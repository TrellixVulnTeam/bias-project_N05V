,conference_year,category,title,author,abstract,download_url,keywords
0,1992,"Explanationand Tutoring",Understanding Causal Descriptions of Physical Systems,Gary C. Borchardt,"This paper introduces the causal reconstruction task-the task of reading a causal description of a physical system, forming an internal model of the specified behavior, and answering questions demonstrating comprehension and reasoning on the basis of the input description. A representation called transition space is introduced, in which events are depicted as path fragments in a space of ""transitions,"" or complexes of changes in the attributes of participating objects. By identifying partial matches between the transition space representations of events, a program called PATHFINDER is able to perform causal reconstruction on short causal descriptions presented in simplified English. Simple transformations applied to event representations prior to matching enable the program to bridge discontinuities arising from the writer’s use of analogy or abstraction. The operation of PATHFINDER is illustrated in the context of a simple causal description extracted from the Encyclopedia Americana, involving exposure of film in a camera.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-001.pdf,
1,1992,"Explanationand Tutoring",Generating Cross-References for Multimedia Explanation,"Kathleen R. McKeown, Steven K. Feiner, Jacques Robin, Dorée D. Seligmann, Michael Tanenblatt","When explanations include multiple media, such as text and illustrations, a reference to an object can be made through a combination of media. We call part of a presentation that references material elsewhere a cross-reference. We are concerned here with how textual expressions can refer to parts of accompanying illustrations. The illustration to which a cross-reference refers should also satisfy the specific goal of identifying an object for the user. Thus, producing an effective cross-reference not only involves text generation, but may also entail modifying or replacing an existing illustration and in some cases, generating an illustration where previously none was needed. In this paper, we describe the different types of cross-references that COMET (Coordinated Multimedia Explanation Testbed) generates and show the roles that both its text and graphics generators play in this process.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-002.pdf,
2,1992,"Explanationand Tutoring",Results of Encoding Knowledge with Tutor Construction Tools,"Tom Murray, Beverly Park Woolf","We have developed and evaluated a set of tutor construction tools which enabled three computer-naive educators to build, test and modify an intelligent tutoring system. The tools constitute a knowledge acquisition interface for representing and rapid prototyping both domain and tutoring knowledge. A formative evaluation is described which lasted nearly two years and involved 20 students. This research aims to understand and support the knowledge acquisition process in education and to facilitate browsing and modification of knowledge. Results of a person-hour analysis of throughput factors are provided along with knowledge representation and engineering issues for developing knowledge acquisition interfaces in education.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-003.pdf,
3,1992,"Explanationand Tutoring",Steps from Explanation Planning to Model Construction Dialogues,"Daniel Suthers, Beverly Woolf, Matthew Cornell","Human explanatory dialogue is an activity in which participants interactively construct explanatory models of the topic phenomenon. However, current explanation planning technology does not support such dialogue. In this paper we describe contributions in the areas of discourse planning architectures, heuristics for knowledge communication, and user interface design that take steps towards addressing this problem. First, our explanation planning architecture independently applies various constraints on the content and organization of explanation, avoiding the inflexibility and contextual assumptions of schematic discourse plans. Second, certain planning operators simulate a human explainer’s efforts to choose and incrementally develop models of the topic phenomenon. Third, dialogue occurs in the medium of a ""live information"" interface designed to serve as the representational medium through which the activities of the machine and human are coupled. Collectively these contributions facilitate interactive model construction in human-machine dialogue.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-004.pdf,
4,1992,Learning: Constructive and Linguistic,A Connectionist Parser with Recursive Sentence Structure and Lexical Disambiguation,George Berg,"In order to be taken seriously, connectionist natural language processing systems must be able to parse syntactically complex sentences. Current connectionist parsers either ignore structure or impose prior restrictions on the structural complexity of the sentences they can process - either number of phrases or the ""depth"" of the sentence structure. XERIC networks, presented here, are distributed representation connectionist parsers which can analyze and represent syntactically varied sentences, including ones with recursive phrase structure constructs. No a priori limits are placed on the depth or length of sentences by the architecture. XERIC networks use recurrent networks to read words one at a time. RAAM-style reduced descriptions and X-Bar grammar are used to make an economical syntactic representation scheme. This is combined with a training technique which allows XERIC to use multiple, virtual copies of its RAAM decoder network to learn to parse and represent sentence structure using gradient-descent methods. XERIC networks also perform number-person disambiguation and lexical disambiguation. Results show that the networks train to a few percent error for sentences up to a phrase-nesting depth of ten or more and that this performance generalizes well.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-005.pdf,
5,1992,Learning: Constructive and Linguistic,Learning to Disambiguate Relative Pronouns,Claire Cardie,"In this paper we show how a natural language system can learn to find the antecedents of relative pronouns. We use a well-known conceptual clustering system to create a case-based memory that predicts the antecedent of a wh-word given a description of the clause that precedes it. Our automated approach duplicates the performance of hand-coded rules. In addition, it requires only minimal syntactic parsing capabilities and a very general semantic feature set for describing nouns. Human intervention is needed only during the training phase. Thus, it is possible to compile relative pronoun disambiguation heuristics tuned to the syntactic and semantic preferences of a new domain with relative ease. Moreover, we believe that the technique provides a general approach for the automated acquisition of additional disambiguation heuristics for natural language systems, especially for problems that require the assimilation of syntactic and semantic knowledge.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-006.pdf,
6,1992,Learning: Constructive and Linguistic,Discrimination-Based Constructive Induction of Logic Programs,"Boonserm Kijsirikul, Masayuki Numao, Masamichi Shimura","This paper presents a new approach to constructive induction, Discrimination-Based Constructive induction(DBC), which invents useful predicates in learning relations. Triggered by failure of selective induction, DBC finds a minimal set of variables forming a new predicate that discriminates between positive and negative examples, and induces a definition of the invented predicate. If necessary, it also induces subpredicates for the definition. Experimental results show that DBC learns meaningful predicates without any interactive guidance.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-007.pdf,
7,1992,Learning: Constructive and Linguistic,Learning Relations by Pathfinding,"Bradley L. Richards, Raymond J. Mooney","First-order learning systems (e.g., FOlL, FOCL, FORTE) generally rely on hill-climbing heuristics in order to avoid the combinatorial explosion inherent in learning first-order concepts. However, hill-climbing leaves these systems vulnerable to local maxima and local plateaus. We present a method, called relational pathfinding, which has proven highly effective in escaping local maxima and crossing local plateaus. We present our algorithm and provide learning results in two domains: family relationships and qualitative model building.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-008.pdf,
8,1992,Learning: Discovery,Symmetry as Bias: Rediscovering Special Relativity,Michael Lowry,"This paper describes a rational reconstruction of Einstein’s discovery of special relativity, validated through an implementation: the Erlanger program. Einstein’s discovery of special relativity revolutionized both the content of physics and the research strategy used by theoretical physicists. This research strategy entails a mutual bootstrapping process between a hypothesis space for biases, defined through different postulated symmetries of the universe, and a hypothesis space for physical theories. The invariance principle mutually constrains these two spaces. The invariance principle enables detecting when an evolving physical theory becomes inconsistent with its bias, and also when the biases for theories describing different phenomena are inconsistent. Structural properties of the invariance principle facilitate generating a new bias when an inconsistency is detected. After a new bias is generated, this principle facilitates reformulating the old, inconsistent theory by treating the latter as a limiting approximation.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-009.pdf,
9,1992,Learning: Discovery,Theory-Driven Discovery of Reaction Pathways in the MECHEM System,Raúl E. Valdés-Perez,"One goal of machine discovery is to automate creative tasks from human scientific practice. This paper describes a project to automate in a general manner the theory-driven discovery of reaction pathways in chemistry and biology. We have designed a system - called MECHEM - that proposes credible pathway hypotheses from data ordinarily available to the chemist. MECHEM has been applied to reactions drawn from the history of biochemistry, from recent industrial chemistry as reported in journals, and from organic chemistry textbooks. The paper first explains the chemical problem and discusses previous AI treatments. Then are presented the architecture of the system, the key algorithmic ideas, and the heuristics used to explore the very large space of chemical pathways. The system’s efficacy is demonstrated on a biochemical reaction studied earlier by Kulkarni and Simon in the KEKADA system, and on another reaction from industrial chemistry. Our project has also resulted in separate novel contributions to chemical knowledge, demonstrating that we have not simplified the task for our convenience, but have addressed its full complexity.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-010.pdf,
10,1992,Learning: Discovery,Discovery of Equations: Experimental Evaluation of Convergence,"Robert Zembowicz, Jan M. Zytkow","Systems that discover empirical equations from data require large scale testing to become a reliable research tool. In the central part of this paper we discuss two convergence tests for large scale evaluation of equation finders and we demonstrate that our system, which we introduce earlier, has the desired convergence properties. Our system can detect a broad range of equations useful in different sciences, and can be easily expanded by addition of new variable transformations. Previous systems, such as BACON or ABACUS, disregarded or oversimplified the problems of error analysis and error propagation, leading to paradoxical results and impeding the true world applications. Our system treats experimental error in a systematic and statistically sound manner. It propagates error to the transformed variables and assigns error to parameters in equations. It uses errors in weighted least squares fitting, in the evaluation of equations, including their acceptance, rejection and ranking, and uses parameter error to eliminate spurious parameters. The system detects equivalent terms (variables) and equations, and it removes the repetitions. This is important for convergence tests and system efficiency. Thanks to the modular structure, our system can be easily expanded, modified, and used to simulate other equation finders.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-011.pdf,
11,1992,Learning: Discovery,Operational Definition Refinement: A Discovery Process,"Jan M. Zytkow, Jieming Zhu, Robert Zembowicz","Operational definitions link scientific attributes to experimental situations, prescribing for the experimenter the actions and measurements needed to measure or control attribute values. While very important in real science, operational procedures have been neglected in machine discovery. We argue that in the preparatory stage of the empirical discovery process each operational definition must be adjusted to the experimental task at hand. This is done in the interest of error reduction and repeatability of measurements. Both small error and high repeatability are instrumental in theory formation. We demonstrate that operational procedure refinement is a discovery process that resembles the discovery of scientific laws. We demonstrate how the discovery task can be reduced to an application of the FAHRENHEIT discovery system. A new type of independent variables, the experiment refinement variables, have been introduced to make the application of FAHRENHEIT theoretically valid. This new extension to FAHRENHEIT uses simple operational procedures, as well as the system’s experimentation and theory formation capabilities to collect real data in a science laboratory and to build theories of error and repeatability that are used to refine the operational procedures. We present the application of FAHRENHEIT in the context of dispensing liquids in a chemistry laboratory.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-012.pdf,
12,1992,Learning: Inductive,Learning in FOL with a Similarity Measure,Gilles Bisson,"There are still very few systems performing a Similarity Based Learning and using a First Order Logic (FOL) representation. This limitation comes from the intrinsic complexity of the learning processes in FOL and from the difficulty to deal with numerical knowledge in this representation. In this paper, we show that major learning processes, namely generalization and clustering, can be solved in a homogeneous way by using a similarity measure. As this measure is defined, the similarity computation comes down to a problem of solving a set of equations in several unknowns. The representation language used to express our examples is a subset of FOL allowing to express both quantitative knowledge and a relevance scale on the predicates.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-013.pdf,
13,1992,Learning: Inductive,Learning to Learn Decision Trees,"Vlad G. Dabija, Katsuhiko Tsujino, Shogo Nishida","Decision trees are widely used in machine learning and knowledge acquisition systems. However, there is no optimal or even unanimously accepted strategy of obtaining ""good"" such trees, and most of the generated trees suffer from improprieties, i.e. inadequacies in representing knowledge. The final goal of the research reported here is to formulate a theory for the decision trees domain, that is a set of heuristics (on which a majority of experts will agree) which will describe a good decision tree, as well as a set of heuristics specifying how to obtain optimal trees. In order to achieve this goal we have designed a recursive architecture learning system, which monitors an interactive knowledge acquisition system based on decision trees and driven by explanatory reasoning, and incrementally acquires from the experts using it the knowledge used to build the decision trees domain theory. This theory is also represented as a set of decision trees, and may be domain dependent. Our system acquires knowledge to define the notion of good/bad decision trees and to measure their quality, as well as knowledge needed to guide domain experts in constructing good decision trees. The partial theory acquired at each moment is also used by the basic knowledge acquisition system in its tree generation process, thus constantly improving its performance.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-014.pdf,
14,1992,Learning: Inductive,A Personal Learning Apprentice,"Lisa Dent, Jesus Boticario, Tom Mitchell, David Zabowski, John McDermott","Personalized knowledge-based systems have not yet become widespread, despite their potential for valuable assistance in many daily tasks. This is due, in part, to the high cost of developing and maintaining customized knowledge bases. The construction of personal assistants as learning apprentices -- interactive assistants that learn continually from their users -- is one approach which could dramatically reduce the cost of knowledge-based advisors. We present one such personal learning apprentice, called CAP, which assists in managing a meeting calendar. CAP has been used since June 1991 by a secretary in our work place to manage a faculty member’s meeting calendar, and is the first instance of a fielded learning apprentice in routine use. This paper describes the organization of CAP, its performance in initial field tests, and more general lessons learned from this effort about learning apprentice systems.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-015.pdf,
15,1992,Learning: Inductive,The Attribute Selection Problem in Decision Tree Generation,"Usama M. Fayyad, Keki B. Irani","We address the problem of selecting an attribute and some of its values for branching during the top-down generation of decision trees. We study the class of impurity measures, members of which are typically used in the literature for selecting attributes during decision tree generation (e.g. entropy in ID3, GID3*, and CART; Gini Index in CART). We argue that this class of measures is not particularly suitable for use in classification learning. We define a new class of measures, called C-SEP, that we argue is better suited for the purposes of class separation. A new measure from C-SEP is formulated and some of its desirable properties are shown. Finally, we demonstrate empirically that the new algorithm, O-BTree, that uses this measure indeed produces better decision trees than algorithms that use impurity measures.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-016.pdf,
16,1992,Learning: Inductive,COGIN: Symbolic Induction with Genetic Algorithms,"David Perry Greene, Stephen F. Smith","COGIN is a system designed for induction of symbolic decision models from pre-classed examples based on the use of genetic algorithms (GAS). Much research in symbolic induction has focused on techniques for reducing classification inaccuracies that arise from inherent limits of underlying incremental search techniques. Genetic Algorithms offer an intriguing alternative to step-wise model construction, relying instead on model evolution through global competition. The difficulty is in providing an effective framework for the GA to be practically applied to complex induction problems. COGIN merges traditional induction concepts with genetic search to provide such a framework, and recent experimental results have demonstrated its advantage relative to basic stepwise inductive approaches. In this paper, we describe the essential elements of the COGIN approach and present a favorable comparison of COGIN results with those produced by a more sophisticated stepwise approach (with support post processing) on standardized multiplexor problems.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-017.pdf,
17,1992,Learning: Inductive,Polynomial-Time Learning with Version Spaces,Haym Hirsh,"Although version spaces provide a useful conceptual tool for inductive concept learning, they often face severe computational difficulties when implemented. For example, the G set of traditional boundary-set implementations of version spaces can have size exponential in the amount of data for even the most simple conjunctive description languages [Haussler, 1988]. This paper presents a new representation for version spaces that is more general than the traditional boundary-set representation, yet has worst-case time complexity that is polynomial in the amount of data when used for learning from attribute-value data with tree-structured feature hierarchies (which includes languages like Haussler’s). The central idea underlying this new representation is to maintain the traditional S boundary set as usual, but use a list N of negative data rather than keeping a G set as is typically done.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-018.pdf,
18,1992,Learning: Inductive,ChiMerge: Discretization of Numeric Attributes,Randy Kerber,"Many classification algorithms require that the training data contain only discrete attributes. To use such an algorithm when there are numeric attributes, all numeric values must first be converted into discrete values-a process called discretization. This paper describes ChiMerge, a general, robust algorithm that uses the x2 statistic to discretize (quantize) numeric attributes.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-019.pdf,
19,1992,Learning: Inductive,The Feature Selection Problem: Traditional Methods and a New Algorithm,"Kenji Kira, Larry A. Rendell","For real-world concept learning problems, feature selection is important to speed up learning and to improve concept quality. We review and analyze past approaches to feature selection and note their strengths and weaknesses. We then introduce and theoretically examine a new algorithm Relief which selects relevant features using a statistical method. Relief does not depend on heuristics, is accurate even if features interact, and is noise-tolerant. It requires only linear time in the number of given features and the number of training instances, regardless of the target concept complexity. The algorithm also has certain limitations such as non-optimal feature set size. Ways to overcome the limitations are suggested. We also report the test results of comparison between Relief and other feature selection algorithms. The empirical results support the theoretical analysis, suggesting a practical approach to feature selection for real-world problems.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-020.pdf,
20,1992,Learning: Inductive,Discrete Sequence Prediction and its Applications,Philip Laird,"Learning from experience to predict sequences of discrete symbols is a fundamental problem in machine learning with many applications. We present a simple and practical algorithm (TDAG) for discrete sequence prediction, verify its performance on data compression tasks, and apply it to problem of dynamically optimizing Prolog programs for good average-case behavior.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-021.pdf,
21,1992,Learning: Inductive,Classifier Learning from Noisy Data as Probabilistic Evidence Combination,"Steven W. Norton, Haym Hirsh","This paper presents an approach to learning from noisy data that views the problem as one of reasoning under uncertainty, where prior knowledge of the noise process is applied to compute a posteriori probabilities over the hypothesis space. In preliminary experiments this maximum a posteriori (MAP) approach exhibits a learning rate advantage over the C4.5 algorithm that is statistically significant.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-022.pdf,
22,1992,Learning: Inductive,Sparse Data and the Effect of Overfitting Avoidance in Decision Tree Induction,Cullen Schaffer,"Overfitting avoidance in induction has often been treated as if it statistically increases expected predictive accuracy. In fact, there is no statistical basis for believing it will have this effect. Overfitting avoidance is simply a form of bias and, as such, its effect on expected accuracy depends, not on statistics, but on the degree to which this bias is appropriate to a problem-generating domain. This paper identifies one important factor that affects the degree to which the bias of overfitting avoidance is appropriate-the abundance of training data relative to the complexity of the relationship to be induced-and shows empirically how it determines whether such methods as pessimistic and cross-validated cost-complexity pruning will increase or decrease predictive accuracy in decision tree induction. The effect of sparse data is illustrated first in an artificial domain and then in more realistic examples drawn from the UCI machine learning database repository.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-023.pdf,
23,1992,Learning: Inductive,Complementary Discrimination Learning with Decision Lists,Wei-Min Shen,"This paper describes the integration of a learning mechanism called complementary discrimination learning with a knowledge representation schema called decision lists. There are two main results of such an integration. One is an efficient representation for complementary concepts that is crucial for complementary discrimination style Iearning. The other is the first behaviorally incremental algorithm, called CDLZ, for learning decision lists. Theoretical analysis and experiments in several domains have shown that CDL2 is more efficient than many existing symbolic or neural network learning algorithms, and can learn multiple concepts from noisy and inconsistent data.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-024.pdf,
24,1992,Learning: Neural Network and Hybrid,A Framework for Integrating Fault Diagnosis and Incremental Knowledge Acquisition in Connectionist Expert Systems,"Joo-Hwee Lim, Ho-Chung Lui, Pei-Zhuang Wang","In this paper, we propose a framework for integrating fault diagnosis and incremental knowledge acquisition in connectionist expert systems. A new case solved by the Diagnostic Function is formulated as a new example for the Learning Function to learn incrementally. The Diagnostic Function is composed of a neural networks-based Example Module and a symbolic-based Rule Module. While the Example Module is always first invoked to provide the short-cut solution, the Rule Module provides extensive coverage of cases to handle odd cases when Example Module fails. Two applications based on the proposed framework will also be briefly mentioned.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-025.pdf,
25,1992,Learning: Neural Network and Hybrid,Using Knowledge-Based Neural Networks to Improve Algorithms: Refining the Chou-Fasman Algorithm for Protein Folding,"Richard Maclin, Jude W. Shavlik","We describe a method for using machine learning to refine algorithms represented as generalized finite-state automata. The knowledge in an automaton is translated into an artificial neural network, and then refined with backpropagation on a set of examples. Our technique for translating an automaton into a network extends KBANN, a system that translates a set of propositional rules into a corresponding neural network. The extended system, FSKBANN, allows one to refine the large class of algorithms that can be represented as state-based processes. As a test, we use FSKBANN to refine the Chou-Fasman algorithm, a method for predicting how globular proteins fold. Empirical evidence shows the refined algorithm FSKBANN produces is statistically significantly more accurate than both the original Chou-Fasman algorithm and a neural network trained using the standard approach.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-026.pdf,
26,1992,Learning: Neural Network and Hybrid,Adapting Bias by Gradient Descent: An Incremental Version of Delta-Bar-Delta,Richard S. Sutton,"Appropriate bias is widely viewed as the key to efficient learning and generalization. I present a new algorithm, the Incremental Delta-Bar-Delta (IDBD) algorithm, for the learning of appropriate biases based on previous learning experience. The IDBD algorithm is developed for the case of a simple, linear learning system-the LMS or delta rule with a separate learning-rate parameter for each input. The IDBD algorithm adjusts the learning-rate parameters, which are an important form of bias for this system. Because bias in this approach is adapted based on previous learning experience, the appropriate testbeds are drifting or non-stationary learning tasks. For particular tasks of this type, I show that the IDBD algorithm performs better than ordinary LMS and in fact finds the optimal learning rates. The IDBD algorithm extends and improves over prior work by Jacobs and by me in that it is fully incremental and has only a single free parameter. This paper also extends previous work by presenting a derivation of the IDBD algorithm as gradient descent in the space of learning-rate parameters. Finally, I offer a novel interpretation of the IDBD algorithm as an incremental form of hold-one-out cross validation.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-027.pdf,
27,1992,Learning: Neural Network and Hybrid,Using Symbolic Learning to Improve Knowledge-Based Neural Networks,"Geoffrey G. Towell, Jude W. Shavlik","The previously-described KBANN system integrates existing knowledge into neural networks by defining the network topology and setting initial link weights. Standard neural learning techniques can then be used to train such networks, thereby refining the information upon which the network is based. However, standard neural learning techniques are reputed to have difficulty training networks with multiple layers of hidden units; KBANN commonly creates such networks. In addition, standard neural learning techniques ignore some of the information contained in the networks created by KBANN. This paper describes a symbolic inductive learning algorithm for training such networks that uses this previously-ignored information and which helps to address the problems of training ""deep"" networks. Empirical evidence shows that this method improves not only learning speed, but also the ability of networks to generalize correctly to testing examples.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-028.pdf,
28,1992,Learning: Robotic,Reinforcement Learning with Perceptual Aliasing: The Perceptual Distinctions Approach,Lonnie Chrisman,"It is known that Perceptual Aliasing may significantly diminish the effectiveness of reinforcement learning algorithms [Whitehead and Ballard, 1991]. Perceptual aliasing occurs when multiple situations that are indistinguishable from immediate perceptual input require different responses from the system. For example, if a robot can only see forward, yet the presence of a battery charger behind it determines whether or not it should backup, immediate perception alone is insufficient for determining the most appropriate action. It is problematic since reinforcement algorithms typically learn a control policy from immediate perceptual input to the optimal choice of action. This paper introduces the predictive distinctions approach to compensate for perceptual aliasing caused from incomplete perception of the world. An additional component, a predictive model, is utilized to track aspects of the world that may not be visible at all times. In addition to the control policy, the model must also be learned, and to allow for stochastic actions and noisy perception, a probabilistic model is learned from experience. In the process, the system must discover, on its own, the important distinctions in the world. Experimental results are given for a simple simulated domain, and additional issues are discussed.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-029.pdf,
29,1992,Learning: Robotic,Acquisition of Automatic Activity through Practice: Changes in Sensory Input,"Jack Gelfand, Marshall Flax, Raymond Endres, Stephen Lane, David Handelman","This paper will present computer models of three robotic motion planning and learning systems which use a multi-sensory learning strategy for learning and control. In these systems machine vision input is used to plan and execute movements utilizing an algorithmic controller while at the same time neural networks learn the control of those motions using feedback provided by position and velocity sensors in the actuators. A specific advantage of this approach is that, in addition to the system learning a more automatic behavior, it employs a computationally less costly sensory system more tightly coupled from perception to action.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-030.pdf,
30,1992,Learning: Robotic,Automatic Programming of Robots Using Genetic Programming,"John R. Koza, James P. Rice","The goal in automatic programming is to get a computer to perform a task by telling it what needs to be done, rather than by explicitly programming it. This paper considers the task of automatically generating a computer program to enable an autonomous mobile robot to perform the task of moving a box from the middle of an irregular shaped room to the wall. We compare the ability of the recently developed genetic programming paradigm to produce such a program to the reported ability of reinforcement learning techniques, such as Q learning, to produce such a program in the style of the subsumption architecture. The computational requirements of reinforcement learning necessitates considerable human knowledge and intervention, whereas genetic programming comes much closer to achieving the goal of getting the computer to perform the task without explicitly programming it. The solution produced by genetic programming emerges as a result of Darwinian natural selection and genetic crossover (sexual recombination) in a population of computer programs. The process is driven by a fitness measure which communicates the nature of the task to the computer and its learning paradigm.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-031.pdf,
31,1992,Learning: Robotic,Reinforcement Learning with a Hierarchy of Abstract Models,Satinder P. Singh,"Reinforcement learning (RL) algorithms have traditionally been thought of as trial and error learning methods that use actual control experience to incrementally improve a control policy. Sutton’s DYNA architecture demonstrated that RL algorithms can work as well using simulated experience from an environment model, and that the resulting computation was similar to doing one-step lookahead planning. Inspired by the literature on hierarchical planning, I propose learning a hierarchy of models of the environment that abstract temporal detail as a means of improving the scalability of RL algorithms. I present H-DYNA (Hierarchical DY NA), an extension to Sutton’s DYNA architecture that is able to learn such a hierarchy of abstract models. H-DYNA differs from hierarchical planners in two ways: first, the abstract models are learned using experience gained while learning to solve other tasks in the same environment, and second, the abstract models can be used to solve stochastic control tasks. Simulations on a set of compositionally-structured navigation tasks show that H-DYNA can learn to solve them faster than conventional RL algorithms. The abstract models also serve as mechanisms for achieving transfer of learning across multiple tasks.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-032.pdf,
32,1992,Learning: Theory,Inferring Finite Automata with Stochastic Output Functions and an Application to Map Learning,"Thomas Dean, Kenneth Basye, Leslie Kaelbling, Evangelos Kokkevis, Oded Maron, Dana Angluin, Sean Engelson","We assume that it is useful for a robot to construct a spatial representation of its environment for navigation purposes. In addition, we assume that robots, like people, make occasional errors in perceiving the spatial features of their environment. Typical perceptual errors include confusing two distinct locations or failing to identify the same location seen at different times. We are interested in the consequences of perceptual uncertainty in terms of the time and space required to learn a map with a given accuracy. We measure accuracy in terms of the probability that the robot correctly identifies a particular underlying spatial configuration. We derive considerable power by providing the robot with routines that allow it to identify landmarks on the basis of local features. We provide a mathematical model of the problem and algorithms that are guaranteed to learn the underlying spatial configuration for a given class of environments with probability 1 - 5 in time polynomial in l/S and some measure of the structural complexity of the environment and the robot’s ability to discern that structure. Our algorithms apply to a variety of environments that can be modeled as labeled graphs or deterministic finite automata.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-033.pdf,
33,1992,Learning: Theory,Oblivious PAC Learning of Concept Hierarchies,"Thomas Dean, Kenneth Basye, Leslie Kaelbling, Evangelos Kokkevis, Oded Maron, Dana Angluin, Sean Engelson","In this paper we introduce an extension of the Probably Approximately Correct (PAC) learning model to study the problem of learning inclusion hierarchies of concepts (sometimes called is-a hierarchies) from random examples. Using only the hypothesis representations output over many different runs of a learning algorithm, we wish to reconstruct the partial order (with respect to generality) among the different target concepts used to train the algorithm. We give an efficient algorithm for this problem with the property that each run is oblivious of all other runs: each run can take place in isolation, without access to any examples except those of the current target concept, and without access to the current pool of hypothesis representations. Thus, additional mechanisms providing shared information between runs are not necessary for the inference of some nontrivial hierarchies.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-034.pdf,
34,1992,Learning: Theory,An Analysis of Bayesian Classifiers,"Pat Langley, Wayne Iba, Kevin Thompson","In this paper we present an average-case analysis of the Bayesian classifier, a simple induction algorithm that fares remarkably well on many learning tasks. Our analysis assumes a monotone conjunctive target concept, and independent, noise-free Boolean attributes. We calculate the probability that the algorithm will induce an arbitrary pair of concept descriptions and then use this to compute the probability of correct classification over the instance space. The analysis takes into account the number of training instances, the number of attributes, the distribution of these attributes, and the level of class noise. We also explore the behavioral implications of the analysis by presenting predicted learning curves for artificial domains, and give experimental results on these domains as a check on our reasoning.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-035.pdf,
35,1992,Learning: Theory,A Theory of Unsupervised Speedup Learning,Prasad Tadepalli,"Speedup learning seeks to improve the efficiency of search-based problem solvers. In this paper, we propose a new theoretical model of speedup learning which captures systems that improve problem solving performance by solving a user-given set of problems. We also use this model to motivate the notion of ""batch problem solving,"" and argue that it is more congenial to learning than sequential problem solving. Our theoretical results are applicable to all serially decomposable domains. We empirically validate our results in the domain of Eight Puzzle.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-036.pdf,
36,1992,Learning: Utility and Bias,COMPOSER: A Probabilistic Solution to the Utility Problem in Speed-Up Learning,"Jonathan Gratch, Gerald DeJong","In machine learning there is considerable interest in techniques which improve planning ability. Initial investigations have identified a wide variety of techniques to address this issue. Progress has been hampered by the utility problem, a basic tradeoff between the benefit of learned knowledge and the cost to locate and apply relevant knowledge. In this paper we describe the COMPOSER system which embodies a probabilistic solution to the utility problem. We outline the statistical foundations of our approach and compare it against four other approaches which appear in the literature.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-037.pdf,
37,1992,Learning: Utility and Bias,A Statistical Approach to Solving the EBL Utility Problem,"Russell Greiner, Igor Jurisica","Many ""learning from experience"" systems use information extracted from problem solving experiences to modify a performance element PE, forming a new element PE' that can solve these and similar problems more efficiently. However, as transformations that improve performance on one set of problems can degrade performance on other sets, the new PE' is not always better than tile original PE; this depends on the distribution of problems. We therefore seek the performance element whose expected perfornamce, over this distribution, is optimal. Unfortunately, the actual distribution, which is needed to determine which element is optimal, is usually not known. Moreover, the task of finding the optimal element, even knowing the distribution, is intractable for most interesting spaces of elements. This paper presents a method, PALO, that side-steps these problems by using a set of samples to estimate the unknown distribution, and by using a set of transformations to hill-climb to a local optimum. This process is based on a mathematically rigorous form of utility analysis: in particular, it uses statistical techniques to determine whether the result of a proposed transformation will be better than the original system. We also present an efficient way of implementing this learning system in the contest of a general class of performance elements, and include empirical evidence that this approach can work effectively.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-038.pdf,
38,1992,Learning: Utility and Bias,Empirical Analysis of the General Utility Problem in Machine Learning,Lawrence B. Holder,The overfit problem in inductive learning and the utility problem in speedup learning both describe a common behavior of machine learning methods: the eventual degradation of performance due to increasing amounts of learned knowledge. Plotting the performance of the changing knowledge during execution of a learning method (the performance response) reveals similar curves for several methods. The performance response generally indicates an increase to a single peak followed by a more gradual decrease in performance. The similarity in performance responses suggests a model relating performance to the amount of learned knowledge. This paper provides empirical evidence for the existence of a general model by plotting the performance responses of several learning programs. Formal models of the performance response are also discussed. These models can be used to control the amount of learning and avoid degradation of performance.,https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-039.pdf,
39,1992,Learning: Utility and Bias,Inductive Policy,"Foster John Provost, Bruce G. Buchanan","The concept of inductive bias can be broken down into the underlying assumptions of the domain, the particular implementation choices that restrict or order the space of hypotheses considered by the learning program (the bias choices), and the inductive policy that links the two. We define inductive policy as the strategy used to make bias choices based on the underlying assumptions. Inductive policy decisions involve addressing tradeoffs with respect to different bias choices. Without addressing these tradeoffs, bias choices will be made arbitrarily. From the standpoint of inductive policy, we discuss two issues not addressed much in the machine learning literature. First we discuss batch learning with a strict time constraint, and present an initial study with respect to trading off predictive accuracy for speed of learning. Next we discuss the issue of learning in a domain where different types of errors have different associated costs (risks). We show that by using different inductive policies accuracy can be traded off for safety. We also show how the value for the latter tradeoff can be represented explicitly in a system that adjusts bias choices with respect to a particular inductive policy.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-040.pdf,
40,1992,Multi-Agent Coordination,Constrained Intelligent Action: Planning Under the Influence of a Master Agent,"Eithan Ephrati, Jeffrey S. Rosenschein","In this paper we analyze a particular model of control among intelligent agents, that of non-absolute control. Non-absolute control involves a ""supervisor"" agent that issues orders to a ""subordinate"" agent. An example might be a human agent on Earth directing the activities of a Mars-based semi-autonomous vehicle. Both agents operate with essentially the same goals. The subordinate agent, however, is assumed to have access to some information that the supervisor does not have. The agent is thus expected to exercise its judgment in following orders (i.e., following the true intent of the supervisor, to the best of its ability). After presenting our model, we discuss the planning problem: how would a subordinate agent choose among alternative plans? Our solutions focus on evaluating the distance between candidate plans.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-041.pdf,
41,1992,Multi-Agent Coordination,Using Joint Responsibility to Coordinate Collaborative Problem Solving in Dynamic Environments,"N. R. Jennings, E. H. Mamdani","Joint responsibility is a new meta-level description of how cooperating agents should behave when engaged in collaborative problem solving. It is independent of any specific planning or consensus forming mechanism, but can be mapped down to such a level. An application of the framework to the real world problem of electricity transportation management is given and its implementation is discussed. A comparative analysis of responsibility and two other group organisational structures, selfish problem solvers and communities in which collaborative behaviour emerges from interactions, is undertaken. The aim being to evaluate their relative performance characteristics in dynamic and unpredictable environments in which decisions are taken using partial, imprecise views of the system.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-042.pdf,
42,1992,Multi-Agent Coordination,On the Synthesis of Useful Social Laws for Artificial Agent Societies (Preliminary Report),"Yoav Shoham, Moshe Tennenholtz","We present a general model of social law in a computational system, and investigate some of its properties. The contribution of this paper is twofold. First, we argue that the notion of social law is not epiphenomenal, but rather should be built into the action representation; we then offer such a representation. Second, we investigate the complexity of automatically deriving useful social laws in this model, given descriptions of the agents’ capabilities, and the goals they might encounter. We show that in general the problem is NP-complete, and identify precise conditions under which it becomes polynomial.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-043.pdf,
43,1992,Multi-Agent Coordination,A General--Equilibrium Approach to Distributed Transportation Planning,Michael P. Wellman,"Market price mechanisms from economics constitute a well-understood framework for coordinating decentralized decision processes with minimal communication. WALRAS is a general ""market-oriented programming"" environment for the construction and analysis of distributed planning systems, based on general-equilibrium theory. The environment provides basic constructs for defining computational market structures, and a procedure for deriving their corresponding competitive equilibria. In a particular realization of this approach for a simplified form of distributed transportation planning, we see that careful construction of the decision process according to economic principles can lead to effective decentralization, and that the behavior of the system can be meaningfully analyzed in economic terms.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-044.pdf,
44,1992,Natural Language: Intepretation,An Approach to the Representation of Iterative Situations,Michael J. Almeida,"Iterative sentences such as Mary knocked on the door four times, John played the sonata every other day, and Mary was often busy can be understood as asserting that some situation type is either repeated a certain number of times or with a certain frequency. The semantic content of iterative sentences has been standardly represented by some logical formula which quantifies over instances of a non-iterative situation type. The principal claim of this paper, and the basis of the representations proposed in it, is that we also require iterative situation types and instances in order to completely handle the range of possible interpretations of iterative sentences.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-045.pdf,
45,1992,Natural Language: Intepretation,"Actions, Beliefs and Intentions in Rationale Clauses and Means Clauses",Cecile T. Balkanski,"Utterances that include rationale clauses and means clauses display a variety of features that affect their interpretation, as well as the subsequent discourse. Of particular importance is the information that is conveyed about agents’ beliefs and intentions with respect to the actions they talk about or perform. Hence, for a language interpretation system to handle these utterances, it must identify the relevant features of each construction and draw appropriate inferences about the agents’ mental states with respect to the actions and action relations that are involved. This paper describes an interpretation model that satisfies this need by providing a set of interpretation rules and showing how these rules allow for the derivation of the appropriate set of beliefs and intentions associated with each construction.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-046.pdf,
46,1992,Natural Language: Intepretation,"Actions, Beliefs and Intentions in Rationale Clauses and Means Clauses",Daniel Jurafsky,"Models of parsing or of sentence interpretation generally fall into one of three paradigms. The Zinguistic paradigm is concerned with computational models of linguistic theories, the computational paradigm with the computationally best process for computing the meaning or structure of a sentence, and the psychological paradigm with psychological modeling of human interpretation of language. Rarely have models attempted to cross disciplinary boundaries and meet the criteria imposed by each of these paradigms. This paper describes a model of human sentence intepretation which addresses the fundamental goals of each of these paradigms. These goals include the need to produce a high-level and semantically rich representation of the meaning of the sentence, to include a motivated and declarative theory of linguistic knowledge which captures linguistic generalizations, and to account in a principled manner for psycholinguistic results.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-047.pdf,
47,1992,Natural Language: Intepretation,Literal Meaning and the Comprehension of Metaphors,"Steven L. Lytinen, Robert R. Burridge, Jeffrey D. Kirtner","Based on psychological studies which show that metaphors and other nonliteral constructions are comprehended in the same amount of time as comparable literal constructions, some researchers have concluded that literal meaning is not computed during metaphor comprehension. In this paper, we suggest that the empirical evidence does not rule out the possibility that literal meaning is constructed. We present a computational model of metaphor comprehension which is consistent with the data, but in which literal meaning is computed. This model has been implemented as part of a unification-based natural language processing system, called LINK.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-048.pdf,
48,1992,Natural Language: Parsing,Parsing Run Amok: Relation-Driven Control for Text Analysis,Paul S. Jacobs,"Traditional syntactic models of parsing have been inadequate for task-driven processing of extended text, because they spend most of their time on misdirected linguistic analysis, leading to problems with both efficiency and coverage. Statistical and domain-driven processing offer compelling possibilities, but only as a complement to syntactic processing. For semantically-oriented tasks such as data extraction from text, the problem is how to combine the coverage of these ""weaker"" methods with the detail and accuracy of traditional lingusitic analysis. A good approach is to focus linguistic analysis on relations that directly impact the semantic results, detaching these relations from the complete constituents to which they belong. This approach results in a faster, more robust, and potentially more accura.te parser for real text.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-049.pdf,
49,1992,Natural Language: Parsing,A Probabilistic Parser Applied to Software Testing Documents,"Mark A. Jones, Jason Eisner","We describe an approach to training a statistical parser from a bracketed corpus, and demonstrate its use in a software testing application that translates English specifications into an automated testing language. A grammar is not explicitly specified; the rules and contextual probabilities of occurrence are automatically generated from the corpus. The parser is extremely successful at producing and identifying the correct parse, and nearly deterministic in the number of parses that it produces. To compensate for undertraining, the parser also uses general, linguistic subtheories which aid in guessing some types of novel structures.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-050.pdf,
50,1992,Natural Language: Parsing,Classifying Texts Using Relevancy Signatures,"Ellen Riloff, Wendy Lehnert","Text processing for complex domains such as terrorism is complicated by the difficulty of being able to reliably distinguish relevant and irrelevant texts. We have discovered a simple and effective filter, the Relevancy Signatures Algorithm, and demonstrated its performance in the domain of terrorist event descriptions. The Relevancy Signatures Algorithm is based on the natural language processing technique of selective concept extraction, and relies on text representations that reflect predictable patterns of linguistic context. This paper describes text classification experiments conducted in the domain of terrorism using the MUC-3 text corpus. A customized dictionary of about 6,000 words provides the lexical knowledge base needed to discriminate relevant texts, and the CIRCUS sentence analyzer generates relevancy signatures as an effortless side-effect of its normal sentence analysis. Although we suspect that the training base available to us from the MUC-3 corpus may not be large enough to provide optimal training, we were nevertheless able to attain relevancy discriminations for significant levels of recall (ranging from 11% to 47%) with 100% precision in half of our test runs.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-051.pdf,
51,1992,Natural Language: Parsing,Shipping Departments vs. Shipping Pacemakers: Using Thematic Analysis to Improve Tagging Accuracy,Uri Zernik,"Thematic analysis is best manifested by contrasting collocations such as ""shipping pacemakers"" vs. ""shipping departments"". While in the first pair, the pacemakers are being shipped, in the second one, the departments are probably engaged in some shipping activity, but are not being shipped. Text pre-processors, intended to inject corpus-based intuition into the parsing process, must adequately distinguish between such cases. Although statistical tagging [Church et al., 1989; Meteer et al., 1991; Brill, 1992; Cutting et al., 1992] has attained impressive results overall, the analysis of multiple-content-word strings (i.e., collocations) has presented a weakness, and caused accuracy degradation. To provide acceptable coverage (i.e., 90% of collocations), a tagger must have accessible a large database ( i.e., 250,000 pairs) of individually analyzed collocations. Consequently, training must be based on a corpus ranging well over 50 million words. Since such large corpus does not exist in a tagged form, training must be from raw corpus. In this paper we present an algorithm for text tagging based on thematic analysis. The algorithm yields high-accuracy results. We provide empirical results: The program NLcp (NL corpus processing) acquired a 250,000 thematic-relation database through the 85-million word Wall-Street Journal Corpus. It was tested over the Tipster 66,000-word Joint-Venture corpus.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-052.pdf,
52,1992,Perception,Computation of Upper-Bounds for Stochastic Context-Free Languages,"A. Corazza, R. De Mori, G. Satta","Automatic speech understanding and automatic speech recognition extract different kinds of information from the input signal. The result of the former must be evaluated on the basis of the response of the system while the result of the latter is the word sequence which best matches the input signal. In both cases search has to be performed based on scores of interpretation hypotheses. A scoring method is presented based on stochastic context-free grammars. The method gives optimal upper-bounds for the computation of the ""best"" derivation trees of a sentence. This method allows language models to be built based on stochastic context-free grammars and their use with an admissible search algorithm that interprets a speech signal with left-to-right or middle-out strategies. Theoretical and computational aspects are discussed.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-053.pdf,
53,1992,Perception,A Computational Model for Face Location Based on Cognitive Principles,"Venu Govindaraju, Sargur N. Srihari, David Sher","The human face is an object that is easily located in complex scenes by infants and adults alike. Yet the development of an automated system to perform this task is extremely challenging. This paper is concerned with the development of a computational model for locating human faces in newspaper photographs based on cognitive research in human perceptual development. In the process of learning to recognize objects in the visual world, one could assume that natural growth favors the development of the abilities to detect the more essential features first. Hence, a study of the progress of an infant’s visual abilities can be used to categorize the potential features in terms of their importance. The face locator developed by the authors takes a hypothesis gelaerate and test approach to the task of finding the locations of people’s faces in digitized pictures. Information from the accompanying caption is used in the verification phase. The system successfully located all faces in 44 of the 60 (73%) test newspaper photographs.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-054.pdf,
54,1992,Perception,Grouping Iso-Velocity Points for Ego-Motion Recovery,"Yibing Yang, Alan Yuille","The instantaneous image motion field due to a camera moving through a static environment encodes information about ego-motion and environmental layout. For pure translational motion, the motion field has a unique point termed focus of expansion/contraction where the image velocity vanishes. We reveal the fact that for an arbitrary 3D motion the zero-velocity points, whose number can be large, have the regularity of being approximately cocircular. More generally, all the image points with the same velocity u are located approximately on a circle (termed the iso-velocity circle (IVC)) determined solely by u and the ego-motion, except for the pathological cases in which the circle degenerates into a straight line. While IVCs can be recovered from 3 or more pairs of iso-velocity points in the motion field using a linear method, estimating ego-motion reduces to solving systems of linear equations constraining iso-velocity point pairs (Yang 1992).",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-055.pdf,
55,1992,Planning,Cultural Support for Improvisation,"Philip E. Agre, Ian D. Horswill","We present a novel object-centered formalization of action which allows us to define an interesting class of tasks, called cooking tasks, which can be performed without backtracking. Since backtracking is unnecessary, actions can be selected incrementally using a greedy method without having to precompute a plan. Such an approach is efficient and rapidly adjusts to unforeseen circumstances. Our argument is that cooking tasks are widely encountered in everyday life because of the special properties of a given culture’s artifacts. In other words, culture has structured the world so as to make it easier to live in. We present an implementation of these ideas, experimental results, and control experiments using a standard nonlinear planner.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-056.pdf,
56,1992,Planning,The Expected Value of Hierarchical Problem-Solving,"Fahiem Bacchus, Qiang Yang","n the best case using an abstraction hierarchy in problem-solving can yield an exponential speed-up in search efficiency. Such a speed-up is predicted by various analytical models developed in the literature, and efficiency gains of this order have been confirmed empirically. However, these models assume that the Downward Refinement Property (DRP) holds. When this property holds, backtracking never need occur across abstraction levels. When it fails, search may have to consider many different abstract solutions before finding one that can be refined to a concrete solution. In this paper we provide an analysis of the expected search complexity without assuming the DRP. We find that our model predicts a phase boundary where abstraction provides no benefit: if the probability that an abstract solution can be refined is very low or very high, search with abstraction yields significant speed up. However, in the phase boundary area where the probability takes on an intermediate value search efficiency is not nec- essarily improved. The phenomenon of a phase boundary-where search is hardest agrees with recent empirical studies of Cheeseman et al. [CKT91].",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-057.pdf,
57,1992,Planning,Achieving the Functionality of Filter Conditions in a Partial Order Planner,"Gregg Collins, Louise Pryor","One of the most common modifications made to the standard STRIPS action representation is the inclusion of filter conditions. A key function of such filter conditions is to distinguish between operators that represent different context-dependent effects for the same action. We consider how filter conditions may be used to provide this functionality in a complete and correct partial order planner. We conclude that they are not effective, and that in general the use of filter conditions is incompatible with the basic assumptions that lie behind partial order planning. We present an alternative mechanism, using the secondary preconditions of Pednault (1988, 1991) to represent context-dependent effects. The use of secondary preconditions is effective, and preserves completeness and correctness.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-058.pdf,
58,1992,Planning,On the Complexity of Domain-Independent Planning,"Kutluhan Erol, Dana S. Nau, V. S. Subrahmanian","In this paper, we examine how the complexity of domain-independent planning with STRIPS-Style operators depends on the nature of the planning operators. We show how the time complexity varies depending on a wide variety of conditions: whether or not delete lists are allowed; whether or not negative preconditions are allowed; whether or not the predicates are restricted to be propositions (i.e., 0-ary); whether the planning operators are given as part of the input to the planning problem, or instead are fixed in advance.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-059.pdf,
59,1992,Planning,Analyzing Failure Recovery to Improve Planner Design,Adele E. Howe,"Plans fail for many reasons. During planner development, failure can often be traced to actions of the planner itself. Failure recovery analysis is a procedure for analyzing execution traces of failure recovery to discover how the planner’s actions may be causing failures. The four step procedure involves statistically analyzing execution data for dependencies between actions and failures, mapping those dependencies to plan structures, explaining how the structures might produce the observed dependencies, and recommending modifications. The procedure is demonstrated by applying it to explain how a particular recovery action may lead to a particular failure in the Phoenix planner. The planner is modified based on the recommendations of the analysis, and the modifications are shown to improve the planner’s performance by removing a source of failure and so reducing the overall incidence of failure.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-060.pdf,
60,1992,Planning,Constrained Decision Revision,Charles Petrie,This paper synthesizes general constraint satisfaction and classical AI planning into a theory of incremental change that accounts for multiple objectives and contingencies. The hypothesis is that this is a new and useful paradigm for problem solving and re-solving. A truth maintenance-based architecture derived from the theory is useful for contingent assignment problems such as logistics planning.,https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-061.pdf,
61,1992,Planning,Learning from Goal Interactions in Planning: Goal Stack Analysis and Generalization,"Kwang Ryel Ryu, Keki B. Irani","This paper presents a methodology which enables the derivation of goal ordering rules from the analysis of problem failures. We examine all the planning actions that lead to failures. If there are restrictions imposed by a problem state on taking possible actions, the restrictions manifest themselves in the form of a restricted set of possible bindings. Our method makes use of this observation to derive general control rules which are guaranteed to be correct. The overhead involved in learning is low because our method examines only the goal stacks retrieved from the leaf nodes of a failure search tree rather than the whole tree. Empirical tests show that the rules derived by our system PAL, after sufficient training, performs as well as or better than those derived by systems such as PRODIGY/EBL and STATIC.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-062.pdf,
62,1992,"ProblemSolving: Constraint Satisfaction",Efficient Propositional Constraint Propagation,Mukesh Dalal,"We present an efficient method for inferring facts from a propositional knowledge base, which is not required to be in conjunctive normal form. This logically-incomplete method, called propositional fact propagation, is more powerful and efficient than some forms of boolean constraint propagation. Hence, it can be used for tractable deductive reasoning in many AI applications, including various truth maintenance systems. We also use propositional fact propagation to define a weak logical entailment relation that is more powerful and efficient than some others presented in the literature. Among other applications, this new entailment relation can be used for efficiently answering queries posed to a knowledge base, and for modeling beliefs held by a resource-limited agent.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-063.pdf,
63,1992,"ProblemSolving: Constraint Satisfaction",Semantic Evaluation as Constraint Network Consistency,Nicholas J. Haddock,"This paper investigates a problem of natural language processing from the perspective of reasoning work on constraint satisfaction. We formulate the task of computing singular, definite reference to a known contextual entity as a constraint satisfaction problem. We argue that such referential constraint problems have a structure which is often simpler than the general case, and can therefore often be solved by the sole use of low-power network consistency techniques. To illustrate, we define a linguistic fragment which provably generates tree-structured constraint problems. This enables us to conclude that the limited operation of strong arc consistency is sufficient to resolve the class of noun phrase defined by the fragment.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-063.pdf,
64,1992,"ProblemSolving: Constraint Satisfaction",An Efficient Cross Product Representation of the Constraint Satisfaction Problem Search Space,"Paul D. Hubbe, Eugene C. Freuder","Constraint satisfaction problems involve finding values for variables subject to constraints on which combinations of values are permitted. They arise in a wide variety of domains, ranging from scene analysis to temporal reasoning. We present a new representation for partial solutions as cross products of sets of values. This representation can be used to improve the performance of standard algorithms, especially when seeking all solutions or discovering that none exist.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-065.pdf,
65,1992,"ProblemSolving: Constraint Satisfaction",On the Density of Solutions in Equilibrium Points for the Queens Problem,Paul Morris,"There has been recent, interest in applying hill-climbing or iterative improvement methods to constraint satisfaction problems. An important issue for such methods is the likelihood of encountering a non-solution equilibrium (locally optimal) point. We present analytic techniques for determining the relative densities of solutions and equilibrium points with respect to these algorithms. The analysis explains empirically observed data for the n-queens problem, and provides insight into the potential effectiveness of these methods for other problems.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-066.pdf,
66,1992,"ProblemSolving: Constraint Satisfaction",An Improved Connectionist Activation Function for Energy Minimization,"Gadi Pinkas, Rina Dechter","Symmetric networks that are based on energy minimization, such as Boltzmann machines or Hopfield nets, are used extensively for optimization, constraint satisfaction, and approximation of NP-hard problems. Nevertheless, finding a global minimum for the energy function is not guaranteed, and even a local minimum may take an exponential number of steps. We propose an improvement to the standard activation function used for such networks. The improved algorithm guarantees that a global minimum is found in linear time for tree-like subnetworks. The algorithm is uniform and does not assume that the network is a tree. It performs no worse than the standard algorithms for any network topology. In the case where there are trees growing from a cyclic subnetwork, the new algorithm performs better than the standard algorithms by avoiding local minima along the trees and by optimizing the free energy of these trees in linear time. The algorithm is self-stabilizing for trees (cycle-free undirected graphs) and remains correct under various scheduling demons. However, no uniform protocol exists to optimize trees under a pure distributed demon and no such protocol exists for cyclic networks under central demon.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-067.pdf,
67,1992,"ProblemSolving: Constraint Satisfaction",A New Method for Solving Hard Satisfiability Problems,"Bart Selman, Hector Levesque, David Mitchell","We introduce a greedy local search procedure called GSAT for solving propositional satisfiability problems. Our experiments show that this procedure can be used to solve hard, randomly generated problems that are an order of magnitude larger than those that can be handled by more traditional approaches such as the Davis-Putnam procedure or resolution. We also show that GSAT can solve structured satisfiability problems quickly. In particular, we solve encodings of graph coloring problems, N-queens, and Boolean induction. General application strategies and limitations of the approach are also discussed. GSAT is best viewed as a model-finding procedure. Its good performance suggests that it may be advantageous to reformulate reasoning tasks that have traditionally been viewed as theorem-proving problems as model-finding tasks.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-068.pdf,
68,1992,"ProblemSolving: Constraint Satisfaction",On the Minimality and Decomposability of Constraint Networks,Peter van Beek,"Constraint networks have been shown to be useful in formulating such diverse problems as scene labeling, natural language parsing, and temporal reasoning. Given a constraint network, we often wish to (i) find a solution that satisfies the constraints and (ii) find the corresponding minimal network where the constraints are as explicit as possible. Both tasks are known to be NP-complete in the general case. Task (i) is usually solved using a backtracking algorithm, and task (ii) is often solved only approximately by enforcing various levels of local consistency. In this paper, we identify a property of binary constraints called row convexity and show its usefulness in deciding when a form of local consistency called path consistency is sufficient to guarantee a network is both minimal and decomposable. Decomposable networks have the property that a solution can be found without backtracking. We show that the row convexity property can be tested for efficiently and we show, by examining applications of constraint networks discussed in the literature, that our results are useful in practice. Thus, we identify a large class of constraint networks for which we can solve both tasks (i) and (ii) efficiently.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-069.pdf,
69,1992,"ProblemSolving: Constraint Satisfaction",Solving Constraint Satisfaction Problems Using Finite State Automata,Nageshwara Rao Vempaty,"In this paper, we explore the idea of representing CSPs using techniques from formal language theory. The solution set of a CSP can be expressed as a regular language; we propose the minimized deterministic finite state automaton (MDFA) recognizing this language as a canonical representation for the CSP. This representation has a number of advantages. Explicit (enumerated) constraints can be stored in lesser space than traditional techniques. Implicit constraints and networks of constraints can be composed from explicit ones by using a complete algebra of boolean operators like AND, OR, NOT, etc., applied in an arbitrary manner. Such constraints are stored in the same way as explicit constraints - by using MDFAs. This capability allows our technique to construct networks of constraints incrementally. After constructing this representation, answering queries like satisfiability, validity, equivalence, etc., becomes trivial as this representation is canonical. Thus, MDFAs serve as a means to represent constraints as well as to reason with them. While this technique is not a panacea for solving CSPs, experiments demonstrate that it is much better than previously known techniques on certain types of problems.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-070.pdf,
70,1992,Problem Solving: Hardness and Easiness,Hard and Easy Distributions of SAT Problems,"David Mitchell, Bart Selman, Hector Levesque","We report results from large-scale experiments in satisfiability testing. As has been observed by others, testing the satisfiability of random formulas often appears surprisingly easy. Here we show that by using the right distribution of instances, and appropriate parameter values, it is possible to generate random formulas that are hard, that is, for which satisfiability testing is quite difficult. Our results provide a benchmark for the evaluation of satisfiability-testing procedures.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-071.pdf,
71,1992,Problem Solving: Hardness and Easiness,How Long Will It Take?,"Ron Musick, Stuart Russell","We present a method for approximating the expected number of steps required by a heuristic search algorithm to reach a goal from any initial state in a problem space. The method is based on a mapping from the original state space to an abstract space in which states are characterized only by a syntactic ""distance"" from the nearest goal. Modeling the search algorithm as a Markov process in the abstract space yields a simple system of equations for the solution time for each state. We derive some insight into the behavior of search algorithms by examining some closed form solutions for these equations; we also show that many problem spaces have a clearly de-lineated ""easy zone"", inside which problems are trivial and outside which problems are impossible. The theory is borne out by experiments with both Markov and non-Markov search algorithms. Our results also bear on recent experimental data suggesting that heuristic repair algorithms can solve large constraint satisfaction problems easily, given a preprocessor that generates a sufficiently good initial state.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-072.pdf,
72,1992,Problem Solving: Hardness and Easiness,Using Deep Structure to Locate Hard Problems,"Colin P. Williams, Tad Hogg","One usually writes A.I. programs to be used on a range of examples which, although similar in kind, differ in detail. This paper shows how to predict where, in a space of problem instances, the hardest problems are to be found and where the fluctuations in difficulty are greatest. Our key insight is to shift emphasis from modelling sophisticated algorithms directly to rnodelling a search space which captures their principal effects. This allows us to analyze complex A.I. problems in a simple and intuitive way. We present a sample analysis, compare our model’s quantitative predictions with data obtained independently and describe how to exploit the results to estimate the value of preprocessing. Finally, we circumscribe the kind problems to which the methodology is suited.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-073.pdf,
73,1992,Problem Solving: Real-Time,Run-Time Prediction for Production Systems,"Franz Barachini, Hans Mistelberger, Anoop Gupta","A major obstacle to the widespread use of expert systems in real-time domains is the non-predictability of response times. While some researchers have addressed this issue by optimizing response time through better algorithms or parallel hardware, there has been little research towards run-time prediction in order to meet user defined deadlines. To cope with the latter, real-time expert systems must provide mechanisms for estimating run-time required to react to external events. As a starting point for our investigations we chose the RETE algorithm, which is widely used for real-time production systems. In spite of RETE’s combinatorial worst case match behavior we introduce a method for estimating match-time in the RETE network. This paper shows that simple profiling methods do not work well, but by going to a finer granularity, we can get much better execution time predictions for basic actions as well as for complete right hand sides of rules. Our method is dynamically applied during the run-time of the production system by using continuously updated statistical data of individual nodes in the RETE network.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-074.pdf,
74,1992,Problem Solving: Real-Time,Can Real-Time Search Algorithms Meet Deadlines?,"Babak Hamidzadeh, Shashi Shekhar","A real-time AI problem solver performs a task or a set of tasks in two phases: planning and execution. Under real-time constraints, a real-time AI problem solver must balance the planning and the execution phases of its operation to comply with deadlines. This paper provides a methodology for specification and analysis of real-time AI problems and problem solvers. This methodology is demonstrated via domain analysis of the real-time path planning problem and via algorithm analysis of DYNORAII and RTA* [ 1]. We provide new results on worst-case complexity of the problem. We also provide experimental evaluation of DYNORAII and RTA* for deadline compliance.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-075.pdf,
75,1992,Problem Solving: Real-Time,Comparison of Three Algorithms for Ensuring Serializable Executions in Parallel Production Systems,"James G. Schmolze, Daniel Neiman","To speed up production systems, researchers have developed parallel algorithms that execute multiple instantiations simultaneously. Unfortunately, without special controls, such systems can produce results that could not have been produced by any serial execution. We present and compare three different algorithms that guarantee a serializable result in such systems. Our goal is to analyze the overhead that serialization incurs. All three algorithms perform synchronization at the level of instantiations, not rules, and are targeted for shared-memory machines. One algorithm operates synchronously while the other two operate asynchronously. Of the latter two, one synchronizes instantiations using compiled tests that were determined from an offline analysis while the other uses a novel locking scheme that requires no such analysis. Our examination of performance shows that asynchronous execution is clearly faster than synchronous execution and that the locking method is somewhat faster than the method using compiled tests. Moreover, we predict that the synchronization and/or locking needed to guarantee serializability will limit speedup no matter how many processors are used.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-076.pdf,
76,1992,Problem Solving: Real-Time,Real-time Metareasoning with Dynamic Trade-off Evaluation,"Ursula M. Schwuttke, Les Gasser","This paper describes dynamic trade-off evaluation (DTE), a new technique that has been developed to improve the performance of real-time problem solving systems. The DTE technique is most suitable for automation environments in which the requirement for meeting time constraints is of equal importance to that of providing optimally intelligent solutions. In such environments, the demands of high input data volumes and short response times can rapidly overwhelm traditional AI systems. DTE is based on the recognition that in time-constrained environments, compromises to optimal problem solving (in favor of timeliness) must often be made in the form of trade-offs. Towards this end, DTE combines knowledge-based techniques with decision theory to 1) dynamically modify system behavior and 2) adapt the decision criteria that determine how such modifications are made. The performance of DTE has been evaluated in the context of several types of real-time trade-offs in spacecraft monitoring problems. One such application has demonstrated that DTE can be used to dynamically vary the data that is monitored, making it possible to detect and correctly analyze all anomalous data by examining only a subset of the total input data. In carefully structured experimental evaluations that use real spacecraft data and real decision making, DTE provides the ability to handle a three-fold increase in input data (in real-time) without loss of performance.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-077.pdf,
77,1992,Problem Solving: Search and Expert Systems,On Optimal Game Tree Propagation for Imperfect Players,Eric B. Baum,"We consider the approach to game playing where one looks ahead in a game tree, evaluates heuristically the probability of winning at the leaves, and then propagates this evaluation up the tree. We show that minimax does not make optimal use of information contained in the leaf evaluations, and in fact misvalues the position associated with all nodes. This occurs because when actually playing a position down the game tree, a player would be able to search beyond the boundaries of the original search, and so has access to additional information. The remark that such extra information will exist, allows better use of the information contained in the leaf evaluations even though we do not have access to the extra information itself. Our analysis implies that, while minimax is approximately correct near the top of the game tree, near the bottom a formula closer to the probability product formula is better. We propose a simple model of how deep search yields extra information about the chances of winning in a position. Within the context of this model, we write down the formula for propagating information up the tree which is correct at all levels. We generalize our results to the case when the outcomes at the leaves are correlated and also to games like chess where there are three possible outcomes: Win, Lose, and Draw. Experiments demonstrate our formula’s superiority to minimax and probability product in the game of Kalah.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-078.pdf,
78,1992,Problem Solving: Search and Expert Systems,Improved Decision-Making in Game Trees: Recovering from Pathology,"Arthur L. Delcher, Simon Kasif","In this paper we address the problem of making correct decisions in the context of game-playing. Specifically, we address the problem of reducing or eliminating pathology in game trees. However, the framework used in the paper applies to decision making that depends on evaluating complex Boolean expressions. The main contribution of this paper is in casting general evaluation of game trees as belief propagation in causal trees. This allows us to draw several theoretically and practically interesting corollaries. In the Bayesian framework we typically do not want to ignore any evidence, even if it may be inaccurate. Therefore, we evaluate the game tree on several levels rather than just the deepest one. Choosing the correct move in a game can be implemented in a straightforward fashion by an efficient linear-time algorithm adapted from the procedure for belief propagation in causal trees. We propose a probabilistically sound heuristic that allows us to reduce the effects of pathology significantly.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-079.pdf,
79,1992,Problem Solving: Search and Expert Systems,Modeling Accounting Systems to Support Multiple Tasks: A Progress Report,Walter Hamscher,"A domain model in SAVILE represents the steps involved in producing and processing financial data in a company, using an ontology appropriate for several reasoning tasks in accounting and auditing. SAVILE is an implemented program that demonstrates the adequacy and appropriateness of this ontology of financial data processing for evaluating internal controls, designing tests, and other audit planning related tasks. This paper discusses the rationale, syntax, semantics, and implementation of the ontology as it stands today.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-080.pdf,
80,1992,Problem Solving: Search and Expert Systems,Moving Target Search with Intelligence,Toru Ishida,"We previously proposed the moving target search (MTS) algorithm, where the location of the goal may change during the course of the search. MTS is the first search algorithm concerned with problem solving in a dynamically changing environment. However, since we constructed the algorithm with the minimum operations necessary for guaranteeing its completeness, the algorithm as proposed is neither efficient nor intelligent. In this paper, we introduce innovative notions created in the area of resource- bounded planning into the formal search algorithm, MTS. Our goal is to improve the efficiency of MTS, while retaining its completeness. Notions that are introduced are (1) commitment to goals, and (2) deliberation for selecting plans. Evaluation results demonstrate that the intelligent MTS is 10 to 20 times more efficient than the original MTS in uncertain situations.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-081.pdf,
81,1992,Problem Solving: Search and Expert Systems,Linear-Space Best-First Search: Summary of Results,Richard E. Korf,"Best-first search is a general search algorithm that, always expands next a frontier node of lowest cost. Its applicability, however, is limited by its exponential memory requirement. Iterative deepening, a previous approach to this problem, does not expand nodes in best-first order if the cost function can decrease along a path. We present a linear-space best-first search algorithm (RBFS) that always explores new nodes in best-first order, regardless of the cost function, and expands fewer nodes than iterative deepening with a nondecreasing cost function. On the sliding-tile puzzles, RBFS with a weighted evaluation function dramatically reduces computation time with only a small penalty in solution cost. In general, RBFS reduces the space complexity of best-first search from exponential to linear, at the cost of only constant factor in time complexity in our experiments.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-082.pdf,
82,1992,Problem Solving: Search and Expert Systems,Performance of IDA on Trees and Graphs,"A. Mahanti, S. Ghosh, D. S. Nau, L. N. Kanal, A. K. Pal","We present the following results about IDA* and related algorithms: We show that IDA* is not asymptotically optimal in all of the cases where it was thought to be so. In particular, there are trees satisfying all of the conditions previously thought to guarantee asymptotic optimality for IDA*, such that IDA* will expand more than O(N) nodes, where N is the number of nodes eligible for expansion by A*. We present a new set of necessary and sufficient conditions to guarantee that IDA* expands O(N) nodes on trees.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-083.pdf,
83,1992,Problem Solving: Search and Expert Systems,An Average-Case Analysis of Branch-and-Bound with Applications: Summary of Results,"Weixiong Zhang, Richard E. Korf","Motivated by an anomaly in branch-and-bound (BnB) search, we analyze its average-case complexity. We first delineate exponential vs polynomial average-case complexities of BnB. When best-first BnB is of linear complexity, we show that depth-first BnB has polynomial complexity. For problems on which best-first BnB haa exponential complexity, we obtain an expression for the heuristic branching factor. Next, we apply our analysis to explain an anomaly in lookahead search on sliding-tile puzzles, and to predict the existence of an average-case complexity transition of BnB on the Asymmetric Traveling Salesman Problem. Finally, by formulating IDA* as costbounded BnB, we show its aaverage-case optimality, which also implies that RBFS is optimal on average.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-084.pdf,
84,1992,"Representationand Reasoning: Abduction and Diagnosis",Dynamic MAP Calculations for Abduction,"Eugene Charniak, Eugene Santos, Jr.","We present a dynamic algorithm for MAP calculations. The algorithm is based upon Santos’s technique (Santos 1991b) of transforming minimal-cost-proof problems into linear-programming problems. The algorithm is dynamic in the sense that it is able to use the results from an earlier, near by, problem to lessen its search time. Results are presented which clearly suggest that this is a powerful technique for dynamic abduction problems.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-085.pdf,
85,1992,"Representationand Reasoning: Abduction and Diagnosis",Consistency-Based Diagnosis in Physiological Domains,Keith L. Downing,"This research attempts to span the gap between the AI in medicine (AIM) and consistency-based diagnosis (CBD) communities by applying CBD to physiology. The highly-regulated nature of physiological systems challenges standard CBD algorithms, which are not tailored for complex dynamic systems. To combat this problem, we separate static from dynamic analysis, so that CBD is performed over the steady-state constraints at only a selected set of time slices. Regulatory models help link static inter-slice diagnoses into a complete dynamic account of the physiological progression. This provides a simpler approach to CBD in dynamic systems that (a) preserves information-reuse capabilities, (b) extends information-theoretic probing, and (c) adds a new capability to CBD: the detection of dynamic faults (i.e., those that do not necessarily persist throughout diagnosis) .",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-086.pdf,
86,1992,"Representationand Reasoning: Abduction and Diagnosis",Adaptive Model-Based Diagnostic Mechanism Using a Hierarchical Model Scheme,"Yoichiro Nakakuki, Yoshiyuki Koseki, Midori Tanaka","This paper describes an adaptive model-based diagnostic mechanism. Although model-based systems are more robust than heuristic-based expert systems, they generally require more computation time. Time consumption can be significantly reduced by using a hierarchical model scheme, which presents views of the device at several different levels of detail. We argue that in order to employ hierarchical models effectively, it is necessary to make economically rational choices concerning the trade-off between the cost of a diagnosis and its precision. The mechanism presented here makes these choices using a model diagnosabiliiy criterion which estimates how much information could be gained by using a candidate model. It takes into account several important parameters, including the level of diagnosis precision required by the user, the computational resources available, the cost of observations, and the phase of the diagnosis. Experimental results demonstrate the effectiveness of the proposed mechanism.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-087.pdf,
87,1992,"Representationand Reasoning: Abduction and Diagnosis",Reasoning MPE to Multiply Connected Belief Networks Using Message Passing,Bon K. Sy,This paper presents an efficient algorithm for finding l (>= 2) MPE in singly-connected networks and the extension of this algorithm for multiply-connected networks.,https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-088.pdf,
88,1992,Representation and Reasoning: Action and Change,Formalizing Reasoning about Change: A Qualitative Reasoning Approach (Preliminary Report),"James M. Crawford, David W. Etherington","The development of a formal logic for reasoning about change has proven to be surprisingly difficult. Furthermore, the logics that have been developed have found surprisingly little application in those fields, such as Qualitative Reasoning, that are concerned with building programs that emulate human common-sense reasoning about change. In this paper, we argue that a basic tenet of qualitative reasoning practice-the separation of modeling and simulation-obviates many of the difficulties faced by previous attempts to formalize reasoning about change. Our analysis helps explain why the QR community has been nonplussed by some of the problems studied in the nonmonotonic reasoning community. Further, the formalism we present provides both the beginnings of a formal foundation for qualitative reasoning, and a framework in which to study a number of open problems in qualitative reasoning.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-089.pdf,
89,1992,Representation and Reasoning: Action and Change,Deriving Properties of Belief Update from Theories of Action,"Alvaro del Val, Yoav Shoham","Two areas that have attracted much interest in recent years, belief update and reasoning about action, have so far been largely disjoint. Indeed, at first glance there appears to be little connection between them. In this paper we argue that this first impression is wrong; specifically, we show that the postulates for belief update recently proposed in [Katsuno and Mendelzon, 1991], can in fact be analytically derived, using the formal theory of action proposed in [Lin and Shoham, 1991].",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-090.pdf,
90,1992,Representation and Reasoning: Action and Change,Concurrent Actions in the Situation Calculus,"Fangzhen Lin, Yoav Shoham","We propose a representation of concurrent actions; rather than invent a new formalism, we model them within the standard situation calculus by introducing the notions of global actions and primitive actions, whose relationship is analogous to that between situations and fluents. The result is a framework in which situations and actions play quite symmetric roles. The rich structure of actions gives rise to a new problem, which, due to this symmetry between actions and situations, is analogous to the traditional frame problem. In [Lin and Shoham 1991] we provided a solution to the frame problem based on a formal adequacy criterion called ""epistemological completeness."" Here we show how to solve the new problem based on the same adequacy criterion.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-091.pdf,
91,1992,Representation and Reasoning: Action and Change,Nonmonotonic Sorts for Feature Structures,Mark A. Young,"There have been many recent attempts to incorporate defaults into unification-based grammar formalisms. What these attempts have in common is that they all lose one of the most desirable properties of feature systems: namely, presentation order independence. This paper describes a method of dealing with defaults that retains order independence. The method works by making a strong distinction between strict and default information. The addition of nonmonotonic sorts allows default information to be carried in the feature structure while retaining a simple, deterministic unification operation. Monotonic feature structures are rederived through a satisfaction relation that is abstract in that it depends only on the ordering information for sorts.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-092.pdf,
92,1992,Representation and Reasoning: Belief,From Statistics to Beliefs,"Fahiem Bacchus, Adam Grove, Daphne Koller, Joseph Y. Halpern","An intelligent agent uses known facts, including statistical knowledge, to assign depees of belief to assertions it is uncertain about. We investigate three principled techniques for doing this. All three are applications of the principle of indifference, because they assign equal degree of belief to all basic ""situations"" consistent with the knowledge base. They differ because there are competing intuitions about what the basic situations are. Various natural patterns of reasoning, such as the preference for the most specific statistical data available, turn out to follow from some or all of the techniques. This is an improvement over earlier theories, such as work on direct inference and reference classes, which arbitrarily postulate these patterns without offering any deeper explanations or guarantees of consistency. The three methods we investigate have surprising characterisations: there are connections to the principle of maximum entropy, a principle of maximal independence, and a ""center of mass"" principle. There are also unexpected connections between the three, that help us understand why the specific language chosen (for the knowledge base) is much more critical in inductive reasoning of the sort we consider than it is in traditional deductive reasoning.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-093.pdf,
93,1992,Representation and Reasoning: Belief,A Logic for Revision and Subjunctive Queries,Craig Boutilier,"We present a logic for belief revision in which revision of a theory by a sentence is represented using a conditional connective. The conditional is not primitive, but rather defined using two unary modal operators. Our approach captures and extends the classic AGM model without relying on the Limit Assumption. Reasoning about counterfactual or hypothetical situations is also crucial for AI. Existing logics for such subjunctive queries are lacking in several respects, however, primarily in failing to make explicit the epistemic nature of such queries. We present a logical model for subjunctives based on our logic of revision that appeals explicitly to the Ramsey test. We discuss a framework for answering subjunctive queries, and show how integrity constraints on the revision process can be expressed.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-094.pdf,
94,1992,Representation and Reasoning: Belief,Lexical Imprecision in Fuzzy Constraint Networks,"James Bowen, Robert Lai, Dennis Bahler","We define fuzzy constraint networks and prove a theorem about their relationship to fuzzy logic. Then we introduce Khayyam, a fuzzy constraint-based programming language in which any sentence in the first-order fuzzy predicate calculus is a well-formed constraint statement. Finally, using Khayyam to address an equipment selection application, we illustrate the expressive power of fuzzy constraint-based languages.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-095.pdf,
95,1992,Representation and Reasoning: Belief,A Symbolic Generalization of Probability Theory,"Adnan Y. Darwiche, Matthew L. Ginsberg","This paper demonstrates that it is possible to relax the commitment to numeric degrees of belief while retaining the desirable features of the Bayesian approach for representing and changing states of belief. We first present an abstract representation of states of belief and an associated notion of conditionalization that subsume their Bayesian counterparts. Next, we provide some symbolic and numeric instances of states of belief and their conditionalizations. Finally, we show that patterns of belief change that make Bayesianism so appealing do hold in our framework.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-096.pdf,
96,1992,Representation and Reasoning: Belief,A Logic of Knowledge and Belief for Recursive Modeling: A Preliminary Report,"Piotr J. Gmytrasiewicz, Edmund H. Durfee","To make informed decisions in a multiagent environment, an agent needs to model itself, the world, and the other agents, including the models that those other agents might be employing. We present a framework for recursive modeling that uses possible worlds semantics, and is based on extending the Kripke structure so that an agent can model the information it thinks that another agent has in each of the possible worlds, which in turn can be modeled with Kripke structures. Using recursive nesting, we can define the propositional attitudes of agents to distinguish between the concepts of knowledge and belief. Through the Three Wise Men example, we show how our framework is useful for deductive reasoning, and we suggest that it might provide a meeting ground between decision theoretic and deductive methods for multiagent reasoning.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-097.pdf,
97,1992,Representation and Reasoning: Belief,Ideal Introspective Belief,Kurt Konolige,"Autoepistemic (AE) logic is a formal system characterizing agents that have complete introspective access to their own beliefs. AE logic relies on a fixed point definition that has two significant parts. The first part is a set of assumptions or hypotheses about the contents of the fixed point. The second part is a set of reflection principles that link sentences with statements about their provability. We characterize a family of ideal AE reasoners in terms of the minimal hypotheses that they can make, and the weakest and strongest reflection principles that they can have, while still maintaining the interpretation of AE logic as self-belief. These results can help in analyzing metatheoretic systems in logic programming.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-098.pdf,
98,1992,Representation and Reasoning: Belief,A Belief-Function Logic,Alessandro Saffiotti,"We present BFL, a hybrid logic for representing uncertain knowledge. BFL attaches a quantified notion of belief - based on Dempster-Shafer’s theory of belief functions - to classical first-order logic. The language of BFL is composed of objects of the form F:[a,b], where F is a first-order sentence, and Q and b are numbers in the [O,l] interval (with a>=b). Intuitively, a measures the strength of our belief in the truth of F, and (l-b) that in its falseness. A number of properties of first-order logic nicely generalize to BFL; in return, BFL gives us a new perspective on some important points of Dempster-Shafer theory (e.g., the role of Dempster’s combination rule).",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-099.pdf,
99,1992,Representation and Reasoning: Belief,Combining Circumscription and Modal Logic,Jacques Wainer,"This paper discusses the logic LKM which extends circumscription into an epistemic domain. This extension will allow us to define circumscription of predicates that appear within the context of a modal operator. In fact, LKM can be seen as a method of extending any first-order nonmonotonic logic whose semantic definition is based on a partial-order among models, into a new nonmonotonic logic defined for a modal language, whose modal operator (K) follows an underlying S5 or weak-S5 semantics. One interesting use of this nonmonotonic logic is to model nonmonotonic aspects of the communication between agents.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-100.pdf,
100,1992,Representation and Reasoning: Case-Based,Generating Dialectical Examples Automatically,"Kevin D. Ashley, Vincent Aleven","We identify and illustrate five important kinds of Dialectical Examples, standard configurations of cases which enable an arguer to justify rhetorical assertions effectively by example. Our computer program generates Argument Contexts, collections of cases that instantiate Dialectical Examples from an on-line database of cases according to a user’s general specifications. The Argument Context generation program provides a human or automated tutor a stock of Dialectical Examples to teach novice advocates (first year law students) how to recognize, carry out and respond to the associated rhetorical moves. Although generating such examples is very hard for humans even when dealing with small numbers of cases, our program generates and organizes such examples quickly and effectively. In a preliminary experiment, we employed program-generated Argument Contexts manually to teach basic argument skills to first year law students with good results. Our ability to define such complex examples declaratively in terms of logical expressions of Loom concepts and relations affords a number of advantages over previous work.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-101.pdf,
101,1992,Representation and Reasoning: Case-Based,Common Sense Retrieval,A. Julian Craddock,"An important and readily available source of knowledge for common sense reasoning is partial descriptions of specific experiences. Knowledge bases (KBs) containing such information are called episodic knowledge buses (EKB). Aggregations of episodic knowledge provide common sense knowledge about the unobserved properties of 'new' experiences. Such knowledge is retrieved by applying statistics to a relevant subset of the EKB called the reference class. I study a manner in which a corpus of experiences can be represented to allow common sense retrieval which is: 1. Flexible enough to allow the common sense reasoner to deal with 'new' experiences, and 2. In the simplest case, reduces to efficient database look-up. I define two first order dialects, L and QL. L is used to represent experiences in an episodic knowledge base. An extension, QL is used for writing queries to EKBs l.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-102.pdf,
102,1992,Representation and Reasoning: Case-Based,When Should a Cheetah Remind You of a Bat? Reminding in Case-Based Teaching,Daniel C. Edelson,"Case-based teaching systems, like good human teachers, tell stories in order to help students learn. A case-based teaching system engages a student in a challenging task and monitors his actions looking for opportunities to tell stories that will assist the learning process. In order to produce stories at the appropriate moment, a case-based teaching system must have a library of stories that are indexed according to how they should be used and a set of reminding strategies to retrieve stories when they are relevant. In this paper, I discuss CreANIMate, a biology tutor that uses stories to help teach elementary school students about animal morphology. In particular, I discuss the reminding strategies and indexing schemes that enable the system to achieve its educational objectives. These reminding strategies are example remindings, similarity-based remindings, and expectation violation remindings.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-103.pdf,
103,1992,Representation and Reasoning: Case-Based,Model-Based Case Adaptation,Eric K. Jones,"In this paper, we demonstrate an important role for model-based reasoning in case adaptation. Model-based reasoning can allow a case-based reasoner to apply cases to a wider range of problems than would otherwise be possible. We focus on case adaptation in BRAINSTORMER, a planner that uses abstract advice to help it plan in the domain of political and military policy as it relates to terrorism. We show that by equipping a case adapter with an explicit causal model of the planning process, cases presented as advice can be flexibly applied to difficulties that arise at a variety of different stages of planning.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-104.pdf,
104,1992,Representation and Reasoning: Qualitative,Qualitative Simulation Based on a Logical Formalism of Space and Time,"Z. Cui, A. G. Cohn, D. A. Randell","We describe an envisionment-based qualitative simulation program. The program implements part of an axiomatic, first order theory that has been developed to represent and reason about space and time. Topological information from the modelled domain is expressed as sets of distinct topological relations holding between sets of objects. These form the qualitative states in the underlying theory and simulation. Processes in the theory are represented as paths in the envisionment tree. The algorithm is illustrated with an example of a simulation of phagocytosis and exocytosis - two processes used by unicellular organisms for garnering food and expelling waste material respectively.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-105.pdf,
105,1992,Representation and Reasoning: Qualitative,Self-Explanatory Simulations: Scaling Up to Large Models,"Kenneth D. Forbus, Brian Falkenhainer","Qualitative reasoners have been hamstrung by the inability to analyze large models. This includes self-explanatory simulators, which tightly integrate qualitative and numerical models to provide both precision and explanatory power. While they have important potential applications in training, instruction, and conceptual design, a critical step towards realizing this potential is the ability to build simulators for medium-sized systems (i.e., on the order of ten to twenty independent parameters). This paper describes a new method for developing self-explanatory simulators which scales up. While our method involves qualitative analysis, it does not rely on envisioning or any other form of qualitative simulation. We describe the results of an implemented system which uses this method, and analyze its limitations and potential.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-106.pdf,
106,1992,Representation and Reasoning: Qualitative,Towards a Qualitative Lagrangian Theory of Fluid Flow,Gordon Skorstad,"Choosing between multiple ontological perspectives is crucial for reasoning about the physical world. Choosing the wrong perspective can make a reasoning task impossible. This paper introduces a Lagrangian plug flow ontology (PF) for reasoning about thermodynamic fluid flow. We show that this ontology captures continuously changing behaviors of flowing fluids not represented in currently implemented ontologies. These behaviors are essential for understanding thermodynamic applications such as power cycles, refrigeration, liquefaction, throttling and flow through nozzles. We express the ontology within the framework of Qualitative Process (QP) theory. To derive our QP theory for plug flow, we use the method of causal clustering to find causal interpretations of thermodynamic equations. We also incorporate qualitative versions of standard thermodynamic relations, including the second law of thermodynamics and Clapeyron’s equation.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-107.pdf,
107,1992,Representation and Reasoning: Qualitative,On the Qualitative Structure of a Mechanical Assembly,"Randall H. Wilson, Jean-Claude Latombe","A mechanical assembly is usually described by the geometry of its parts and the spatial relations defining their positions. This model does not directly provide the information needed to reason about assembly and disassembly motions. We propose another representation, the non-directional blocking graph, which describes the qualitative internal structure of the assembly. This representation makes explicit how the parts prevent each other from being moved in every possible direction of motion. It derives from the observation that the infinite set of motion directions can be partitioned into a finite arrangement of subsets such that over each subset the interferences among the parts remain qualitatively the same. We describe how this structure can be efficiently computed from the geometric model of the assembly. The (dis)assembly motions considered include infinitesimal and extended translations in two and three dimensions, and infinitesimal rigid motions.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-108.pdf,
108,1992,Representation and Reasoning: Qualitative Model Construction,Causal Approximations,P. Pandurang Nayak,"Adequate problem representations require the identification of abstractions and approximations that are well suited to the task at hand. In this paper we introduce a new class of approximations, called cuusal approximations, that are commonly found in modeling the physical world. Causal approximations support the efficient generation of parsimonious causal explanations, which play an important role in reasoning about engineered devices. The central problem to be solved in generating parsimonious causal explanations is the identification of a simplest model that explains the phenomenon of interest. We formalize this problem and show that it is, in general, intractable. In this formalization, simplicity of models is based on the intuition that using more approximate models of fewer phenomena leads to simpler models. We then show that when all the approximations are causal approximations, the above problem can be solved in polynomial time.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-109.pdf,
109,1992,Representation and Reasoning: Qualitative Model Construction,Automated Model Selection Using Context-Dependent Behaviors,"P. Pandurang Nayak, Leo Joskowicz, Sanjaya Addanki","Effective reasoning about complex engineered devices requires device models that are both adequate for the task and computationally efficient. This paper presents a method for constructing simple and adequate device models by selecting appropriate models for each of the device’s components. Appropriate component models are determined by the context in which the device operates. We introduce context-dependent behaviors (CDBS), a component behavior model representation for encapsulating contextual modeling constraints. We show how CDBs are used in the model selection process by exploiting constraints from three sources: the structural and behavioral contexts of the components, and the expected behavior of the device. We describe an implemented program for selecting a simplest adequate model. The inputs are the structure of the device, the expected device behavior, and a library of CDBs. The output is a set of component CDBs forming a structurally and behaviorally consistent device model that achieves the expected behavior.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-110.pdf,
110,1992,Representation and Reasoning: Qualitative Model Construction,Learning Engineering Models with the Minimum Description Length Principle,"R. Bharat Rao, Stephen C-Y. Lu","This paper discusses discovery of mathematical models from engineering data sets. KEDS, a Knowledge-based Equation Discovery System, identifies several potentially overlapping regions in the problem space, each associated with an equation of different complexity and accuracy. The minimum description length principle, together with the KEDS algorithm, is used to guide the partitioning of the problem space. The KEDS-MDL algorithm has been tested on discovering models for predicting the performance efficiencies of an internal combustion engine.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-111.pdf,
111,1992,Representation and Reasoning: Qualitative Model Construction,Automatic Abduction of Qualitative Models,"Bradley L. Richards, Benjamin J. Kuipers, Ina Kraan","We describe a method of automatically abducing qualitative models from descriptions of behaviors. We generate, from either quantitative or qualitative data, models in the form of qualitative differential equations suitable for use by QSIM. Constraints are generated and filtered both by comparison with the input behaviors and by dimensional analysis. If the user provides complete information on the input behaviors and the dimensions of the input variables, the resulting model is unique, maximally constrained, and guaranteed to reproduce tbe input behaviors. lf the user provides incomplete information, our method will still generate a model which reproduces tbe input behaviors, but the model may no longer be unique. Incompleteness can take several forms: missing dimensions, values of variables, or entire variables.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-112.pdf,
112,1992,Representation and Reasoning: Temporal,Complexity Results for Serial Decomposability,Tom Bylander,"Korf (1985) presents a method for learning macro-operators and shows that the method is applicable to serially decomposable problems. In this paper I analyze the computational complexity of serial decomposability. Assuming that operators take polynomial time, it is NP-complete to determine if an operator (or set of operators) is not serially decomposable, whether or not an ordering of state variables is given. In addition to serial decomposability of operators, a serially decomposable probIem requires that the set of solvable states is closed under the operators. It is PSPACEcomplete to determine if a given ""finite state-variable problem"" is serially decomposable. In fact, every solvable instance of a PSPACE problem can be converted to a serially decomposable problem. Furthermore, given a bound on the size of the input, every problem in PSPACE can be transformed to a probIem that is nearly serially-decomposable, i.e., the problem is serially decomposable except for closure of solvable states or a unique goal state.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-113.pdf,
113,1992,Representation and Reasoning: Temporal,Temporal Reasoning in Sequence Graphs,Jürgen Dorn,"Temporal reasoning is widely used in AI, especially for natural language processing. Existing methods for temporal reasoning are extremely expensive in time and space, because complete graphs are used. We present an approach of temporal reasoning for expert systems in technical applications that reduces the amount of time and space by using sequence graphs. A sequence graph consists of one or more sequence chains and other intervals that are connected only loosely with these chains. Sequence chains are based on the observation that in technical applications many events occur sequentially. The uninterrupted execution of technical processes for a long time is characteristic for technical applications. To relate the first intervals in the application with the last ones makes no sense. In sequence graphs only these relations are stored that are needed for further propagation. In contrast to other algorithms which use incomplete graphs, no information is lost and the reduction of complexity is significant. Additionally, the representation is more transparent, because the ""flow"" of time is modelled.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-114.pdf,
114,1992,Representation and Reasoning: Temporal,Algorithms and Complexity for Reasoning about Time,"Martin Charles Golumbic, Ron Shamir","Interval consistency problems deal with events, each of which is assumed to be an interval on the real line or on any other linearly ordered set. This paper deals with problems in reasoning about such intervals when the precise topological relationships between them is unknown or only partially specified. This work unifies notions of interval algebras for temporal reasoning in artificial intelligence with those of interval orders and interval graphs in combinatorics, obtaining new algorithmic and complexity results of interest to both disciplines. Several versions of the satisfiability, minimum labeling and all consistent solutions problems for temporal (interval) data are investigated. The satisfiability question is shown to be NP-complete even when restricting the possible interval relationships to subsets of the relations intersection and precedence only. On the other hand, we give efficient algorithm for several other restrictions of the problem. Many of these problems are also important in molecular biology, archaeology, and resolving mutual-exclusion constraints in circuit design.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-115.pdf,
115,1992,Representation and Reasoning: Temporal,On the Computational Complexity of Temporal Projection and Plan Validation,"Bernhard Nebel, Christer Bäckström","One kind of temporal reasoning is temporal projection-the computation of the consequences of a set of events. This problem is related to a number of other temporal reasoning tasks such as story understanding, planning, and plan validation. We show that one particular simple case of temporal projection on partially ordered events turns out to be harder than previously conjectured. However, given the restrictions of this problem, story understanding, planning, and plan validation appear to be easy. In fact, we show that plan validation, one of the intended applications of temporal projection, is tractable for an even larger class of plans.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-116.pdf,
116,1992,Representation and Reasoning: Terminological,Computing Least Common Subsumers in Description Logics,"William W. Cohen, Alex Borgida, Haym Hirsh","Description logics are a popular formalism for knowledge representation and reasoning. This paper introduces a new operation for description logics: computing the ""least common subsumer"" of a pair of descriptions. This operation computes the largest set of commonalities between two descriptions. After arguing for the usefulness of this operation, we analyze it by relating computation of the least common subsumer to the well-understood problem of testing subsumption; a close connection is shown in the restricted case of ""structural subsumption"". We also present a method for computing the least common subsumer of ""attribute chain equalities"", and analyze the tractability of computing the least common subsumer of a set of descriptions -an important operation in inductive learning.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-117.pdf,
117,1992,Representation and Reasoning: Terminological,A Non-Well-Founded Approach to Terminological Cycles,"Robert Dionne, Eric Mays, Frank J. Oles","In this paper, we propose a new approach to intensional semantics of term subsumption languages. We introduce concept algebras, whose signatures are given by sets of primitive concepts, roles, and the operations of the language. For a given set of variables, standard results give us free algebras. We next define, for a given set of concept definitions, a term algebra, as the quotient of the free algebra by a congruence generated by the definitions. The ordering on this algebra is called descriptive subsumption (la). We also construct a universal concept algebra, as a non-well-founded set given by the greatest fixed point of a certain equation. The ordering on this algebra is called structural subsumption (?a). We prove there are unique mappings from the free algebras, to each of these, and establish that our method for classifying cycles in a term subsumption language, K-REP, consists of constructing accessible pointed graphs, representing terms in the universal concept algebra, and checking a simulation relation between terms.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-118.pdf,
118,1992,Representation and Reasoning: Terminological,An Empirical Analysis of Terminological Representation Systems,"Jochen Heinsohn, Daniel Kudenko, Bernhard Nebel, Hans-Jürgen Profitlich","The family of terminological representation systems has its roots in the representation system KL-ONE. Since the development of this system more than a dozen similar representation systems have been developed by various research groups. These systems vary along a number of dimensions. In this paper, we present the results of an empirical analysis of six such systems. Surprisingly, the systems turned out to be quite diverse leading to problems when transporting knowledge bases from one system to another. Additionally, the runtime performance between different systems and knowledge bases varied more than we expected. Finally, our empirical runtime performance results give an idea of what runtime performance to expect from such representation systems. These findings complement previously reported analytical results about the computational complexity of reasoning in such systems.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-119.pdf,
119,1992,Representation and Reasoning: Terminological,Recognition Algorithms for the Loom Classifier,"Robert M. MacGregor, David Brill","Most of today’s terminological representation systems implement hybrid reasoning architectures wherein a concept classifier is employed to reason about concept definitions, and a separate recognizer is invoked to compute instantiation relations between concepts and instances. Whereas most of the existing recognizer algorithms designed to maximally exploit the reasoning supplied by the concept classifier, LOOM has experimented with recognition strategies that place less emphasis on the classifier, and rely more on the abilities of LOOM’s backward chaining query facility. This paper presents the results of experiments that test the performance of the LOOM algorithms. These results suggest that, at least for some applications, the LOOM approach to recognition is likely to outperform the classical approach. They also indicate that for some applications, much better performance can be achieved by eliminating the recognizer entirely, in favor of a purely backward chaining architecture. We conclude that no single recognition algorithm or strategy is best for all applications, and that an architecture that offers a choice of inference modes is likely to be more useful than one that offers only a single style of reasoning.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-120.pdf,
120,1992,Representation and Reasoning: Tractability,An Improved Incremental Algorithm for Generating Prime Implicates,Johan de Kleer,"Prime implicates have become a widely used tool in AI. The prime implicates of a set of clauses can be computed by repeatedly resolving pairs of clauses, adding the resulting resolvents to the set and removing subsumed clauses. Unfortunately, this brute-force approach performs far more resolution steps than necessary. Tison provided a method to avoid many of the resolution steps and Kean and Tsiknis developed an optimized incremental version. Unfortunately, both these algorithms focus only on reducing the number of resolution steps required to compute the prime implicates. The actual running time of the algorithms depends critically on the number and expense of the subsumption checks they require. This paper describes a method based on a simplification of Kean and Tsiknis’ algorithm using an entirely different data structure to represent the data base of clauses. The new algorithm uses a form of discrimination net called tries to represent the clausal data base which produces an improvement in running time on all known examples with a dramatic improvement in running time on larger examples.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-121.pdf,
121,1992,Representation and Reasoning: Tractability,Forming Concepts for Fast Inference,"Henry Kautz, Bart Selman","Knowledge compilation speeds inference by creating tractable approximations of a knowledge base, but this advantage is lost if the approximations are too large. We show how learning concept generalizations can allow for a more compact representation of the tractable theory. We also give a general induction rule for generating such concept generalizations. Finally, we prove that unless NP E non-uniform P, not all theories have small Horn least upper-bound approximations.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-122.pdf,
122,1992,Representation and Reasoning: Tractability,The Complexity of Propositional Default Logics,Jonathan Stillman,"We characterize the complexity of several typical problems in propositional default logics. In particular, we examine the complexity of extension membership, extension existence, and extension entailment problems. We show that the extension existence problem is X; complete, even for semi-normal theories, and that the extension membership and entailment problems are X; complete and IIp2; complete respectively, even when restricted to normal default theories. These results contribute to our understanding of the computational relationship between propositional default logics and other formalisms for nonmonotonic reasoning, e.g., autoepistemic logic and McDermott and Doyle’s NML, as well as their relationship to problems outside the realm of nonmonotonic reasoning.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-123.pdf,
123,1992,Robot Navigation,A Reactive Robot System for Find and Fetch Tasks in an Outdoor Environment,"R. Peter Bonasso, H. James Antonisse, Marc G. Slack","This paper describes the results of using a reactive control software architecture for a mobile robot retrieval task in an outdoor environment. The software architecture draws from the ideas of universal plans and subsumption’s layered control, producing reaction plans that exploit low-level competences as operators. The retrieval task requires the robot to locate and navigate to a donor agent, receive an object from the donor, and return. The implementation employs the concept of navigation templates (NaTs) to construct and update an obstacle space from which navigation plans are developed and continually revised. Selective perception is employed among an infrared beacon detector which determines the bearing to the donor, a real-time stereo vision system which obtains the range, and ultrasonic sensors which monitor for obstacles en route. The perception routines achieve a robust, controlled switching among sensor modes as defined by the reaction plan of the robot. In demonstration runs in an outdoor parking lot, the robot located the donor object while avoiding obstacles and executed the retrieval task among a variety of moving and stationary objects, including moving cars, without stopping its traversal motion. The architecture was previously reported to be effective for simple navigation and pick and place tasks using ultrasonics. Thus the results reported herein indicate that the architecture will scale well to more complex tasks using a variety of sensors.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-124.pdf,
124,1992,Robot Navigation,Integrating Planning and Reacting in a Heterogeneous Asynchronous Architecture for Controlling Real-World Mobile Robots,Erann Gat,"This paper presents a heterogeneous, asynchronous architecture for controlling autonomous mobile robots which is capable of controlling a robot performing multiple tasks in real time in noisy, unpredictable environments. The architecture produces behavior which is reliable, task-directed (and taskable), and reactive to contingencies. Experiments on real and simulated real-world robots are described. The architecture smoothly integrates planning and reacting by performing these two functions asynchronously using heterogeneous architectural elements, and using the results of planning to guide the robot’s actions but not to control them directly. The architecture can thus be viewed as a concrete implementation of Agre and Chapman’s plans-as-communications theory. The central result of this work is to show that completely unmodified classical AI programming methodologies using centralized world models can be usefully incorporated into real-world embedded reactive systems.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-125.pdf,
125,1992,Robot Navigation,Landmark-Based Robot Navigation,"Anthony Lazanas, Jean-Claude Latombe","To operate in the real world robots must deal with errors in control and sensing. Achieving goals despite these errors requires complex motion planning and plan monitoring. We present a reduced version of the general problem and a complete planner that solves it in polynomial time. The basic concept underlying this planner is that of a landmark. Within the field of influence of a landmark, robot control and sensing are perfect. Outside any such field control is imperfect and sensing is null. In order to make sure that the above assumptions hold, we may have to specifically engineer the robot workspace. Thus, for the first time, workspace engineering is seen as a means to make planning problems tractable. The planner was implemented and experimental results are presented. An interesting feature of the planner is that it always returns a universal plan in the form of a collection of reaction rules. This plan can be used even when the input problem has no guaranteed solution, or when unexpected events occur during plan execution.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-126.pdf,
126,1992,Robot Navigation,Reactive Navigation through Rough Terrain: Experimental Results,"David P. Miller, Rajiv S. Desai, Erann Gat, Robert Ivlev, John Loch","This paper describes a series of experiments that were performed on the Rocky III robot. Rocky III is a small autonomous rover capable of navigating through rough outdoor terrain to a predesignated area, searching that area for soft soil, acquiring a soil sample, and depositing the sample in a container at its home base. The robot is programmed according to a reactive behavior-control paradigm using the ALFA programming language. This style of programming produces robust autonomous performance while requiring significantly less computational resources than more traditional mobile robot control systems. The code for Rocky III runs on an 8-bit processor and uses about 10k of memory.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-127.pdf,
127,1992,Scaling Up,"Learning 10,000 Chunks: What’s It Like Out There?","Bob Doorenbos, Milind Tambe, Allen Newell","This paper describes an initial exploration into large learning systems, i.e., systems that learn a large number of rules. Given the well-known utility problem in learning systems, efficiency questions are a major concern. But the questions are much broader than just efficiency, e.g., will the effectiveness of the learned rules change with scale? This investigation uses a single problem-solving and learning system, Dispatcher-Soar, to begin to get answers to these questions. Dispatcher-Soar has currently learned 10,112 new productions, on top of an initial system of 1,819 productions, so its total size is 11,931 productions. This represents one of the largest production systems in existence, and by far the largest number of rules ever learned by an AI system. This paper presents a variety of data from our experiments with Dispatcher-Soar and raises important questions for large learning systems.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-128.pdf,
128,1992,Scaling Up,Mega-Classification: Discovering Motifs in Massive Datastreams,"Nomi L. Harris, Lawrence Hunter, David J. States","We report on the development and application of an efficient unsupervised learning procedure for the classification of an unsegmented datastream, given a set of probabilistic binary similarity judgments between regions in the stream. Our method is effective on very large databases, and tolerates the presence of noise in the similarity judgements and in the extents of similar regions. We applied this method to the problem of finding the sequence-level building blocks of proteins. After verifying the effectiveuess of the clusterer by testing it on synthetic protein data with known evolutionary history, we applied the method to a large protein sequence database (a datastream of more than IO^7 elements) and found about 10,000 protein sequence classes. The motifs defined by these classes are of biological interest, and have the potential to supplement or replace the existing manual annotation of protein sequence databases.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-129.pdf,
129,1992,Scaling Up,Building Large-Scale and Corporate-Wide Case-Based Systems: Integration of the Organizational and Machine Executable Algorithms,"Hiroaki Kitano, Akihiro Shibata, Hideo Shimazu, Juichirou Kajihara, Atsumi Sato","This paper reports a case study on a large-scale and corporate-wide case-based system. Unlike most papers for the AAAI conference, which exclusively focus on algorithms and models executed on computer systems, this paper heavily involves organizational activities and structures as a part of algorithms in the system. It is our claim that successful corporate-wide deployment of the case-base system must involve organizational efforts as a part of an algorithmic loop in the system in a broad sense. We have established a corporate-wide case acquisition algorithm, which is performed by persons, and developed the SQUAD Software Quality Control Advisor system, which facilitates sharing and spreading of experiences corporate-wide. The major findings were that the key for the success is not necessary in complex and sophisticated AI theories, in fact, we use very simple algorithms, but the integration of mechanisms and algorithms executed by machines and persons involved.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-130.pdf,
130,1992,Scaling Up,Wafer Scale Integration for Massively Parallel Memory-Based Reasoning,"Hiroaki Kitano, Moritoshi Yasunaga","In this paper, we describe a design of wafer-scale integration for massively parallel memory-based reasoning (WSI-MBR). WSI-MBR attains about 2 million parallelism on a single 8 inch wafer using the state-of-the-art fabrication technologies. While WSI-MBR is specialized to memory-based reasoning, which is one of the mainstream approachs in massively parallel artificial intelligence research, the level of parallelism attained far surpasses any existing massively parallel hardware. Combination of memory array and analog weight computing circuits enable us to attain super high-density implementation with nanoseconds order inference time. Simulation results indicates that inherent robustness of the memory-based reasoning paradigm overcomes the possible precision degradation and fabrication defects in the wafer-scale integration. Also, the WSI-MBR provides a compact (desk-top size) massively parallel computing environment.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-131.pdf,
131,1992,Invited Talks,"What Your Computer Really Needs to Know, You Learned in Kindergarten",Edmund H. Durfee,"Research in distributed AI has led to computational techniques for providing AI systems with rudimentary social skills. This paper gives a brief survey of distributed AI, describing the work that strives for social skills that a person might acquire in kindergarten, and highlighting important unresolved problems facing the field.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-132.pdf,
132,1992,Invited Talks,Reasoning as Remembering: The Theory and Practice of CBR,Kristian Hammond,"Over the past few years, Case-Based Reasoning (CBR) has grown from a Yalecentric view of cognition to a solid sub-area that is supported by wide-spread academic research and industrial development. Unfortunately, the definition of what is and is not CBR remains seriously ambiguous. In this talk, I will look at different takes on CBR and suggest a definition that, oddly enough, doesn’t include the use of ""cases "". In particular I will look at:CBR as nothing new.CBR as an alternative cognitive model.CBR as an approach to knowledge engineering.CBR as a new set of assumptions.CBR as a new set of modeling goals.In the end, I argue that the CBR is part of a larger model of long-term agency. This model is distinguished by the view of agents and environments as dynamic entities that change to fit each other over time. Along with arguing for a new view of autonomous agents, this model supports the main tenant of CBR, that reasoning and learning must be linked within any intelligent system.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-133.pdf,
133,1992,Invited Talks,Artificial Intelligence and Molecular Biology,Lawrence Hunter,"The twin goals of artificial intelligence research are to understand the computational mechanisms underlying human-like thought and behavior, and to design and synthesize comparable machines. Despite their inherent generality, these goals are best served by decomposing this overwhelmingly complex task into more tractable subproblems, often addressing quite specific behaviors or cognitive abilities. These decompositions generate the collection of tasks and domains that defines AI research at a particular point in time.",https://aaai.org/Library/AAAI/1992/../../../Papers/AAAI/1992/AAAI92-134.pdf,
