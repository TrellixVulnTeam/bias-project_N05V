,conference_year,category,title,author,abstract,download_url,keywords
0,1983,Knowledge Representation and Problem Solving,An Overview of Meta-Level Architecture,Michael R. Genesereth,"One of the biggest problems in AT programming is the difficulty of specifying control. Meta-level architecture is a knowledge engineering approach to coping with this difficulty. The key feature of the architecture is a declarative control language that allows one to write partial specifications of program behavior. This flexibility facilitates incremental system dcvclopment and the integration of disparate architectures like demons, object-oriented programming, and controlled deduction. This paper presents the language, describes an appropriate, and cliscusses the issues of compiling. It illustrales the architecture with a variety of examples and reports some experience in using the architecture in building expert systems.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-001.pdf,
1,1983,Knowledge Representation and Problem Solving,Finding All of the Solutions to a Problem,David E. Smith,"This paper describes a method of cutting off reasoning when all of the answers to a problem have been found. Briefly, the method involves keeping and maintaining information about the sizes of important sets and using this information to determine when all of the answers to a problem have been found. We show how this information can be dynamically calculated and kept accurate in a changing world. Additional complexity is encouniered when this maintenance mixed with independent meta-level reasoning for pruning search spaces.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-002.pdf,
2,1983,Knowledge Representation and Problem Solving,Communication and Interaction in Multi-Agent Planning,Michael Georgeff,"A method for synthesizing multi-agent plans from simpler single-agent plans is described. The idea is to insert communication acts into the single-agent plans so that agents can synchronize activities and avoid harmful interactions. Unlike most previous planning systems, actions are represented by sequesnces of states, rather than as simple state change operators. This allows the expression of more complex kinds of interaction than would otherwise be possible. An efficient method of interaction and safety analysis is then developed and used to identify critical regions in the plans. An essential feature of the method is that the analysis is performed without generating all possible interleavings of the plans, thus avoiding a combinatorial explosion. Finally, commiunication primitives are inserted into the plans and a supervisor process created to handle synchronization.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-003.pdf,
3,1983,Knowledge Representation and Problem Solving,Data Dependencies on Inequalities,Drew McDermott,"Numerical inequalities present new challenges to data-base systems that keep track of ""dependencies,"" or reasons for beliefs. Care must be taken in interpreting an inequality as an assertion, since occasionally a ""strong"" interpretation is needed, that the inequality is best known bound on a quantity. Such inequalities often have many proofs, so that the proper response to their erasure is often to look for an alternative proof. Fortunately, abstraction techniques developed by data-dependency theorists are robust enough that they can be extended fairly easily to handle these problems. The key abstractions involved are the ""ddnode,"" an abstract assertion as seen by the data-dependency system, and its associated ""signal function,"" which performs indexing, re-deduction, and garbage-collection functions. Such signal functions must have priorities, so that they don’t clobber each other when they run.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-004.pdf,
4,1983,Knowledge Representation and Problem Solving,KRYPTON: Integrating Terminology and Assertion,"Ronald J. Brachman, Hector J. Levesque, Richard E. Fikes","The demands placed on a knowledge representation scheme by a knowl-edge-based system are generally not all met by any of today’s can-didates. Representation languages based on frames or semantic net-works have intuitive appeal for forming descriptions but tend to have severely limited assertional power, and are often fraught with am-biguous readings. Those based on first-order logic are less limited assertionally, but are restricted to primitive, unrelated terms. We have attempted to overcome these limitations in a new, hybrid knowledge representation system, called ""KRYPTON"". KRYPTON has two rep- resentation languages, a frame-based one for forming domain-specific descriptive terms and a logic-based one for making statements about the world. We here summarize the two languages, a functional inter-face to the system, and an implementation in terms of a taxonomy of frames and its interaction with a first-order theorem prover.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-005.pdf,
5,1983,Knowledge Representation and Problem Solving,The Denotational Semantics of Horn Clauses as a Production System,"J.-L. Lassez, M. Maher","We show how one of Nilsson’s tenets on rule-based production systems, when applied to Horn clause programs, leads to a denotational semantics. This formalism, in turn provides a striking illustra-tion of a second Nilsson tenet.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-006.pdf,
6,1983,Knowledge Representation and Problem Solving,Theory Resolution: Building in Nonequational Theories,Mark E. Stickel,"Theory resolution constitutes a set of complete proce-dures for building nonequational theories into a resolution theorem-proving program so that axioms of the theory need never be resolved upon. Total theory resolution uses a deci- sion procedure that is capable of determining inconsistency of any set of clauses using predicates in the theory. Partial theory resolution employs a weaker decision procedure that can determine potential inconsistency of a pair of literals. Applications include the building in of both mathematical and special decision procedures, such as for the taxonomic information furnished by a knowledge representation sys-tem.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-007.pdf,
7,1983,Knowledge Representation and Problem Solving,Improving the Expressiveness of Many Sorted Logic,Anthony G. Cohn,"Many sorted logics can allow an increase in deductive efficiency by eliminating useless branches of the search space, but are usually formulated so that their expressive power is severely limited. The many sorted logic described here is unusual in that the quantifiers are unsorted; the restriction on the range of a quantified variable derives from the argument positions of the function and predicate symbols that it occupies; associated with every non-logical symbol is a sorting function which describes how its sort varies with the sorts of its inputs; polymorphic functions and predicates are thus easily expressible and statements usually requiring several assertions may be compactly expressed by a single assertion. The sort structure may be an arbitrary lattice. Increased expressiveness is obtained by allowing the sort of a term to be a more general sort than the sort of the argument position it occupies. Furthermore, by allowing three boolean sorts (representing ""true,"" ""false"" and ""either true or false"" ), it is sometimes possible to detect that a formula is contradictory or tautologous without resort to general inference rules. Inference rules for a resolution based system are discussed; these can be proved to be both sound and complete.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-008.pdf,
8,1983,Knowledge Representation and Problem Solving,The Bayesian Basis of Common Sense Medical Diagnosis,Eugene Charniak,"In the paper, we show that the objections most frequently related against the use of Bayesian statistics within the AI-in-Medicine community do not seem to hold. In particular, we will show that the independence assumptions required to make Bayesian statistics computationally feasiable are not nearly as damaging as has been claimed. We will also argue that Bayesian statistics is perfectly compatible with heuristic solu-tions to the multiple disease problem.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-009.pdf,
9,1983,Knowledge Representation and Problem Solving,Analyzing the Roles of Descriptions and Actions in Open Systems,"Carl Hewitt, Peter de Jong","This paper analyzes relationships between the roles of descriptions and actions in large scale, open ended, geographically distributed, concurrent systems. Rather than atempt to deal with the complexities and ambiguities of currently implemented descriptive languages, we concentrate our analysis on what can be expressed in the underlying frameworks such as the lambda calculus and first order logic. By this means we conclude that descriptions and actions complement one another; neither being sufficient unto itself. This paper provides a basis to begin the analysis of the very subtle relationships that hold between descriptions and actions in Open Systems.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-010.pdf,
10,1983,Knowledge Representation and Problem Solving,Proving the Correctness of Digital Hardware Designs,Harry G. Barrow,VERIFY is a PROLOG program that attempts to prove the correct-ness of a digital design. It does so by showing that the behavior inferred from the interconnection of its parts and their behaviors is equivalent to the specified behavior. It has successfully verified large designs in-volving many thousands of transistors.,https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-011.pdf,
11,1983,Knowledge Representation and Problem Solving,A Chess Program That Chunks,"Murray Campbell, Hans Berliner","CHUNKER is a ches program that uses chunked knowledge to achieve success. Its domain is a subset of king and pawn endings in chess that has been studied for over 300 years. CHUNKER has a large library of chunk instances where chunk type has a property list and each instance has a set of values for these properties. This allows CHUNKER to reason about positions that come up in the search that would otherwise have to handled by means of additional search. Thus the program is able to sohe the most difficult problem of its present domain (a problem that would require 45 ply of search and on the order of 1Oi3 years of CPU time to be solved by the best of present day chess programs) in 18 ply and one minute of CPU time. Further, CHUNKER is undoubtedly the world' s foremost expert in its domain, and has discovercd 2 mistakes in the literature and has been instrumental in discovering a new theorem about the domain that allows the assessing of positions with a new degree of ease and confidence. In this paper we describe CHUNKER’s structure and performance, and discuss our plans for extending it to play the whole domain of king and pawn endings.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-012.pdf,
12,1983,Knowledge Representation and Problem Solving,The Decomposition of a Large Domain: Reasoning About Machines,Craig Stanfill,"The world of machines is divided into a hierarchy of seven sub-worlds, ranging from algebra to causality. Separate representations and experts are constructed for each sub-world; these experts are then integrated into an expert system. The result is Mack, a system which produces qualitative models of simple machines from purely geometric representations.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-013.pdf,
13,1983,Knowledge Representation and Problem Solving,Reasoning About State From Causation and Time in a Medical Domain,William J. Long,"The reasoning needed for diagnosis and patient management in a medical domain requires the ability to determine both the aspects of ttle patient state that are definitely known and those that are possible given what is known about the patient. This paper discusses a mechanism for including the time constraints of causal relationships in the representation and the increased discriminatory power of the reasoning mechanisms when the time relationships are used appropriately. It is further argued that in such a domain where time bounds are weak there is often more information in the relationships between times than in the time values themselves. Thus, it is often necessary to reason from the relationships rather than by comparing time values. A program utilizing the mechanisms outlined is currently under development.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-014.pdf,
14,1983,Knowledge Representation and Problem Solving,The Use of Qualitative and Quantitative Simulations,Reid G. Simmons,We describe a technique called imagining which uses a combination of qualitative and quantitative simulation techniques to solve a problem where neither alone would suffice. We illustrate the imagining technique using the domain of geologic interpretation and argue for why the two types of simulation are necessary for problems of this sort. We also discuss the strengths of each simulation technique and how they support each other in the problem solving process.,https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-015.pdf,
15,1983,Knowledge Representation and Problem Solving,An Automatic Algorithm Designer: An Initial Implementation,"Elaine Kant, Allen Newell","This paper outlines a specification for an algorithm-design system (based on previous work involving protocol analysis) and describes an implementation of the specification that is a combination frame and production system. In the implementation, design occurs in two problem spaces -- one about algorithms and one about the task-domain. The partially worked out algorithms are represented as configurations of data-flow components. A small number of general-purpose operators construct and modify the representations. These operators are adapted to different situations by instantiation and means-ends analysis rules. The data-flow space also includes symbolic and test-case execution rules that drive the component-refinement orocess by exposing both problems and opportunities. A domain space about geometric images supports test,case execution, domain-specific problem solving, recognition and discovery.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-016.pdf,
16,1983,Knowledge Representation and Problem Solving,On Inheritance Hierarchies With Exceptions,"David W. Etherington, Raymond Reiter","Using default logic, we formalize NETL-like inheritance hierar-chies with exceptions. This provides a number of benefits: (1) A precise semantics for such hierarchies. (2) A provably correct (with respect to the proof theory of default logic) inference algorithm for acyclic networks. (3) A guarantee that acyclic networks have extensions. (4) A provably correct quasi-parallel inference algorithm for such networks.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-017.pdf,
17,1983,Knowledge Representation and Problem Solving,Default Reasoning as Likelihood Reasoning,Elaine Rich,"Several attempts to define formal logics for some type of default reasoning have been made. All of these logics share the property that in any given state, a proposition p is either held to be true, it is held to be false, or no belief about it is held. But, if we ask what default reasoning really is, we see that it is form of likelihood reasoning. The goal of this paper is to show that if default reasoning is treated as likelihood reasoning (similar to that of Mycin), then natural solutions emerge for several of the problems that are encountered when default reasoning is used. This is shown by presenting 7 such problems and showing how they are solved.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-018.pdf,
18,1983,Knowledge Representation and Problem Solving,Default Reasoning Using Monotonic Logic: A Modest Proposal,Jane Terry Nutter,"This paper presents a simple extension of first order predicate logic to include a default operator. Rules of inference governing the operator are specified, and a model theory for interpreting sentences involving default operstor is developed based on standard Tarslian semantics. The Resulting system is trivially sound. It is argued that (a) this logic provides an adequate basis for default reasoning in A.I. systems, and (b) unlike most logics proposed for this purpose, it retains the virtues of standard first order logic, including both montonicity and simplicity.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-019.pdf,
19,1983,Knowledge Representation and Problem Solving,A Theorem-Prover for a Decidable Subset of Default Logic,"Philippe Besnard, Rene Quiniou, Patrice Quinton","Non-monotonic logic is an attempt to take into account such notions as incomplete knowledge and theory evolution. However the decidable theorem-prover issue has been so far unexplored. We propose such a theorem-prover for default logic with a restriction on the first-order formulae it deals with. This theorem-prover is based on the generalisation of a resolution technique named saturation, which was initially designed to test the consistency of a set of first-order formulae. We have proved that our algorithm is complete and that it always terminates for the selected subset of first-order formulae.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-020.pdf,
20,1983,Knowledge Representation and Problem Solving,Derivational Analogy and Its Role in Problem Solving,Jaime G. Carbonell,"Derivational analogy, a method of solving problems based upon the transfer of past experience to new problem situations, is discussed in the context of other general approaches to problem solving. The experience transfer process consists of recreating lines of reasoning, including decision sequences and accompanying justifications, that proved effective in solving particular problems requiring similar initial analysis. The derivational analogy approach is advocated as a means of implementing reasoning from individual cases in expert systems.'",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-021.pdf,
21,1983,Cognitive Modeling,Three Dimensions of Design Development,Neil M. Goldman,"Formal specifications are difficult to understand for a number of reasons. When the developer of a large specification explains it to another person, he typically includes informatlon in his explanation that is is not present, even implicitly. in the specification itself. One useful form of information presents the specification in terms of an evolution from simpler specificattons. TypIcally a specification was actually produced by a series of evolutionary steps reflected in the explanation. This paper suggests three dimensions of evolution that can be used to structure specification developments: structural granularity, temporal granularity. and coverage. Their use in a particular example is demonstrated.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-022.pdf,
22,1983,Cognitive Modeling,Six Problems for Story Understanders,Peter Norvig,"Story understanding programs have been classified as script-based processors, goal-based pro-cessors or multi-level processors. Each program introduces a new knowledge structure and invents a mechanism to make inferences and manage memory for that knowledge structure. This can lead to a prol-iferation of incomplete, incompatible processing mechanisms. The alternative presented here is to concentrate on the processing mechanism. It is sug-gested that a single inferencing scheme can deal with all knowledge structures in a uniform manner. Six basic problems that such a processor must address are presented and discussed.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-023.pdf,
23,1983,Cognitive Modeling,Planning and Goal Interaction: The Use of Past Solutions in Present Situations,Kristian J. Hammond,"This paper presents WOK, a cased-based planner that makes use of memory structures based on goal interactions. WOK generates original plans, (which take the form of recipes), in the domain of Szechuan cooking, by modifying existing plans that are stored and then retrieved on the basis of the goal interactions that they deal with. The paper suggests an organization and indexing strategy that allows the retrieval and use of plans that overarch sets of goals rather than just individual goal situations. It also demonstrates how episodic knowledge can be used to guide planning and avoid past failures.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-024.pdf,
24,1983,Cognitive Modeling,A Model of Learning by Incremental Analogical Reasoning and Debugging,Mark H. Burstein,"This paper presents a model of analogical reasoning for learning. The model is based on two main ideas. First, that reasoning from an analogy presented by a teacher while explaining an unfamiliar concept is often determined by the causal abstractions known by the student to apply in the familiar domain referred to. Secondly, that such analogies, once introduced, are extended incrementally, in attempts to account for new situations by recalling additional situations from the same base domain. Protocols suggest that this latter process is quite useful but extremely error-prone. CARL, a computer program that learns semantic representations for assignment statements of the BASIC programming language is described as an illustration this kind of analogical reasoning. The model maps and debugs inferences drawn from several commonly used analogies to assignment, in response to presented examples.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-025.pdf,
25,1983,Cognitive Modeling,Modeling Human Knowledge of Routes: Partial Knowledge and Individual Variation,Benjamin Kuipers,"Commonsense knowledge of large-scale space (the cognitive map) includes several different types of knowledge: of sensorimotor, topological, and metrical spatial relationships. Sensorimotor knowledge is defined as that knowledge which is necessary to reconstruct a route from memory after travel along that route in a large-scale environment. A representation for route knowledge is proposed with sufficiently robust performance properties to be useful as commonsense knowledge. Its states of partial knowledge are shown to correspond to those observed in humans. We also define and explore the space of all possible variants of this representation, to derive empirical predictions about the nature of individual variation.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-026.pdf,
26,1983,Cognitive Modeling,STRATEGIST: A Program That Models Strategy-Driven and Content-Driven Inference Behavior,"Richard H. Granger, Kurt P. Eiselt, Jennifer K. Holbrook","In the course of understanding a text, different readers use different inference strategies to guide their choice of interpretations of the events in the text. This is in contrast to previous computer models of understanding, which all use the same (single) strategy while concentrating on details of content-driven inference. The separate strategies are theorized to be composed of the same component inference process, but of different rules for application of the processes. The use of different strategies occasionally results in different intrepretations of a single text. This paper presents both the results in different interpretations of a single text. This paper presents both the results of new experimental data and a working computer program, called STRATEGIST, that models both strategy-driven and content-driven inference behavior. The rules which make up two of these strategies are presented.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-027.pdf,
27,1983,Cognitive Modeling,Learning Operator Semantics by Analogy,"Sarah A. Douglas, Thomas P. Moran",This paper proposes a cognitive model for human procedural skill acquisition based on problem solving in problem spaces and the use of analogy for building the representation of operator semantics. Protocol data of computer-naive subjects learning the EMACS text editor suggests that they use their knowledge of typewriting to decide which commands to use in performing editing tasks. We propose a formal method of analysis that compares operators in two problem spaces (based on posrcondirion similarity) and generates misconceptions (based on pre- and postcondition differences). Comparing these predicted misconceptions with error data and verbal comments in problem solving episodes validates this analysis.,https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-028.pdf,
28,1983,Cognitive Modeling,An Analysis of a Welfare Eligibility Determination Interview: A Planning Approach,Eswaran Subrahmanian,"The purpose of this research is to identify strategies and plans used by welfare caseworkers in order to build an expert system to determine welfare eligibility. Here we use the framework of proposed by Hobbs and Evans(1979) to analyze the conversational plans of a welfare caseworker while conducting an eligibility interview. We study the plans used by the caseworker, show the interaction between goals and themes, and study the influence of constraints imposed on the interview by the statutory Welfare Eligibility Rules. We identify some of the pre-structured plans in this constrained conversational domain in our effort to define the range of choices available in a welfare eligibility interview.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-029.pdf,
29,1983,Vision and Robotics,A Variational Approach to Edge Detection,John Canny,"The problem of detecting intensity changes in images is canonical in vision. Edge detection operators are typically designed to optimally estimate first or second derivative over some (usually small) support. Other criteria such as output signal to noise ratio or bandwidth have also been argued for. This paper describes an attempt to formulate set of edge detection criteria that capture as directly as possible the desirable properties of the detector. Variational techniques are used to find 2 solution over the space of all possible functions. The first criterion is that the detector have low probability of error i.e. failing to mark edges or falsely marking non-edges. The second is that the marked points should be as close as possible to the centre of the true edge. The third criterion is that there should be low probability of more than one response to a single edge. The third criterion is claimed to be new, and it became necessary when an operator designed using the first two criteria was found to have excessive multiple responses. The edge model that will be considered here is 2 one-dlmensional step edge in white Gaussian noise although the same technique has been applied to an extended impulse or ridge profile. The result is a one dimensional operator that approximates the first derivative of a Gaussian. Its extension to two dimensions is also discussed.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-030.pdf,
30,1983,Vision and Robotics,Surface Constraints From Linear Extents,John R. Kender,"This paper demonstrates how image features of linear extent (lengths and spacings) generate nearly image-independent constraints on underlying surface orientations. General constraints are derived from the shape-from-texture paradigm; then, certain special cases are shown to be especially useful. Under orthography, the assumption that two extents are equal is shown to be identical to the assumption that an image angle is a right angle (i.e. orthographic extent is a form of slope or skewed symmetry). Under perspective, if image extents are assumed equal and parallel, extent again degenerates into slope. In the gereral perspective case, the shape constraints are usually complex fourth-order equations, but they often simplify--even to graphic constructions in the image space itself. If image extents are colinear and assumed equal, the constraint equations reduce to second order with several graphic analogs. If extents are adjacent as well, the equations are first order and the derived construction (the ""jack-knife method"") is particularly straightfoward and general. This metheod works not only on measures of extent per texel, but also on reciprocal measures: texels per extent. Several examples and discussion indicate that the methods are robust, deriving surface information cheaply, without search, where other methods must fail.*",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-031.pdf,
31,1983,Vision and Robotics,An Iterative Method for Reconstructing Convex Polyhedra from External Guassian Images,James J. Little,"In computing a scene description from an image, a useful intermediate representation of a scene object is given by the orientation and area of the constituent surface facets, termed the Extended Gaussian Image (EGI) of the object. The EGI of a convex object uniquely represents that object. We are con- cerned with the computational task of reconstructing the shape of scene objects from their Extended Gaussian Images, where the objects are restricted to convex polyhedra. We present an iterative method for reconstructing convex polyhe- dra from their Extended Gaussian Images.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-032.pdf,
32,1983,Vision and Robotics,Two Results Concerning Ambiguity in Shape From Shading,Michael J. Brooks,"Two shape from shading problems are con-sidered, one involving an image of a plane and the other an image of a hemisphere. The former is shown to be ambiguous because it can be generated by an infinite number of ruled surfaces. The latter, in contrast, is shown to have only the hemisphere and its reversal as solutions, although some subregions of the image are shown to be infinitely ambiguous.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-033.pdf,
33,1983,Vision and Robotics,Perceptual Organization as a Basis for Visual Recognition,"David G. Lowe, Thomas O. Binford","Evidence is presented showing that bottom-up grouping of image features is usually prerequisite to the recognition and in interpretation of images. We describe three functions of these groupings: 1) segmentation, 2) three-dimensional in-terpretation, and 3) stable descriptions for accessing object models. Several principles are hypothesized for determin-ing which image relations should be formed: relations are significant to the extent that they are unlikely to have arisen by accident from the surrounding distribution of features, relations can only be formed where there are few alterna-tives within the same proximity, and relations must be based on properties which are invariant over a range of imaging conditions. Using these principles we develop an algorithm for curve segmentation which detects significant structure at multiple resolutions, including the linking of segments on the basis of curvilinearity. The algorithm is able to detect structures vhich no single-resolution algorithm could detect. Its performance is demonstrated on synthetic and natural image data.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-034.pdf,
34,1983,Vision and Robotics,Model-Based Interpretation of Range Imagery,"Darwin T. Kuan, Robert J. Drazovich","This paper describes a model-based approach to interpreting laser range imagery. It discusses the object modeling, model-driven prediction, and feature-to-model matching aspects of the problem. The model objects are represented by a viewpoint- independent volumetric model based on generalized cylinders. Predictions of 3-D image features and their relations are generated from object models on multiple levels. These predictions give guidance for goal-directed shape extraction from low level image features. Interpretation proceeds by compar- ing the extracted image features with object models in a coarse to fine hierarchy. Since the 3-D information is available from the range image, the actual measurements are used for feature-to-model ma tch ing . A limited prototype system has been developed, preliminary results on prediction and interpretation are shown, and future research directions are discussed.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-035.pdf,
35,1983,Vision and Robotics,A Design Method for Relaxation Labeling Applications,Robert A. Hummel,"A summary of mathematical results developing theory of consistency in ambiguous labelings is presented. This theory allows the relaxation labeling algorithm, introduced in [Rosenfeld, Hummel, Zucker, 19761, to be interpreted as a method for finding consistent labelings, and allows specific applications to be tailored in accordance with intended design goals. We discuss, with a couple of examples, a design methodology for using this theory for practical applications.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-036.pdf,
36,1983,Vision and Robotics,Appropriate Lengths Between Phalanges of Multijointed Fingers for Stable Grasping,"Tokuji Okada, Takeo Kanade","An appropriate arrangement of finger joints is very important in designing multijointed fingers since the stability of grasping an object greatly depends on that arrangement. Multijointed fingers can grasp an abject with many points of contact each of which is pressed against the object as if wrapping up that object. The amount of the wrapped up area and the form of the finger when an objected is grasped are and therefore important factors for determining the stability of the grasping. We propose the wrap-up rate to be used for the evaluation of the stability of grasping by using these factors. We consider twenty eight models for the finger having three joints, and perform a simulation of their ability to grasp various shapes stably. Based on the simulation results, an appropriate arrangement of lengths between phalanges for a multijointed finger is presented.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-037.pdf,
37,1983,Vision and Robotics,Find-Path for a PUMA-Class Robot,Rodney A. Brooks,"Collision free motions for a manipulator with revolute joints (e.g. a PUMA) are planned through an obstacle littered workspace by first describing free space in two ways: as freeways for the hand and payload ensemble and as freeways for the up-perarm. Freeways match volumes swept out by manipulator motions and can be ""inverted"" to find a class of topologically equivalent path segments. The two freeway spaces are searched concurrently under protection of constraints determined by motion of the forearm.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-038.pdf,
38,1983,Vision and Robotics,Rule Based Strategies for Image Interpretation,"T. E. Weymouth, J. S. Griffith, A. R. Hanson, E. M. Riseman","We present an interpretation system which utilizes world knowledge in the form of simple object hypothesis rules, and more complex interpretation strategies attached to object and scene schemata, to reduce the ambiguities in image measurements. These rules involve sets of partially redundant features each of which defines an area of feature space which represents a ""vote"" for an object. Convergent evidence from multiple interpretation strategies is organized by top-down control mechanisms in the context of a partial interpretation. One such strategy extends a kernel interpretation derived through the selection of object exemplars, which represent the most reliable image specific hypotheses of a general object class, resulting in the extension of partial interpretations from islands of reliability.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-039.pdf,
39,1983,Natural Language,Recursion in TEXT and Its Use in Language Generation,Kathleen R. McKeown,"In this paper, I show how textual structure is recursive in nature; that is, the same rhetorical strategies that are available for constructing the text’s macro-structure are available for constructing its sub-sequences as well, resulting in a hierarchically structured text. The recursive formalism presented can be used by a generation system to vary the amount of detail it presents for the same discourse goal in different situations.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-040.pdf,
40,1983,Natural Language,Repairing Miscommunication: Relaxation in Reference,Bradley A. Goodman,"In natural language interactions, a speaker and a listener cannot be assured to have the same beliefs, contexts, backgrounds or goals. This leads to difficulties and mistakes when a listener tries to interpret a speaker’s utterance. One principal source of trouble is the description constructed by the speaker to refer to an actual object in the world. The description can be imprecise, confused, ambiguous or overly specific; it might be interpreted under the wrong context. This paper explores the problem of resolving such reference failures in the context of the task of assembling a toy water pump. We are using actual protocols to drive the design of a program that plays the part of an apprentice who must interpret the instructions of an expert and carry them out. A primary means for the apprentice to repair such descriptions is by relaxing parts of the description.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-041.pdf,
41,1983,Natural Language,Tracking User Goals in an Information-Seeking Environment,Sandra Carberry,"This paper presents a model for hypothesizing and tracking the changing task-level goals of a speaker during the course of an information-seeking dialogue. It allows a complex set of domain-dependent plans, forming a hierarchial structure of component goals and actions. Our model builds the user’s plan as the dialogue progresses, maintains both a local and a global plan context, and differentiates between past goals and goals currently pursued by the user. This research is part of a project to develop a robust natural language interface. If an utter-ance cannot be interpreted normally or a response cannot be generated due to pragmatic overshoot, the strong expectations about the utterance pro- vided by our context model can be used as an aid in processing the input and producing useful responses.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-042.pdf,
42,1983,Natural Language,Reasons for Beliefs in Understanding: Applications of Non-Monotonic Dependencies to Story Processing,Paul O'Rorke,"Many of the inferences and decisions which contribute to understanding involve fallible assumptions. When these assumptions are under-mined, computational models of comprehension should respond rationally. This paper crossbreeds AI research on problem solving and understanding to produce a hybrid model (""reasoned understand-ing""). In particular, the paper shows how non-monotonic dependencies [Doyle79] enable a schema-based story processor to adjust to new information requiring the retraction of assumptions.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-043.pdf,
43,1983,Natural Language,RESEARCHER: An Overview,Michael Lebowitz,"Described in this paper is a computer system, RESEARCHER, being developed at Columbia that reads natural language text in the form of patent abstracts and creates a permanent long-term memory based on concepts generalized from these texts, forming an intelligent information system. This paper is intended to give an overview of RESEARCHER. We will describe briefly the four main areas dealt with in the design of RESEARCHER: 1) knowledge representation where a canonical scheme for representing physical objects has been developed, 2) memory-based text processing, 3) and generalization and generalization-based memory organization that treats concept formation as an integral part of understanding, and 4) generalization-based question and answering.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-044.pdf,
44,1983,Natural Language,Phonotactic and Lexical Constraints in Speech Recognition,"Daniel P. Huttenlocher, Victor W. Zue","We demonstrate a method for partitioning a large lexicon into small equivalence classes, based on sequential phonetic and prosodic constraints. The representation is attractive for speech recognition systems because it allows all but a small number word candidates to be excluded, using only gross phonetic and prosodic information. The approach is a robust one in that the representation is relatively insensitive to phonetic variability and recognition error.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-045.pdf,
45,1983,Natural Language,Deterministic and Bottom-Up Parsing in Prolog,"Edward P. Stabler, Jr.","It is well known that top-down backtracking context free parsers are easy to write in Prolog, and that these parsers can be extended to give them the power of ATN’s. This report shows that a number of other familiar parser designs can be very naturally implemented in Prolog. The top-down parsers can easily be constrained to do deterministic parsing of LL(k) languages. Bottom-up backtrack parsers can also be elegantly implemented and similarly constrained to do deterministic LR(k) parsing. Very natural extensions of these LR(k) parser designs suffice for deterministic parsing of natural languages of the sort carried out by the Marcus(1980) parser.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-046.pdf,
46,1983,Natural Language,"MCHART: A Flexible, Modular Chart Parsing System",Henry Thompson,"One of the most attractive properties of the active chart parsing methodology (Kay 1980; Thompson and Ritchie 1983) is the distinction it makes possible between essential bookkeeping mechanisms, scheduling issues, and details of grammatical formalisms. MCHART is a framework within which active chart parsing systems can be constructed. It provides the essential book- keeping mechanisms, and carefully structured interfaces for the specification of scheduling and grammatical formalism. The resulting flexibility makes it useful both for pedagogical purposes and for quick prototyping. The system is available in UCILISP, FranzLisp, and Interlisp versions, together with a simple lexicon facility, example parsers and detailed documentation.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-047.pdf,
47,1983,Natural Language,Inference-Driven Semantic Analysis,Martha Stone Palmer,"A primary problem in the area of natural language processing is the problem of semantic analysis, This involves both formalizing the general and domain-dependent semantic information relevant to the task involved, and developing a uniform method for access to that information. Natural language interfaces are gen-erally also required to have access to the syntactic analysis of a sentence as well as knowledge of the prior discourse to produce a semantic representation ade-quate for the task. This paper briefly describes previous approaches to semantic analysis, specifically those approaches which can be described as using templates, and corresponding multiple levels of representation. It then presents an alternative to the template approach, inference-driven semantic analysis, which can perform the same tasks but without needing as many levels of representation.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-048.pdf,
48,1983,Natural Language,Mapping Between Semantic Representations Using Horn Clauses,Ralph M. Weischedel,"Even after an unambiguous semantic interpretation has been computed for a sentence in context, there are at least three reasons that a system may map the semantic representation R into another form S. 1. The terms of R, while reflecting the user view, may require deeper understanding, e.g. may require a version S where metaphors have been analyzed. 2. Transformations of R may be more appropriate for the underlying application system, e.g. S may be a more nearly optimal form. These transformations may not be linguisticly motivated. 3. Some transformations structural context. depend on non- Design considerations may favor factoring the process into two stages, for reasons of understandability or for easier transportability of the components. This paper describes the use of Horn clauses for the three classes of transformations listed above. The transformations are part of a system that converts the English description of a software module into a formal specification, i.e. an abstract data type.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-049.pdf,
49,1983,Natural Language,Constraining a Deterministic Parser,"J. Bachenko, D. Hindle, E. Fitzpatrick","At the Naval Research Laboratory, we are build-ing a deterministic parser, based on principles pro- posed by Marcus, that can be used in interpreting military message narrative, A central goal of our project is to make the parser useful for real-time applications by constraining the parser’s actions and so enhancing its efficiency. In this paper, we propose that a parser can determine the correct structures for English without looking past the ""left corner"" of a constituent, i.e. the leftmost element of the constituent along with its lexical category (e.g. N, V, Adj). We show that this Left Corner Con-straint, which has been built into our parser, leads quite naturally to a description of verb comple-ments in English that is consistent with the tidings of recent linguistic theory, in particular, Chomsky' s government and binding (GB) framework.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-050.pdf,
50,1983,Natural Language,QE-III: A Formal Approach to Natural Language Querying,James Clifford,"In this Paper we present an overview of QE-III, a language designed for natural-language querying of historical databases. QE-III is defined formally with a Montague Grammar, extended to provide an interpretation for questions and temporal reference. Moreover, in addition to the traditional syntactic formal pragmatic interpretation for-the semantic components, a sentences of QE-III is also defined.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-051.pdf,
51,1983,Natural Language,An Overview of the Penman Text Generation System,William C. Mann,"The problem of programming computers to produce natural language explanations and other texts on demand is an active research area in artificial intelligence. In the past, research systems designed for this purpose have been limited by the weakness of their linguistic bases, especially their grammars, and their techniques often cannot be transferred to new knowledge domains.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-052.pdf,
52,1983,Natural Language,Interactive Script Instantiation,Michael J. Pazzani,"The KNOBS [ENGELMAN 80] planning system is an experimental expert system which assists a user by instantiating a stereotypical solution to his problem. SNUKA, the natural language understanding component of KNOBS, can engage in a dialog with the user to allow him to enter components of a plan or to ask questions about the contents of a database which describes the planning world. User input is processed with respect to several knowledge sources including word definitions, scripts which describe the relationships among the scenes of the problem solution, and four production system rule bases which determine the proper database access for answering questions, infer missing meaning elements, describe how to conduct a conversation, and monitor the topic of the conversation. SNUKA differs from GUS [BOBROW 77], a dialog system similar to SNUKA in its goals, in its use of a script to guide the conversation, interpret indirect answers to questions, determine the referents of nominals, perform inferences to answer the user’s questions, and decide upon the order of asking questions of the user to maintain a coherent conversation. SNUKA differs from other script-based language understanders such as SAM [CULLINGFORD 78] and FRUMP [DEJONG 79] in its role as a conversational participant instead of a story understander.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-053.pdf,
53,1983,Learning,Episodic Learning,"Dennis Kibler, Bruce Porter",A system is described which learns to compose sequences of operators into episodes for problem solving. The system incrementally learns when and why operators are applied. Episodes are segmented so that they are generalizable and reusable. The idea of augmenting the instance language with higher level concepts is introduced. The technique of perturbation is described for discovering the essential features for a rule with minimal teacher guidance. The approach is applied to the domain of solving simultaneous linear equations.,https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-054.pdf,
54,1983,Learning,"Human Procedural Skill Acquisition: Theory, Model and Psychological Validation",Kurt VanLehn,"It is widely held that ordinary natural language conversations are governed by tacit conventions. called felicity conditions or conversational postulates (Austin. 1962: Grice. 1975: Gordon and Lakoff, 1975). Learning a procedural skill is also a communication act. The teacher communicates a procedure to the student over the course of several lessons. The central idea of the theory to be presented is that there are specific felicity conditions that govern learning. In particular, five newly discovered felicity conditions govern the kind of skill acquisition studied here. The theory has been embedded in a learning model. a large Al-based computer program. The model' s performance has been compared to data from several thousand students learning ordinary mathematical procedures: subtracting multidigit numbers, adding fractions, and solving simple algebraic equations. A key criterion for the theory is that the set of procedures that the model learns should exactly match the set of procedures that students actually acquire, including their ""buggy"" procedures. However, much more is needed for psychological validation of this theory, or any complex Al-based theory, than merely testing its predictions. The method used with this theory is presented.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-055.pdf,
55,1983,Learning,A Production System for Learning Plans From an Expert,D. Paul Benjamin,"This paper described a method for constructing expert systems in which control information is automatically built from the actions of an expert trainer. This control information consists of sequencing and goal information consists of sequencing and goal information which is extracted from the trainer by a 'planning expert', and generalized by a 'gereralization expert'. A set of extensions to the OPS5 system is dedcribed which facilitates the implementation of this approach within OPS5. These extensions permit the use of meta productions to affect the conflict resolution procedure; the control information is expressed as a set of such meta productions. Prelininary experiments with the system are described.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-056.pdf,
56,1983,Learning,Operator Decomposability: A New Type of Problem Structure,Richard E. Korf,"This paper describes a structural property of problems that allows an efficient strategy for solving a large number of problem instances to be based on a small amount of knowledge. Specifically, the property of operator decomposability is shown to be a sufficient condition for the effective application of the Macro Problem Solver, a method that represents its knowledge of a problem by a small number of operator sequences. Roughly, operator decomposability exists in a problem to the extent that the effect of an operator on each component of a state can be expressed as a function of only a subset of the components, independent of the remaining state components.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-057.pdf,
57,1983,Learning,Schema Selection and Stochastic Inference in Modular Environments,Paul Smolensky,"Given a set of stimuli presenting views of some environment, how can one characterize the natural modules or ""objects"" that compose the environment? Should a given set of items be encoded as a collection of instances or as a set of rules? Res-tricted formulations of these questions are addressed by analysis within a new mathematical framework that describes stochastic parallel computation. An algorithm is given for simulating this computation once schemas encoding the modules of the environ-ment have been selected. The concept of computational tempera-ture is introduced. As this temperature is lowered, the system appears to display a dramatic tendency to interpret input, even if the evidence for any particular interpretation is very weak.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-058.pdf,
58,1983,Learning,Why AM and Eurisko Appear to Work,"Douglas B. Lenat, John Seely Brown","Seven years ago, the AM program was constructed as an experiment in learning by discovery. Its source of power was a large body of heuristics, rules which guided it toward fruitful topics of investigation, toward profitable experiments to perform, toward plausible hypotheses and definitions. Other heuristics evaluated those discoveries for utility and ""interestingness"", and they were added to AM’s vocabulary of concepts. AM’s ultimate limitation apparently was due to its Inability to discover new, powerful, domain-specific heuristics for the various new fields it uncovered. At that time, it seemed straight-forward to simply add Heuretics (the study of heuristics) as one more field in which to let AM explore, observe, define, and develop. That task -- learning new heuristics by discovery -- turned out to be much more difficult than was realized initially, and we have just now achieved some successes at it. Along the way, it became clearer why AM had succeeded in the first place, and why it was so difficult to use the same paradigm to discover new heuristics. This paper discusses those recent insights. They spawn questions about ""where the meaning really resides"" in the concepts discovered by A.M. This leads to an appreciation of the crucial and unique role of representation in theory formation, a role involving the relationship between Form and Content.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-059.pdf,
59,1983,Learning,"Learning Physical Descriptions From Functional Definitions, Examples, and Precedents","Patrick H. Winston, Boris Katz, Thomas O. Binford, Michael Lowry","It is too hard to tell vision systems what things look like. It is easier to talk about purpose and what things are for. Consequently, we want vision systems to use functional descriptions to identify things, when necessary, and we want them to learn physical descriptions for themselves, when possible, This paper describes a theory that explains how to make such systems work. The theory is a synthesis of two sets of ideas: ideas about learning from precedents and exercises developed at MIT and ideas about physical description developed at Stanford. The strength of the synthesis is illustrated by way of representative experiments. All of these experiments have been performed with an implemented system.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-060.pdf,
60,1983,Learning,A Problem-Solver for Making Advice Operational,Jack Mostow,"One problem with taking advice arises when the advice is expressed in terms of data or actions unavailable to the advice- taker. For example. in the card game Hearts, the advice ""don’t lead a suit in which some opponent has no cards left"" is non- operational because players cannot see their opponents’ cards. Operationalization is the process of converting such advice into an executable (perhaps heuristic) procedure. This paper describes an interactive system, called BAR. that operationalizes advice by applying a series of program transformations. By applying different transformation sequences, BAR can operationalize the same advice in very different ways. BAR uses means-ends analysis and planning in an abstraction space. Rather than using a hand-coded difference table, BAR analyzes the transformations to identify transformation sequences that might help solve a given problem. Thus new transformations can be added without modifying the problem-solver itself. Although user intervention is required to select among alternative plans, BAR reduces the number of alternatives by 10 3 compared to an earlier operationalizer.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-061.pdf,
61,1983,Learning,Generating Hypotheses to Explain Prediction Failures,Steven Salzberg,"Learning from prediction failures is one of the most important types of human learning from experience. In particular, prediction failures provide a constant source of learning. When people expect some event to take place in a certain way and it does not, they generate an explanation of why the unexpected event occurred [Sussman 1975) [Schank 1982]. This explanation requires hypotheses based on the features of the objects and on causal relations between the events in the domain. In some domains, causal knowledge plays a large role; in some, experience determines behavior almost, entirely. This research describes learning in intermediate domains, where causal knowledge is used in conjunction with experience to build new hypotheses and guide behavior. In many cases, causal knowledge of the domain is essential in order to create a correct explanation of a failure. The HANDICAPPER program uses domain knowledge to aid it in building hypotheses about why thoroughbred horses win races. As the program processes more races, it builds and modifies its rules, winning horses. and steadily improves in its ability to pick winning horses.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-062.pdf,
62,1983,Learning,Learning by Re-Expressing Concepts for Efficient Recognition,Richard M. Keller,"Much attention in the field of machine learning has been directed at the problem of inferring concept descriptions from examples. But in many learning situations, we are initially presented with a fully-formed concept description, and our goal IS instead to re-express that description with some particular task in mind. In this paper, we specifically consider the task of recognizing concept instances efficiently. We describe how concepts that are accurate, though computationally inefficient for use in recognizing instances, can be re-expressed in an efficient form through a process we call concept operationalization Various techniques for concept operationalization are illustrated in the context of the LEX learning system.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-063.pdf,
63,1983,Learning,Learning: The Construction of A Posteriori Knowledge Structures,Paul D. Scott,This paper is a critical examination of both the nature of learning and its value in artificial intelligence. After examining alternative definitions it is concluded that learning is in fact any process for the acquisition of synthetic a posteriori knowledge structures. The suggestion that learning will not prove useful in machines is examined and it is argued. that its main application in practical Al systems terns is in providing a means by which a system can acquire knowledge which is not readily formalizable. Finally some of the implications of these conclusions for future Al research are explored.,https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-064.pdf,
64,1983,Learning,"A Doubly Layered, Genetic Penetrance Learning System",Larry A. Rendell,"The author’s original state-space learning system (based on a probabilistic performance measure clustered in feature space) was effective in optimizing parameterized linear evaluation functions. However, more accurate probability estimates would allow stabilization in cases of strong feature interactions. To attain this accuracy and stability, a second level of learning is added, a genetic (parallel) algorithm which supervises multiple activations of the original system. This scheme is aided by the probability clusters themselves. These structures are intermediate between the detailed performance statistics and the more general heuristic, and they estimate an absolute quantity independently of one another. Consequently the system allows both credit localization at this mediating level of knowledge and feature interaction at the derived heuristic level. Early experimental results have been encouraging. As predicted by the analysis, stability is very good.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-065.pdf,
65,1983,Learning,An Analysis of Genetic-Based Pattern Tracking and Cognitive-Based Component Tracking Models of Adaptation,"Elaine Pettit, Kathleen M. Swigger",The objective of this study was a comparison of the effectiveness in adapting to an environment of populations of structures undergoing modifica-tion by four different models: 1) Holland’s (2) genetic operator model; 2) a cognitive (statistical predictive) model; 3) a random point mutation model; and 4) a control (non-altered) model.,https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-066.pdf,
66,1983,Expert Systems,The Design of a Legal Analysis Program,Anne v.d.L. Gardner,"The analysis of legal problems is a relatively new domain for AI. This paper outlines a model of legal reasoning, giving special attention to the unique characteristics of the domain, and describes a program based on the model. Major features include (1) distinguishing between questions the program has enough information to resolve and questions that competent lawyers could argue either way; (2) using incompletely defined (""open-textured"") technical concepts; (3) combining the use of knowledge expressed as rules and knowledge expressed as examples; and (4) combining the use of professional knowledge and commonsense knowledge. All these features may prove important in other domains besides law, but previous AI research has left them largely unexplored.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-067.pdf,
67,1983,Expert Systems,The Advantages of Abstract Control Knowledge in Expert System Design,William J. Clancey,"A poorly designed knowledge base can be as cryptic as an arbitrary program and just as difficult to maintain. Representing control knowledge abstractly, separately from domain facts and relations, makes the design more transparent and explainable. A body of abstract control knowledge provides a generic framework for constructing knowledge bases for related problems in other domains and also provides a useful starting point for studying the nature of strategies.*",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-068.pdf,
68,1983,Expert Systems,The GIST Behavior Explainer,Bill Swartout,"One difficulty in understanding formal specifications is that there are often interactions between pieces of the specification, never explicitly stated, that only become apparent when the specification is analyzed or simulated. Symbolic evaluation has been proposed as a way of making such interactions apparent, but symbolic evaluators often produce enormous execution traces that are tedious and difficult to examine. This paper presents an automated system that employs a number of heuristics to select the most interesting aspects of the trace for presentation. The system uses this information to construct an English description of the trace. Due to the need for summarization and proof reformulation, the direct-translation approach, which worked well in describing specifications statically, is not suitable in this case. This paper describes the system and gives an example of its output.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-069.pdf,
69,1983,Expert Systems,A Comparative Study of Control Strategies for Expert Systems: Age Implementation of Three Variations of PUFF,Nelleke Aiello,"This paper presents the results of comparing three control strategies for expert systems: event driven, expectation driven, and goal driven. Three different versions of a pulmonary function analysis system (PUFF) were implemented, each with a different control strategy. The systems are described and compared for efficiency and naturalness. The advantages and disadvantages of each strategy are discussed. The reasons why one approach, the expectation-drive strategy, is best suited for the PUFF application are summarized.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-070.pdf,
70,1983,Expert Systems,A Rule-Based Approach to Information Retrieval: Some Results and Comments,"Richard M. Tong, Daniel G. Shapiro, Brian P. McCune, Jeffrey S. Dean","This paper is a report of our early efforts to use a rule-based approach in the information retrieval task. We have developed a prototype system that allows the user to specify his or her retrieval concept as a hierarchy of sub-concepts which are then implemented as a set of production rules. The paper contains a brief description of the system and some of the preliminary testing we have done. In particular, we make some observations on the need for an appropriate language for expressing conceptual queries, and on the interactions between rule formulation and uncertainty representation.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-071.pdf,
71,1983,Expert Systems,Expert System Consultation Control Strategy,"James Slagle, Michael Gaynor","User interfaces to expert systems represent a bottleneck since consultation time is proportional to the amount of information the system asks the user to sup- ply. An efficient, rather than exhaustive, strategy to direct user questioning will reduce consultation time and effort. An intelligent strategy to minimize questioning, the merit system, has been successfully implemented in Battle, an expert consultant system developed for the Marine Corps. The merit strategy enables Battle to focus the consultation process on the most meritorious ques-tions allowing the military commander to respond quickly awith the most pertinent information The merit system, originally defined for logical functions in the Multiple program, has been extended to the Mycin style of propagation and to the method of subjective Bayesian assignments used by Prospector. A procedure for merit calculations with any differentiable, real-valued assign-ment function is presented. Our experience has shown that merit values provide an efficient flow of control for expert consultation.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-072.pdf,
72,1983,Expert Systems,Diagnosis Via Causal Reasoning: Paths of Interaction and the Locality Principle,Randall Davis,"Interest has grown recently in developing expert systems that reason ""from first principles"", i.e., capable of the kind of problem solving exhibited by an engineer who can diagnose a malfunctioning device by reference to its schematics, even though he may never have seen that device before. In developing such a system for troubleshooting digital electronics, we have argued for the importance of pathways of causal interaction as a key concept. We have also suggested using a layered set of interaction paths as a way of constraining and guiding the diagnostic process. We report here on the implementation and use of these ideas. We show how they make it possible for our system to generate a few sharply constrained hypotheses in diagnosing a bridge fault. Abstracting from this example, we find a number of interesting general principles at work. We suggest that diagnosis can be viewed as the interaction of simulation and inference and we find that the concept of locality proves to be extremely useful in understanding why bridge faults are difficult to diagnose and why multiple representations are useful.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-073.pdf,
73,1983,Expert Systems,A New Inference Method for Frame-Based Expert Systems,"James A. Reggia, Dana S. Nau, Pearl Y. Wang","This paper introduces a new frame-based model of diagnostic reasoning which is based on a generalization of the classic set covering problem in mathematics. The model directly handles multiple simultaneous disorders, it can be formalized, it is intuitively plausible, it provides an approach to partial matching, and it is justifiable in terms of past empirical studies of human diagnostic reasoning. We are using this model as an inference method in diagnostic expert systems, and contrast it with the inference methods used in previous similar systems.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-074.pdf,
74,1983,Expert Systems,Analysis of Physiological Behavior Using a Causal Model Based on First Principles,John C. Kunz,"The objective of this research is to demonstrate a methodology for the design and use of a physiological model in a computer program that suggests medical decisions. The physiological model is based on first principles and facts of physiology and anatomy, and it includes inference rules for analysis of casual relations between physiological events. The model is used to analyze physiological behavior, identify the effects of abnormalities, suggest appropriate therapies, and predict the results of therapy. This methodology integrates heuristic knowledge traditionally used in artificial intelligence programs with mathematical knowledge traditionally used in mathematical modeling programs. In recognition of its origins in artificial intelligence and mathematical modeling, the system is named AI/MM. This paper briefly introduces the knowledge representation and examples of the system analysis of behavior in the domain of renal physiology.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-075.pdf,
75,1983,Expert Systems,An Intelligent Aid for Circuit Redesign,"Tom M. Mitchell, Louis I. Steinberg, Smadar Kedar-Cabelli, Van E. Kelly, Jeffrey Shulman, Timothy Weinrich","Digital circuit redesign is a task that requires knowledge of circuit structure, function, and purpose, and of the interrelationships among these We describe a knowledge-based system, REDESIGN, which assists In the redesign of digital circuits to meet altered functional specifications. REDESIGN assists the user in focusing on an appropriate portion of the circuit, generating possible local changes within the circuit, ranking these possible changes, and detecting undesirable side-effects of redesigns. lt provides this assistance by combining two modes of reasoning about circuits: (1) causal reasoning involving analysis of circuit operation, and (2) reasoning about the purposes, or roles, of various circuit modules within the larger circuit. We describe these two modes of reasoning, and the way in which they are combined by REDESIGN to provide aid in circuit redesign.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-076.pdf,
76,1983,Expert Systems,TALIB: An IC Layout Design Assistant,"Jin Kim, John McDermott","This paper describes a knowledge-based system for automatically synthesizing integrated circuit layouts for NMOS cells. The desired cell layouts are specified in terms of their general structural and functional characteristics. From these initial specifications, the system produces correct and compact cell layouts. The system performs this task by generating plan steps at different levels of abstraction and opportunistically refining each plan step at one level to more specific steps at a lower level. Although the implementation of this system has focused on NMOS technology, the techniques used are not restricted to that technology.'",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-077.pdf,
77,1983,Expert Systems,Using Structural and Functional Information in Diagnostic Design,Walter Hamscher,"We wish to design a diagnostic for a device from knowledge of its structure and function. The diagnostic should achieve both coverage of the faults that can occur in the device, and should strive to achieve specifically in its diagnosis when it detects a fault. A system is described that uses a simple model of hardware structure and function, representing the device in terms of its internal primitive functions and connections. The system designs a diagnostic in three steps. First, an extension of path sensitization is used to design a test for each of the connections in the device. Next, the resulting tests are improved by increasing their specificity. Finally the tests are ordered so that each relies on the fewest possible connections. We describe an implementation of the first of these steps and show an example of the results for a simple device.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-078.pdf,
78,1983,Expert Systems,Abstract Explanations of Strategy in a Diagnostic Consultation System,Diane Warner Hasling,"This paper presents the explanation system for NEOMYCIN* , a medical consultation program. A consultation program plays the role of an expert to assist a user in solving a problem. An explanation of strategy describes the plan the program is using to reach a solution. Such an explanation is usually concrete, referring to aspects of the current problem situation. Abstract explanations articulate a general principle, which can be applied in different situations; such explanations arc useful in teaching and in explaining by analogy. We describe the aspects of NEOMYCIN that make abstract strategic explanations possible-the representation of strategic knowledge explicitly and separately from domain knowledge--and demonstrate how this representation can be used to generate explanations.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-079.pdf,
79,1983,Search,A Theory of Game Trees,"Chun-Hung Tzeng, Paul W. Purdom, Jr.","A theory of heuristic game tree search and evalua-tion functions for estimating minimax values is developed. The result is quite different from the tradi-tional minimax approach to game playing, and it leads to product-propagation rules for backing up values when subpositions in the game are independent. In this theory Nau’s paradox is avoided and deeper searching leads to better moves if one has reasonable evaluation functions.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-080.pdf,
80,1983,Search,The Optimality of A* Revisited,"Rina Dechter, Judea Pearl","This paper examines the optimality of A*, in the sense of expanding the least number of distinct nodes, over three classes of algorithms which return solutions of comparable costs to that found by A*. We first show that A* is optimal over those algorithms guaranteed to And a solution at least as good as A*’s for every heuristic assignment h. Second, we consider a wider class of algorithms which, like A*, are guaranteed to find an optimal solution (i.e., admissible) if all cost estimates are optimistic (i.e., h(h*). On this class we show that A* is not optimal and that no optimal algorithm exists unless h is also consistent, in which case A* is optimal. Finally we show that A* is optimal over the subclass of best-first algorithms which are admissible whenever h(h*).",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-081.pdf,
81,1983,Search,Solving the General Consistent Labeling (or Constraint Satisfaction) Problem: Two Algorithms and Their Expected Complexities,Bernard Nadel,"The Consistent Labeling Problem is of considerable importance in Artificial Intelligence, Operations Research and Symbolic Logic. It has received much attention, but most work has addressed the specialized binary form of the problem. Furthermore, none of the relatively few papers that treat the general problem have dealt analytically with the issue of complexity. In this paper we present two algorithms for solving the general Consistent Labeling Problem and for each of these the expected complexity is given under a simple statistical model for the distribution of problems. This model is sufficient to expose certain interesting aspects of complexity for the two algorithms. Work currently in progress will address more subtle aspects by extension to more refined satistical models.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-082.pdf,
82,1983,Search,"The Composite Decision Process: A Unifying Formulation for Heuristic Search, Dynamic Programming and Branch and Bound Procedures +","Vipin Kumar, Laveen Kanal","In this short paper we present a brief exposition of a composite decision process -- our unifying formulation of search procedures -- which provides new insights concerning the relationships among heuristic search, dynamic programming and branch and bound procedures.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-083.pdf,
83,1983,Search,Non-Minimax Search Strategies for Use Against Fallible Opponents,"Andrew L. Reibman, Bruce W. Ballard","Most previous research on the use of search for minimax game playing has focused on improving search efficiency rather than on better utilizing available information. In a previous paper we developed models of imperfect opponent play based on a notion we call playing strength. In this paper, we use the insights acquired in our study of imperfect play and ideas expressed in papers by Slagle and Dixon, Ballard, Nau, and Pearl to develop alternatives to the conventional minimax strategy. We demonstrate that, in particular situations, against both perfect and imperfect opponents, our strategy yields an improvement comparable to or exceeding that provided by an additional ply of search.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-084.pdf,
84,1983,Search,Intelligent Control Using Integrity Constraints,"Madhur Kohli, Jack Minker","This paper describes how integrity con-straints, whether user supplied or automatically generated during the search, and analysis of failures can be used to improve the execution of function free logic programs. Integrity con-straints are used to guide both the forward and backward execution of the Programs. This work applies to arbitrary node and literal selection functions and is thus transparent to the fact whether the logic program is executed Sequentially or in parallel.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-085.pdf,
85,1983,Search,Predicting the Performance of Distributed Knowledge-Based Systems: A Modeling Approach,Jasmina Pavlin,"A model of a distributed knowledge-based system is presented. The model captures the features specific to those systems, such as alternative paths to the solution, utilization of inexact and/or incomplete knowledge and data, dynamic task creation, complex subproblem dependencies and focusing aspect of problem solving. The model is applied to the analysis of communication policies in a distributed interpretation system. The result of the analysis is the best policy for the given environment and system conditions. Another use of the model as a real-time simulation tool is suggested.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-086.pdf,
86,1983,Support Hardware and Software,"Massively Parallel Architectures for Al: NETL, Thistle, and Boltzmann Machines","Scott E. Fahlman, Geoffrey E. Hinton, Terrence J. Sejnowski","It is becoming increasingly apparent that some aspects of intelligent behavior require enormous computational power and that some sort of massively parallel computing architecture is the most plausible way to deliver such power. Parallelism, rather than raw speed of the computing elements. seems to be the way that the brain gets such jobs done. But even if the need for massive parallelism is admitted, there is still the question of what kind of parallel architecture best fits the needs of various AI tasks. In this paper we will attempt to isolate a number of basic computational tasks that an intelligent system must perform. We will describe several families of massively parallel computing architectures, and we will see which of these computational tasks can be handled by each of these families. In particular, we will describe a new architecture, which we call the Boltzmann machine, whose abilities appear to include a number of tasks that are inefficient or impossible on the other architectures.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-087.pdf,
87,1983,Support Hardware and Software,YAPS: A Production Rule System Meets Objects,Elizabeth Allen,"This paper describes an antecedent-driven pro-duction system, YAPS (Yet Another Production Sys-tem) which encodes the left hand sides of produc-ion rules into a discrimination net in a manner similar to that used by Forgy ([Forgy 811, [Forgy 791) in OPS5. YAPS, however, gives the user more flexibility in the structure of facts in the data- base, the kinds of tests that can appear on the left hand side of production rules and the actions that can appear on the right hand side of the rules. This flexibility is realized without sac-rificing the efficiency gained by OPS5 through its discrimination net implementation. The paper also discusses how YAPS can be used in conjunction with object oriented programming systems to yield a sys-tem in which rules can talk about objects and objects can have daemons attached to them. It discusses methods of dividing YAPS into independent rule sets sharing global facts.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-088.pdf,
88,1983,Support Hardware and Software,Specification-Based Computing Environments,"Robert Balzer, David Dyer, Matthew Morgenstern, Robert Neches","This paper considers the improvements that could result from basing future computing environments on specification languages rather than programming languages. Our goal is to identify those capabilities which will significantly enhance the user’s ability to benefit from the computing environment. We have identified five such capabilities: Search. Coordination. Automation, Evolution. and Inter-User Interactions. They will be directly supported by the computing environment. Hence. each represents a ""freedom"" that users will enjoy without having to program them (i.e., be concerned with the details of how they are achieved). They form both the conceptual and the practical basis for this computing environment. A prototype computing environment has been built which supports the first three of these capabilities and which supports a simple but real service.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-089.pdf,
89,1983,Support Hardware and Software,An Object-Oriented Simulator for the Apiary,Henry Lieberman,"This paper describes a simulator for the proposed Apiary, an object-oriented, message passing parallel machine for artificial intelligence applications, using the QCKV model of computation. The simulator implements an interpreter for the lowest level ""virtual machine language"" of the Apiary, specifying computations in terms of creating objects and sending messages rather than loading and storing registers. The simulator is itself programmed in the object-oriented style advocated by the actor philosophy, allowing experimentation with alternative implementation mechanisms without disturbing the behavior of the simulation. Technical details in the paper assume some familiarity with object-oriented programming and the actor formalism.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-090.pdf,
90,1983,Support Hardware and Software,Knowledge-Based Programming Using Abstract Data Types,"Gordon S. Novak, Jr.","Features of the GLISP programming system that support knowledge-based programming are described. These include compile-time expansion of object-centered programs, interpretation of messages and operations relative to data type, inheritance of properties and behavior from multiple superclasses, type inference and propagation, conditional compilation, symbolic optimization of compiled code, instantiation of generic programs for particular data types, combination of partial algorithms from separate sources, knowledge-based inspection and editing of data, menu-driven interactive programming, and transportability between Lisp dialects and machines. GLISP is fully implemented for the major dialects of Lisp and is available over the ARPANET.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-091.pdf,
91,1983,Support Hardware and Software,IMPULSE: A Display Oriented Editor for STROBE,"Eric Schoen, Reid G. Smith","In this paper, we discuss a display-oriented editor to aid in the construction of knowledge-based systems. We also report on our experiences concerning the utility of the editor.",https://aaai.org/Library/AAAI/1983/../../../Papers/AAAI/1983/AAAI83-092.pdf,
