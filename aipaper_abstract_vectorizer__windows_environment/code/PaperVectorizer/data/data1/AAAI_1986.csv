,conference_year,category,title,author,abstract,download_url,keywords
0,1986,Automated Reasoning,Reasoning with Simplifying Assumptions: A Methodology and Example,"Yishai Feldman, Charles Rich","Simplifying assumptions are a powerful technique for dealing with complexity, which is used in all branches of science and engineering. This work develops a formal account of this technique in the context of heuristic search and automated reasoning. We also present a methodology for choosing appropriate simplifying assumptions in specific domains, and demonstrate the use of this methodology with an example of reasoning about typed partial functions in an automated programming assistant.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-001.pdf,
1,1986,Automated Reasoning,"Tweety--Still Flying: Some Remarks on Abnormal Birds, Applicable Rules and a Default Prover",Gerhard Brewka,"This paper describes FAULTY, a default prover for a decidable subset of predicate calculus. FAULTY is based on McDermott’s and Doyle’s Nonmonotonic Logic I und avoids the well-known weakness of this logic by a restriction to specific theories, which are sufficient for default reasoning purposes, however. The dafaults are represented in a way that allows explicit control of their applicability. By blocking the applicability of a default the problem of interacting defaults can be avoided.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-002.pdf,
2,1986,Automated Reasoning,Representing Actions with an Assumption-Based Truth Maintenance System,"Paul Morris, Robert Nado","The Assumption-based Truth Maintenance System, introduced by de Kleer, is a powerful new tool for organizing a search through a space of alternatives. However, the ATMS is oriented towards inferential problem solving, and provides no special mechanisms for modeling actions or state changes. We describe an approach to applying the ATMS to the task of representing the effects of actions. The approach extends traditional tree-structured context mechanisms to allow context merges. It also takes advantage of the underlying ATMS to detect inconsistent contexts and to maintain derived results. Some results are presented concerning possible approaches to the treatment of merges in questionable circumstances. Finally, the analysis of actions in terms of a truth maintenance system suggests the need for a more elaborate treatment of contradiction in such systems than exists at present.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-003.pdf,
3,1986,Automated Reasoning,Automatic Compilation of Logical Specifications into Efficient Programs,Donald Cohen,"We describe an automatic programmer, or ""compiler"" which accepts as input a predicate calculus specification of a set to generate or a condition to test, along with a description of the underlying representation of the data. This compiler searches a space of possible algorithms for the one that is expected to be most efficient. We describe the knowledge that is and is not available to this compiler, and its corresponding capabilities and limitations. This compiler is now regularly used to produce large programs.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-004.pdf,
4,1986,Automated Reasoning,Factual Knowledge for Developing Concurrent Programs,"Andrzej Skowron, Alberto Pettorossi","We propose a system for the derivation of algorithms which allows us to use ""factual knowledge"" for the development of concurrent programs. From preliminary program versions the system can derive new versions which have higher performances and can be evaluated by communicating agents in a parallel architecture. The knowledge about the facts or properties of the programs is also used for the improvement of the system itself.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-005.pdf,
5,1986,Automated Reasoning,Generalized Plan Recognition,"Henry A. Kautz, James F. Allen","This paper outlines a new theory of plan recognition that is significantly more powerful than previous approaches. Concurrent actions, shared steps between actions, and disjunctive information are all handled. The theory allows one to draw conclusions based on the class of possible plans being performed, rather than having to prematurely commit to a single interpretation. The theory employs circumscription to transform a first-order theory of action into an action taxonomy, which can be used to logically deduce the complex action(s) an agent is performing.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-006.pdf,
6,1986,Automated Reasoning,A Logic of Deliberation,Marvin Belzer,"Deliberation typically involves the formation of a plan or intention from a set of values and beliefs. I suggest that deliberation, or ""practical reasoning,"" is a form of normative reasoning and that the understanding and construction of reasoning systems that can deliberate and act intentionally presupposes a theory of normative reasoning. The language and semantics of a deontic logic is used to develop a theory of defeasible reasoning in normative systems and belief systems. This theory may be applied in action theory and to artificial intelligence by identifying expressions of values, beliefs, and intentions with various types of modal sentences from the language.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-007.pdf,
7,1986,Automated Reasoning,Physics for Robots,James G. Schmolze,"Robots that plan to perform everyday tasks need knowledge of everyday physics. Physics For Robots (PFR) is a representation of part of everyday physics directed towards this need. It includes general concepts and theories, and it has been applied to tasks in cooking. PFR goes beyond most AI planning representation schemes by including natural processes that the robot can control. It also includes a theory of material composition so robots can identify and reason about physical objects that break apart, come together, mix, or go out of existence. Following on Naive Physics (NP), issues about reasoning mechanisms are temporarily postponed, allowing a focus on the characterization of knowledge. However, PFR departs from NP in two ways. (1) PFR characterizes the robot’s capabilities to act and perceive, and (2) PFR replaces the NP goal of developing models of actual common sense knowledge. Instead, PFR includes all and only the knowledge that robots need for planning, which is determined by analyzing proofs showing the effectiveness of robot I/O programs.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-008.pdf,
8,1986,Automated Reasoning,Cooperation without Communication,"Michael R. Genesereth, Matthew L. Ginsberg, Jeffrey S. Rosenschein","Intelligent agents must be able to interact even without the benefit of communication. In this paper we examine various constraints on the actions of agents in such situations and discuss the effects of these constraints on their derived utility. In particular, we define and analyze basic rationality; we consider various assumptions about independence; and we demonstrate the advantages of extending the definition of rationality from individual actions to decision procedures.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-009.pdf,
9,1986,Automated Reasoning,Incremental Planning to Control a Blackboard-based Problem Solver,"Edmund H. Durfee, Victor R. Lesser","To control problem solving activity, a planner must resolve uncertainty about which specific long-term goals (solutions) to pursue and about which sequences of actions will best achieve those goals. In this paper, we describe a planner that abstracts the problem solving state to recognize possible competing and compatible solutions and to roughly predict the importance and expense of developing these solutions. With this information, the planner plans sequences of problem solving activities that most efficiently resolve its uncertainty about which of the possible solutions to work toward. The planner only details actions for the near future because the results of these actions will influence how (and whether) a plan should be pursued. As problem solving proceeds, the planner adds new details to the plan incrementally, and monitors and repairs the plan to insure it achieves its goals whenever possible. Through experiments, we illustrate how these new mechanisms significantly improve problem solving decisions and reduce overall computation. We briefly discuss our current research directions, including how these mechanisms can improve a problem solver’s real-time response and can enhance cooperation in a distributed problem solving network.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-010.pdf,
10,1986,Automated Reasoning,An Adaptive Planner,Richard Alterman,"This paper is about an approach to the flexible utilization of old plans called adaptive planning. An adaptive planner can take advantage of the details associated with specific plans, while still maintaining the flexibility of a planner that works from general plans. Key elements in the theory of adaptive planning are its treatment of background knowledge and the introduction of a notion of planning by situation matching.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-011.pdf,
11,1986,Automated Reasoning,The Representation of Events in Multiagent Domains,Michael P. Georgeff,"The purpose of this paper is to construct a model of actions and events suited to reasoning about domains involving multiple agents or dynamic environments. A model is constructed that provides for simultaneous action, and the kind of facts necessary for reasoning about such actions are described. A model-based law of persistence is introduced to describe how actions affect the world. No frame axioms or syntactic frame rules are involved in the specification of any given action, thus allowing a proper model-theoretic semantics for the representation. Some serious deficiencies with existing approaches to reasoning about multiple agents are also identified. Finally, it is shown how the law of persistence, together with a notion of causality, makes it possible to retain a simple model of action while avoiding most of the difficulties associated with the frame problem.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-012.pdf,
12,1986,Automated Reasoning,Planning with Abstraction,Josh Tenenberg,"Intelligent problem solvers for complex domains must have the capability of reasoning abstractly about tasks that they are called upon to solve. The method of abstraction presented here allows one to reason analogically and hierarchically, making both the task of formalizing domain theories easier for the system designer, as well as allowing for increased computational efficiencies. It is believed that reasoning about concepts that share structure is essential to improving the performance of automated planning systems by allowing one to apply previous computational effort expended in the solution of one problem to a broad range of new problems.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-013.pdf,
13,1986,Automated Reasoning,Generating Perception Requests and Expectations to Verify the Execution of Plans,"Richard J. Doyle, David Atkinson, Rajkumar Doshi","This paper addresses the problem of verifying plan execution. An implemented computer program which is part of the execution monitoring process for an experimental robot system is described. The program analyzes a plan and automatically inserts appropriate perception requests into the plan and generates anticipated sensor values. Real-time confirmation of these expectations implies successful plan execution. The implemented plan verification strategy and knowledge representation are described. Several issues and extensions of the method are discussed, including a language for plan verification, heuristics for constraining plan verification, and methods for analyzing plans at multiple levels of abstraction to determine context-dependent verification strategies.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-014.pdf,
14,1986,Automated Reasoning,A Representation of Action Structures,"Erik Sandewall, Ralph Ronnquist","We consider structures of actions which are partially ordered for time, which may occur in parallel, and which have lasting effects on the state of the world. Such action structures are of interest for problem-solving with multiple actors, and for understanding narrative texts where several things are going on at the same time. They are also of interest for other branches of computer science besides AI. Actions in the action structure are characterized in terms of preconditions, postconditions, and prevail conditions, where the prevail condition is a requirement on what must hold for the duration of the action. All three conditions are partial states of the world, and therefore elements of a lattice. We develop the formalism, give an example, and specify formally the criterion for admissible action' structures, where postconditions of earlier actions serve as prevail- or preconditions of later actions in a coherent way, and there are no conflicting attempts to change (""update"" a feature in the world.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-015.pdf,
15,1986,Automated Reasoning,Order of Magnitude Reasoning,Olivier Raiman,"This paper presents a methodology for extending representation and reasoning in Qualitative Physics. This methodology is presently used for various applications. The qualitative modeling of a physical system is weakened by the lack of quantitative information. This may lead a qualitative analysis to ambiguity. One of the aims of this methodology is to cope with the lack of quantitative information. The main idea is to reproduce the physicist’s ability to evaluate the influence of different phenomena according to their relative order of magnitude and to use this information to distinguish among radically different ways in which a physical system may behave. A formal system, FOG, is described in order to represent and structure this kind of apparently vague and intuitive knowledge so that it can be used for qualitative reasoning. The validity of FOG for an interpretation in a mathematical theory called Non-Standard Analysis is then proven. Last, it is shown how FOG structures the quantity-space.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-016.pdf,
16,1986,Automated Reasoning,Doing Time: Putting Qualitative Reasoning on Firmer Ground,Brian C. Williams,"Recent work in qualitative reasoning has focused on predicting the dynamic behavior of continuous physical systems. Significant headway has been made in identifying the principles necessary to predict this class of behavior. However, the predictive inference engines based on these principles are limited in their ability to reason about time. This paper presents a general approach to behavioral prediction which overcomes many of these limitations. Generality results from a clean separation between principles relating to time, continuity, and qualitative representations. The resulting inference mechanism, based on propagation of constraints, is applicable to a wide class of physical systems exhibiting discrete or continuous behavior, and can be used with a variety of representations (e.g., digital, quantitative, qualitative or symbolic abstractions). In addition, it provides a framework in which to explore a broad range of tasks including prediction, explanation, diagnosis, and design.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-017.pdf,
17,1986,Automated Reasoning,Interpreting Measurements of Physical Systems,Kenneth D. Forbus,"An unsolved problem in qualitative physics is generating a qualitative understanding of how a physical system is behaving from raw data, especially numerical data taken across time, to reveal changing internal state. Yet providing this ability to ""read gauges"" is a critical step towards building the next generation of intelligent computer-aided engineering systems and allowing robots to work in unconstrained envirionments. This paper presents a theory to solve this problem. Importantly, the theory is domain independent and will work with any system of qualitative physics. It requires only a qualitative description of the domain capable of supporting envisioning and domain-specific techniques for providing an initial qualitative description of numerical measurements. The theory has been fully implemented, and an extended example using Qualitative Process theory is presented.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-018.pdf,
18,1986,Automated Reasoning,"""Commonsense"" Arithmetic Reasoning",Reid Simmons,"""Arithmetic reasoning"" can range in complexity from simple integer arithmetic to powerful symbolic algebraic reasoning of the sort done by MACSYMA. We describe an arithmetic reasoning system of intermediate complexity called the Quantity Lattice. In a computationally efficient manner the Quantity Lattice integrates qualitative and quantitative reasoning, and combines inequality reasoning with reasoning about simple arithmetic expressions, such as addition or multiplication. The system has proven useful in doing simulation and analysis in several domains, including geology and semiconductor fabrication, by supporting useful forms of reasoning about time and the changes that happen when processes occur.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-019.pdf,
19,1986,Automated Reasoning,A Reasoning Model Based on an Extended Dempster-Shafer Theory,John Yen,"The Dempster-Shafer (D-S) theory of evidence suggests a coherent approach to aggregate evidence bearing on groups of mutually exclusive hypotheses; however, the uncertain relationships between evidence and hypotheses are difficult to represent in applications of the theory. In this paper, we extend the multivalued mapping in the D-S theory to a probabilistic one that uses conditional probabilities to express the uncertain associations. In addition, Dempster’s rule is used to combine belief update rather than absolute belief to obtain results consistent with Bayes’ theorem. The combined belief intervals form probability bounds under two conditional independence assumptions. Our model can be applied to expert systems that contain sets of mutually exclusive and exhaustive hypotheses, which may or may not form hierarchies.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-020.pdf,
20,1986,Automated Reasoning,Reasoning about Multiple Faults,"Johan de Kleer, Brian Williams","Diagnostic tasks require determining the differences between a model of an artifact and the artifact itself The differences between the manifested behavior of the artifact and the predicted behavior of the model guide the search for the differences between the artifact and its model. The diagnostic procedure presented in this paper is model-based, inferring the behavior of the composite device from knowledge of the structure and function of the individual components comprising the device. The system (GDE - General Diagnostic Engine) has been implemented and tested on examples in the domain of troubleshooting digital circuits. This research makes several novel contributions: First, the system diagnoses failures due to multiple faults. Second, failure candidates are represented and manipulated in terms of minimal sets of violated assumptions, resulting in an efficient diagnostic procedure. Third, the diagnostic procedure is incremental, reflecting the iterative nature of diagnosis. Finally, a clear separation is drawn between diagnosis and behavior prediction, resulting in a domain (and inference procedure) independent diagnostic procedure.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-021.pdf,
21,1986,Automated Reasoning,Plausibility of Diagnostic Hypotheses: The Nature of Simplicity,"Yun Peng, James A. Reggia","In general diagnostic problems multiple disorders can occur simultaneously. AI systems have traditionally handled the potential combinatorial explosion of possible hypotheses in such problems by focusing attention on a few ""most plausible"" ones. This raises the issue of establishing what makes one hypothesis more plausible than others. Typically a hypothesis (a set of disorders) must not only account for the given manifestations, but it must also satisfy some notion of simplicity (or coherency, or parsimony, etc) to be considered. While various criteria for simplicity have been proposed in the past, these have been based on intuitive and subjective grounds. In this paper, we address the issue of if and when several previously-proposed criteria of parsimony are reasonable in the sense that they are guaranteed to at least identify the most probable hypothesis. Hypothesis likelihood is calculated using a recent extension of Bayesian classification theory for multimembership classification in causal diagnostic domains. The significance of this result is that it is now possible to decide objectively a priori the appropriateness of different criteria for simplicity in developing an inference method for certain classes of general diagnostic problems.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-022.pdf,
22,1986,Search,A Unified Theory of Heuristic Evaluation Functions and its Application to Learning,"Jens Christensen, Richard E. Korf","We present a characterization of heuristic evaluation functions which unifies their treatment in single-agent problems and two-person games. The central result is that a useful heuristic function is one which determines the outcome of a search and is invariant along a solution path. This local characterization of heuristics can be used to predict the effectiveness of given heuristics and to automatically learn useful heuristic functions for problems. In one experiment, a set of relative weights for the different chess pieces was automatically learned.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-023.pdf,
23,1986,Search,Choosing Directions for Rules,"Richard Treitel, Michael Genesereth","In ""expert systems"" and other applications of logic programming, the issue arises of whether to use rules for forward or backward inference, i.e. whether deduction should be driven by the facts available to the program or the questions that are put to it. Often some mixture of the two is cheaper than using either mode exclusively. We show that, under two restrictive assumptions, optimal choices of directions for the rules can be made in time polynomial in the number of rules in a recursion-free logic program. If we abandon either of these restrictions, the optimal choice is NP-complete. A broad range of cost measures can be used, and can be combined with bounds on some element of the total cost.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-024.pdf,
24,1986,Search,An Algorithmic Solution of N-Person Games,"Carol Luckhardt, K. Irani","Two-person, perfect information, constant sum games have been studied in Artificial Intelligence. This paper opens up the issue of playing n-person games and proposes a procedure for constant sum or non-constant sum games. It is proved that a procedure, maxn, locates an equilibrium point given the entire game tree. The minimax procedure for 2-person games using look ahead finds a saddle point of approximations, while maxn finds an equilibrium point of the values of the evaluation function for n-person games using look ahead. Mazn is further analyzed with respect to some pruning schemes.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-025.pdf,
25,1986,Search,Making Best Use of Available Memory When Searching Game Trees,"Subir Bhattacharya, Amitava Bagchi","When searching game trees, Algorithm SSS* examines fewer terminal nodes than the alphabeta procedure, but has the disadvantage that the storage space required by it is much greater. ITERSSS* is a modified version of SSS* that does not suffer from this limitation. The memory M that is available for use by the OPEN list can be fed as a parameter to ITERSSS* at run time. For successful operation M must lie above a threshold value MO . But MO is small in magnitude and is of the same order as the memory requirement of the alphabeta procedure. The number of terminal nodes of the game tree examined by ITERSSS* is a func- tion of M, but is never greater than the number of terminals examined by the alphabeta procedure. For large enough M, ITERSSS* is identical in operation to SSS*.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-026.pdf,
26,1986,Search,Finding a Shortest Solution for the N x N Extension of the 15-PUZZLE Is Intractable,"Daniel Ratner, Manfred Warmuth","The 8-puzzle and the 15-puzzle have been used for many years as a domain for testing heuristic search techniques. From experience it is known that these puzzles are ""difficult"" and therefore useful for testing search techniques. In this paper we give strong evidence that these puzzles are indeed good test problems. We extend the 8-puzzle and the 15-puzzle to a nxn board and show that finding a shortest solution for the extended puzzle is NP-hard and thus computationally infeasible. We also present an approximation algorithm for transforming boards that is guaranteed to use no more than c*L(SP) moves, where L(SP) is the length of the shortest solution and c is a constant which is independent of the given boards and their size n .",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-027.pdf,
27,1986,Search,Joint and LPA*: Combination of Approximation and Search,"Daniel Ratner, Ira Pohl","This paper describes two new algorithms, Joint and LPA*, which can be used to solve difficult combinatorial problems heuristically. The algorithms find reasonably short solution paths and are very fast. The algorithms work in polynomial time in the length of the solution. The algorithms have been benchmarked on the 15-puzzle, whose generalization has recently been shown to be NP hard, and outperform other known methods within this context.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-028.pdf,
28,1986,Search,Learning While Searching in Constraint-Satisfaction-Problems,Rina Dechter,"The popular use of backtracking as a control strategy for theorem proving in PROLOG and in Truth-Maintenance-Systems (TMS) led to increased interest in various schemes for enhancing the efficiency of backtrack search. Researchers have referred to these enhancement schemes by the names ""Intelligent Backtracking"" (in PROLOG), ""Dependency-directed-backtracking"" (in TMS) and others. Those improvements center on the issue of ""jumping-back"" to the source of the problem in front of dead-end situations. This paper examines another issue (much less explored) which arises in dead-ends. Specifically, we concentrate on the idea of constraint recording, namely, analyzing and storing the reasons for the dead-ends, and using them to guide future decisions, so that the same conflicts will not arise again. We view constraint recording as a process of learning, and examine several possible learning schemes studying the tradeoffs between the amount of learning and the improvement in search efficiency.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-029.pdf,
29,1986,Search,Query Answering in Circumscriptive and Closed-World Theories,Teodor C. Przymusinski,"Among various approaches to handling incomplete and negative information in knowledge representation systems based on predicate logic, McCarthy’s circumscription appears to be the most powerful. In this paper we describe a decidable algorithm to answer queries in circumscriptive theories. The algorithm is based on a modified version of ordered linear resolution, which constitutes a sound and complete procedure to determine whether there exists a minimal model satisfying a given formula. The Closed-World Assumption and its generalizations, GCWA and ECWA, can be considered as a special form of circumscription. Consequently, our method also applies to answering queries in theories using the Closed-World Assumption or its generalizations. For the sake of clarity, we restrict our attention to theories consisting of ground clauses. Our algorithm, however, has a natural extension to theories consisting of arbitrary clauses.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-030.pdf,
30,1986,Search,Indefinite and GCWA Inference in Indefinite Deductive Databases,"Lawrence J. Henschen, Hyung-Sik Park","This paper presents several basic results on compiling indefinite and GCWA (Generalized Closed World Assumption) inference in IDDB (Indefinite Dedutive Databases). We do not allow function symbols, but do allow non-Horn clauses. Further, although the GCWA is used to derive negative assumptions, we do also allow negative clauses to occur explicitly. We show a fundamental relationship between indefiniteness and indefinite inference. We consider three representation alternatives to separate the CDB (Clausa1 DB) from the RDB (Relationa1 DB). We present the basic ideas for compiling indefinite and GCWA inference on CDB and evaluating it through the RDB. Finally, we introduce decomposition theorems to evaluate disjunctive and conjunctive queries.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-031.pdf,
31,1986,Search,An Integration of Resolution and Natural Deduction Theorem Proving,"Dale Miller, Amy Felty","We present a high-level approach to the integration of such different theorem proving technologies as resolution and natural deduction. This system represents natural deduction proofs as X-terms and resolution refutations as the types of such X-terms. These type structures, called expansion trees, are essentially formulas in which substitution terms are attached to quantifiers. As such, this approach to proofs and their types extends the formulas-as-type notion found in proof theory. The LCF notion of tactics and tacticals can also be extended to incorporate proofs as typed X-terms. Such extended tacticals can be used to program different interactive and automatic natural deduction theorem provers. Explicit representation of proofs as typed values within a programming language provides several capabilities not generally found in other theorem proving systems. For example, it is possible to write a tactic which can take the type specified by a resolution refutation and automatically construct a complete natural deduction proof. Such a capability can be of use in the development of user oriented explanation facilities.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-032.pdf,
32,1986,Search,Parallel Logical Inference and Energy Minimization,Dana H. Ballard,"The inference capabilities of humans suggest that they might be using algorithms with high degrees of parallelism. This paper develops a completely parallel connectionist inference mechanism. The mechanism handles obvious inferences, where each clause is only used once, but may be extendable to harder cases. The main contribution of this paper is to show formally that some inference can be reduced to an energy minimization problem in a way that is potentially useful.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-033.pdf,
33,1986,Search,A Framework for Comparing Alternative Formalisms for Plausible Reasoning,"Eric J. Horvitz, David E. Heckerman, Curtis P. Langlotz","We present a logical relationship between a small number of intuitive properties for measures of belief and the axioms of probability theory. The relationship was first demonstrated several decades ago but has remained obscure. We introduce the proof and discuss its relevance to research on reasoning under uncertainty in artificial intelligence. In particular, we demonstrate that the logical relationship can facilitate the identification of differences among alternative plausible reasoning methodologies. Finally, we make use of the relationship to examine popular non-probabilistic strategies.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-034.pdf,
34,1986,Search,Using Decision Theory to Justify Heuristics,"Curtis P. Langlotz, Edward H. Shortliffe, Lawrence M. Fagan","We present a method for using decision theory to evaluate the merit of individual situation -- action heuristics. The design of a decision-theoretic approach to the analysis of heuristics is illustrated in the context of a rule from the MYCIN system. Using calculations and plots generated by an automated decision making tool, decision-theoretic insights are shown that are of practical use to the knowledge engineer. The relevance of this approach to previous discussions of heuristics is discussed. We suggest that a synthesis of artificial intelligence and decision theory will enhance the ability of expert systems to provide justifications for their decisions, and may increase the problem solving domains in which expert systems can be used.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-035.pdf,
35,1986,Search,Causal and Plausible Reasoning in Expert Systems,Gerald Shao-Hung Liu,This study sets out to establish a unified framework for causal and plausible reasoning. We identify a primitive set of causal roles which a condition may play in the inference. We also extend Dempster-Shafer theory to compose the belief in conclusion by the belief in rules and the belief in conditions. The combined framework permits us to express and propagate a scale of belief certainties in the context of individual roles. Both the causation aspect and the certainty aspect of an inference are now accounted for in a coherent way.,https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-036.pdf,
36,1986,Search,Advances in Rete Pattern Matching,"Marshall Schor, Timothy Daly, Ho Soo Lee, Beth Tibbitts","A central algorithm in production systems is the pattern match among rule predicates and current data. Systems like OPS5 and its various derivatives use the RETE algorithm for this function. This paper describes and analyses several augmentations of the basic RETE algorithm that are incorporated into an experimental production system, YES/OPS, which achieve significant improvement in efficiency and rule clarity.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-037.pdf,
37,1986,Search,Bayesian Inference without Point Estimates,Paul Snow,"It is conventional to apply Bayes’ formula only to point estimates of the prior probabilities. This convention is unnecessarily restrictive. The analyst may prefer to estimate that the priors belong to some set of probability vectors. Set estimates allow the non-paradoxical expression of ignorance and support rigorous inference on such everyday assertions as ""one event is more likely than another"" or that an event ""usually"" occurs. Bayes’ formula can revise set estimates, often at little computational cost beyond that needed for point priors. Set estimates can also inform statistical decisions, although disagreement exists about what decision methods are best.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-038.pdf,
38,1986,Search,Implementation of and Experiments with a Variable Precision Logic Inference System,Peter Haddawy,A system capable of performing approximate inferences under time constraints is presented. Censored production rules are used to represent both domain and control information. These are given a probabilistic semantics and reasoning is performed using a scheme based on Dempster-Shafer theory. Examples show the naturalness of the representation and the flexibility of the system. Suggestions for further research are offered.,https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-039.pdf,
39,1986,Search,Multi-Valued Logics,Matthew L. Ginsberg,"A great deal of recent theoretical work in inference has involved extending classical logic in some way. I argue that these extensions share two properties: firstly, the formal addition of truth values encoding intermediate levels of validity between true (i.e., valid) and false (i.e., invalid) and, secondly, the addition of truth values encoding intermediate levels of certainty between true or false on the one hand (complete information) and unknown (no information) on the other. Each of these properties can be described by associating lattice structures to the collection of truth values involved; this observation lead us to describe a general framework of which both default logics and truth maintenance systems are special cases.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-040.pdf,
40,1986,Cognitive Modeling and Education,Editorial Comprehension in OpEd through Argument Units,"Sergio J. Alvarado, Michael G. Dyer, Margot Flowers","This paper presents a theory of reasoning and argument comprehension currently implemented in OpEd, a computer system that reads short politico-economic editorials and answers questions about the editorial contents. We believe that all arguments are composed of a fixed number of abstract argument structures, which we call Argument Units (AUs). Thus, argument comprehension is viewed in OpEd fundamentally as the process of recognizing, instantiating, and applying argument units. Here we discuss: (a) the knowledge and processes necessary to understand opinions, arguments, and issues which arise in politico-economic editorials; and (b) the relation of this research to previous work in natural language understanding. A description of OpEd and examples of its current input/output behavior are also presented in this paper.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-041.pdf,
41,1986,Cognitive Modeling and Education,Uniform Parsing and Inferencing for Learning,"Charles E. Martin, Christopher K. Riesbeck","In previous papers we have argued for the complete integration of natural language understanding with the rest of the cognitive system. Given a set of richly indexed memory structures, we have claimed that parsing is a general memory search process guided by predictive patterns of lexical and conceptual items which are a part of those memory structures. In this paper, we demonstrate that our architecture for language understanding is capable of implementing the memory search processes required to make complex inferences not directly associated with parsing. The uniform format of the knowledge representation and search process provide a foundation for learning research.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-042.pdf,
42,1986,Cognitive Modeling and Education,Mixing Binary and Continuous Connection Schemes for Knowledge Access,"Noel E. Sharkey, R. F. E. Sutcliffe, W. R. Wobcke","We present BACAS, a Binary and Continuous Activation System which is a parallel process content-addressable memory model. BACAS is designed for the representation and retrieval of ""knowledge of the world"" for automatic natural language understanding. In its present form, BACAS is a two-layered system with 10 K-structures (like scripts) in the binary output macro-layer represented by 46 Threshold Knowledge Units and 184 processing elements (like action events) in the continuous activation micro-layer. We discuss the problems of combining two types of connection system and describe a simulation in which the system moves from one pattern to the next in response to external input. A new tool for connection systems, the pulse-out, is introduced. This is a device which replaces the Boltzmann Machine in creating energy leaps. The pulse-out also has the advantage, in the current system, of setting the state of the system a short Hamming distance from an appropriate pattern.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-043.pdf,
43,1986,Cognitive Modeling and Education,CHEF: A Model of Case-Based Planning,Kristian J. Hammond,"Case-based planning is based on the idea that a machine planner should make use of its own past experience in developing new plans, relying on its memories instead of a base of rules. Memories of past successes are accessed and modified to create new plans. Memories of past failures are used to warn the planner of impending problems and memories of past repairs are called upon to tell the planner how to deal with them. Successful plans are stored in memory, indexed by the goals they satisfy and the problems they avoid. Failures are also stored, indexed by the features in the world that predict them. By storing failures as well as successes, the planner is able to anticipate and avoid future plan failures. These ideas of memory, learning and planning are implemented in the case-based planner CHEF, which creates new plans in the domain of Szechwan cooking.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-044.pdf,
44,1986,Cognitive Modeling and Education,The Structure-Mapping Engine,"Brian Falkenhainer, Kenneth D. Forbus, Dedre Gentner","This paper describes the Structure-Mapping Engine (SME), a cognitive simulation program for studying human analogical processing. SME is based on Gentner’s Structure-Mapping theory of analogy, and provides a ""tool kit"" for constructing matching algorithms consistent with this theory. This flexibility enhances cognitive simulation studies by simplifying experimentation. Furthermore, SME is very efficient, making it a candidate component for machine learning systems as well. We review the Structure-Mapping theory and describe the design of the engine. Next we demonstrate some examples of its operation. Finally, we discuss our plans for using SME in cognitive simulation studies.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-045.pdf,
45,1986,Cognitive Modeling and Education,SNePS Considered as a Fully Intensional Propositional Semantic Network,"Stuart C. Shapiro, William J. Rapaport","We present a formal syntax and semantics for SNePS considered as the (modeled) mind of a cognitive agent. The semantics is based on a Meinongian theory of the intensional objects of thought that is appropriate for AI considered as ""computational philosophy"" or ""computational psychology"".",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-046.pdf,
46,1986,Cognitive Modeling and Education,Quantitative Analysis of Analogy by Similarity,Stuart J. Russell,"In the absence of specific relevance information, the traditional assumption in the study of analogy has been that the most similar analogue is most likely to provide the correct solutions; a justification for this assumption has been lacking, as has any relation between the similarity measure used and the probability of correctness of the analogy. We show how a statistical analysis can be performed to give the probability that a given source will provide a successful analogy, using only the assumption that there are some relevant features somewhere in the source and target descriptions. The predicted variation of the probability with source-target similarity corresponds closely to empirical analogy data obtained by Shepard for human and animal subjects for a wide variety of domains. The utility of analogy by similarity seems to rest on some very fundamental assumptions about the nature of our representations.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-047.pdf,
47,1986,Cognitive Modeling and Education,Hypotheticals as Heuristic Device,"Edwina L. Rissland, Kevin D. Ashley","In this paper we examine the use of hypotheticals as a heuristic device to assist a case-based reasoner test the strengths, weaknesses, and ramifications of an analysis or argument by exploring and augmenting the space of known cases and indirectly, the attendant spaces of doctrine and argument. Our program, HYPO, works in the task domain of the law, particularly, the area of trade secret protection for software. We describe how HYPO generates a constellation of legally-meaningful hypothetical fact situations (""hypos"") which are ""near"" a given fact situation. This is done in two steps: analysis of the given situation and then generation of the hypos. We discuss the heuristics HYPO currently uses, which include: (1) make a case weaker or stronger; (2) generate an extreme case; (3) enable a near miss; (4) manipulate a near win; and (5) generate a case on a related ""dimension"".",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-048.pdf,
48,1986,Cognitive Modeling and Education,Can a System Be Intelligent if It Never Gives a Damn?,Thomas Edelson,"I explore whether all types of cognitive faculties are possible in a system which does not also have ""affective faculties"" such as motives and emotions. Attention is focused on the human cognitive faculty of understanding the human affective faculties, and it is suggested that we do this in part by using ourselves as models of other people. Therefore any system which could perform as well as we do on this task would incorporate much knowledge about human affective faculties, and would probably have embedded within it a ""model human"" which would possess affective faculties. However, this would not necessarily manifest itself directly in the embedding system’s behavior; thus the embedding system, unlike the embedded one, need not have affective faculties. That’s a relief, because we might prefer that it didn’t.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-049.pdf,
49,1986,Cognitive Modeling and Education,Debugging User Conceptions of Interpretation Processes,"M. J. Coombs, R. Hartley, J. G. Stell","The use of high level declarative languages has been advocated since they allow problems to be expressed in terms of their domain facts, leaving details of execution to the language interpreter. While this is a significant advantage, it is frequently difficult to learn the procedural constraints imposed by the interpreter. Thus, declarative failures may arise from misunderstanding the implicit procedural content of a program. This paper argues for a constructive approach to identifying poor understanding of procedural interpretation, and presents a prototype diagnostic system for Prolog.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-050.pdf,
50,1986,Cognitive Modeling and Education,Imposing Structure on Linear Programming Problems: An Empirical Analysis of Expert and Novice Models,"Wanda Orlikowski, Vasant Dhar","Research on expert-novice differences falls into two complementary classes. The first assumes that novice skills are a subset of those of the expert, represented by the same vocabulary of concepts. The second approach emphasizes novices’ misconceptions and the different meanings they tend to attribute to concepts. Our evidence, based on observations of problem solving behavior of experts and novices in the area of mathematical programming, reveals both type of differences: while novices are to some extent underdeveloped experts, they also attribute different meanings to concepts. The research suggests that experts’ concepts can be characterized as being more differentiated than those of novices, where the differentiation enables experts to categorize problem descriptions accurately into standard archetypes and facilitates attribution of correct meanings to problem features. Our results are based on twenty-five protocols obtained from experts and novices attempting to structure problem descriptions into mathematical programming models. We have developed a model of knowledge in the LP domain that accommodates a continuum of expertise ranging from that of the expert who has a highly specialized vocabulary of LP concepts to that of a novice whose vocabulary might be limited to high school algebra. We discuss the normative implications of this model for pedagogical strategies employed by instructors, textbooks and intelligent tutoring systems.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-051.pdf,
51,1986,Cognitive Modeling and Education,Intelligent Tutoring Systems Based Upon Qualitative Model Evolutions,"Barbara Y. White, John R. Frederiksen","One promising educational application of computers derives from their ability to dynamically simulate physical phenomena. Such systems permit students to explore, for instance, electrical circuit behavior or particle dynamics. In the past, these simulations have been based upon quantitative models. However, recent work in artificial intelligence has created techniques for basing such simulations on qualitative reasoning. Qualitative models not only simulate the phenomena of the domain, but also permit instructional systems to generate explanations of the behavior under study. Sequences of such models, that attempt to capture the progression from novice to expert reasoning, permit instructional systems to select problems and generate explanations that increase in complexity at an appropriate rate for each student. Since the acquisition of a qualitative understanding of the laws of physics and their implications is an important component of understanding physical phenomena, it is argued that systems based upon qualitative model progressions can play a valuable role in science education.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-052.pdf,
52,1986,Cognitive Modeling and Education,An Analysis of Tutorial Reasoning about Programming Bugs,"David Littman, Jeannine Pinto, Elliot Soloway","A significant portion of tutorial interactions revolve around the bugs a student makes. When a tutor performs an intervention to help a student fix a programming bug, the problem of deciding which intervention to perform requires extensive reasoning. In this paper, we identify five tutorial considerations tutors appear to use when they reason about how to construct tutorial interventions for students’ bugs. Using data collected from human tutors working in the domain of introductory computer programming, we identify the knowledge tutors use when they reason about the five considerations and show that tutors are consistent in the ways that they use the kinds of knowledge to reason about students’ bugs. In this paper we illustrate our findings of tutorial consistency by showing that tutors are consistent in how they reason about bug criticality and bug categories. We suggest some implications of these empirical findings for the construction of intelligent tutoring systems.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-053.pdf,
53,1986,Knowledge Representation,"Default Reasoning, Nonmonotonic Logics, and the Frame Problem","Steve Hanks, Drew McDermott","Nonmonotonic formal systems have been proposed as an extension to classical first-order logic that will capture the process of human ""default reasoning"" or ""plausible inference"" through their inference mechanisms just as modus ponens provides a model for deductive reasoning. But although the technical properties of these logics have been studied in detail and many examples of human default reasoning have been identified, for the most part these logics have not actually been applied to practical problems to see whether they produce the expected results. We provide axioms for a simple problem in temporal reasoning which has long been identified as a case of default reasoning, thus presumably amenable to representation in nonmonotonic logic. Upon examining the resulting nonmonotonic theories, however, we find that the inferences permitted by the logics are not those we had intended when we wrote the axioms, and in fact are much weaker. This problem is shown to be independent of the logic used; nor does it depend on any particular temporal representation. Upon analyzing the failure we find that the nonmonotonic logics we considered are inherently incapable of representing this kind of default reasoning. Finally we discuss two recent proposals for solving this problem.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-054.pdf,
54,1986,Knowledge Representation,Inference in a Topically Organized Semantic Net,"Johannes de Haan, Lenhart K. Schubert","A semantic net system in which knowledge is topically organized around concepts has been under development at the University of Alberta for some time. The system is capable of automatic topical classification of modal logic input sentences, concept and topic oriented retrieval, and property inheritance of a general sort. This paper presents an inference method which efficiently determines yes or no answers to relatively simple questions about knowledge in the net. It is a deductive, resolution based method, enhanced by a set of special inference methods, and relies on the classification and retrieval mechanisms of the net to maintain its effectiveness, unencumbered by the volume or diversity of knowledge in the net.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-055.pdf,
55,1986,Knowledge Representation,On the Logic of Probabilistic Dependencies,Judea Pearl,"This paper uncovers the axiomatic basis for the probabilistic relation ""x is independent of y, given z"" and offers it as a formal definition of informational dependency. Given an initial set of such independence relationships, the axioms established permits us to infer new independencies by non-numeric, logical manipulations. Additionally, the paper legitimizes the use of inference networks to represent probabilistic dependencies by establishing a clear correspondence between the two relational structures. Given an arbitrary probabilistic model, P, we demonstrate a construction of a unique edge-minimum graph G such that each time we observe a vertex x separated from y by a subset S of vertices, we can be guaranteed that variables x and y are independent in P, given the values of the variables in S.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-056.pdf,
56,1986,Knowledge Representation,A Four-Valued Semantics for Frame-Based Description Languages,Peter F. Patel-Schneider,"One severe problem in frame-based description languages is that computing subsumption in computationally intractable for languages of reasonable expressive power. Several partial solutions to this problem are used in knowledge representation systems that incorporate such languages, but none of these solutions are satisfactory if the system is to be of general use in representing knowledge. A new solution to this problem is to use a weaker, four-valued semantics for frame-based description languages, thus legitimizing a smaller set of subsumption relationships. In this way a computationally tractable but expressively powerful knowledge representation system incorporating a frame-based description language can be built.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-057.pdf,
57,1986,Knowledge Representation,On the Parallel Complexity of Some Constraint Satisfaction Problems,Simon Kasif,"Constraint satisfaction networks have been shown to be a very useful tool for knowledge representation in Artificial Intelligence applications. These networks often utilize local constraint propagation techniques to achieve global consistency (consistent labelling in vision). Such methods have been used extensively in the context of image understanding and interpretation, as well as planning, natural language analysis and commonsense reasoning. In this paper we study the parallel complexity of discrete relaxation, one of the most commonly used constraint satisfaction techniques. Since the constraint propagation procedures such as discrete relaxation appear to operate locally, it has been previously believed that the relaxation approach for achieving global consistency has a natural parallel solution. Our analysis suggests that a parallel solution is unlikely to improve by much the known sequential solutions. Specifically, we prove that the problem solved by discrete relaxation is log-space complete for P (the class of polynomial time deterministic sequential algorithms). Intuitively, this implies that discrete relaxation is inherently sequential and it is unlikely that we can solve the polynomial time version of the consistent labelling problem in logarithmic time by using only a polynomial number of processors. Some practical implications of our result are discussed.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-058.pdf,
58,1986,Knowledge Representation,Primitives and Units for Time Specification,Peter Ladkin,"We work in a calculus of intervals, formulated by James Allen for convex intervals, and by ourselves for unions of convex intervals [All2,Lad2]. We investigate the primitive relations and operations needed for implementing such calculi in a system which includes some set theory, and which allows the assertional definition of operators in Horn clause fashion. We indicate how standard temporal logic may be rephrased in the interval calculus, and present a formalisation of a system of time units in the interval framework. We are implementing the primitives in the REFINETM system.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-059.pdf,
59,1986,Knowledge Representation,Time Representation: A Taxonomy of Internal Relations,Peter Ladkin,"James Allen in [All2] formulated a calculus of convex time intervals, which is being applied to commonsense reasoning by Allen, Pat Hayes, Henry Kautz and others [AllKau, AllHay]. For many purposes in AI, we need more general time intervals. We present a taxonomy of important binary relations between intervals which are unions of convex intervals, and we provide examples of these relations applied to the description of tasks and events. These relations appear to be necessary for such description. Finally, we provide logical definitions of a taxonomy of general binary relations between non-convex intervals.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-060.pdf,
60,1986,Knowledge Representation,A Representation for Collections of Temporal Intervals,"Bruce Leban, David McDonald, David Forster","Temporal representation and reasoning are necessary components of systems that consider events that occur in the real world. This work explores ways of considering collections of intervals of time. This line of research is motivated by related work being done by our research group on appointment scheduling and time management. Natural language expressions that refer to collections of intervals are used naturally and routinely in these contexts, and an effective means of representing them is essential. Previous studies, which considered intervals primarily in isolation, have difficulties in representing some classes of expressions. This occurs not only with expressions that explicitly refer to collections of intervals, such as ""the first of every month,"" but also with expressions that do so only implicitly, such as the U.S. Election Day: ""the first Tuesday after the first Monday in November."" The traditional solution to this problem has been to provide special means of specifying those forms that are judged to be the most useful (to the exclusion of all other forms). The ""collection representation"" builds on previous work in temporal representation by introducing operators that allow the representation of collections of intervals, whether they occur explicitly or implicitly in the expression. The operators introduced are natural extensions of the relations and operations on intervals. The representation has potential use in scheduling in three areas: graphical display, natural language translation, and reasoning.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-061.pdf,
61,1986,Knowledge Representation,A Representation for Temporal Sequence and Duration in Massively Parallel Networks,Hon Wai Chun,"One of the major representational problems in massively parallel or connectionist models is the difficulty of representing temporal constraints. Temporal constraints are important and crucial sources of information for event perception in general. This paper describes a novel scheme which provides massively parallel models with the ability to represent and recognize temporal constraints such as sequence and duration by exploiting link to link interactions. This relatively unexplored yet powerful mechanism is used to represent rule-like constraints and behaviors. The temporal sequence of a set of nodes is defined as the constraints or the temporal context, in which these nodes should be activated. This representation is quite robust in the sense that it captures subtleties in both the strength and scope (order) of temporal constraints. Duration is also represented using a similar mechanism. The duration of a concept is represented as a memory trace of the activation of this concept. The state of this trace can be used to generate a fuzzy set like classification of the duration.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-062.pdf,
62,1986,Knowledge Representation,Constraint Propagation Algorithms for Temporal Reasoning,"Marc Vilain, Henry Kautz","This paper considers computational aspects of several temporal representation languages. It investigates an interval-based representation, and a point-based one. Computing the consequences of temporal assertions is shown to be computationally intractable in the interval-based representation, but not in the point-based one. However, a fragment of the interval language can be expressed using the point language and benefits from the tractability of the latter.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-063.pdf,
63,1986,Knowledge Representation,Propagating Temporal Constraints for Scheduling,Jean-Francois Rit,"We give in this article a general frame for propagating temporal constraints over events, these events being exclusively considered as sets of possible occurrences (SOPOs). These SOPOs are the numerical expression of an uncertainty about the exact occurrence of an event, while this exact occurrence is constrained by the possible occurrence of other events. This key-problem of scheduling is an instance of the consistent labeling problem and is known to be NP-complete. We introduce a graphical representation of SOPOs which is a useful tool to understand and help solving the problem. We give a constraint propagation algorithm which is a Waltz-type filtering algorithm. Theoretically, it does not discard all the inconsistent occurrences; however, under a number of relatively weak assumptions, the problem can be transformed into a solvable one.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-064.pdf,
64,1986,Knowledge Representation,"Chronological Ignorance: Time, Nonmonotonicity, Necessity, and Causal Theories",Yoav Shoham,"Concerned with the problem of reasoning efficiently about change within a formal system, we identify the initiation problem. The solution to it which we offer, called the logic of chronological ignorance, combines temporal logic, nonmonotonic logic, and the modal logic of necessity. We identify a class of theories, called causal theories, which have elegant model-theoretic and complexity properties in the new logic.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-065.pdf,
65,1986,Knowledge Representation,A Comparison of the Commonsense and Fixed Point Theories of Nonmonotonicity,Frank M. Brown,"The mathematical fixed point theories of nonmonotonic reasoning are examined and compared to a commonsense theory of nonmonotonic reasoning which models our intuitive ability to reason about defaults. It is shown that all of the known problems of the fixed point theories are solved by the commonsense theory. The concepts of this commonsense theory do not involve mathematical fixed points, but instead are explicitly defined in a monotonic modal quantificational logic which captures the modal notion of logical truth.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-066.pdf,
66,1986,Knowledge Representation,The Logic of Persistence,Henry A. Kautz,"A recent paper [Hanks1985] examines temporal reasoning as an example of default reasoning. They conclude that all current systems of default reasoning, including non-monotonic logic, default logic, and circumscription, are inadequate for reasoning about persistence. I present a way of representing persistence in a framework based on a generalization of circumscription, which captures Hanks and McDermott’s procedural representation.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-067.pdf,
67,1986,Knowledge Representation,Pointwise Circumscription: Preliminary Report,Vladimir Lifschitz,"Circumscription is the minimization of predicates subject to restrictions expressed by predicate formulas. We propose a modified notion of circumscription so that, instead of being a single minimality condition, it becomes an ""infinite conjunction"" of ""local"" minimality conditions; each of these conditions expresses the impossibility of changing the value of a predicate from true to false at one point. We argue that this ""pointwise"" circumscription is conceptually simpler than the traditional ""global"" approach and, at the same time, leads to generalizations with the additional flexibility needed in applications to the theory of commonsense reasoning.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-068.pdf,
68,1986,Knowledge Representation,A Viewpoint Distinction in the Representation of Propositional Attitudes,John A. Barnden,"A representation scheme can be used by a cognitive agent as a basis for its normal, inbuilt cognitive processes. Also, a representation scheme can serve as a means for describing cognitive agents, in particular their ""mental"" states. A scheme can serve this second function either when it is itself naturally used by a cognitive agent (that reasons about agents), or when it is merely an artificial, theoretical tool used by a researcher. In designing a representation scheme one must pay very careful attention to two related questions: the question of whether, for any given agent, the scheme is used by the agent or is used to describe the agent (or both); and the question of whether the scheme is being used as a theoretical tool as well as, perhaps, being used by agents). I show by example that representational pitfalls can be encountered when these questions are not clearly addressed. The examples revolve around Creary’s logic-based scheme and Maida and Shapiro’s semantic network scheme, both of which were designed primarily to facilitate the representation of propositional attitudes (beliefs, hopes, desires, etc.). However, the general points have wider application to schemes for propositional attitude representation. By appeal mainly to the Maida and Shapiro case I demonstrate also that it is possible to be misled by the ambiguity of whether ""to represent"" means ""to denote"" or ""to be an ambassador/representative/abstraction of"".",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-069.pdf,
69,1986,Knowledge Representation,"Self-Reference, Knowledge, Belief, and Modality",Donald Perlis,"An apparently negative result of Montague has diverted research in formal modalities away from syntactic (""first-order"") approaches, encouraging rather weak and semantically complex modal formalisms, especially in representing epistemic notions. We show that, Montague notwithstanding, consistent and straightforward first-order syntactic treatments of modality are possible, especially for belief and knowledge; that the usual modal treatments are on no firmer ground than first-order ones when endowed with self-reference; and that in the latter case there still are remedies.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-070.pdf,
70,1986,Knowledge Representation,Is Belief Revision Harder Than You Thought?,Marianne Winslett,"Suppose one wishes to construct, use, and maintain a knowledge base (KB) of beliefs about the real world, even though the facts about that world are only partially known. In the AI domain, this problem arises when an agent has a base set of beliefs that reflect partial knowledge about the world, and then tries to incorporate new, possibly contradictory knowledge into this set of beliefs. We choose to represent such a KB as a logical theory, and view the models of the theory as representing possible states of the world that are consistent with the agent’s beliefs. How can new information be incorporated into the KB? For example, given the new information that ""b or c is true,"" how can one get rid of all outdated information about b and c, add the new information, and yet in the process not disturb any other information in the KB? The burden may be placed on the user or other omniscient authority to determine exactly what to add and remove from the KB. But what' s really needed is a way to specify the desired change intensionally, by stating some well-formed formula that the state of the world is now known to satisfy and letting the KB algorithms automatically accomplish that change. This paper explores a technique for updating KBs containing incomplete extensional information. Our approach embeds the incomplete KB and the incoming information in the language of mathematical logic. We present semantics and algorithms for our operators, and discuss the computational complexity of the algorithms. We show that the incorporation of new information is difficult even without the problems associated with justification of prior conclusions and inferences and identification of outdated inference rules and axioms.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-071.pdf,
71,1986,Knowledge Representation,What Can Machines Know? On the Epistemic Properties of Machines,"Ronald Fagin, Joseph Halpern, Moshe Vardi","It has been argued that knowledge is a useful tool for designing and analyzing complex systems in AI. The notion of knowledge that seems most relevant in this context is an external, information-based notion that can be shown to satisfy all the axioms of the modal logic S5. We carefully examine the properties of this notion of knowledge, and show that they depend crucially, and in subtle ways, on assumptions we make about the system. We present a formal model in which we can capture the types of assumptions frequently made about systems (such as whether they are deterministic or nondeterministic, whether knowledge is cumulative, and whether or not the environment affects the transitions of the system). We then show that under some assumptions certain states of knowledge are not attainable, and the axioms of S5 do not completely characterize the properties of knowledge; extra axioms are needed. We provide complete axiomatizations for knowledge in a number of cases of interest.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-072.pdf,
72,1986,Learning,A Metalinguistic Approach to the Construction of Knowledge Base Refinement Systems,Allen Ginsberg,"A variety of approaches to knowledge base refinement [3, 8] and rule acquisition [4] have appeared recently. This paper is concerned with the means by which alternative refinement systems themselves may be specified, developed, and studied. The anticipated virtues of taking a metalinguistic approach to these tasks are described, and shown to be confirmed by experience with an actual refinement metalanguage, RM.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-073.pdf,
73,1986,Learning,Rule Refinement Using the Probabilistic Rule Generator,"Won D. Lee, Sylvian Ray","This work treats the case of expert-originated hypotheses which are to be modified or refined by training event data. The method accepts the hypotheses in the form of weighted VL expressions and uses the probabilistic rule generator, PRG. The theory of operation, verified by experimental results, provides for any degree of hypothesis modification, ranging from minor perturbation to complete replacement according to supplied confidence weightings.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-074.pdf,
74,1986,Learning,On Debugging Rule Sets When Reasoning Under Uncertainty,"David C. Wilkins, Bruce G. Buchanan","Heuristic inference rules with a measure of strength less than certainty have an unusual property: better individual rules do not necessarily lead to a better overall rule set. All less-than-certain rules contribute evidence towards erroneous conclusions for some problem instances, and the distribution of these erroneous conclusions over the instances is not necessarily related to individual rule quality. This has important consequences for automatic machine learning of rules, since rule selection is usually based on measures of quality of individual rules. In this paper, we explain why the most obvious and intuitively reasonable solution to this problem, incremental modification and deletion of rules responsible for wrong conclusions a la Teiresias, is not always appropriate. In our experience, it usually fails to converge to an optimal set of rules. Given a set of heuristic rules, we explain why the best rule set should be considered to be the element of the power set of rules that yields a global minimum error with respect to generating erroneous positive and negative conclusions. This selection process is modeled as a bipartite graph minimization problem and shown to be NP-complete. A solution method is described, the Antidote Algorithm, that performs a model-directed search of the rule space. On an example from medical diagnosis, the Antidote Algorithm significantly reduced the number of misdiagnoses when applied to a rule set generated from 104 training instances.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-075.pdf,
75,1986,Learning,Discovering Functional Formulas through Changing Representation Base,Mieczyslaw M. Kokar,"This paper deals with computer generation of numerical functional formulas describing results of scientific experiments (measurements). It describes the methodology for generating functional physical laws called COPER (Kokar 1985a). This method generates only so called ""meaningful functions"", i.e., such that fulfill some syntactic conditions. In the case of physical laws these conditions are described in the theory of dimensional analysis, which provides rules for grouping arguments of a function into a (smaller) number of dimensionless monomials. These monomials constitute new arguments for which a functional formula is generated. COPER takes advantage of the fact that the grouping is not unique since it depends on which of the initial arguments are chosen as so called ""dimensional base"" (representation base). For a given functional formula the final result depends on the base. In its search for a functional formula COPER first performs a search through different representation bases for a fixed form of the function before going into more complex functional formulas. It appears that for most of the physical laws only two classes of functional formulas - linear functions and second degree polynomials - need to be considered to generate a formula exactly matching the law under consideration.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-076.pdf,
76,1986,Learning,Selecting Appropriate Representations for Learning from Examples,"Nicholas S. Flann, Thomas G. Dietterich","The task of inductive learning from examples places constraints on the representation of training instances and concepts. These constraints are different from, and often incompatible with, the constraints placed on the representation by the performance task. This incompatibility explains why previous researchers have found it so difficult to construct good representations for inductive learning-they were trying to achieve a compromise between these two sets of constraints. To address this problem, we have developed a learning system that employs two different representations: one for learning and one for performance. The learning system accepts training instances in the ""performance representation,"" converts them into a ""learning representation"" where they are inductively generalized, and then maps the learned concept back into the ""performance representation."" The advantages of this approach are (a) many fewer training instances are required to learn the concept, (b) the biases of the learning program are very simple, and (c) the learning system requires virtually no ""vocabulary engineering"" to learn concepts in a new domain.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-077.pdf,
77,1986,Learning,Optimal Allocation of Very Limited Search Resources,David Mutchler,"This paper presents a probabilistic model for studying the question: given n search resources, where in the search tree should they be expended? Specifically, a least-cost root-to-leaf path is sought in a random tree. The tree is known to be binary and complete to depth N. Arc costs are independently set either to 1 (with probability p ) or to 0 (with probability 1-p ). The cost of a leaf is the sum of the arc costs on the path from the root to that leaf. The searcher (scout) can learn n arc values. How should these scarce resources be dynamically allocated to minimize the average cost of the leaf selected? A natural decision rule for the scout is to allocate resources to arcs that lie above leaves whose current expected cost is minimal. The bad-news theorem says that situations exist for which this rule is nonoptimal, no matter what the value of n. The good-news theorem counters this: for a large class of situations, the aforementioned rule is an optimal decision rule if p < 0.5 and within a constant of optimal if p > 0.5. This report discusses the lessons provided by these two theorems and presents the proof of the bad-news theorem.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-078.pdf,
78,1986,Learning,Inductive Inference by Refinement,P. D. Laird,"A model is presented for the class of inductive inference problems that are solved by refinement algorithms - that is, algorithms that modify a hypothesis by making it more general or more specific in response to examples. The separate effects of the syntax (rule space) and semantics, and the relevant orderings on these, are precisely specified. Relations called refinement operators are defined, one for generalization and one for specialization. General and particular properties of these relations are considered, and algorithm schemas for top-down and bottom-up inference are given. Finally, difficulties common to refinement algorithms are reviewed.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-079.pdf,
79,1986,Learning,Preliminary Steps Toward the Automation of Induction,Stuart Russell,"Rational inductive behaviour is strongly influenced by existing knowledge of the world. This paper begins to elucidate the formal relationship between the base-level induction to be attempted, the direct evidence for it (positive and negative instances) and the indirect evidence (higher-level regularities in the world). By constructing a program to search the space of forms of higher-level regularity WC discover some important new forms which have direct application to analogy, single-instance generalization and enumerative induction in general. We outline a theory which we hope is the first step towards the construction of powerful and robust learning systems.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-080.pdf,
80,1986,Learning,Quantifying the Inductive Bias in Concept Learning (extended abstract),David Haussler,"We show that the notion of bias in inductive concept learning can be quantified in a way that directly relates to learning performance, and that this quantitative theory of bias can provide guidance in the design of effective learning algorithms. We apply this idea by measuring some common language biases, including restriction to conjunctive concepts and conjunctive concepts with internal disjunction, and, guided by these measurements, develop learning algorithms for these classes of concepts that have provably good convergence properties.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-081.pdf,
81,1986,Learning,The FERMI System: Inducing Iterative Macro-Operators from Experience,"Patricia W. Cheng, Jaime G. Carbonell","Automated methods of exploiting past experience to reduce search vary from analogical transfer to chunking control knowledge. In the latter category, various forms of composing problem-solving operators into larger units have been explored. However, the automated formulation of effective macro-operators requires more than the storage and parametrization of individual linear operator sequences. This paper addresses the issue of acquiring conditional and iterative operators, presenting a concrete example implemented in the FERMI problem-solving system. In essence, the process combines empirical recognition of cyclic patterns in the problem-solving trace with analytic validation and subsequent formulation of general iterative rules. Such rules can prove extremely effective in reducing search beyond linear macro-operators produced by past techniques.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-082.pdf,
82,1986,Learning,A Case Study of Incremental Concept Induction,"Jeffrey C. Schlimmer, Douglas Fisher","Application of machine induction techniques in complex domains promises to push the computational limits of nonincremental, search intensive induction methods. Learning effectiveness in complex domains requires the development of incremental, cost effective methods. However, discussion of dimensions for comparing the utilily of differing incremental methods has been lacking. In this paper we introduce 3 dimensions for characterizing incremental concept induction systems which relate to the cost and quality of learning. The dimensions are used to compare the respective merits of 4 incremental variants of Quinlan’s learning from examples program, ID3. This comparison indicates that cost effective induction can be obtained, without significantly detracting from the quality of induced knowledge.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-083.pdf,
83,1986,Learning,Beyond Incremental Processing: Tracking Concept Drift,"Jeffrey Schlimmer, Richard Granger, Jr.","Learning in complex, changing environments requires methods that are able to tolerate noise (less than perfect feedback) and drift (concepts that change over time). These two aspects of complex environments interact with each other: when some particular learned predictor fails to correctly predict the expected outcome (or when the outcome occurs without having been preceded by the learned predictor), a learner must be able to determine whether the situation is an instance of noise or an indication that the concept is beginning to drift. We present a learning method that is able to learn complex Boolean characterizations while tolerating noise and drift. An analysis of the algorithm illustrates why it has these desirable behaviors, and empirical results from an implementation (called STAGGER) are presented to show its ability to track changing concepts over time.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-084.pdf,
84,1986,Learning,Conceptual Clustering Using Relational Information,Bernd Nordhausen,"Work in conceptual clustering has focused on creating cIasses from objects with a fixed set of features, such as color or size. In this paper we describe a system which uses relations between the objects being clustered as well as features of the objects to form a hierarchy tree of classes. Unlike previous conceptual clustering systems, this algorithm can define new attributes. Using relational information the system is able to find object classifications not possible with conventional conceptual clustering methods.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-085.pdf,
85,1986,Learning,Generating Predictions to Aid the Scientific Discovery Process,Randy Jones,"NGLAUBER is a system which models the scientific discovery of qualitative empirical laws. As such, it falls into the category of scientific discovery systems. However, NGLAUBER can also be viewed as a conceptual clustering system since it forms classes of objects and characterizes these classes. NGLAUBER differs from existing scientific discovery and conceptual clustering systems in a number of ways. I. It uses an incremental method to group objects into classes. 2. These classes are formed based on the relationships between objects rather than just the attributes of objects. 3 The system describes the relationships between classes rather than simply describing the classes. 4. Most importantly, NGLAUBER proposes experiments by predicting future data. The experiments help the system guide itself through the search for regularities in the data.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-086.pdf,
86,1986,Learning,Factorization in Experiment Generation,"Devika Subramanian, Joan Feigenbaum","Experiment generation is an important part of incremental concept learning. One basic function of experimentation is to gather data to refine the existing space of hypotheses[DB83]. Here we examine the class of experiments that accomplish this, called discrimination experiments, and propose factoring as a technique for generating them efficiently.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-087.pdf,
87,1986,Learning,A Case-Based Reasoning System for Subjective Assessment,William M. Bain,"People tend to improve their abilities to reason about situations by amassing experiences in reasoning. Resorting to previous instances of similar situations for guidance is known as case-based reasoning. This paper presents JUDGE, a computer model of judges who sentence criminals. The task is viewed as one in which people learn empirically from the process of producing relative assessments of input situations with respect to several concerns, with little external feedback. People can perform such subjective tasks by at least trying to keep their assessments consistent. For assessment tasks, this reasoning style involves comparing a previous similar situation with an input one, and then extracting an assessment for the new input, based on both the assessment previously assigned to the older example, and differences found between them. The system also stores input items to reflect their relationships to situations already contained in memory.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-088.pdf,
88,1986,Learning,STAHLp: Belief Revision in Scientific Discovery,"Donald Rose, Pat Langley","In this paper we describe the STAHLp system for inferring components of chemical substances - i.e., constructing componential models. STAHLp is a descendant of the STAHLp system (Zytkow and Simon, 1986); both use chemical reactions and any known models in order to construct new models. However, STAHLp employs a more unified and effective strategy for preventing, detecting, and recovering from erroneous inferences. This strategy is based partly upon the assumption-based method (de Kleer, 1984) of recording the source beliefs, or premises, which lead to each inferred belief (i.e., reaction or model). STAHL’s multiple methods for detecting and recovering from erroneous inferences have been reduced to one method in STAHLp, which can hypothesize faulty premises, revise them, and proceed to construct new models. The hypotheses made during belief revision can be viewed as interpretations from competing theories; how they are chosen thus determines how theories evolve after repeated revisions. We analyze this issue with an example involving the shift from phlogiston to oxygen theory.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-089.pdf,
89,1986,Learning,Not the Path to Perdition: The Utility of Similarity-Based Learning,Michael Lebowitz,"A large portion of the research in machine learning has involved a paradigm of comparing many examples and analyzing them in terms of similarities and differences, assuming that the resulting generalizations will have applicability to new examples. While such research has been very successful, it is by no means obvious why similarity-based generalizations should be useful, since they may simply reflect coincidences. Proponents of explanation-based learning, a new, knowledge-intensive method of examining single examples to derive generalizations based on underlying causal models, could contend that their methods are more fundamentally grounded, and that there is no need to look for similarities across examples. In this paper, we present the issues, and then show why similarity-based methods are important. We present four reasons why robust machine learning must involve the integration of similarity-based and explanation-based methods. We argue that: 1) it may not always be practical or even possible to determine a causal explanation; 2) similarity usually implies causality; 3) similarity-based generalizations can be refined over time; 4) similarity-based and explanation-based methods complement each other in important ways.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-090.pdf,
90,1986,Learning,Constructing and Refining Causal Explanations from an Inconsistent Domain Theory,Richard J. Doyle,"Recent work in the field of machine learning has demonstrated the utility of explanation formation as a guide to generalization. Most of these investigations have concentrated on the formation of explanations from consistent domain theories. I present an approach to forming explanations from domain theories which are inconsistent due to the presence of abstractions which suppress potentially relevant detail. In this approach, explanations are constructed to support reasoning tasks and are refined in a failure-driven manner. The elaboration of explanations is guided by the structuring of domain theories into layers of abstractions. This work is part of a larger effort to develop a causal modelling system which forms explanations of the underlying causal relations in physical systems. This system utilizes an inconsistent, common-sense theory of the mechanisms which operate in physical systems.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-091.pdf,
91,1986,Learning,The Role of Prior Causal Theories in Generalization,"Michael J. Pazzani, Michael Dyer, Margot Flowers","OCCAM is a program which organizes memories of events and learns by creating generalizations describing the reasons for the outcomes of the events. OCCAM integrates two sources of information when forming a generalization: 1) Correlational information which reveals perceived regularities in events. 2) Prior causal theories which explain regularities in events. The former has been extensively studied in machine learning. Recently, there has been interest in explanation-based learning in which the latter source of information is utilized. In OCCAM, prior causal theories are preferred to correlational information when forming generalizations. This strategy is supported by a number of empirical investigations. Generalization rules are used to suggest causal and intentional relational relationships. In familiar domains, these relationships are confirmed or denied by prior causal theories which differentiate the relevant and irrelevant features. In unfamiliar domains, the postulated causal and intentional relationships serve as a basis for the construction of causal theories.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-092.pdf,
92,1986,Learning,A Domain Independent Explanation-Based Generalizer,"Raymond J. Mooney, Scott Bennett","A domain independent technique for generalizing a broad class of explanations is described. This method is compared and contrasted with other approaches to generalizing explanations, including an abstract version of the algorithm used in the STRIPS system and the EBG technique recently developed by Mitchell, Keller, and Kedar-Cabelli. We have tested this generalization technique on a number examples in different domains, and present detailed descriptions of several of these.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-093.pdf,
93,1986,Learning,Learning to Anticipate and Avoid Planning Problems through the Explanation of Failures,Kristian Hammond,"This paper presents an approach to learning during planning that focuses on learning to predict planning problems through an analysis of the planner’s own failures. The need to predict failures in order to avoid them is argued and a method for learning the features that predict problems from a causal analysis of planning failures is discussed. A further argument is also given concerning the natural integration of this approach to learning with an overall theory of case-based planning. An implementation of these learning ideas is presented in the case-based planner CHEF, which creates new plans from old in the domain of Szechwan cooking. The CHEF planner uses an anticipate and avoid approach to planning problems that is sharply contrasted with the create and debug approach taken by existing planners.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-094.pdf,
94,1986,Learning,Mapping Explanation-Based Generalization onto Soar,"Paul Rosenbloom, John Laird","Explanation-based generalization (EBG) is a powerful approach to concept formation in which a justifiable concept definition is acquired from a single training example and an underlying theory of how the example is an instance of the concept. Soar is an attempt to build a general cognitive architecture combining general learning, problem solving, and memory capabilities. It includes an independently developed learning mechanism, called chunking, that is similar to but not the same as explanation-based generalization. In this article we clarify the relationship between the explanation-based generalization framework and the Soar/chunking combination by showing how the EBG framework maps onto Soar, how several EBG concept-formation tasks are implemented in Soar, and how the Soar approach suggests answers to some of the outstanding issues in explanation-based generalization.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-095.pdf,
95,1986,Learning,Learning by Failing to Explain,Robert J. Hall,"Explanation-based Generalization depends on having an explanation on which to base generalization. Thus, a system with an incomplete or intractable explanatory mechanism will not be able to generalize some examples. It is not necessary, in those cases, to give up and resort to purely empirical generalization methods, because the system may already know almost everything it needs to explain the precedent. Learning by Failing to Explain is a method which exploits current knowledge to prune complex precedents and rules, isolating their mysterious parts. This paper describes two techniques for Learning by Failing to Explain: Precedent Analysis, partial analysis of a precedent or rule to isolate the mysterious new technique(s) it embodies; and Rule Re-analysis, re-analyzing old rules in terms of new rules to obtain a more general set.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-096.pdf,
96,1986,Natural Language,Dynamically Combining Syntax and Semantics in Natural Language Processing,Steven Lytinen,"A controversy has existed over the interaction of syntax and semantics in natural language understanding systems. According to theories of integrated parsing, syntactic and semantic processing should take place simultaneously, with the parsing process driven by a single rule base which contains both syntactic and semantic knowledge. This is in sharp contrast to traditional linguistic approaches to language analysis, in which syntactic and semantic processing are performed separately from one another, driven by completely separate sets of syntactic and semantic rules. This paper presents an approach to natural language understanding which is a compromise between these two views. It is an integrated approach, in the sense that syntactic and semantic processing take place at the same time. However, unlike previous integrated systems, the approach described here uses largely separate bodies of syntactic and semantic knowledge, which are combined only at the time of processing.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-097.pdf,
97,1986,Natural Language,Dual Frames: A New Tool for Semantic Parsing,"Jean-Louis Binot, Daniel Ribbens","The dual frames method is a new tool for specifying and establishing semantic dependencies, which has been implemented in a parser of French called SABA. This method offers solutions to some typical problems of semantic parsing strategies - such as the difficulty of coping with different types of sentence structures and the amount of work needed to specify the vocabulary of a new domain - by providing a general and flexible tool which can handle all the kinds of meaningful terms which can appear in a sentence.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-098.pdf,
98,1986,Natural Language,A Neat Theory of Marker Passing,Eugene Charniak,"We describe here the theory behind the language comprehension program Wimp. Wimp understands by first finding paths between the open-class words in a sentence using a marker passing, or spreading-activation, technique. This paper is primarily concerned with the ""meaning"" (or interpretation) of such paths. We argue that they are best thought of as backbones of proofs that the terms (words) at either end of the paths exist in the story and show how viewing paths in this way naturally leads to the kinds of inferences which are normally thought to characterize ""understanding."" In a companion paper we show how this interpretation also accomplishes much of the work normally expected in the parsing of language (noun-phrase reference, word-sense disambiguation, etc) so we only briefly touch on this topic here. Wimp has been implemented and works on all of the examples herein.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-099.pdf,
99,1986,Natural Language,Using Commonsense Knowledge to Disambiguate Prepositional Phrase Modifiers,"Kathleen Dahlgren, J. McDowell","This paper describes a method using commonsense knowledge for discarding spurious syntactic ambiguities introduced by post-verbal prepositional phrase attachment during parsing. A completely naive parser will generate three parses for sentences of the form NP-V-Det-N-PP. The prepositions alone are insufficiently precise in meaning to guide selection among competing parses. The method is imbedded in the Kind Types System (KT) which employs commonsense knowledge of concepts, including prototype and inherent features (generic information) and ontological classifications. The generic information is drawn from published psycholinguistic studies on how average people typically view the world. This method is employed in preference strategies which appeal to the meaning of the preposition combined with information about the verbs and nouns associated with it drawn from the text and from the generic and ontological databases. These determine which syntactic structures generated by a semantically naive parser are commonsensically plausible. The method was successful in 93% of cases tested.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-100.pdf,
100,1986,Natural Language,Beyond Exploratory Programming: A Methodology and Environment for Conceptual Natural Language Processing,"Philip Johnson, Wendy Lehnert","This paper presents an attempt to synthesize a methodology and environment which has features both of traditional software development methodologies and exploratory programming environments. The environment aids the development of conceptual natural language analyzers, a problem area where neither of these approaches alone adequately supports the construction of modifiable and maintainable systems. The paper describes problems with traditional approaches, the new ""parallel"" development methodology, and its supporting environment, called the PLUMber’s Apprentice.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-101.pdf,
101,1986,Natural Language,Are There Preference Trade-offs in Attachment Decisions?,Lenhart K. Schubert,"The paper argues for an affirmative answer to the question, against the view that correct attachment decisions can be made by a serial process that considers alternatives in some order and accepts the first ""satisfactory"" alternative. The pitfall in serial strategies is that they are apt to finalize their choice while ""the best is yet to come"".",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-102.pdf,
102,1986,Natural Language,Comprehension-Driven Generation of Meta-Technical Utterances in Math Tutoring,"Ingrid Zukerman, Judea Pearl","A technical discussion often contains conversational expressions like ""however,"" ""as I have stated before,"" ""next,"" etc. These expressions, denoted Meta-technical Utterances (MTUs) carry important information which the listener uses to speed up the comprehension process. In this research we model the meaning of MTUs in terms of their anticipated effect on the listener comprehension, and use these predictions to select MTUs and weave them into a computer generated discourse. This paradigm was implemented in a system called FIGMENT, which generates commentaries on the solution of algebraic equations.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-103.pdf,
103,1986,Natural Language,A Logical-Form and Knowledge-Base Design for Natural Language Generation,"Norman K. Sondheimer, Bernhard Nebel","This paper presents a technique for interpreting output demands by a natural language sentence generator in a formally transparent and efficient way. These demands are stated in a logical language. A network knowledge base organizes the concepts of the application domain into categories known to the generator. The logical expressions are interpreted by the generator using the knowledge base and a restricted, but efficient, hybrid knowledge representation system. This design has been used to allow the NIGEL generator to interpret statements in a first-order predicate calculus using the NIKL and KL-TWO knowledge representation systems. The success of this experiment has led to plans for the inclusion of this design in both the evolving Penman natural generator and the Janus natural language interface.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-104.pdf,
104,1986,Natural Language,Understanding Plan Ellipsis,Diane J. Litman,"This paper presents an extended and unified approach to the interpretation of sentence fragments and elliptical utterances within the context of a plan-based theory of dialogue understanding. The approach integrates knowledge about plans and knowledge about discourse, enabling the treatment of a variety of difficult linguistic phenomena within a single framework while maintaining the computational advantages of the plan-based approach.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-105.pdf,
105,1986,Perception and Robotics,A Simple Motion Planning Algorithm for General Robot Manipulators,Tomas Lozano-Perez,"This paper presents a simple and efficient algorithm, using configuration space, to plan collision-free motions for general manipulators. We describe an implementation of the algorithm for manipulators made up of revolute joints. The configuration-space obstacles for an n degree-of-freedom manipulator are approximated by sets of n-1 dimensional slices, recursively built up from one dimensional slices. This obstacle representation leads to an efficient approximation of the free space outside of the configuration-space obstacles.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-106.pdf,
106,1986,Perception and Robotics,Tactile Recognition by Probing: Identifying a Polygon on a Plane,"R. E. Ellis, Edward M. Riseman, Allen R. Hanson","An outstanding problem in model-based recognition of objects by robot systems is how the system should proceed when the acquired data are insufficient to identify uniquely the model instance and model pose that best interpret the object. In this paper, we consider the situation in which some tactile data about the object are already available, but can be ambiguously interpreted. The problem is thus to acquire and process new tactile data in a sequential and eflicient manner, so that the object can be recognised and its location and orientation determined. An object model, in this initial analysis of the problem, is a polygon located on a plane; the case of planar objects presents some interesting problems, and is also an important prelude to recognition of three-dimensional (polyhedral) objects.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-107.pdf,
107,1986,Perception and Robotics,Abstraction and Representation of Continuous Variables in Connectionist Networks,Eric Saund,"A method is presented for using connectionist networks of simple computing elements to discover a particular type of constraint in multidimensional data. Suppose that some data source provides samples consisting of n-dimensional feature-vectors, but that this data all happens to lie on an m-dimensional surface embedded in the n-dimensional feature space. Then occurrences of data can be more concisely described by specifying an m-dimensional location on the embedded surface than by reciting all n components of the feature vector. The recoding of data in such a way is a form of abstraction. This paper describes a method for performing this type of abstraction in connectionist networks of simple computing elements. We present a scheme for representing the values of continuous (scalar) variables in subsets of units. The backpropagation weight updating method for training connectionist networks is extended by the use of auxiliary pressure in order to coax hidden units into the prescribed representation for scalar-valued variables.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-108.pdf,
108,1986,Perception and Robotics,SIMD Tree Algorithms for Image Correlation,"Hussein A. Ibrahim, John R. Kender, David Elliot Shaw","This paper examines the applicability of fine-grained ""pure"" tree SIMD machines, which are amenable to highly efficient VLSI implementation, to image correlation which is a representative of low-level image window- based operations. A particular massively parallel machine called NON-VON is used for purposes of explication and performance evaluation. Several algorithms are presented for image shifting and correlation operations. Novel algorithmic techniques are described, such as vertical pipelining, subproblem partitioning, associative matching, and data duplication that effectively exploit the massive parallelism available in fine-grained SIMD tree machines. Limitations of SIMD pure tree machines are also addressed. They tend to correspond to situations in which the root of the tree may become a significant communication bottleneck, or in situations in which MIMD techniques would be more effective than the SIMD approaches considered in this paper. Performance results have been projected for the NON-VON machine (using only its tree connections, in order to address the issues of concern in this paper).",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-109.pdf,
109,1986,Perception and Robotics,On the Reconstruction of a Scene from Two Unregistered Images,Harit Trivedi,"It is sometimes desirable to compute depth from unregistered pairs of images. I show that it is possible to calculate the two 'epicentres’ and the relation governing pairs of epipolar lines, given 8 corresponding points in the two images in any coordinate system. This reduces the matching problem to one dimensional searches along pairs of epipolar lines and can be readily automated using any stereo algorithm. Depth, however, does not seem to be derivable without extra information. I show how to compute depth in two such instances, each involving two 'pieces’ of information.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-110.pdf,
110,1986,Perception and Robotics,Depth and Flow From Motion Energy,David J. Heeger,"This paper presents a model of motion perception that utilizes the output of motion-sensitive spatiotemporal filters. The power spectrum of a moving texture occupies a tilted plane in the spatiotemporal-frequency domain. The model uses 3-D (space-time) Gabor filters to sample this power spectrum. By combining the outputs of several such filters, the model estimates the velocity of the moving texture - without first computing component (or normal) velocity. A parallel implementation of the model encodes velocity as the peak in a distribution of velocity-sensitive units. For a fixed 3-D rigid-body motion, depth values parameterize a line through image-velocity space. The model estimates depth by finding the peak in the distribution of velocity-sensitive units lying along this line. In this way, depth and velocity are simultaneously extracted.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-111.pdf,
111,1986,Perception and Robotics,Shape from Darkness: Deriving Surface Information from Dynamic Shadows,"John R. Kender, Earl M. Smith","We present a new method, shape from darkness, for extracting surface shape information based on object self-shadowing under moving light sources. It is motivated by the problem of human perception of fractal textures under perspective. One-dimensional dynamic shadows are analyzed in the continuous case, and their behavior is categorized into three exhaustive shadow classes. The continuous problem is shown to be solved by the integration of ordinary differential equations, using information captured in a new image representation called the suntrace. The discretization of the one-dimensional problem introduces uncertainty in the discrete suntrace; however it is successfully recast as the satisfaction of 8n constraint equations in 2n unknowns. A form of relaxation appears to quickly converge these constraints to accurate surface reconstructions; we give several examples on simulated images. The shape from darkness method has two advantages: it does not require a reflectance map, and it works on non-smooth surfaces. We conclude with a discussion on the method’s accuracy and practicality, its relation to human perception, and its future extensions.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-112.pdf,
112,1986,Perception and Robotics,3-D Motion Recovery From Time-Varying Optical Flows,"Kwangyoen Wohn, Jian Wu","Previous research on analyzing time-varying image sequences has concentrated on finding the necessary (and sufficient) conditions for a unique 3-D solution. While such an approach provides useful theoretical insight, the resulting algorithms turn out to be too sensitive to be of pratical use. We claim that any robust algorithm must improve the 3-D solution adaptively over time. As the first step toward such a paradigm, in this paper we present an algorithm for 3-D motion computation, given time-varying optical flow fields. The surface of the object in the scene is assumed to be locally planar. It is also assumed that 3-D velocity vectors are piecewise constant over three consecutive frames (or 2 snapshots of flow field). Our formulation relates 3-D motion and object geometry with the optical flow vector as well as its spatial and temporal derivatives. The deformation parameters of the first kind, or equivalently, the first-order flow approximation (in space and time) is sufficient to recover rigid body motion and local surface structure from the local instantaneous flow field. We also demonstrate, through a sensitivity analysis carried out for synthetic and natural motions in space, that 3-D inference can be made reliably.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-113.pdf,
113,1986,Perception and Robotics,A Stochastic Approach to Stereo Vision,Stephen T. Barnard,"A stochastic optimization approach to stereo matching is presented. Unlike conventional correlation matching and feature matching, the approach provides a dense array of disparities, eliminating the need for interpolation. First, the stereo matching problem is defined in terms of finding a disparity map that satisfies two competing constraints: (1) matched points should have similar image intensity, and (2) the disparity map should be smooth. These constraints are expressed in an ""energy"" function that can be evaluated locally. A simulated annealing algorithm is used to find a disparity map that has very low energy (i.e., in which both constraints have simultaneously been approximately satisfied). Annealing allows the large-scale structure of the disparity map to emerge at higher temperatures, and avoids the problem of converging too quickly on a local minimum. Results are shown for a sparse random-dot stereogram, a vertical aerial stereogram (shown in comparison to ground truth), and an oblique ground-level scene with occlusion boundaries.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-114.pdf,
114,1986,Perception and Robotics,Determining the 3-D Motion of a Rigid Surface Patch Without Correspondence under Perspective Projection,"John Aloimonos, Isidore Rigoutsos","A method is presented for the recovery of the 3-D motion parameters of a rigidly moving textured surface. The novelty of the method is based on the following two facts : 1) no point-to-point correspondences are used, and 2) ""stereo"" and ""motion"" are combined in such a way that no correspondence between the left and the right stereo pairs is required.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-115.pdf,
115,1986,Perception and Robotics,A Stereo Integral Equation,Grahame B. Smith,"A new approach to the formulation and solution of the problem of recovering scene topography from a stereo image pair is presented. The approach circumvents the need to solve the correspondence problem, returning a solution that makes surface interpolation unnecessary. The methodology demonstrates a way of handling image analysis problems that differs from the usual linear-system approach. We exploit the use of nonlinear functions of local image measurements to constrain and infer global solutions that must be consistent with such measurements. Because the solution techniques we present entail certain computational difficulties, significant work still lies ahead before they can be routinely applied to image analysis tasks.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-116.pdf,
116,1986,Perception and Robotics,Parts: Structured Descriptions of Shape,Alex P. Pentland,"A shape representation is presented that has been shown competent to accurately describe an extensive variety of natural forms (e g., people, mountains, clouds, trees), as well as man-made forms, in a succinct and natural manner. The approach taken in this representational system is to describe scene structure at a scale that is similar to our naive perceptual notion of ""a part,' by use of descriptions that reflect a possible formative history of the object, e.g., how the object might have been constructed from lumps of clay. For this representation to be useful it must be possible to recover such descriptions from image data; we show that the primitive elements of this representation may be recovered in an overconstrained and therefore potentially reliable manner.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-117.pdf,
117,1986,Perception and Robotics,Constraint-Theorems on the Prototypification of Shape,Michael Leyton,"Mathematical results are presented that strongly constrain the prototypification of complex shape. Such shape requires local prototypification in two senses: (1) prototypification occurs in parallel at different parts of the figure, and (2) prototypification varies differentially (smoothly) across an individual part. With respect to (1), we present a theorem that states that every Hoffman-Richards codon has a unique Brady Smooth Local Symmetry. The theorem solves the issue of defining units for parallel decomposition, for it implies that a codon is the minimal unit with respect to the existence of prototypification via symmetry, and is maximal with respect to prototypification via non-ambiguous symmetry. Concerning issue (2) above, a further theorem is offered that severely limits the possible shapes that result from the sequential application of prototypifying operations to smoothly varying deformation. This second result explains why considerably fewer prototype classes exist than one would otherwise expect.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-118.pdf,
118,1986,Perception and Robotics,Linear Image Features in Stereopsis,Michael Kass,"Most proposed algorithms for solving the stereo correspondence problem have used matching based in some way on linear image features. Here the geometric effect of a change in viewing position on the output of a linear filter is modeled. A simple local computation is shown to provide confidence intervals for the difference between filter outputs at corresponding points. Examples of the use of the confidence interval are provided. For some widely used filters, the confidence intervals are tightest at isolated vertical step edges, lending support to the idea of using edge-like features in stereopsis. However, the same conclusion does not apply to image regions with more complicated variation on the scale of the filter support.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-119.pdf,
119,1986,Perception and Robotics,Signal Matching Through Scale Space,"Andrew Witkin, Demetri Terzopoulos, Michael Kass","Given a collection of similar signals that have been deformed with respect to each other, the general signal matching problem is to recover the deformation. We formulate the problem as the minimieation of an energy measure that combines a smoothness term and a similarity term. The minimieation reduces to a dynamic system governed by a set of coupled, first-order differential equations. The dynamic system finds an optimal solution at a coarse scale and then tracks it continuouslv to a fine scale. Among the major themes in recent work on visual signal matching have been the notions of matching as constrained optimization, of variational surface reconstruction, and of coarse-to-fine matching. Our solution captures these in a precise, succinct, and unified form. Results are presented for one-dimensional signals, a motion sequence, and a stereo pair.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-120.pdf,
120,1986,AI and Education,Teaching a Complex Industrial Process,"Beverly Woolf, Darrell Blegen, Johan Jansen, Arie Verloop","Computer training for industry is often not capable of providing advice custom-tailored for a specific student and a specific learning situation. In this paper we describe an intelligent computer-aided system that provides multiple explanations and tutoring facilities tempered to the individual student in an industrial setting. The tutor is based on a mathematically accurate formulation of the kraft recovery boiler and provides an interactive simulation complete with help, hints, explanations, and tutoring. The approach is extensible to a wide variety of engineering and industrial problems in which the goal is to train an operator to control a complex system and to solve difficult ""real time"" emergencies.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-121.pdf,
121,1986,AI Language and Architectures,The Butterfly(TM) Lisp System,"Seth Steinberg, Don Allen, Laura Bagnall, Curtis Scott","This paper describes the Common Lisp system that BBN is developing for its Butterfly multiprocessor. The BBN Butterfly is a shared memory multiprocessor which may contain up to 256 processor nodes. The system provides a shared heap, parallel garbage collector, and window based I/O system. The future construct is used to specify parallelism.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-122.pdf,
122,1986,AI Language and Architectures,CIS: A Massively Concurrent Rule-Based System,Guy E. Blelloch,"Recently researchers have suggested several computational models in which, one programs by specifying large networks of simple devices. Such models are interesting because they go to the roots of concurrency - the circuit level. A problem with the models is that it is unclear how to program large systems and expensive to implement many features that are taken for granted m symbolic programming languages. This paper describes the Concurrent Inference System (CIS), and its implementation on a massively concurrent network model of computation. It shows how much of the functionality of current rule-based systems can be implemented in a straightforward manner within such models. Unlike conventional implementations of rule-based systems in which the inference engine and rule sets are clearly divided at run time, CIS compiles the rules into a large static concurrent network of very simple devices. In this network the rules and inference engine are no longer distinct. The Thinking Machines Corporation, Connection Machine - a 65,536 processor SIMD computer - is then used to run the network. On the current implementation, real time user system interaction is possible with up to 100,000 rules.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-123.pdf,
123,1986,AI Language and Architectures,A Software and Hardware Environment for Developing AI Applications on Parallel Processors,R. Bisiani,"This paper describes and reports on the use of an environment, called Agora, that supports the construction of large, computationally expensive and loosely-structured systems, e.g. knowledge-based systems for speech and vision understanding. Agora can be customized to support the programming model that is more suitable for a given application. Agora has been designed explicitly to support multiple languages and highly parallel computations. Systems built with Agora can be executed on a number of general purpose and custom multiprocessor architectures.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-124.pdf,
124,1986,AI Language and Architectures,Connection Machine Stereomatching,Michael Drumheller,"This paper describes a parallel real-time stereomatching algorithm and its implementation on the Connection Machine computer, a new massively parallel computing system. The main features of the algorithm are 1) real-time performance, 2) the full exploitation of the ordering constraint, 3) a representation that easily maps onto a parallel computer architecture, and 4) the ability to efficiently use a variety of matching primitives. Some results, including timings, are shown for both real and synthetic data. Also discussed are the use of color information and some subtle variations of the basic algorithm.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-125.pdf,
125,1986,AI Language and Architectures,Merging Objects and Logic Programming: Relational Semantics,Herve Gallaire,"This paper proposes new semantics for merging object programming into logic programming. It differs from previous attempts in that it takes a relational view of method evaluation and inheritance mechanisms originating from object programming. A tight integration is presented, an extended rationale for adopting a success/failure semantics of backtrackable methods calls and for authorizing variable object calls is given. New method types dealing with non monotonicity and determinism necessary for this tight integration are discussed. The need for higher functions is justified from a user point of view. as well as from an implementation one. The system POL is only a piece of a more ambitious goal which is to merge logic programming, object programming and semantic data models which can be seen as an attempt to bridge the gap between AI and databases. The paper is restricted to a programming perspective.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-126.pdf,
126,1986,AI Language and Architectures,Domains in Logic Programming,"P. Van Hentenryck, M. Dincbas","When confronted with constraint satisfaction problems (CSP), the ""generate b test"" strategy of Prolog is particularly inefficient. Also, control mechanisms defined for logic programming languages fall short in CSP because of their restricted use of constraints. Indeed, constraints are used passiveIy for testing generated values and not for actively pruning the search space by eliminating combinations of values which cannot appear together in a solution. One remedy is to introduce the domain concept in logic programming language. This allows for an active use of constraints. This extension which does not impede the declarative (logic) reading of logic languages, consists in a modification of the unification, the redefinition of the procedural semantics of some built-in predicates ( # , < , < , > , >) and a new evaluable function and can be implemented efficiently. Without any change to the search procedure and without introducing a new control mechanism, look ahead strategies, more intelligent choices and consistency techniques can be implemented naturally in programs. Moreover, when combined with a delay mechanism, this leads directly to a strategy which applies active constraints as soon as possible.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-127.pdf,
127,1986,AI Language and Architectures,"Comments on Kornfeld’s ""Equality for Prolog"": E-Unification as a Mechanism for Augmenting the Prolog Search Strategy","E. W. Elcock, O. Hoddinott","The search strategy of standard Prolog can lead to a situation in which a predicate has to be evaluated in circumstances where it has an infeasibly large number of instantiations. The work by Kornfeld [8] addressed this important problem by means of an extension of unification which allows Prolog to be augmented by what is essentially a (non-standard) equality theory. This paper uses the notion of the general procedure introduced by van Emden and Lloyd [12] to formalize Kornfeld’s work. In particular, the formalization is used to make a careful analysis and evaluation of Kornfeld’s solution to the problem of delayed evaluation.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-128.pdf,
128,1986,Applications,Saturn: An Automatic Test Generation System for Digital Circuits,Narinder Singh,"This paper describes a novel test generation system, called Saturn, for testing digital circuits. The system differs from existing test generation systems in that it allows a designer to specify the structure and behavior of a design at a collection of abstraction levels that mirror the design refinement process. The system exploits the abstract design formulations to increase the efficiency of test generation by ignoring irrelevant detail whenever possible. These capabilities are made possible by using general representation and reasoning methods based on logic, which provide a declarative representation of a design, and permit using a single inference procedure for reasoning both forwards and backwards through the design for test generation.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-129.pdf,
129,1986,Applications,An Expert System for Chorale Harmonization,Kemal Ebcioglu,"We have designed an expert system called CHORAL, for harmonizing four-part chorales in the style of J.S. Bach. The system contains over 270 rules, expressed in a form of first order predicate calculus, for representing the knowledge required for harmonizing a given melody. The rules observe the chorale from multiple viewpoints, such as the chord skeleton, individual melodic lines of each voice, and the Schenkerian voice leading within the descant and bass. The program harmonizes chorales using a generate-and-test method with intelligent backtracking. A substantial number of heuristics are used for biasing the search toward musical solutions. Examples of program output are given in the paper. BSL, a new and efficient logic programming language which is fundamentally different from Prolog, was designed to implement the CHORAL system.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-130.pdf,
130,1986,Applications,A Rule-Based System for Document Understanding,"Debashish Niyogi, Sargur N. Srihari","A rule-based system to make inferences about document images is introduced. Given a digitized document image, the system controls the analysis of the document, and identifies all the different printed regions in the document image. Logical ""blocks"" of information on the document image are interpreted and classified by this system which then produces as output an editable description of the entire document. The system uses a goal-directed top down appoach, and utilizes a three-level rule hierarchy to implement its control strategy.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-131.pdf,
131,1986,Applications,Qualitative Simulation of Semiconductor Fabrication,"John Mohammed, Reid Simmons","As part of a larger effort aimed at providing symbolic, computer-aided tools for semiconductor fabrication experts, we have developed qualitative models of the operations performed during semiconductor manufacture. By qualitatively simulating a sequence of these models we generate a description of how a wafer is affected by the operations. This description encodes the entire history of processing for the wafer and causally relates the attributes that describe the structures on the wafer to the processing operations responsible for creating those structures. These causal relationships can be used to support many reasoning tasks in the semiconductor fabrication domain, including synthesis of new recipes, and diagnosis of failures in operating fabrication lines.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-132.pdf,
132,1986,Applications,Knowledge-Based Simulation of a Glass Annealing Process: An AI Application in the Glass Industry,"Richard A. Herrod, Jeff Rickel","This paper describes a knowledge-based simulation system for a glass annealing process. The long ovens, known as lehrs, in which annealing takes place are not well understood by their operators. In fact, only a few experts can predict the effects of a change in the lehr controls. Attempts to simulate the behavior of the lehr using conventional methods have not been successful due to the size and complexity of the lehr. Our knowledge-based approach is capable of both simulating the glass temperature curve in an annealing lehr and planning the necessary lehr control settings to achieve a desired curve. It consists of two cooperating expert systems, one rule-based and the other frame-based. The system also includes a high-bandwidth graphics display which allows operators to interactively test control-setting changes and ask for the control settings which meet desired specifications. A description of the domain, a history of the development, and details of the design are all presented, along with lessons learned from the experience.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-133.pdf,
133,1986,Applications,Plan Recognition for Airborne Tactical Decision Making,"Jerome Azarewicz, Glenn Fala, Ralph Fink, Christof Heithecker","Airborne tactical decision making is degraded as a result of sophisticated threat capabilities, high data rates and uncertainties, and the necessity for timely response. Under investigation at the Naval Air Development Center is the concept of a plan recognition model to assist the tactical decision maker in interpreting and predicting the activities of enemy platforms.* On-going work in the field of plan recognition was surveyed, knowledge acquisition conducted, and a prototype plan recognition model has emerged. The model is a hierarchical, blackboard based adaptation of a more general architecture of cognition. The model attempts to overcome some of the perceived shortfalls of other approaches relative to the complexities of the tactical situation. Extensions to accommodate uncertain events and elusive goals in multi-hypothesis situations are the focus of current activities.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-134.pdf,
134,1986,Applications,Application of Knowledge Based Systems Technology to Triple Quadrupole Mass Spectrometry,"Hal Brand, Carla Wong","The complexity of chemical instrumentation is such that automation of certain instrument functions by conventional algorithmic means is either very difficult or completely unsuitable. This paper details work in progress on the application of knowledge based systems technology to the tuning of a complex analytical instrument, a triple quadrupole mass spectrometer (TQMS) . The knowledge representation schemes and interface design between the expert system and the TQMS instrument are discussed. Preliminary results of optimizing the TQMS on chemical standards are presented.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-135.pdf,
135,1986,Applications,Designing for Manufacturability in Riveted Joints,"A. R. Kilhoffer, K. G. Kempf","The study of human experts in the areas of design and manufacturing has lead to two hypotheses concerning the problem solving methods which these engineers utilize to attack difficult problems. The basis of both hypotheses is a modular approach to problem solving. One hypothesis addresses the nature of the modules utilized while the other hypothesis deals with the organization of the modules. A knowledge-based system has been designed and implemented under the philosophy expressed in these hypotheses. The domain is the design and manufacture of riveted joints in sheet metal. Special emphasis is given to the integration of design knowledge and manufacturing knowledge for the concept of ""designing for manufacturability"". The implementation is described in some detail and two example problems are presented with their solutions.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-136.pdf,
136,1986,Applications,Design and Experimentation of an Expert System for Programming In-the-Large,"Giovanni Guida, Marco Guida, Sergio Gusmeroli, Marco Somalvico","The purpose of this paper is the illustration of the results obtained in a research project, devoted to design an expert system assisting the programmer in-the-large in his activity of problem analysis and software design: the ESAP (Expert System for Automatic Programming) (Guida et al. (1984); Guida et al. (1985)). The environment where the ESAP has been designed refers to a new rearrangement of the softuare Life-cycLe, in which several tools for automating software production are available. We call this environment Software Factory of the Future CSFF), as illustrated in Figure 1.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-137.pdf,
137,1986,Applications,A Graph-Oriented Knowledge Representation and Unification Technique for Automatically Selecting and Invoking Software Functions,"William F. Kaemmerer, James A. Larson","An interface to information systems that can automatically select, sequence, and invoke the sources needed to satisfy a user’s request can have great practical value. It can spare the user from the need to know what information is available from each of the sources, and how to access them. We have developed and implemented a graph-oriented technique for representing software modules and databases, along with unification and search algorithms that enable an interface to perform this automatic programming function. The approach works for a large class of useful requests, in a tractable amount of run time. The approach permits the logical integration of pre-existing batch application programs and databases. It may also be used in other situations requiring automatic selection of software functions to obtain information specified in a declarative expression.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-138.pdf,
138,1986,Applications,SCAT: An Automatic-Programming Tool for Telecommunications Software,"S. Barra, O. Ghisio, F. Manucci","The size, complexity and long life-time of telecommunications software, e.g. the programs for store program control (SPC) telephone exchanges, call for an increased software productivity and maintainability other than an improved quality. The availability of programming support environments based on standardized specification and programming languages greatly improves the software development process. Artificial Intelligence techniques are very promising aiming at further improvements and can provide a short-term payoff especially within an evolutionary approach leading up to an hybrid programming environment, i.e. a software environment made of both conventional and intelligent tools. The paper describes an intelligent tool, dubbed SCAT, based on ideas exploited by various automatic programming systems, like CHI, Programmer’s Apprentice and DEDALUS. SCAT is strictly related to the telecommunications domain, thus it differs from other systems in the domain specifity. SCAT partly automatizes the most crucial phase in the software development process, i.e. the transition from project’s detailed specification to the actual software implementation. SCAT has been tested in a few experimental software developments and in an actual application,i.e. the message handling system (MHS) to be made available in the Italian public packet switching network (ITAPAC).",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-139.pdf,
139,1986,Applications,PIES: An Engineer’s Do-lt-Yourself Knowledge System for Interpretation of Parametric Test Data,"Jeff Yung-Choa Pan, Jay M. Tenenbaum","PIES is a knowledge system for interpreting the parametric test data collected at the end of complex semiconductor fabrication processes. The system transforms hundreds of measurements into a concise statement of the overall health of the process, and the nature and probable cause of any anomalies. A key feature of PIES is the structure of the knowledge-base, which reflects the way fabrication engineers reason causally about semiconductor failures. This structure permits fabrication engineers to do their own knowledge engineering, building the knowledge base, and then maintaining it to reflect process modifications and operating experience. The approach appears applicable to other process control and diagnosis tasks.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-140.pdf,
140,1986,Applications,StarPlan II: Evolution of an Expert System,"Ronald W. Siemens, Marilyn Golden, Jay C. Ferguson","An expert system for satellite anomaly resolution must perform monitoring, situation assessment, diagnosis, goal determination and planning functions in real time. StarPlan is such a system being developed at the Ford Aerospace Sunnyvale Operation. This paper details the evolution of the StarPlan architecture from a rule-based system in which multiple ""experts"" classified and resolved anomalies to a more generic architecture that utilizes an object model of the domain to perform fault diagnosis using causal reasoning. The StarPlan I architecture is described; the lessons learned in StarPlan I implementation are discussed; and the architecture of StarPlan II is presented.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-141.pdf,
141,1986,Applications,System Integration of Knowledge-Based Maintenance Aids,"Christopher A. Powell, Cynthia K. Pickering, Keith T. Wescourt","There are many examples of knowledge-based fault diagnosis advisors for corrective maintenance of complex equipment. However, such advisors are only part of an overall maintenance solution. To be used effectively, diagnostic advisors must be integrated with other existing and forthcoming systems, such as Automated Test Equipment and maintenance databases. Successful fielding of knowledge-based systems requires consideration of integration issues throughout the design process.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-142.pdf,
142,1986,Applications,A Knowledge-Based Framework for Design,"Sanjay Mittal, Agustin Araya","Many design problems can be formulated as a process of searching a ""well-defined"" space of artifacts with similar functionality. The dimensions of such spaces are largely known and are constrained by relations obtained from the implicit functionality of the designed artifact. After identifying the kinds of knowledge that mediate the search for acceptable designs, a computational framework is presented that organizes the required knowledge as design plans. A problem solver is described that executes these plans. The problem solver extends the notion of dependency-directed backtracking with an advice mechanism. This mechanism allows information from a constraint failure to be used as advice in modifying a partial design. An expert system for designing paper transports inside copiers has been successfully built based on this framework.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-143.pdf,
143,1986,Applications,Knowledge Engineering Issues in VLSI Synthesis,"W. H. Wolf, T. J. Kowalski, M. C. McFarland","This paper explores VLSI synthesis and the role that traditional AI methods can play in solving this problem. VLSI synthesis is hard because interactions among decisions at different levels of abstraction make design choices difficult to identify and evaluate. Our knowledge engineering strategy tackles this problem by organizing knowledge to encourage reasoning about the design through multiple levels of abstraction. We divide design knowledge into three categories: knowledge about modules used to design chips; knowledge used to distinguish and select modules; and knowledge about how to compose new designs from modules. We discuss the uses of procedural and declarative knowledge in each type of knowledge, the types of knowledge useful in each category, and efficient representations for them.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-144.pdf,
144,1986,Applications,Artificial Intelligence and Design: A Mechanical Engineering View,John R. Dixon,"Most AI research into design has been based on or directed to the electrical circuit domain. This paper presents a mechanical engineer’s view. Design of mechanical parts and products differs from design of electrical circuits in several fundamental ways: materials selection, sensitivity to manufacturing issues, non-modularity, high coupling of form and function, and especially the role of 3-D geometry. These differences, and the role of analysis in mechanical design, are discussed. A model for design is also presented based on the basically iterative nature of the design process. A brief summary of the related research at the University of Massachusetts into application of AI to mechanical design is included.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-145.pdf,
145,1986,Applications,Integration of Multiple Knowledge Sources in ALADIN: An Alloy Design System,"M. D. Rychener, I. Hulthage, M. S. Fox, M. L. Farinacci","ALADIN' is a knowledge-based system that aids metallurgists in the design of new aluminum alloys. Alloy design is characterized by creativity, intuition and conceptual reasoning. The application of artificial intelligence to this domain poses a number of challenges, including: how to focus the search, how to deal with subproblem interactions, how to integrate multiple, incomplete design models and how to represent complex, metallurgical structure knowledge. In this paper, our approach to dealing with these problems is described.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-146.pdf,
146,1986,Automated Reasoning,Generating Tests by Exploiting Designed Behavior,Mark Harper Shirley,One of the hardest problems in digital circuit design is test pattern generation for a complex device. This is difficult in part because it requires reasoning about how to control a device whose behavior can be extremely complex. Knowledge of the specific operations that the device was designed to perform can help solve this problem. The key observation is that a device’s designed behavior is often far more limited than the device’s potential behavior. This limitation translates into a reduction of the search necessary to achieve planning goals. We describe an implemented program based on this idea.,https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-147.pdf,
147,1986,Automated Reasoning,Evidential Reasoning with Temporal Aspects,Thomas C. Fall,"In the real world, one usually cannot gather enough evidence to completely determine the activity of a system. Typically, different pieces of evidence tell you about different aspects of the system with different certainties. Correlating these at a given point of time is a much studied problem. The problem becomes even less tractable when the evidence is acquired at different points in time. The system described in this paper uses frame-like objects called ""models"" that propagate the effects of a piece of evidence through time and uses Gordon and Shortliffe’s theory to combine the effects of the active models. These models do not require either a great deal of storage or that the evidence be processed in temporal order. Further, they seem to be a construct that the experts in our problems easily relate to. Results with test problems are consistent with the estimates of experts and run in not unreasonable time.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-148.pdf,
148,1986,Automated Reasoning,A Framework for Evidential-Reasoning Systems,"John D. Lowrance, Thomas D. Garvey, Thomas M. Strat","Evidential reasoning is a body of techniques that supports automated reasoning from evidence. It is based upon the Dempster-Shafer theory of belief functions. Both the formal basis and a framework for the implementation of automated reasoning systems based upon these techniques are presented. The formal and practical approaches are divided into four parts (1) specifying a set of distinct propositional spaces, each of which delimits a set of possible world situations (2) specifying the interrelationships among these propositional spaces (3) representing bodies of evidence as belief distributions over these propositional spaces and (4) establishing paths for the bodies of evidence to move through these propositional spaces by means of evidential operations, eventually converging on spaces where the target questions can be answered.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-149.pdf,
149,1986,Automated Reasoning,PROTEAN: Deriving Protein Structure from Constraints,"Barbara Hayes-Roth, Bruce Buchanan, Olivier Lichtarge, Mike Hewitt, Russ Altman, James Brinkley, Craig Cornelius, Bruce Duncan, Oleg Jardetzky","PROTEAN is an evolving knowledge-based system that is intended to identify the three-dimensional conformations of proteins in solution. Using a variety of empirically derived constraints, PROTEAN must identify legal positions for each of a protein’s constituent structures (e.g., atoms, amino acids, helices) in three-dimensional space. In fact, because protein-structure analysis is an underconstrained problem, PROTEAN must identify the entire family of conformations allowed by available constraints. In this paper, we discuss PROTEAN’s approach to the protein-structure analysis problem and its current implementation within the BBl blackboard architecture.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-150.pdf,
150,1986,Automated Reasoning,Back to Backtracking: Controlling the ATMS,"Johan de Kleer, Brian Williams","The ATMS (Assumption-Based Truth Maintenance System) provides a very general facility for all types of default reasoning. One of the principal advantages of the ATMS is that all of the possible (usually mutually inconsistent) solutions or partial solutions are directly available to the problem solver. By exploiting this capability of the ATMS, the problem solver can efficiently work on all solutions simultaneously and avoid the computational expense of backtracking. However, for some applications this ATMS capability is more of a hindrance than a help and some form of backtracking is necessary. This paper first outlines some of the reasons why backtracking is still necessary, and presents a powerful backtracking algorithm which we have implemented which backtracks more efficiently than other approaches.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-151.pdf,
151,1986,Automated Reasoning,Knowledge-Based Validity Maintenance for Production Systems,"Philip R. Schaefer, Isil H. Bozma, Randall D. Beer","In many problem domains, an action may be taken by an expert, which, due to new inferences or a changing domain situation, should be retracted. To this end, an effective problem solver will need to use some kind of validity-maintenance system, so that it can gracefully recover from invalid previous decisions. Unfortunately, the standard IF/THEN paradigm often used to encode expert behavior does not readily allow the expression and processing of this validity knowledge. We present a new extension to that rule paradigm which can be used to augment production-rule-based systems with validity maintenance capabilities, and demonstrate a straightforward algorithm for its interpretation.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-152.pdf,
152,1986,Automated Reasoning,A Parallel Self-Modifying Default Reasoning System,"Jack Minker, Donald Perlis, Krishnan Subramanian","As a step in our efforts toward the study of real-time monitoring of the inferential process in reasoning systems, we have devised a method of representing knowledge for the purpose of default reasoning. A meta-level implementation that permits effective monitoring of the deductive process as it proceeds, providing information on the state of the answer procurement process, has been developed on the Parallel Inference System (PRISM) at the University of Maryland. Also described is an implementation in PROLOG (and to be incorporated in the above) of a learning feature used to calculate, for purposes of issuing default answers, the current depth of inference for a query from that obtained from similar queries posed earlier.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-153.pdf,
153,1986,Automated Reasoning,Towards Explicit Integration of Knowledge in Expert Systems: An Analysis of MYClN’s Therapy Selection Algorithm,"Jack Mostow, William Swartout","The knowledge integration problem arises in rule-based expert systems when two or more recommendations made by right-hand sides of rules must be combined. Current expert systems address this problem either by engineering the rule set to avoid it, or by using a single integration technique built into the interpreter, e.g., certainty factor combination. We argue that multiple techniques are needed and that their use -- and underlying assumptions -- should be made explicit. We identify some of the techniques used in MYCIN’s therapy selection algorithm to integrate the diverse goals it attempts to satisfy, and suggest how knowledge of such techniques could be used to support construction, explanation, and maintenance of expert systems.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-154.pdf,
154,1986,Automated Reasoning,The Shifting Terminological Space: An Impediment to Evolvability,"William Swartout, Robert Neches","In an expert system, rules or methods interact by creating situations to which other rules or methods respond. We call the language in which these situations are represented the terminological space. In most expert systems, terms in this language often lack an independent definition, in which case they are implicitly defined by the way the rules or methods react to them. We argue that this hampers evolution, and argue for a separate, independently defined terminological space that is automatically maintained.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-155.pdf,
155,1986,Automated Reasoning,Using Qualitative Reasoning to Understand Financial Arithmetic,"Chidanand Apte, SeJune Hong","This paper describes a general mechanism for the qualitative interpretation of simple arithmetic relations. This mechanism is useful for the understanding and reasoning about domains that can be modeled by systems of simple arithmetic equations. Our representation attempts to model the underlying arithmetic in its complete detail. Reasoning from these forms provides the completeness and consistency that cannot be always guaranteed by a pure production-rule based system. We describe an experimental architecture for Equation Reasoning (ER), and illustrate its applicability using examples from the financial domain.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-156.pdf,
156,1986,Knowledge Acquisition,MOLE: A Knowledge Acquisition Tool that Uses its Head,"Larry Eshelman, John McDermott","MOLE can help domain experts build a heuristic classification problem-solver by working with them to generate an initial knowledge base and then detect and remedy deficiencies in it. By exploiting several heuristic assumptions about the world, MOLE is able to minimize the information it needs to elicit from the domain expert. In particular, by using static techniques of analysis, MOLE is able to infer support values and fill in gaps when a knowledge base is under-specified. And by using dynamic techniques of analysis, MOLE is able to interactively refine the knowledge base.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-157.pdf,
157,1986,Knowledge Acquisition,Problem Features that Influence the Design of Expert Systems,"Paul J. Kline, Steven B. Dolins","An analysis was made of a set of design guidelines for expert systems. These guidelines relate problem characteristics to appropriate AI implementation techniques. The analysis indicates there are five general problem features that are important for the proper use of a wide variety of AI implementation techniques. By being aware of these problem features, knowledge engineers improve their chances of coming up with the right design for expert systems. Awareness of these problem features should also help knowledge engineers take full advantage of new AI techniques as they emerge.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-158.pdf,
158,1986,Knowledge Acquisition,Knowledge Level Engineering Ontological Analysis,"James H. Alexander, Michael J. Freiling, Sheryl J. Shulman, Jeffery L. Staley, Steven Rehfuss, Steven L. Messick","Knowledge engineering suffers from a lack of formal tools for understanding domains of interest. Current practice relies on an intuitive, informal approach for collecting expert knowledge and formulating it into a representation scheme adequate for symbolic processing. Implicit in this process, the knowledge engineer formulates a model of the domain, and creates formal data structures (knowledge base) and procedures (inference engine) to solve the task at hand. Newell (1982) has proposed that there should be a knowledge level analysis to aid the development of AI systems in general and knowledge-based expert systems in particular. This paper describes a methodology, called ontological analysis, which provides this level of analysis. The methodology consists of an analysis tool and its principles of use that result in a formal specification of the knowledge elements in a task domain.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-159.pdf,
159,1986,Knowledge Acquisition,Framework for Prototyping Expert Systems for Financial Applications,"Jacob Y. Friedman, Atul Jain","Analysis of difficulties in transferring expert systems technology into the financial industry applications suggests that speed-up of the prototyping phase can significantly reduce the cost and length of the entire development process. We suggest a prototype concept that is generic for certain types of financial applications and can serve as both a catalyst for the knowledge engineering process and a laboratory for knowledge gathering, validation and maintenance. We developed software tools to provide a framework for rapid prototyping by financial professionals with basic computer training.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-160.pdf,
160,1986,Knowledge Representation,Recent Developments in NIKL,"Thomas S. Kaczmarek, Raymond Bates, Gabriel Robins","NIKL (a New Implementation of KL-ONE) is one of the members of the KL-ONE family of knowledge representation languages. NIKL has been in use for several years and our experiences have led us to define and implement various extensions to the language, its support environment and the implementation. Our experiences are particular to the use of NIKL. However, the requirements that we have discovered are relevant to any intelligent system that must reason about terminology. This article reports on the extensions that we have found necessary based on experiences in several different testbeds. The motivations for the extensions and future plans are also presented.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-161.pdf,
161,1986,Knowledge Representation,A Hybrid Structured Object and Constraint Representation Language,David R. Harris,SOCLE is a hybrid representation system in which cells of constraints are identified with slots of frame networks. Constraint formulas are maintained with respect to slots in frame networks and in turn provide for the dependency regulation of values on the frames. This paper illustrates the use of SOCLE and outlines the control structure decisions made for its design and implementation.,https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-162.pdf,
162,1986,Knowledge Representation,A Knowledge Representation Technique for Systems Dealing with Hardware Configuration,Jeff Pierick,A representation language combining the attributes of both rule-based systems and frame-based systems is discussed within the context of developing systems for computer hardware configuration. It is believed that the combination of these two common approaches to knowledge representation provides many advantages over the strict use of either of the two approaches alone.,https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-163.pdf,
163,1986,Knowledge Representation,AGNESS: A Generalized Network-based Expert System Shell,"James Slagle, Michael Wick, Marius Poliac","AGNESS is an expert system shell developed at the University of Minnesota. AGNESS is more general than other shells. It uses a computation network to represent expert defined rules, and can handle any well-defined inference method. The system works with non-numeric as well as numeric data, and shares constructs whenever possible to achieve increased storage efficiency. AGNESS uses a menu-driven user interface, and has several features that make the system friendly and convenient to use. The system includes eight explanation queries designed to increase the amount of information available to the user, the expert, and the knowledge engineer while remaining simple enough to be included in most of today’s expert system shells. AGNESS has been tested on several domains ranging from simplified problems to real world medical analysis.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-164.pdf,
164,1986,Knowledge Representation,SYNTEL(TM): Knowledge Programming Using Functional Representations,"Rene Reboh, Tore Risch","SYNTEL is a novel knowledge representation language that provides traditional features of expert system shells within a pure functional programming paradigm. However, it differs sharply from existing functional languages in many ways, ranging from its ability to deal with uncertainty to its evaluation procedures. A very flexible user-interface definition facility is tightly integrated with the SYNTEL interpreter, giving the knowledge engineer full control over both form and content of the end-user system. SYNTEL is fully implemented and has been successfully used to develop large knowledge bases dealing with problems of risk assessment.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-165.pdf,
165,1986,Knowledge Representation,GBB: A Generic Blackboard Development System,"Daniel D. Corkill, Kevin Q. Gallagher, Kelly F, Murray","This paper describes a generic blackboard development system (GBB) that unifies many characteristics of the blackboard systems constructed to date. The goal of GBB is to provide flexibility, ease of implementation, and efficient execution of the resulting application system. Efficient insertion/retrieval of blackboard objects is achieved using a language for specifying the detailed structure of the blackboard as well as how that structure is to be implemented for a specific application. These specifications are used to generate a blackboard database kernel tailored to the application. GBB consists of two distinct subsystems: a blackboard database development subsystem and a control shell. This paper focuses on the database support and pattern matching capabilities of GBB, and presents the concepts and functionality used in providing an efficient blackboard database development subsystem.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-166.pdf,
166,1986,Knowledge Representation,ISCS-A Tool Kit for Constructing Knowledge-based System Configurators,"Harry Wu, Hon Wai Chun, Alejandro Mimo","This paper describes an integrated programming environment which is specially tailored to the development of knowledge-based system configurators. It is designed with three major objectives: to provide an integrated representational framework for the various knowledge sources relevant to the configuration task, to assist a knowledge engineer in the development and administration of the knowledge base, and to aid a knowledge engineer in the actual construction of a system configurator. Particular attention is placed on the engineering aspects of the system construction process. Specifically, we describe how knowledge acquisition is eased through the use of a configuration language which is specially designed to represent the various knowledge sources for configuration, how knowledge encoding and modification may be aided by the knowledge engineer assistant module, and how the development of user interface may be aided by a generic user interface generator. A prototype of the special purpose environment is implemented on a XEROX 1108 workstation and is being used to develop configuration expert systems at Honeywell.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-167.pdf,
167,1986,Learning,A System Which Uses Examples to Learn VLSI Structure Manipulation,"Richard H. Lathrop, Robert S. Kirk","Focusing especially on the later stages of the design task, when a complete (or nearly so) design is being optimized at the structural level prior to final physical layout, we identify some aspects of the VLSI domain which complicate efficient design both for human and machine agents. We describe a simple but useful idea and a prototype implemented system which partially addresses these problems. Examples are conveyed graphically from an existing design. The system automatically learns a design precedent enabling it to infer local hierarchy corresponding to the example in new designs. The teacher may also substitute alternative actions, described to the system in its native Y hardware description language, which the system remembers and can also apply later. CONSTELLATION can infer local hierarchy; undo and rationalize clever local tricks and work-arounds; search for situations in which a specific local optimization can be applied; and modify the circuit as described by the teacher. The system is in experimental use in a production environment.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-168.pdf,
168,1986,Learning,Refining the Knowledge Base of a Diagnostic Expert System: An Application of Failure-Driven Learning,Michael Pazzani,"This paper discusses an application of failure-driven learning to the construction of the knowledge base of a diagnostic expert system. Diagnosis heuristics (i.e., efficient rules which encode empirical associations between atypical device behavior and device failures) are learned from information implicit in device models. This approach is desireable since less effort is required to obtain information about device functionality and connectivity to define device models than to encode and debug diagnosis heuristics from a domain expert. We give results of applying this technique in an expert system for the diagnosis of failures in the attitude control system of the DSCS-III satellite. The system is fully implemented in a combination of LISP and PROLOG on a Symbolics 3600. The results indicate that realistic applications can be built using this approach. The performance of the diagnostic expert system after learning is equivalent to and, in some cases, better than the performace of the expert system with rules supplied by a domain expert.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-169.pdf,
169,1986,Learning,Learning Arithmetic Problem Solver,"Masamichi Shimura, Seiichiro Sakurai","In this paper we describe a problem solving system with a learning mechanism (Learning Arithmetic Problem Solver, LAPS), which can solve arithmetic problems written in natural languages. Since LAPS has knowledge about arithmetic problems in the form of rules, it can solve many different problems without alteration of the program. When LAPS cannot solve a given problem because of a shortage of knowledge, it asks the user how to solve the problem. According to the user’s advice LAPS acquires knowledge and rules. Using these rules, LAPS can solve problems. Furthermore, LAPS can improve its performance at problem solving by synthesizing rules that are applied.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-170.pdf,
170,1986,Learning,The Multi-Purpose Incremental Learning System AQ15 and Its Testing Application to Three Medical Domains,"Ryszard Michalski, Igor Mozetic, Jiarong Hong, Nada Lavrac","AQ15 is a multi-purpose inductive learning system that uses logic-based, user-oriented knowledge representation, is able to incrementally learn disjunctive concepts from noisy or overlapping examples, and can perform constructive induction (i.e., can generate new attributes in the process of learning). In an experimental application to three medical domains, the program learned decision rules that performed at the level of accuracy of human experts. A surprising and potentially significant result is the demonstration that by applying the proposed method of cover truncation and analogical matching, called TRUNC, one may drastically decrease the complexity of the knowledge base without affecting its performance accuracy.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-171.pdf,
171,1986,Natural Language,Restricting Logic Grammars,Edward P. Stabler,"A parser formalism for natural languages that is so restricted as to rule out the definition of linguistic structures that do not occur in any natural language can make the task of grammar construction easier, whether it is done manually (by a programmer) or automatically (by a grammar induction system). A restrictive grammar formalism for logic programming languages is presented that imposes some of the constraints suggested by recent Chomskian linguistic theory. In spite of these restrictions, this formalism allows for relatively elegant characterizations of natural languages that can be translated into efficient prolog parsers.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-172.pdf,
172,1986,Natural Language,A Parser for Portable NL Interfaces Using Graph-Unification-Based Grammars,Kent Wittenburg,"This paper presents the reasoning behind the selection and design of a parser for the Lingo project on natural language interfaces at MCC. The major factors in the selection of the parsing algorithm were the choices of having a syntactically based grammar, using a graph-unification-based representation language, using Combinatory Categorial Grammars, and adopting a one-to-many mapping from syntactic bracketings to semantic representations in certain cases. The algorithm chosen is a variant of chart parsing that uses a best-first control structure managed on an agenda. It offers flexibility for these natural language processing applications by allowing for best-first tuning of parsing for particular grammars in particular domains while at the same time allowing exhaustive enumeration of the search space during grammar development. Efficiency advantages of this choice for graph-unification-based representation languages are outlined, as well as a number of other advantages that acrue to this approach by virtue of its use of an agenda as a control structure. We also mention two useful refinements to the basic best-first chart parsing algorithm that have been implemented in the Lingo project.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-173.pdf,
173,1986,Natural Language,A Chinese Natural Language Processing System Based Upon the Theory of Empty Categories,"Long Ji Lin, Lin-Shan Lee, James Huang, K. J. Chen","In this paper, we will present a device specially designed on the basis of the theory of empty categories. This device cooperates with a bottom-up parser and is used as an elegant and efficient approach to treat the troublesome problems of the transformations of passivization, relativizatlon; topicalization, ba-transformation and the use of zero pronouns in Chinese natural language. With the aid of the device, the grammar rules for Chinese will be much more simplified and easier to design, and the processing capability can be significantly improved.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-174.pdf,
174,1986,Natural Language,Adapting MUMBLE: Experience with Natural Language Generation,Robert Rubinoff,"This paper describes the construction of a MUMBLE-based [5] tactical component for the TEXT text generation system [7]. This new component, which produces fluent English sentences from the sequence of structured message units output from TEXT’s strategic component, has produced a 60-fold speed-up in sentence production. Adapting MUMBLE required work on each of the three parts of the MUMBLE framework: the interpreter, the grammar, and the dictionary. It also provided some insight into the organization of the generation process and the consequences of MUMBLE’s commitment to a deterministic model.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-175.pdf,
175,1986,Natural Language,Generating Medical Case Reports with the Linguistic String Parser,"Ping-Yang Li, Martha Evens, Daniel Hier","We are building a text generation module for a decision support system designed to assist physicians in the management of stroke. This module produces multi-paragraph reports on stroke cases stored in the Stroke Data Base or on cases being processed by the decision support system. Analysis of human-generated case reports using Sager’s Linguistic String Parser (LSP) led to a characterization of the stroke sublanguage in terms of four components: a Text Grammar for stroke case reports, a set of Stroke Information Formats, a Relational Lexicon for the stroke sublanguage, and a Linguistic String Grammar for this sublanguage. At this point, we have produced free text by using reverse transformations from our LSP grammar to combine fragments into sentences. Our future goal lies in discovering how to generate good paragraphs, using these components as tools.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-176.pdf,
176,1986,Natural Language,A Relational Representation of Modification,Samuel Bayer,The KING KONG parser being developed at The MITRE Corporation combines an argument-structure shorthand with recent work on the relationship between spatial and non-spatial sets of relations and a relational model of abstract relations to produce a robust approach to modifier constructions.,https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-177.pdf,
177,1986,Natural Language,Categorical Disambiguation,Gavan Duffy,"This paper presents an implemented, computationally inexpensive technique for disambiguating categories (parts of speech) by exploiting constraints on possible category combinations. Early resolutions of category ambiguities provide a great deal of leverage, simplifying later resolutions of other types of lexical ambiguity.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-178.pdf,
178,1986,Natural Language,Focusing and Reference Resolution in PUNDIT,"Deborah A, Dahl","This paper describes the use of focusing in the PUNDIT text processing system. Focusing, as discussed by [Sidner1979] ( as well as the closely related concept of centering, as discussed by [Groszl983] ), provides a powerful tool for pronoun resolution. However, its range of application is actually much more general, in that it can be used for several problems in reference resolution. Specifically, in the PUNDIT system, focusing is used for one-ansphora, elided noun phrases, and certain types of definite and indefinite noun phrases, in addition to its use for pronouns. Another important feature in the PUNDIT reference resolution system is that the focusing algorithm is based on syntactic constituents, rather than on thematic roles, as in Sidner’s system. This feature is based on considerations arising from the extension of focusing to cover one-anaphora. These considerations make syntactic focusing a more accurate predictor of the interpretation of one-anaphoric noun phrases without decreasing the accuracy for definite pronouns.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-179.pdf,
179,1986,Natural Language,ATRANS: Automatic Processing of Money Transfer Messages,"Steven Lytinen, Anatole Gershman","Unformatted natural-language money transfer messages play an important role in the international banking system. Manually reading such messages and encoding them in the format understandable by a bank’s automatic payment system is relatively slow and expensive. Due to the very restricted nature of the domain, the problem lends itself naturally to a Conceptual Dependency (CD), script-style solution. This paper illustrates the solutions to a number of problems that arise when an academic theory is applied to a real-world problem. In particular, we concentrate on the problem of context localization in the absence of reliable syntactic clues, such as sentence boundaries.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-180.pdf,
180,1986,Robotics,A Mobile Robot with Onboard Parallel Processor and Large Workspace Arm,"Rodney A. Brooks, Jon Connell, Anita Flynn","The MIT AI Lab’s second mobile robot, MOBOT-2, has a number of unique design features. In this paper we describe two of them in detail. First, MOBOT-2 has an extremely cheap 32 processor distributed control system. The processor system, called BARNACLE, runs asynchronously with no central locus of control. Unlike almost all other parallel processors this one has no expensive communications routing network. The communication topology is determined by a distributed patch panel. All computing is done onboard the robot. Second, MOBOT-2 has an onboard arm. It is lightweight, but has an extremely large working volume. The arm is controlled by the parallel processor.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-181.pdf,
181,1986,Robotics,Robot Navigation in Unknown Terrains of Convex Polygonal Obstacles Using Learned Visibility Graphs,"B. John Oommen, S. S. Iyengar, Nageswara S. V. Rao, R. L. Kashyap","The problem of navigating an autonomous mobile robot through an unexplored terrain of obstacles is the focus of this paper. The case when the obstacles are ""known"" has been extensively studied in literature. The process of robot navigation in completely unexplored terrains involves both learning the information about the obstacle terrain and path planning. We present an algorithm to navigate a point robot in an unexplored terrain that is arbitrarily populated with disjoint convex polygonal obstacles in the plane. The navigation process is constituted by a number of traversals; each traversal is from an arbitrary source point to an arbitrary destination point. Initially, the terrain is explored using a sensor and the paths of traversal made may be sub-optimal. The visibility graph that models the obstacle terrain is incrementally constructed by integrating the information about the paths traversed so far. At any stage of learning, the partially learnt terrain model is represented as a learned visibility graph, and it is updated after each traversal. The proposed algorithm is proven to yield a convergent solution to each path of traversal. It is also shown that the learned visibility graph converges to the visibility graph with probability one, when the source and destination points are chosen randomly. Ultimately, the availability of the complete visibility graph enables the robot to plan globally optimal paths, and also obviates the further usage of sensors.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-182.pdf,
182,1986,Robotics,Planning Sensorless Robot Manipulation of Sliding Objects,"M. A. Peshkin, A. C. Sanderson","The physics of motion of a sliding object can be used to plan sensorless robot manipulation strategies. Prediction of a sliding object’s motion is difficult because the object’s distribution of support on the surface, and the resulting frictional forces, are in general unknown. This paper describes a new approach to the analysis of sliding motion, which finds the set of object motions for all distributions of support. The analysis results in the definition of discrete regions of guaranteed sticking and slipping behavior which lend themselves to use in planning. Unlike previous work our approach produces quantitative bounds on the rate at which predicted motions can occur. To illustrate a manipulation plan which requires quantitative information for its construction, we consider a strategy based on ""herding"" a sliding disk toward a central goal by moving a robot finger in a decreasing spiral about the goal. The optimal spiral is constructed, and its performance discussed.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-183.pdf,
183,1986,Robotics,And/Or Graph Representation of Assembly Plans,"Luiz Homem de Mello, A. C. Sanderson","This paper presents a compact representation of all possible assembly plans of a given product using AND/OR graphs. Such a representation forms the basis for efficient planning algorithms which enable an increase in assembly system flexibility by allowing an intclligcnt robot to pick a course of action according to instantaneous conditions. Two applications are discussed: the selection of the best assembly plan (off-line planning), and opportunistic scheduling (on-line planning). An example of an assembly with four parts illustrates the use of the AND/OR graph representation to find the best assembly plan based on weighing of operations according to complexity of manipulation and stability of subassemblies. In practice, a generic search algorithm, such as the AO* may be used to find this plan. The scheduling efficiency using this representation is compared to fixed sequence and precedence graph representations. The AND/OR graph consistently reduces the average number of operations.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-184.pdf,
184,1986,Vision and Signal Understanding,Noise-Tolerant Range Analysis for Autonomous Navigation,"Aviv Bergman, Cregg K. Cowan","Techniques for detecting horizontal regions, obstacles, ditches, and shoulders along a road from range data are described. The noise level in each scan line of the range image is computed and an adaptive threshold is used for noise compensation. The sources of noise and the scanning geometry for a time-of-flight range sensor are discussed and experimental results of applying these techniques to ERIM range images are presented.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-185.pdf,
185,1986,Vision and Signal Understanding,A Real-Time Road Following and Road Junction Detection Vision System for Autonomous Vehicles,"Darwin Kuan, Gary Phipps, A-Chuan Hsueh","This paper describes a real-time road following and road junction detection vision system for autonomous vehicles. Vision-guided road following requires extracting road boundaries from images in real-time to guide the navigation of autonomous vehicles on the roadway. We use a histogram-based pixel classification algorithm to classify road and non-road regions in the image. The most likely road region is selected and a polygonal representation of the detected road region boundary is used as the input to a geometric reasoning module that performs model-based reasoning to accurately identify consistent road segments and road junctions. In this module, local geometric supports for each road edge segment are collected and recorded and a global consistency checking is performed to obtain a consistent interpretation of the raw data. Limited cases of incorrect image segmentation due to shadows or unusual road conditions can be detected and corrected based on the road model. Similarly, road junctions can be detected using the same principle. The real-time road following vision system has been implemented on a high-speed image processor connected to a host computer. We have tested our road following vision system and vehicle control system on a gravel road. The vehicle can travel up to 8 kilometers per hour speed on the road.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-186.pdf,
186,1986,Vision and Signal Understanding,Object Recognition in Structured and Random Environments: Locating Address Blocks on Mail Pieces,"Ching-Huei Wang, Sargur Srihari","A framework for determining special interest objects in images is presented in the context of determining destination address blocks on images of mail pieces such as letters, magazines, and parcels. The images range from those having a high degree of global spatial structure (e.g., carefully prepared letter mail envelopes which conform to specifications) to those with no structure (e.g., magazines with randomly pasted address labels). A method of planning the use of a large numbers of specialized tools is given. The control utilizes a dependency graph, knowledge rules, and a blackboard.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-187.pdf,
187,1986,Vision and Signal Understanding,A Signal-Symbol Approach to Change Detection,"B. G. Lee, V. T. Tom, M. J. Carlotto","A hybrid (signal-symbol) approach for detecting significant changes in imagery uses a signal-based change detection algorithm followed by a symbol-based change interpreter. The change detection algorithm is based on a linear prediction model which uses small patches from a reference image to locally model the corresponding areas in a newly acquired image, and vice versa. Areas that cannot be accurately modelled because some form of change (signal significant) has occurred are passed on to the change interpreter. The change interpreter contains a set of ""physical cause frames"" which attempt to determine if the change is physically nonsignificant (e.g., due to clouds, shadowing, parallax effects, or partial occlusion). Changes due to nonsignificant changes are eliminated from further consideration. If the physical cause of the change cannot be determined, it is passed on to an image analyst for manual inspection. Preliminary results of work in progress are presented. These results indicate that the methodology is extremely effective in screening out large portions of imagery that do not contain significant change as well as cueing areas which are potentially significant.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-188.pdf,
188,1986,Invited Talks,Viewing History of Science as Compiled Hindsight,Lindley Darden,"This paper argues that the history of science can be used as a source for constructing abstract theory types to aid in solving recurring problem types. Two theory types are discussed, selection theories and instructive theories, which aid in forming hypotheses to solve fitness problems. Providing cases from which to construct theory types is one of several ways in which the history of science can be viewed as ""compiled hindsight.""",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-189.pdf,
189,1986,Invited Talks,What’s Practical When in Natural Language Applications,Gary G. Hendrix,"It is one thing to write research papers or build laboratory software addressing some tough natural-language phenomenon. It is quite another to build natural language capabilities into a product that has to support real people doing real work. This presentation is about the latter. A hard look is taken both at the kinds of applications today’s NL technology is really able to support and at some of the not-so-technical problems that can arise when NL technology is employed. The capabilities of current NL products are evaluated, and insights gained from basic research are reviewed as a guide to forecasting future engineering developments and to identifying major technical obstacles blocking the advancement of applications.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-194.pdf,
190,1986,Panels,Learning in Massively Parallel Nets,Geoffrey Hinton,"The human brain is very different from a conventional digital computer. It relies on massive parallelism rather than raw speed and it stores long-term knowledge by modifying the way its processing elements interact rather than by setting bits in a passive, general purpose memory. It is robust against minor physical damage and it learns from experience instead of being explicitly programmed. We do not yet know how the brain uses the activities of neurons to represent complex, articulated structures, or how the perceptual system turns the raw input into useful internal representations so rapidly. Nor do we know how the brain learns new representational schemes. But over the past few years there have been a lot of new and interesting theories about these issues. Much of the theorizing has been motivated by the belief that the brain is using computational principles which could also be applied to massively parallel artificial systems, if only we knew what the principles were.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-190.pdf,
191,1986,Panels,Financial Expert Systems,"Peter E. Hart, Norton Greenfeld, Walter Reitman, Chuck Williams","Financial decision-making problems represent an important new set of applications for expert systems. While these problems share certain similarities with the scientific and engineering domains that have historically been the focus of expert system research and development, they have their own special characteristics. What distinguishes financial applications? What demands do they place on expert system technology? What research opportunities do they suggest?",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-191.pdf,
192,1986,Panels,Where Should the Intelligence in Intelligent Interfaces be Placed?,"Tom Kaczmarek, Robert Neches, John Seely Brown, Phil Hayes, Tom Malone, Dic Waters, Bob Wilensky, Mike Williams","Obviously, many AI techniques are applicable to building better human-machine interfaces. Intelligent interfaces have explored the use of problem solving, planning, heuristic search, discourse models, user models, knowledge representation, expert systems, and natural language text understanding and generation. The particular techniques researchers have chosen have led to two very different paradigms in intelligent interfaces. On the one hand, knowledge based, ""intelligent apprentice"" systems like the UNIX Consultant seek to provide assistance based on an understanding of the user’s intentions and task domain. On the other hand, ""power tool"" systems like COUSIN emphasize a powerful command set, but leave the responsibility for selecting and applying those commands in the hands of the user. Behind these distinctions are some very different fundamental assumptions about the problems that users need help with, and the AI techniques that can be applied to helping them.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-192.pdf,
193,1986,Panels,Knowledge Representation Meets Knowledge Acquisition: What Are the Needs and Where Is the Leverage?,"Robert Neches, Richard Fikes, Kaz Kulikowski, John McDermott, Ramesh Patil","Working on knowledge acquisition requires making a commitment to a particular knowledge representation. This panel will explore the implications of that commitment, focusing on the ways in which choice of a particular representation can facilitate or hinder the knowledge acquisition task. Of particular interest is the role played by the presence or absence of a formal semantics in the representation language. Knowledge representations used in knowledge acquisition vary in their formality from unstructured LISP through the minimal structure imposed by production rules, all the way up to highly formalized languages like those in the U-ONE family. What this panel seeks to do is to bring together researchers with backgrounds in knowledge-based systems, knowledge acquisition, and knowledge representation.",https://aaai.org/Library/AAAI/1986/../../../Papers/AAAI/1986/AAAI86-193.pdf,
