,conference_year,category,title,author,abstract,download_url,keywords
0,1988,AI and Education,Facilitating Self-Education by Questioning Assumptive Reasoning,Robert Farrell,"Making assumptions limits the depth of inferencechains and reduces the potential for complex interactions, but assumptions that are made implicitly can blind the reasoner to important inferences and interactions. A self-education aidmakes students aware of their assumptions bydemonstrating how these assumptions might beviolated. DECIDER is a self-education aid forhistory that tracks student assumptions using political models and illustrates possible violationsusing dramatic stories and video sequences fromreal historical cases.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-001.pdf,
1,1988,Automatic Programming,"Integrating Multiple Sources of Knowledge Into Designer-Soar, an Automatic Algorithm Designer","David Steier, Allen Newell","Designing algorithms requires diverse knowledge about general problem-solving, algorithm design, implementation techniques, and the application domain. The knowledge can come from a variety of sources, including previous design experience, and the ability to integrate larowledge from such diverse sources appears critical to the success of human algorithm designers. Such integration is feasible in an automatic design system, especially when supported by the general problem-solving and learning mechanisms in the Soar architecture. Our system, Designer-Soar, now designs several simple generate-and-test and divide-and-conquer algorithms. The system already uses several levels of abstraction, generalizes from examples, and learns from experience, transferring knowledge acquired during the design of one algorithm to aid in the design of others.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-002.pdf,
2,1988,Automatic Programming,Invariant Logic. A Calculus for Problem Reformulation,Michael R. Lowry,"Symmetries abound in nature. Observing symmetries often provides the key to discovering internal structure. In problem solving, observing and reasoning about symmetries is a powerful tool for shifting viewpoints on a problem. A calculus for reasoning about problem symmetries has been developed, called Invariant Logic. Invariant Logic is partially implemented in STRATA, a system which synthesizes algorithms through problem reformulation. In STRATA, Invariant Logic is used to reason about generalized problem symmetries for several purposes. The first purpose is as a calculus for generating expressions denoting problem symmetries. The second purpose is problem abstraction - generating abstract problem descriptions which denote models in which the problem symmetries have been collapsed. The third purpose is problem reduction - specializing a problem description by adding constraints in order to realize performance gains.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-003.pdf,
3,1988,Automatic Programming,Being Suspicious: Critiquing Problem Specifications,"Stephen Fickas, P. Nagarajan","One should look closely at problem specifications before attempting solutions: we may find that the specifier has only a vague or even erroneous notion of what is required, that the solution of a more general or more specific problem may be of more use, or simply that the problem as given is misstated. Using software development as an example, we present a knowledge-based system for critiquing one form of problem specification, that of a formal software specification.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-004.pdf,
4,1988,Automatic Programming,Dominic II: Meta-Level Control in Iterative Redesign,"Mark F. Orelup, John R. Dixon, Paul R. Cohen, Melvin K. Simmons","This paper describes the meta-level control system of a program (Dominic) for parametric design of mechanical components by iterative redesign. We view parametric design as search, and thus Dominic is a hill climbing algorithm. However, from experience with Dominic we concluded that modeling engineering design as hill climbing has several limitations. Therefore, a need for meta-level control knowledge exists. To implement meta-level control, we have taken the approach of dynamically modifying the way hill climbing is performed for this task, rather than requiring the addition of domain-specific control knowledge. We have identified the limitations of hill climbing, constructed various generic hill climbing strategies, and developed a meta-strategy to guide the application of the strategies. The program monitors its own performance for unproductive efforts and selects among different strategies to improve its performance as it designs. This meta-level control significantly improves the performance of the program over the performance of an earlier version.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-005.pdf,
5,1988,Constraints,FRM: An Intelligent Assistant for Financial Resource Management,"Andrew Gelman, Susan Altman, Matt Pallakoff, Ketan Doshi, Catherine Manago, Thomas C. Rindfleisch, Bruce G. Buchanan","FRM is an experimental, knowledge-based system that assists in the judgmental aspects of budget planning and financial resource management. Problem solving in this domain requires many kinds of knowledge from many sources. We represent domain knowledge uniformly as constraints and view resource management and planning problems as constraint satisfaction and resolution tasks. We sketch here the financial resources management problem, our approach, and early results, concentrating on constraint representation and management issues in the system.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-006.pdf,
6,1988,Constraints,Belief Maintenance In Dynamic Constraint Networks,"Rina Dechter, Avi Dechter","This paper presents a constraint network formulation of belief maintenance in dynamically changing environments. We focus on the task of computing the degree of support for each proposition, i.e., the number of solutions of the constraint network which are consistent with the proposition. The paper develops an efficient distributed scheme for calculating and revising beliefs in acyclic constraint networks. The suggested process consists of two phases. In the first, called support propagation, each variable updates the number of extensions consistent with each of its values. The second, called contradiction resoluticm, is invoked by a variable upon detecting a contradiction, and identifies a minimal set of assumptions that potentially account for the contradiction.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-007.pdf,
7,1988,Planning,Prevention Techniques for a Temporal Planner,John C. Hogge,"Research in domain independent planning has concentrated on making desired facts (goals) become true, with less attention on preventing undesired facts (negative goals) from becoming true. This document presents preliminary worktowards a temporal-based model of prevention, based on Allen and Koomen’s temporal planner. The temporal model permits more expressive prevention problems and a more powerful prevention-oriented planner. Negative goals are temporally constrained-for instance, we can direct the planner to prevent a fact F from occurring before, after, or during a specific goal, between two temporally separated goals, etc. The planner can find solutions which allow F to become true at any time outside of the specified interval or which never allow F to become true. The temporal model permits the idea of delaying F after the interval, as well as terminating it before the interval. This paper describes a set of prevention techniques which have been implemented in a temporal planner and discusses necessary requirements for a more complete preventive planner.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-008.pdf,
8,1988,Planning,An Analysis of Time-Dependent Planning,"Thomas Dean, Mark Boddy","This paper presents a framework for exploring issues in time-dependent planning: planning in which the time available to respond to predicted events varies, and the decision making required to formulate effective responses is complex. Our analysis of time-dependent planning suggests an approach based on a class of algorithms that we call anytime algorithms. Anytime algorithms can be interrupted at any point during computation to return a result whose utility is a function of computation time. We explore methods for solving time-dependent planning problems based on the properties of anytime algorithms.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-009.pdf,
9,1988,Planning,Extending Conventional Planning Techniques to Handle Actions with Context-Dependent Effects,Edwin P. D. Pednault,"This paper presents a method of solving planning problems that involve actions whose effects change according to the situations in which they are performed. The approach is an extension of the conventional planning methodology in which plans are constructed iteratively by scanning for goals that are not yet satisfied, inserting actions to achieve them, and introducing additional subgoals to be achieved. The necessary extensions to this methodology to handle context-dependent effects are presented from a general, mathematically rigorous standpoint.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-010.pdf,
10,1988,Planning,Goals as Parallel Program Specifications,Leslie Pack Kaelbling,"Classical planning is inappropriate for generating actions in a dynamic world. This paper presents a formalism, called Gapps, that allows a programmer to specify an agent’s behavior using symbolic goal-reduction rules that are compiled into an efficient parallel program. Gapps is designed for use in domains that require real-time response, that cannot be completely characterized by operator descriptions, and that allow multiple actions to be carried out in parallel.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-011.pdf,
11,1988,Planning,Predictability Versus Responsiveness: Coordinating Problem Solvers In Dynamic Domains,"Edmund H. Durfee, Victor R. Lesser","Coordination in dynamic domains involves balancing predictability and responsiveness: agents must be predictable enough to anticipate and plan future interactions while being responsive enough to react to unexpected situations. The partial global planning approach to coordination provides a framework for flexibly balancing these opposing needs. In this approach, agents communicate about their current local plans to build up partial global plans (PGPs) that specify cooperative actions and interactions. When their plans change, agents must decide whether the time and effort of reformulating their PGPs is worthwhile, or whether working predictably with slightly out-of-date PGPs is more cost effective. In this paper, we briefly outline the partial global planning approach, discuss how it flexibly balances predictability and responsiveness, and experimentally show how different balances affect behavior in a simulated problem-solving network.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-012.pdf,
12,1988,Planning,Intelligent Real-Time Monitoring,"Thomas J. Laffey, Scott M. Weitzenkamp, Jackson Y. Read, Simon A. Kao, James L. Schmidt","This paper describes a multi-tasking architecture for performing real-time monitoring and analysis using knowledge-based problem solving techniques. To handle asynchronous inputs and perform in real-time, the system consists of three or more distributed processes which run concurrently and communicate via a message passing scheme. The Data Management Process acquires, compresses, and routes the incoming sensor data to other processes. The Inference Process consists of a high performance inference engine that performs a real-time analysis on the state and health of the physical system. The I/O Process receives sensor data from the Data Management Process and status messages and recommendations from the Inference Process, updates its graphical displays in real time, and acts as the interface to the console operator. The distributed architecture has been interfaced to an actual spacecraft (NASA' s Bubble Space Telescope) and is able to process the incoming telemetry in ""real-time"" (i.e., several hundred data changes per second).",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-013.pdf,
13,1988,Planning,Reactive Plan Revision,"Peng Si Ow, Stephen F. Smith, Alfred Thiriez","This paper describes a multi-tasking architecture for performing real-time monitoring and analysis using knowledge-based problem solving techniques. To handle asynchronous inputs and perform in real-time, the system consists of three or more distributed processes which run concurrently and communicate via a message passing scheme. The Data Management Process acquires, compresses, and routes the incoming sensor data to other processes. The Inference Process consists of a high performance inference engine that performs a real-time analysis on the state and health of the physical system. The I/O Process receives sensor data from the Data Management Process and status messages and recommendations from the Inference Process, updates its graphical displays in real time, and acts as the interface to the console operator. The distributed architecture has been interfaced to an actual spacecraft (NASA' s Bubble Space Telescope) and is able to process the incoming telemetry in ""real-time"" (i.e., several hundred data changes per second).",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-014.pdf,
14,1988,Planning,"Integrating Planning, Execution and Monitoring","Jose A. Ambros-lngerson, Sam Steel","IPEM, for Integrated Planning, Execution and Monitoring, provides a simple, clear and well defined framework to integrate these processes. Representation integration is achieved by naturally incorporating execution and monitoring information into [Chapman, 19871 TWEAK'S partial plan representation. Control integration is obtained by using a production system architecture where IF-THEN rules, referred to as flaws and fixes, specify partial plan transformations. Conflict resolution is done using a scheduler that embodies the current problem solving strategy. Since execution and plan elaboration operations have been designed to be independently applicable, and execution of an action is a scheduling decision like any other, the framework effectively supports interleaving of planning and execution (IPE). This renders a local ability to replan after both unexpected events and execution failure. The framework has served as the basis for an implemented hierarchical, nonlinear planning and execution system that has been tested on numerous examples, on various domains, and has shown to be reliable and robust.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-015.pdf,
15,1988,Planning,Reasoning about Action Using a Possible Models Approach,Marianne Winslett,"Ginsberg and Smith [6, 7] propose a new method for reasoning about action, which they term a possible worlds approach (PWA). The PWA is an elegant, simple, and potentially very powerful domain-independent technique that has proven fruitful in other areas of AI [13, 5]. In the domain of reasoning about action, Ginsberg and Smith offer the PWA as a solution to the frame problem (What facts about the world remain true when an action is performed?) and its dual, the ramification problem [3] (What facts about the world must change when an action is performed?). In addition, Ginsberg and Smith offer the PWA as a solution to the qualification problem (When is it reasonable to assume that an action will succeed?), and claim for the PWA computational advantages over other approaches such as situation calculus. Here and in [16] I show that the PWA fails to solve the frame, ramification, and qualification problems, even with additional simplifying restrictions not imposed by Ginsberg and Smith. The cause of the failure seems to be a lack of distinction in the PWA between the state of the world and the description of the state of the world. I introduce a new approach to reasoning about action, called the possible models approach, and show that the possible models approach works as well as the PWA on the examples of [6, 7] but does not suffer from its deficiencies.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-016.pdf,
16,1988,Planning,A Theory of Debugging Plans and Interpretations,Reid G. Simmons,"We present a theory of debugging applicable for planning and interpretation problems. The debugger analyzes causal explanations for why a bug arises to locate the underlying assumptions upon which the bug depends. A bug is repaired by replacing assumptions, using a small set of domain-independent debugging strategies that reason about the causal explanations and domain models that encode the effects of events. Our analysis of the planning and interpretation tasks indicates that only a small set of assumptions and associated repair strategies are needed to handle a wide range of bugs over a large class of domains. Our debugging approach extends previous work in both debugging and domain-independent planning. The approach, however, is computationally expensive and so is used in the context of the Generate, Test and Debug paradigm, in which the debugger is used only if the heuristic generator produces an incorrect hypothesis.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-017.pdf,
17,1988,Planning,Plan Abstraction Based on Operator Generalization,"John S. Anderson, Arthur M. Farley","We describe a planning system which automatically creates abstract operators while organizing a given set of primitive operators into a taxonomic hierarchy. At the same time, the system creates categories of abstract object types which allow abstract operators to apply to broad classes of functionally similar objects. After the system has found a plan to achieve a particular goal, it replaces each primitive operator in the plan with one of its ancestors from the operator taxonomy. The resulting abstract plan is incorporated into the operator hierarchy as a new abstract operator, an abstract-macro. The next time the planner is faced with a similar task, it can specialize the abstract-macro into a suitable plan by again using the operator taxonomy, this time replacing the abstract operators with appropriate descendants.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-018.pdf,
18,1988,Planning,Geometric Reasoning and Organized Optimization for Automated Process Planning,"Yasuyuki Maeda, Katsuya Shinohara","In order to contribute toward the realization of a practical computer-automated process planning system, this paper discusses two essential subjects: geometric reasoning and optimization capabilities. ESPER provides a solution for these two problems. Geometric reasoning mediates between symbolic reasoning and geometric modeling. It implies recognizing various features in geometric data as well as manipulation of these geometries. Optimization requires effective problem solving strategy and, in addition, cooperation between system inference and users’ interaction. For problem solving, knowledge is defined so as to avoid divergent search by pruning. Furthermore, users’ interactions can be incorporated into the optimization process to obtain a better solution. Through developing the system, it is shown that the methodology proposed here is effective for realizing practical process planning systems.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-019.pdf,
19,1988,Rule-Based Reasoning,Reasoning under Varying and Uncertain Resource Constraints,Eric J. Horvitz,"We describe the use of decision-theory to optimize the value of computation under uncertain and varying resource limitations. The research is motivated by the pursuit of formal models of rational decision making for computational agents, centering on the explicit consideration of preferences and resource availability. We focus here on the importance of identifying the multiattribute structure of partial results generated by approximation methods for making control decisions. Work on simple algorithms and on the control of decision-theoretic inference itself is described.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-020.pdf,
20,1988,Rule-Based Reasoning,Conflict Resolution In Fuzzy Forward-Chaining Production Systems,"James Bowen, Jianchu Kang","Forward-chaining productions have been used to implement some of the most significant expert systems. However, most forward chaining production languages make no provision for dealing with lexical imprecision. This paper briefly presents a language which supports fuzzy matching between condition patterns and facts in working memory. Then discussion is focussed on what part should be played in conflict resolution by the relative truth-values of fuzzily matched production instantiations.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-021.pdf,
21,1988,Search,Parallel Best-First Search of State-Space Graphs: A Summary of Results,"Vipin Kumar, K. Ramesh, V. Nageshwara Rao","This paper presents many different parallel formulations of the A*/Branch-and-Bound search algorithm. The parallel formulations primarily differ in the data structures used. Some formulations are suited only for shared-memory architectures, whereas others are suited for distributed-memory architectures as well. These parallel formulations have been implemented to solve the vertex cover problem and the TSP problem on the BBN Butterfly parallel processor. Using appropriate data structures, we are able to obtain fairly linear speedups for as many as 100 processors. We also discovered problem characteristics that make certain formulations more (or less) suitable for some search problems. Since the best-first search paradigm of A*/Branch-and-Bound is very commonly used, we expect these parallel formulations to be effective for a variety of problems. Concurrent and distributed priority queues used in these parallel formulations can be used in many parallel algorithms other than parallel A*/branch-and-bound.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-022.pdf,
22,1988,Search,Distributed Tree Search and Its Application to Alpha-Beta Pruning,"Chris Ferguson, Richard E. Korf","We propose a parallel tree search algorithm based on the idea of tree-decomposition in which different processors search different parts of the tree. This generic algorithm effectively searches irregular trees using an arbitrary number of processors without shared memory or centralized control. The algorithm is independent of the particular type of tree search, such as single-agent or two-player game, and independent of any particular processor allocation strategy. Uniprocessor depth-first and breadth-first search are special cases of this generic algorithm. The algorithm has been implemented for alpha-beta search in the game of Othello on a 32-node Hypercube multiprocessor. The number of node evaluations grows approximately linearly with the number of processors P, resulting in an overall speedup for alpha-beta with random node ordering of p.75 . Furthermore we present a novel processor allocation strategy, called Bound-and-Branch, for parallel alpha-beta search that achieves linear speedup in the case of perfect node ordering. Using this strategy, an actual speedup of 12 is obtained with 32 processors.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-023.pdf,
23,1988,Search,Some Experiments with Case-Based Search,"Steven Bradtke, Wendy G. Lehnert","Knowedge-based problem solvers traditionally merge knowledge about a domain with more general heuristics in an effort to confront novel problem situations intelligently. While domain knowledge is usually represented in terms of a domain model, the case-based reasoning (CBR) approach to problem solving utilizes domain knowledge in the form of past problem solving experience. In this paper we show how the CBR approach to problem solving forms the basis for a class of heuristic search techniques. Given a search space and operators for moving about the space, we can use a case-base of known problem solutions to guide us through the search. In this way, the case-base operates as a type of evaluation function used to prune the space and facilitate search. We will illustrate these ideas by presenting a CBR search algorithm as applied to the 8-puzzle, along with results from a set of experiments. The experiments evaluate 8-puzzle performance while manipulating different case-bases and case-base encoding techniques as independent variables. Our results indicate that there are general principles operating here which may be of use in a variety of applications where the domain model is weak but experience is strong.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-024.pdf,
24,1988,Search,Real-Time Heuristic Search: New Results,Richard E. Korf,"We present new results from applying the assumptions of two-player game searches, namely limited search horizon and commitment to moves in constant time, to single-agent problem-solving searches. We show that the search depth achievable with alpha pruning for a given amount of computation actually increases with increasing branching factor. We prove that real-time-A* (RTA*) makes locally optimal decisions given the heuristic information available to it on a tree. We then modify RTA* to perform optimally on a graph as well. We also prove that RTA* is guaranteed to find a solution to any solvable problem regardless of the initial heuristic values. In addition, we develop a learning version of RTA* that improves its performance over multiple problem-solving trials, and prove convergence of the learned heuristic values to the exact values. Finally, we demonstrate that these algorithms effectively solve larger problems than have previously been solvable with heuristic search techniques.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-025.pdf,
25,1988,Search,An Exact Best-First Search Procedure for the Constrained Rectangular Guillotine Knapsack Problem,"K. V. Viswanathan, A. Bagchi","The constrained Rectangular Guillotine Knapsack Problem (CRGKP) is a variant of the two-dimensional cutting stock problem. In the CRGKP, a stock rectangle of dimensions (L,W) is given. There are n different types of demanded rectangles, with the ith type ri having length li:, width wi, value vi and demand constraint bi. S must be cut using only orthogonal guillotine cuts to produce ai copies of ri, l <_ i <_ n, so as to maiximize a1v1 + a2v2 +...+ anvn, subject to the constraints ai <_ bi, l <_ i <_ n. All parameters are integers. Here a new best-first search algorithm for the CRGKP is described. The heuristic estimate function is monotone, and optimal solutions are guaranteed. Computational results indicate that this method is superior in performance to the two existing algorithms for the problem.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-026.pdf,
26,1988,Search,Tree-Clustering Schemes for Constraint-Processing,"Rina Dechter, Judea Pearl","The paper offers a systematic way of regrouping constraints into hierarchical structures capable of supporting information retrieval without backtracking. The method involves the formation and preprocessing of an acyclic database that permits a large variety of queries and local perturbations to be processed swiftly, either by sequential backtrack-free procedures, or by distributed constraint-propagation processes.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-027.pdf,
27,1988,Search,A Rearrangement Search Strategy for Determining Propositional Satisfiability,"Ramin Zabih, David McAllester","We present a simple algorithm for determining the satisfiability of propositional formulas in Conjunctive Normal Form. As the procedure searches for a satisfying truth assignment it dynamically rearranges the order in which variables are considered. The choice of which variable to assign a truth value next is guided by an upper bound on the size of the search remaining; the procedure makes the choice which yields the smallest upper bound on the size of the remaining search. We describe several upper bound functions and discuss the tradeoff between accurate upper bound functions and the overhead required to compute the upper bounds. Experimental data shows that for one easily computed upper bound the reduction in the size of the search spa,ce more than compensates for the 0verhea.d involved in selecting the next variable.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-028.pdf,
28,1988,Theorem Proving,Using Specialists to Accelerate General Reasoning,"Stephanie A. Miller, Lenhart K. Schubert","Theorem provers are prone to combinatorial explosions, especially when the reasoning chains needed to establish a desired result are lengthy. To alleviate this problem, special purpose inference methods have been developed that exploit properties of certain domains to shorten chains of reasoning with types, temporal relations, colors, numeric relations, and sets, to name a few. The problem investigated here is how to use these efficient, but limited methods in a more general enviromrrent. Although much research has been done on this problem, most of the resulting systems either restrict what they can represent and reason with, limit the types of special mechanisms that can be used, or are difficult to extend with other specialists. We develop a uniform interface to specialists which allows them to assist a resolution-based theorem prover in function evaluation, literal evaluation, and generalized resolving and factoring. The specialists incorporated into this system include a temporal reasoner, a type hierarchy, a number specialist, a set specialist, and a simple color specialist. Each new specialist was found to make possible fast proofs of questions previously beyond the scope of the theorem prover. Examples from the fully operational hybrid system are included.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-029.pdf,
29,1988,Theorem Proving,Goal-Directed Equation Solving,"Nachum Dershowitz, G. Sivakumar","Solving equations in equational Horn-clause theories is a programming paradigm that combines logic programming and functional programming in a clean manner. Languages like EQLOG, SLOG and RITE, express programs as rewrite rules and use narrowing to solve goals expressed as equations. In this paper, we express equational goal solving by means of a logic program that simulates rewriting of terms. Our goal-directed equation solving procedure is based on ""directed goals"", and combines narrowing with a more top-down approach. We also show how to incorporate a notion of operator derivability to prune some useless paths, while maintaining completeness of the method.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-030.pdf,
30,1988,Theorem Proving,Tableau-Based Theorem Proving In Normal Conditional Logics,"Chris Groeneboer, James P. Delgrande","This paper presents an extension of the semantic tableaux approach to theorem proving for the class of normal conditional logics. These logics are based on a possible worlds semantics, but contain a binary ""variable conditional"" operator => instead of the usual operator for necessity. The truth of A => B depends both on the accessibility relation between worlds, and on the proposition expressed by the antecedent A. Such logics have been shown to be appropriate for representing a wide variety of commonsense assertions, including default and prototypical properties, counterfactuals, notions of obligation, and others. The approach consists in attempting to find a truth assignment which will falsify a sentence or set of sentences. If successful, then a specific falsifying truth assignment is obtained; if not, then the sentence is valid. The approach is arguably more natural and intuitive than those based on proof-theoretic methods. The approach has been proven correct with respect to determining validity in the class of normal conditional logics. In addition, the approach has been implemented and tested on a number of different conditional logics. Various heuristics have been incorporated, and the implementation, while exponential in the worst case, is shown to be reasonably efficient for a large set of test cases.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-031.pdf,
31,1988,Theorem Proving,A General Proof Method for Modal Predicate Logic without the Barcan Formula,"Peter Jackson, Han Reichgelt","We present a general proof method for normal systems of modal predicate logic with identical inference rules for each such logic. Different systems are obtained by changing the conditions under which two formulas are considered complementary. The paper extends previous work in that we are no longer confined. to models in which the Barcan formula and its converse hold. This allows the domain of individuals to vary from world to world. Modifications to the original inference rules are given, and a semantic justification is provided.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-032.pdf,
32,1988,"TruthMaintenance Systems",An Efficient ATMS for Equivalence Relations,"Caroline N. Koff, Nicholas S. Flann, Thomas G. Dietterich","We introduce a specialized ATMS for efficiently computing equivalence relations in multiple contexts. This specialized ATMS overcomes the problems with existing solutions to reasoning with equivalence relations. The most direct implementation of an equivalence relation in the ATMS-encoding the reflexive, transitive and symmetric rules in the consumer architectureproduces redundant equality derivations and requires O(n3) label update attempts (where n is the number of terms in an equivalence class). An alternative implementation is one that employs simple equivalence classes. However, this solution is unacceptable, since the number of classes grows exponentially with the number of distinct assumptions. The specialized ATMS presented here produces no redundant equality derivations, requires only O(n2) label update attempts, and is most efficient when there are many distinct assumptions. This is achieved by exploiting a special relationship that holds among the labels of the equality assertions because of transitivity. The standard dependency structure construction and traversal is replaced by a single pass over each label in a weaker kind of equivalence class. The specialized ATMS has been implemented as part of the logic programming language FORLOG.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-033.pdf,
33,1988,"TruthMaintenance Systems",A General Labeling Algorithm for Assumption-Based Truth Maintenance,Johan de Kleer,"Assumption-based truth maintenance svstems have become a powerful and widely used tool in Artificial Intelligence problem solvers. The basic ATMS is restricted to accepting only horn clause justifications. Although various generalizations have been made and proposed to allow an ATMS to handle more general clauses, they have all involved the addition of complex and difficult to integrate hyperresolution rules. This paper presents an alternative approach based on negated assumptions which integrates simply and cleanly into existing ATMS algorithms and which does not require the use of a hyperresolution rule to ensure label consistency.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-034.pdf,
34,1988,"TruthMaintenance Systems",Focusing the ATMS,"Kenneth D. Forbus, Johan de Kleer","Many problems having enormous search spaces can nevertheless be solved because only a very small fraction of that space need be traversed to find the needed solution(s). The ability of assumption-based truth maintenance systems to rapidly switch and compare contexts is an advantage for such problems, but existing ATMS techniques generally perform badly on them. This paper describes a new strategy, the implied-by strategy, for using the ATMS that efficiently supports problem solving in domains which are infinite and where the inference engine must retain tight control in the course of problem solving. We describe the three mechanisms required to implement this strategy: focus environments, implied-by consumers, and contradiction conaumera. We compare the implied-by strategy to previous ATMS strategies, and demonstrate its effectiveness by performance comparisons in two simple domains. Finally, we discuss the implications for parallel processing and future ATMS design.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-035.pdf,
35,1988,"TruthMaintenance Systems",Massively Parallel Assumption-Based Truth Maintenance,"Michael Dixon, Johan de Kleer","De Kleer' s Assumption-based Truth Maintenance System (ATMS) is a propositional inference engine designed to simplify the construction of problem solvers that search complex search spaces efficiently. The ATMS has become a key component of many problem solvers, and often the primary consumer of computational resources. Although considerable effort has gone into designing and optimizing the LISP implementation, it now appears to be approaching the performance limitations of serial architectures. In this paper we show how the combination of a conventional serial machine and a massively parallel processor can dramatically speed up the ATMS algorithms, providing a very powerful general purpose architecture for problem solving.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-036.pdf,
36,1988,Uncertainty,Evidential Reasoning Using DELIEF,"Debra K. Zarley, Yen-Teh Hsia, Glenn Shafer","The Dempster-Shafer theory of belief functions [Shafer 1976] is an intuitively appealing formalism for reasoning under uncertainty. Several AI implementations have been undertaken [e.g., Lowrance et al. 1986, Biswas and Anand 1987], but the computational complexity of Dempster’s rule has limited the usefulness of such implementations. With the advent of efficient propagation schemes in Markov trees [Shafer et al. 1987], the time is ripe for more powerful systems. This paper discusses DELIEF (Design of bELIEFs), an interactive system that allows the design of belief function arguments via a simple graphical interface. The user of DELIEF constructs a graph, with nodes representing variables and edges representing relations among variables. This graph serves as a default knowledge schema. The user enters belief functions representing evidence pertinent to the individual variables in a specific situation, and the system combines them to obtain beliefs on all variables. The schema may be revised and re-evaluated until the user is satisfied with the result. The Markov tree used for belief propagation is displayed on demand. The system handles Bayesian causal trees [Pearl 1986] as a special case, and it has a special user interface for this case.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-037.pdf,
37,1988,Uncertainty,Belief Maintenance: An Integrated Approach to Uncertainty Management,"Kathryn B. Laskey, Paul E. Lehner",Belief maintenance represents a unified approach to assumption-based and numerical uncertainty management. A formal equivalence is demonstrated between Shafer-Dempster belief theory and assumption-based truth maintenance extended to incorporate a probability calculus on assumptions. Belief propagation through truth maintenance automatically and correctly accounts for non-independencies among propositions due to shared antecedents. Belief maintenance also incorporates an ability to represent and reason with defaults. The result is a framework for non-monotonic reasoning about the application of a quantitative uncertainty calculus.,https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-038.pdf,
38,1988,Uncertainty,A Note on Probabilistic Logic,Mary McLeish,This paper answers a question posed by Nils Nilsson in his paper [9] on Probabilistic Logic: When is the maximum entropy solution to the entailment problem equal to the solution obtained by the projection method? Conditions are given for the relevant matrices and vectors which can be tested without actually computing the two solutions and comparing them. Examples are discussed and some comments made concerning the use and computational problems of probabilistic logic.,https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-039.pdf,
39,1988,"CognitiveModeling",Learning a Second Language,"Steven L. Lyffnen, Carol E. Moon","We present a system, called IMMIGRANT, which learns rules about the grammar of a second language from instructions. We explore the implications of this task on the representation of linguistic knowledge in a natural language understanding system. We conclude that the internal representation of linguistic knowledge used in IMMIGRANT, which is unification-based, is more amenable to language learning from instructions than other representation schemes.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-040.pdf,
40,1988,"CognitiveModeling",Ethical Understanding: Recognizing and Using Belief Conflict In Narrative Processing,John F. Reeves,"Belief conflict patterns (BCPs) are knowledge structures representing the understander’s moral attitude toward problematic interpretations of the events in a story. These structures are used to model interest in stories by contrasting the understanding of stories to the system’s beliefs about the characters and what they have done. Once recognized, BCPs provide a framework for interpreting the rest of the story, and a basis for identifying the theme of the story. The representation of reasons for the attitude that the understander has of characters are called character assessments. Character assessments form the basis for BCPs by giving the understander a prior attitude under which to judge the character’s actions. BCPs organize the subjective reasons that the understander has for why a goal success/failure for a character should or shouldn’t have occurred, and these reasons provide support for the problematic interpretation of the story events. A process model for BCP recognition and how thematic resolution is accomplished is presented. The role of BCPs in a program that models the interpretive understanding of a short ironic story is described.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-041.pdf,
41,1988,"CognitiveModeling",A Computational Account of Basic Level and Typicality Effects,Douglas H. Fisher,"Cognitive psychology has uncovered two effects that have altered traditional views of human classification. Basic level effects suggest that humans prefer concepts at a particular level of generality, while typicality effects indicate that some instances of a class are more readily recognized as such than others. This paper describes a model of memory that accounts for basic level effects, typicality effects, and interactions between them. More generally, computer experiments lay the groundwork for a formal unification of basic level and typicality phenomena.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-042.pdf,
42,1988,"CognitiveModeling",Waiting on Weighting: A Symbolic Least Commitment Approach,"Kevin D. Ashley, Edwina L. Rissland","In this paper we describe an approach to the problem of weighting various factors that contribute to an analysis or outcome of a problem situation and discuss issues about weighting as they touch upon our case-based reasoned HYPO. In HYPO, we take the approach of delaying for as long as possible any assignment of weights-and of symbolically comparing the ""weights"" of competing factors. We call this approach a least commitment weighting scheme.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-043.pdf,
43,1988,"CognitiveModeling",Resolving Goal Conflicts via Negotiation,Kaffa P. Sycara,"In non-cooperative multi-agent planning, resolution of multiple conflicting goals is the result of finding compromise solutions. Previous research has dealt with such multi-agent problems where planning goals are well-specified, subgoals can be enumerated, and the utilities associated with subgoals known. Our research extends the domain of problems to include non-cooperative multi-agent interactions where planning goals are ill-specified, subgoals cannot beenumerated, and the associated utilities are not precisely known. We provide a model of goal conflict resolution through negotiation implemented in the PERSUADER, a program that resolves labor disputes. Negotiation is performed through proposal and modification of goal relaxations. Case-Based Reasoning is integrated with the use of multi-attribute utilities to portray tradeoffs and propose novel goal relaxations and compromises. Persuasive arguments are generated and used as a mechanism to dynamically change the agents’ utilities so that convergence to an acceptable compromise can be achieved.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-044.pdf,
44,1988,"CognitiveModeling",Evaluating Explanations,David B. Leake,"Explanation-based learning (EBL) is a powerful method for category formation. However, EBL systems are only effective if they start with good explanations. The problem of evaluating candidate explanations has received little attention: Current research usually assumes that a single explanation will be available for any situation, and that this explanation will be appropriate. In the real world many explanations can be generated for a given anomaly, only some of which are reasonable. Thus it is crucial to be able to distinguish between good and bad explanations. In people, the criteria for evaluating explanations are dynamic: they reflect context, the explainer’s current knowledge, and his needs for specific information. I present a theory of how these factors affect evaluation of explanations, and describe its implementation in ACCEPTER, a program to evaluate explanations for anomalies detected during story understanding.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-045.pdf,
45,1988,"CognitiveModeling",Reasoning about Evidence In Causal Explanations,Phyllis Koton,"Causal models can provide richly-detailed knowledge bases for producing explanations about behaviors in many domains, a task often termed interpretation or diagnosis. However, producing a causal explanation from the model can be time-consuming. This paper describes a system that solves a new problem by recalling a previous, similar problem and modifying its solution to fit the current problem. Because it is unlikely that any new problem will exactly match a previous one, the system evaluates differences between the problems using a set of evidence principles that allow the system to reason about such concepts as alternate lines of evidence, additional supporting evidence, and inconsistent evidence. If all differences between the new situation and the remembered situation are found to be insignificant, the previous causal explanation is adapted to fit the new case. This technique results in the same solution, but with an average of two orders of magnitude less effort. The evidence principles are domain independent, and the information necessary to apply them to other domain models is described.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-046.pdf,
46,1988,"QualitativeReasoning",MINIMA: A Symbolic Approach to Qualitative Algebraic Reasoning,Brian C. Williams,"The apparently weak properties of a qualitative algebra have lead some to conclude that we must turn instead to extra-mathematical properties of physical systems. We propose instead that a more powerful qualitative algebra is needed, one that merges the algebras on signs and reals. We have invented a hybrid algebra, called Ql, allows us to select abstractions intermediate between traditional qualitative and quantitative algebras. The power of our algebra is demonstrated in three ways: First, analysis of Ql shows that the algebra is robust, sharing many properties of reals, but including several that are unique. Second, these properties enable symbolic manipulation techniques for canonicalization and factorization distinct from those applied to the reals. Finally, these manipulation techniques hold much promise for tasks like design and verification, as suggested by a simple design example.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-047.pdf,
47,1988,"QualitativeReasoning",A Qualitative Approach to Mechanical Constraint,Paul Nielsen,"This paper provides a qualitative analysis of instantaneous, constrained motions in rigid bodies. We develop a symbolic spatial representation to describe the effects of configuration on the dynamic behavior of rigid objects. We also explore the way symbolic shape information may be used to reason about force transmission. This information may be used to provide a static analysis for a given configuration and is an important component of the calculation of behavioral transitions when envisioning device behavior. All results are based on an implementation.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-048.pdf,
48,1988,"QualitativeReasoning",Global Filters for Qualitative Behaviors,Peter Struss,Current methods in qualitative physic sometimes predict behaviors of physical systems that do not correspond to any realvalued solution. One reason is that the merging of distinct behaviors cannot be avoided by local criteria. It is necessary to determine the possible continuations of a qualitative behavior taking into account its complete history. Such global criteria for the partial elimination of spurious solutions are developed for 2nd order differential equations. The application of these filters is shown to reduce the set of behaviors for the mass-spring system predicted by other qualitative physics systems.,https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-049.pdf,
49,1988,"QualitativeReasoning",Generating Global Behaviors Using Deep Knowledge of Local Dynamics,Kenneth Man-Kam Yip,"Even with powerful numerical computers, exploring complex dynamical systems requires significant human effort and judgment to prepare simulations and to interpret numerical results. This paper describes one-aspect of a computer program, KAM, that can autonomously prepare numerical simulations, and can automatically generate high-level, qualitative interpretations of the quantitative results. Given a dynamical system, KAM searches in the phase space for regions where the system exhibits qualitatively distinct behaviors: periodic, almost-periodic, and chaotic motion. KAM uses its knowledge of dynamics to constrain its searches. This knowledge is encoded by a grammar of ' dynamical behavior in which the primitives are geometric orbits, and in which the rules of combination are orbit adjacency constraints. A consistency analysis procedure analogous to Waltz’s constraint satisfaction algorithm is implemented. The utility of the approach is demonstrated by exploring the dynamics of non-linear conservative systems with two degrees of freedom.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-050.pdf,
50,1988,"QualitativeReasoning",Non-Intersection of Trajectories In Qualitative Phase Space: A Global Constraint for Qualitative Simulation,"Wood W. Lee, Benjamin J. Kuipers","The QSIM algorithm is useful for predicting the possible qualitative behaviors of a system, given a qualitative differential equation (QDE) describing its structure and an initial state. Although QSIM is guaranteed to predict all real possibilities, it may also predict spurious behaviors which, if uncontrolled, can lead to an intractably branching tree of behaviors. Prediction of spurious behaviors is due to an interaction between the qualitative level of description and the local state-tostate perspective on the behavior taken by the algorithm. In this paper, we describe the non-intersection constraint, which embodies the requirement that a trajectory in phase space cannot intersect itself. We develop a criterion for applying it to all second order systems. It eliminates a major source of spurious predictions. Using it with the curvature constraint tightens simulation to the point where system-specific constraints can be applied more effectively. We demonstrate this on damped oscillatory systems with potentially nonlinear monotonic restoring force and damping terms. Its introduction represents significant progress towards tightening QSIM simulation.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-051.pdf,
51,1988,"QualitativeReasoning",Exaggeration,Daniel S. Weld,"Exaggeration is a technique for solving comparative analysis problems by considering extreme perturbations to a system. For example, exaggeration answers the question ""What happens to the output temperature of a heat exchanger if fluid flow rate increases?"" by simulating the behavior of an exchanger with infinite flow rate. This paper explains the three phases of the exaggeration algorithm: transform, simulate, and scale. The transform phase takes a comparative analysis problem and generates the description of an exaggerated system. The simulate phase predicts the behavior of the transformed system. Finally, the scale phase compares the original and exaggerated behaviors to answer the original comparative analysis question.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-052.pdf,
52,1988,"QualitativeReasoning",Qualitative Reasoning at Multiple Resolutions,Seshashayee S. Murthy,"In this paper we describe an approach to unify the various quantity spaces that have been proposed in qualitative reasoning with numbers. We work in the domain of physical devices, such as electrical circuits using lumped parameter models. We show how changing the quantity space can be achieved in the course of analysis and how this is similar to dynamically changing the resolution in analysis. We demonstrate the utility of this approach with two examples in the domain of circuit analysis.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-053.pdf,
53,1988,"QualitativeReasoning",Setting up Large-Scale Qualitative Models,"Brian Falkenhainer, Kenneth D. Forbus","A qualitative physics which captures the depth and breadth of an engineer’s knowledge will be orders of magnitude larger than the models of today’s qualitative physics. To build and use such models effectively requires explicit modeIing assumptions to manage complexity. This, in turn, gives rise to the problem of selecting the right qualitative model for some purpose. This paper addresses these issues by describing a set of conventions for modeling sssumptions. Simplifying assumptions decompose a domain into different grain sizes and perspectives which may be reasoned about separately. Operating assumptions reduce the complexity of qualitative simulation by focusing on particular behaviors of interest. We show how these assumptions can be directly represented in Qualitative Process theory, using a multi-grain, multi-slice model of a Navy propulsion plant for illustration. Importantly, we show that model selection can often be performed automatically via partial instantiation. We illustrate this technique with a simple explanation generation program that uses the propulsion plant model to answer questions about physical and functional characteristics of its operation.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-054.pdf,
54,1988,"QualitativeReasoning",Formal Basis for Commonsense Abstraction of Dynamic Systems,"Yumi Iwasaki, Inderpal Bhandari","Abstraction is an essential technique in reasoning about complex systems involving a large number of variables and interconnections. Aggregation of dynamic systems is an abstraction technique whose application is easily observable everyday life. The basic intuition behind aggregation of variables may be summarized as follows: if variables in large dynamic system can be partitioned into subsets such that variables in each subset are more strongly connected to each other than to variables in other subsets, one can describe the short-run behavior of each subsystem independently of other subsystems. Furthermore, one can describe the long-run behavior of the entire system in terms of these subsets instead individual variables, treating each subset as a black box. This paper provides a formal justification for commonsense abstraction based on aggregation of a dynamic system and presents a procedure for doing so.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-055.pdf,
55,1988,"QualitativeReasoning",Causal Ordering in a Mixed Structure,Yumi Iwasaki,"This paper describes a computational approach, based on the theory of causal ordering, for inferring causality from an acausal, formal description of a phenomena. Causal ordering an asymmetric relation among the variables in a self-contained equilibrium and dynamic structure, which seems reflect people’s intuitive notion of causal dependency relations among variables in a system. This paper extends the theory cover models consisting of mixture of dynamic and equilibrium equations. When people’s intuitive causal understanding of a situation is based on a mixed description, the causal ordering produced by the extension reflects this intuititve understanding better than that of an equilibrium description. The paper also discusses the view of a mixed model as an approximation to a completely dynamic model.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-056.pdf,
56,1988,"QualitativeReasoning",Controlling Qualitative Resolution,Jean-Luc Dormoy,"We proposed earlier in [Dormoy and Raiman, 1988] a new way of reasoning about a device, we called ""Assembling a Device"". Starting from a component description (namely confluences), the qualitative resolution rule provides task-oriented global relations which link the physical quantities involved in a device to some selected reference variables. This rule is complete: given any task to be performed (simulation, postdiction,...), it discovers an assemblage, i.e. a set of relations reducing the task to a straightforward propagation. We might thus expect to apply qualitative reasoning to large-scale systems. Unfortunately, the number of potential applications of the resolution rule is likely to increase exponentially as it is being fired. This behavior has to be related to the NP-completeness of the problem which consists of solving a set of confluences. In this paper, we present a heuristic for controlling the resolution rule, i.e. for choosing between its potential applications, and a collection of simple rules for speeding it up. This heuristic has a combinatorial form, but it is based on a simple commonsense idea. At the same time, it is borne out by mathematical results. Theoretically, a qualitative model can be out of its scope, but we have not yet hit upon a physical system with this kind of pathology.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-057.pdf,
57,1988,"QualitativeReasoning",Using Incomplete Quantitative Knowledge In Qualitative Reasoning,"Benjamin Kuipers, Daniel Berleant","Incomplete knowledge of the structure of mechanisms is an important fact of life in reasoning, commonsense or expert, about the physical world. Qualitative simulation captures an important kind of incomplete, ordinal, knowledge, and predicts the set of qualitatively possible behaviors of a mechanism, given a qualitative description of its structure and initial state. However, one frequently has quantitative knowledge as well as qualitative, though seldom enough to specify a numerical simulation. We present a method for incrementally exploiting incomplete quantitative knowledge, by using it to refine the predictions of a qualitative reasoner. Incomplete quantitative descriptions (currently ranges within which unknown values are assumed to lie) are asserted about some landmark values in the quantity spaces of qualitative parameters. Unknown monotonic function constraints may be bounded by numerically computable envelope functions. Implications are derived by local propagation across the constraints in the model. When this refinement process produces a contradiction, a qualitatively plausible behavior is shown to conflict with the quantitative knowledge. When all predicted behaviors of a given model are contradicted, the model is refuted. If a behavior is not refuted, propagation of quantitative information results in a mixed quantitative/qualitative description of behavior that can be compared with other surviving predictions for differential diagnosis.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-058.pdf,
58,1988,Design,Assembling a Device,"Jean-Luc Dormoy, Olivier Raiman","We present here a new way of reasoning on a device based on structure, we call assembling a device. It consists of a symbolic combination of local qualitative constraints (namely confluences) leading to more global relations. Some reference variables are selected according to the task to be performed (simulation, observation, postdiction...). The assembling step produces a set of equations expressing directly ""internal"" quantities as functions of the reference quantities. We call such a set a task-oriented assemblage. Then, determining the non ambiguous variables for a particular assignment of the reference quantities turns out to be straightforward. We can thus expect to perform qualitative reasoning on large systems. The assembling tool is a new rule, we call the qualitative resolution rule. It has agreable properties: (1) interpretation: each application can be interpreted as joining local descriptions to more global ones; (2) completeness: an assemblage provides all the non ambiguous variables for any assignment of reference variables.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-059.pdf,
59,1988,Design,Upgrading Design Systems,"Sarosh Talukdar, Jim Rehg, Rob Woodbury, Alberto Elfes",Design systems can have considerable embedded value. Improvements in such systems are better achieved through upgrades than through complete replacements. To determine how best to make these upgrades requires a systems view of design. Such a view is provided by what we call TAO (test-aspect-operator) graphs. Nodes in these graphs represent . aspects of the artifacts being designed while arcs represent operators (transforms between aspects) and tests (comparisons of aspects). Upgrades can be thought of as the additions of nodes or arcs to an existing TAO graph. To illustrate these ideas we will briefly describe the upgrades that we are making to a system for designing certain automobile parts.,https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-060.pdf,
60,1988,Design,Function Sharing in Mechanical Design,"Karl T. Ulrich, Warren P. Seering","Function sharing in mechanical design is the simultaneous implementation of several functions with a single structural element. If automobiles were designed without function sharing they would be relatively large, expensive and unreliable. But because elements like the sheetmetal body implement many functions (electrical ground, structural support, aerodynamic faring, weather protection, and aesthetics among others) automobiles perform better and cost less than a non-function-sharing alternative. This paper describes how function sharing can be viewed as a computational design procedure that produces efficient designs from modular designs. The function sharing procedure consists of three steps: 1) a structural element is deleted from the design; 2) physical features that can provide alternative implementations of the function(s) of the deleted element are found; 3) modifications are made to the design to accentuate the desired properties of the features found in step 2. We have chosen mechanical devices that can be described functionally as a network of lumped-parameter idealized elements as a domain for exploring function sharing. Such d evices include pressure gauges, accelerometers, and hydraulic cylinders.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-061.pdf,
61,1988,Design,From Kinematics to Shape: An Approach to Innovative Design,"Leo Joskowicz, Sanjaya Addanki","We address the problem of designing the shape of solid objects to satisfy a given set of functional specifications. In particular, we show how to design elementary components of mechanical devices (kinematic pairs) from a description of their desired behavior and a set of constraints. This is done using a backtracking algorithm that modifies (or creates) object shapes by adding and deleting line and arc segments to the objects’ contours. These modifications are guided by the configuration space description of the desired behavior. The algorithm is extended to handle both qualitative and causal descriptions of desired behaviors. This work is based on the theory of shape and kinematics developed in [Joskowicz, 1988].",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-062.pdf,
62,1988,Diagnosis,The Induction of Fault Diagnosis Systems from Qualitative Models,D. A. Pearce,"This paper describes a methodology for the automatic construction of diagnostic expert systems, and its application for fault diagnosis of a satellite' s electrical power subsystem. The synthesised knowledge base is compared with an existing expert system for the same application built using a commercial expert system shell. Both systems have been tested using a real-time satellite simulator which has the capability to fail components. A traditional knowledge-engineering approach involves building a prototype which is refined until satisfactory results are obtained. This process is error-ridden, as even in small systems, rules can conflict, be irrelevant, or missing. It is never clear when a system is complete and validation is always difficult. As an alternative, a fault diagnostic knowledge base can be automatically synthesised from a qualitative model of the device. This is achieved by systematically simulating all component failures. Individual failures are used as examples. A learning algorithm is applied to the examples to output a set of diagnostic rules. The resulting rules are complete and consistent with the qualitative model and diagnose component failures in the model 100% accurately. Validation becomes a higher level problem of ensuring that the qualitative simulation accurately models physical device behaviour.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-063.pdf,
63,1988,Diagnosis,Design for Testability,Peng Wu,"This paper presents an implemented system for modifying digital circuit designs to enhance testability. The key contributions of the work are: (1) setting design for testability in the context of test generation, (2) using failures during test generation to focus on testability problems, (3) indexing from these failures to a set of suggested circuit modifications. This approach does not add testability features to the portions of the circuit that a test generator can already handle, therefore, it promises to reduce the area and performance overhead necessary to achieve testability. While the system currently has only a small body of domain knowledge, it has demonstrated its ability to integrate different DFT techniques and to introduce only sharply focused modifications on a textbook microprocessor, an ability that is missing in previous DFT systems.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-064.pdf,
64,1988,Diagnosis,Specialized Strategies: An Alternative to First Principles in Diagnostic Problem Solving,"Nancy E. Reed, Elizabeth R. Stuck, James B. Moen","We introduce specialized strategies, an alternative level of reasoning, falling in generality between recognition-based reasoning and reasoning from first principles. These strategies are weak methods that are specific to a class of problems that occur in different domains. Specialized strategies are applicable not only to familiar problems in a domain, but also to problems that have not been anticipated. As a result they can provide both broad coverage currently given by ""causal"" reasoning and an efficiency close to that of ""shallow"" reasoning. The specialized strategies use inexact models of the components in the faulty system which contain only diagnostically relevant knowledge. Specialized strategies may be used in expert systems to increase efficiency, reduce brittleness, and decrease knowledge base construction effort compared to other common approaches. Examples are given from the domain of computer hardware diagnosis where two prototype expert systems were implemented.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-065.pdf,
65,1988,Diagnosis,Robust Operative Diagnosis as Problem Solving in a Hypothesis Space,Kathy H. Abbott,"The lack of robustness in current diagnostic systems is an important research issue because it has two major consequences: inability to diagnose novel faults and inability to diagnose more than one type of fault. This paper describes an approach that formulates diagnosis of physical systems in operation (operative diagnosis) as problem solving in a hypothesis space. Such a formulation increases robustness by: (1) incremental hypotheses construction via dynamic inputs, since the fault propagation results in changes in symptoms over time; (2) reasoning at a higher level of abstraction to construct hypotheses, albeit less specific ones, when specific knowledge is not available; and (3) partitioning the space by grouping fault hypotheses according to the type of physical system representation and problem solving techniques used in their construction. The approach was implemented for aircraft subsystems and evaluated on eight actual aircraft accident cases involving engine faults, with very promising results.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-066.pdf,
66,1988,"KnowledgeRepresentation",How to Print a File: An Expert System Approach to Software Knowledge Representation,Peter G. Selfridge,"Representing knowledge of software and software systems is an important research area and a prerequisite to engineering expert-level systems to do software tasks. Printing a file in a UNIX environment is an example of a real-world problem that can pose surprising difficulties to UNIX users. The printing of files is also illustrative of a class of software problems characterized by the recombination of existing programs. Automating the printing process involves desigining knowledge representations to appropriately capture knowledge of both the printing software and the printing process and desigining a reasoning system that uses those knowledge representations in a working implementation. This paper examines the printing problem in detail, presents a model of printing and printing software, and describes an implementation designed to test the model and identify the next set of research issues. The implementation, ESP for Expert System for Software, successfully automates the printing process and illustrates a knowledge-based approach to software problems.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-067.pdf,
67,1988,"KnowledgeRepresentation",Representing Genetic Information with Formal Grammars,David B. Searls,"Genetic information, as expressed in the four-letter code of the DNA of living organisms, represents a complex and richly expressive natural knowledge representation system, capturing procedural information that describes how to create and maintain life. The study of its semantics (i.e., the field of molecular biology) has yielded a wealth of information, but its syntax has been elaborated primarily at the lowest lexical levels, without benefit of formal computational approaches that might help to organize its description and analysis. This paper discusses such an approach, using generative grammars to express the information in DNA sequences in a declarative, hierarchical manner. A prototype implemented in a Prolog-based Definite Clause Grammar system is presented, which allows such declarative descriptions to be used directly for analysis of genetic information by parsing DNA. Examples are given of the utility of this method in the domain, and speed-ups and extensions are also proposed.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-068.pdf,
68,1988,"KnowledgeRepresentation",Overview of an Approach to Representation Design,"Jeffrey Van Baalen, Randall Davis","It has long been acknowledged that having a good representation is key in effective problem solving. But what is a ""good"" representation? We describe an approach to representation design for problem solving that answers this question for a class of problems called analytical reasoning problems. These problems are typically very difficult for general problem solvers, like theorem provers, to solve. Yet people solve them quite easily by designing a specialized representation for each problem and using it to aid the solution process. Our approach is motivated, in large part, by observations of the problem solving behavior of people. The implementation based on this approach takes as input a straightforward predicate calculus translation of the problem, tries to gather any necessary additional information, decides what to represent and how, designs the representations, then creates and runs a LISP program that uses those representations to produce a solution. The specialized representation created is a structure whose syntax captures the semantics of the problem domain and whose behavior enforces those semantics.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-069.pdf,
69,1988,"KnowledgeRepresentation",Mechanisms for Reasoning about Sets,"Michael P. Wellman, Reid G. Simmons","The SEt Reasoning Facility (SERF) integrates mechanisms for propagating membership propositions, deriving relations between sets, and reasoning about closure and cardinality into an efficient utility package for reasoning about sets. Assertions about relations between sets are compiled into a constraint network defined entirely in terms of union, complement, and emptiness constraints. The constraint network supports multiple modes of inference, such as local propagation of membership propositions and graph search for set relations using a transitivity table. SERF permits closure assertions of the form ""all members of set S are known"" and utilizes this capability to permit selective applications of closed-world assumptions. Cardinality constraints are handled by a general quantity reasoner. An example from geologic interpretation illustrates the value of mutually constraining sources of information in a typical application of reasoning about sets in commonsense problem-solving.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-070.pdf,
70,1988,Inheritance,A Deductive Pattern Matcher,Robert M. MacGregor,"This paper describes the design of a pattern matcher for a knowledge representation system called LOOM. The pattern matcher has a very rich pattern-forming language, and is logic-based, with a deductive mechanism which includes a truth-maintenance component as an integral part of the pattern-matching logic. The technology behind the LOOM matcher uses an inference engine called a classifier to perform the matches. The LOOM matcher is more expressive and more complete than previous classification- based pattern-matchers, and is expected to be significantly more efficient.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-071.pdf,
71,1988,Inheritance,A Model and Representation for Type Information and Its Use in Reasoning with Defaults,Lin Padgham,"Typing schemes which allow inheritance from super to sub types are a common way of representing information about the world. There are various systems and theories which use such representations plus some inferencing rules to deduce properties of objects, about which the system has only partial information. Many such systems have problems related to multiple inheritance, and have some difficulty in drawing conclusions which we as humans see as intuitively simple. We present a model of typing based on a lattice of feature descriptors. A type is represented by two important points in the lattice representing core and default information. The use of two points allows some information to be monotonic whilst other information is nonmonotonic. We give some operations which can be used in default reasoning about an object, based on knowledge about the relationships between the points in the lattice which are defined as types. We then work through some specific examples, showing the conclusions which we reach with this system. We compare the expressiveness of our system to some of the well known work in the area of default reasoning using inheritance.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-072.pdf,
72,1988,Inheritance,Beyond ISA: Structures for Plausible Inference in Semantic Networks,"Paul R. Cohen, Cynthia L. Loiselle","We present a method for automatically deriving plausible inference rules from relations in a knowledge base. We describe two empirical studies of these rules. First, we derived approximately 300 plausible inference rules, generated over 3000 specific inferences, and presented them to human subjects to discover which rules were plausible. The second study tested the hypothesis that the plausibility of these rules can be predicted by whether they obey a kind of transitivity. The paper discusses four sources of variance in subjects’ judgments, and concludes that relatively little knowledge is needed to achieve moderately accurate predictions of these judgments.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-073.pdf,
73,1988,Inheritance,Tractable Theories of Multiple Defeasible Inheritance in Ordinary Nonmonotonic Logics,Brian A. Haugh,"A suggestion by John McCarthy for general formulations of multiple defeasible inheritance in ordinary nonmonotonic logic is examined and found to suffer from a variety of technical problems, including 1) its restriction to object/class/property networks, 2) unintuitive results in ""Nixon diamond""-type networks, 3) unnecessary closed-world assumptions, and 4) susceptibility to unintended models when generalized. A family of theories is presented that substantially revises McCarthy’s formulation to avoid these problems and restrictions. Finally, an inference control strategy for computing the theory is identified whose tractability is ensured by a variety of techniques including incremental computation of abnormalities and truth maintenance.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-074.pdf,
74,1988,Inheritance,Mixing Strict and Defeasible Inheritance,"John F. Horty, Richmond H. Thomason","Commonsense or expert knowledge of any rich domain involves an intricate mixture of strict and defeasible information. The importance of representing defeasible information in an inheritance system has been widely recognized, but it is not enough for a system to represent only defeasible information: without the ability to represent strict information as well, the system cannot represent definitional relations among concepts. As a response to this difficulty, we present a well-defined and intuitively attractive theory of inheritance for IS-A hierarchies containing strict and defeasible links types mixed together.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-075.pdf,
75,1988,Inheritance,Nonmonotonlc Inheritance and Generic Reflexives,"David S. Touretzky, Richmond H. Thomason","Generic reflexive statements such as Elephants dove themselves have traditionally been formalized using some variant of predicate logic, with variables to mark coreferentiality. We present a radically different semantics for reflexives, based on nonmonotonic inheritance and an extension to Touretzky' s inferential distance ordering. Our system can derive new generic reflexive statements as well as statements about individuals. And unlike the leading predicate logic-based approaches, our formalism does not use variables; this brings it closer in structure to ac tual human languages. The significance of this work for AI is its demonstration of the benefits of a non-classical knowledge representation for analyzing commonsense reasoning phenomena.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-076.pdf,
76,1988,"NonmonotonlcLogic",Hierarchic Autoepistemic Theories for Nonmonotonic Reasoning,Kurt Konolige,"Nonmonotonic logics are meant to be a formalization of nonmonotonic reasoning. However, for the most part they fail to capture in a perspicuous fashion two of the most important aspects of such reasoning: the explicit computational nature of nonmonotonic inference, and the assignment of preferences among competing inferences. We propose a method of nonmonotonic reasoning in which the notion of inference from specific bodies of evidence plays a fundamental role. The formalization is based on autoepistemic logic, but introduces additional structure, a hierarchy of evidential subtheories. The method offers a natural formalization of many different applications of nomnonotonic reasoning, including reasoning about action, speech acts, belief revision, and various situations involving competing defaults.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-077.pdf,
77,1988,"NonmonotonlcLogic",On the Relationship Between Logic Programming and Nonmonotonlc Reasoning,Teodor C. Przymusinski,"In spite of the existence of a close relationship between logic programming and non-monotonic reasoning, in the past the two research areas have progressed largely independently of each other. Recently, however, a new declarative semantics of logic programs has been proposed and it has been shown to be equivalent to suitable forms of four major non-monotonic formalisms: McCarthy’s circumscription, Reiter’s closed world assumption, Moore’s autoepistemic logic and Reiter’s default logic. The importance of these results stems not only from the fact that they shed new light on the relationship between logic programming and non-monotonic reasoning, but also from the fact that they establish a close relationship between four major formalizations of non-monotonic reasoning for an important class of theories.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-078.pdf,
78,1988,"NonmonotonlcLogic",On the Logic of Defaults,Hector Geffner,We present an alternative interpretation of defaults which draws on probability theory and notions of relevance. The result is a system made up of a body of six rules which appears to overcome some of the weaknesses of other non-monotonic logics proposed in AI. We also analyze several examples and discuss some of the issues that require further research.,https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-079.pdf,
79,1988,"NonmonotonlcLogic",Compiling Circumscriptive Theories into Logic Programs: Preliminary Report,"Michael Gelfond, Vladimir Lifschitz","We study the possibility of reducing some special cases of circumscription to logic programming. The description of a given circumscriptive theory T can be sometimes transformed into a logic program II, so that, by running II, we can determine whether a given ground literal is provable in T. The method is applicable, in particular, to some formalizations of tree-structured inheritance systems with exceptions.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-080.pdf,
80,1988,"NonmonotonlcLogic",On Reducing Parallel Circumscription,"Li Yan Yuan, Cheng Hui Wang","Three levels of circumscription have been proposed by McCathy to formalize common sense knowledge and non-monotonic reasoning in general-purpose database and knowledge base systems. That is, basic circumscription, parallel circumscription, and priority circumscription. Basic circumscription is a special case of parallel circumscription while parallel circumscription is a special case of priority circumscription. Lifschitz has reduced priority circumscription into parallel circumscription, i.e., represented priority circumscription as a conjunction of some parallel circumscription formulas. In this paper, we have reduced parallel circumscription into basic circumscription under some restriction, i.e., parallel circumscription of a Z-conflict free first order logic formula can be represented as a conjunction of some basic circumscription formulea.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-081.pdf,
81,1988,"NonmonotonlcLogic",Some Computational Aspects of Circumscription,"Phokion G. Kolaitis, Christos H. Papadimitriou","We explore the effects of a circumscribing first-order formulae from a computational standpoint. First, extending work of V. Lifschitz, we show that the circumscription of any existential first-order formula is equivalent to a first-order formula. After this, we establish that a set of universal Horn clauses has a first-order circumscription if and only if it is bounded (when considered as a logic program); thus it is undecidable to tell whether such formulae have first-order circumscription. Finally, we show that there are first-order formulae whose circumscription has a coNP-complete model-checking problem.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-082.pdf,
82,1988,"NonmonotonlcLogic",A Circumscriptive Theorem Prover: Preliminary Report,Matthew L. Ginsberg,"We discuss the application of an assumption-based truth maintenance system to the construction of a circumscriptive theorem prover, showing that the connection discovered by Reiter and de Kleer between assumption-based truth maintenance and prime implicants relates to the notions of minimality appearing in nonmonotonic reasoning. The ideas we present have been implemented, and the resulting system is applied to the canonical birds flying example and to the Yale shooting problem. In both cases, the implementation returns the circumscriptively correct answer.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-083.pdf,
83,1988,"NonstandardLogics",Investigations into a Theory of Knowledge Base Revision: Preliminary Report,Mukesh Dalal,"A fundamental problem in knowledge representation is how to revise knowledge when new, contradictory information is obtained. This paper formulates some desirable principles of knowledge revision, and investigates a new theory of knowledge revision that realizes these principles. This theory of revision can be explained at the knowledge level, in purely model-theoretic terms. A syntactic characterization of the proposed approach is also presented. We illustrate its application through examples and compare it with several other approaches.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-084.pdf,
84,1988,"NonstandardLogics",A Logic for Hypothetical Reasoning,Anthony J. Bonner,"This paper shows that classical logic is inappropriate for hypothetical reasoning and develops an alternative logic for this purpose. The paper focuses on a form of hypothetical reasoning which appears computationally tractable. Specifically, Horn-clause logic is augmented with rules, called embedded implications, which can hypothetically add atomic formulas to a rulebase. By introducing the notion of rulebase independence, it is shown that these rules can express hypothetical queries which classical logic cannot. By adopting methods from modal logic, these rules are then shown to be intuitionistic. In particular, they form a subset of intuitionistic logic having semantic properties similar to those of Horn-clause logic.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-085.pdf,
85,1988,"NonstandardLogics",Adding Number Restrictions to a Four-Valued Terminological Logic,Anthony J. Bonner,"An intuitive four-valued semantics can be used to develop expressively powerful terminological logits which have tractable subsumption. If a four-valued identity is also used, number restrictions can be added to the logic while retaining tracts bility. The subsumptions supported by the logic are a type of ""structural"" subsumption, where each structural component of one concept must have an analogue in the other concept. Structural subsumption captures an important set of subsumptions, similar to the subsumptions computed in KL-ONE and NIKL. This shows that the trade-off between expressive power and computational tractability which plagues terminological logics based on standard, two-valued semantics can be defeated while still retaining a useful and semantically supported set of subsumptions.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-086.pdf,
86,1988,"NonstandardLogics",Normal Multimodal Logics,Laurent Catach,"This paper studies what we call normal multimodal logics, which are general modal systems with an arbitrary set of normal modal operators. We emphasize the importance of non-simple systems, for which some interaction axioms are considered. A list of such acceptable axioms is proposed, among which the induction axiom has a special behavior. The class of multimodal logics that can be built with these axioms generalizes many existing modal, temporal, dynamic and epistemic systems, and could also suggest new formalizations using modal logics. The main result is a general determination theorem for these multimodal systems, which establishes a correspondence between our axioms and conditions over Kripke frames; this should avoid the need for showing determination each time a new system is considered.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-087.pdf,
87,1988,"TemporalReasoning",The Persistence of Derived Information,"Karen L. Myers, David E. Smith","Work on the problem of reasoning about change has focused on the persistence of nonderived information, while neglecting the effects of inference within individual states. In this paper, we illustrate how such inferences add a new dimension of complexity to reasoning about change and show that failure to allow for such inferences can result in an unwarranted loss of derived information. The difficulties arise with a class of deductions having the property that their conclusions should be allowed to persist even though some components of the justifications involved may no longer be valid. We describe this notion of components of a justification being inessential to the persistence of that justification. A solution to the persistence problem is presented in terms of a default frame axiom that is sensitive to both justification information and specifications of inessentiality.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-088.pdf,
88,1988,"TemporalReasoning",Representing and Computing Temporally Scoped Beliefs,Steve Hanks,"Planning effective courses of action requires making predictions about what the world may be like at the time the actions are to be performed. Making these predictions requires a temporal representation, and-assuming a world that is not entirely predictable and an agent that is not omniscient --a representation of the uncertainty that will characterize its incomplete knowledge of the world. We provide in this paper a representation and calculus for computing an agent’s strength of belief in a proposition at a point in time, based on (possibly imperfect) observations about that proposition and information about the tendency of the proposition to persist over time.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-089.pdf,
89,1988,"TemporalReasoning","Stable Closures, Defeasible Logic and Contradiction Tolerant Reasoning",Paul Morris,"A solution to the Yale shooting problem has been previously proposed that uses so-called non-normal defaults. This approach produces a single extension. One disadvantage, however, is . that new conflicting information causes the extension to collapse. In this paper we propose a new formal counterpart to the intuitive notion of a reasonable set of beliefs. The new formalization reduces to the previous one when there are no conflicts. However, when fresh conflicting information is added, instead of collapsing it produces a revised interpretation similar to that obtained by dependency-directed backtracking in a truth maintenance system. Consideration of the relationship to relevance logic motivates the development of a new formalism for default reasoning, called Defeasible Logic, which behaves like Autoepistemic Logic, but may be more intuitive.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-090.pdf,
90,1988,"TemporalReasoning",Satisfying First-Order Constraints About Time Intervals,Peter B. Ladkin,"James Allen defined a calculus of time intervals in [A1183], as a representation of temporal knowledge that could be used in AI. We shall call this the Interval Culculus. In his paper, Allen investigated specification and constraint satisfaction in the Interval Calculus. Other constraint-satisfaction algorithms for intervals have considered subclasses of Boolean formulas only. The methods herein extend consistency-checking and constraint-satisfaction procedures to finitely many arbitrary quantified formulas in the Interval Calculus. We use a first-order theory from [LadMad87.1, LadMad88.1], that precisely corresponds to Allen’s calculus. We show that every first-order constraint expressible in this theory is equivalent to a Boolean constraint of a particular restricted form. We use this result to obtain a procedure for detecting consistency of arbitrary quantified formulas, and finding intervals that satisfy arbitrary consistent formulas of the Interval Calculus.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-091.pdf,
91,1988,"TemporalReasoning",Why Things Go Wrong: A Formal Theory of Causal Reasoning,"Leora Morgenstern, Lynn Andrea Stein","This paper presents a theory of generalized temporal reasoning. We focus on the related problems of1. Temporal Projection-determining all the facts true in a chronicle, given a partial description of that chronicle, and2. Explanation-figuring out what went wrong if an unexpected outcome occurs.We present a non-monotonic temporal logic based on the notion that actions only happen if they are motivated. We demonstrate that this theory handles generalized temporal projection correctly, and in particular, solves the Yale Shooting Problem and a related class of problems. We then show how our model lends itself to a very natural characterization of the concept of an adequate explanation for an unexpected outcome.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-092.pdf,
92,1988,"TemporalReasoning",Probabilistic Temporal Reasoning,"Thomas Dean, Keiji Kanazawa","Reasoning about change requires predicting how long a proposition, having become true, will continue to be so. Lacking perfect knowledge, an agent may be constrained to believe that a proposition persists indefinitely simply because there is no way for the agent to infer a contravening proposition with certainty. In this paper, we describe a theory of causal reasoning under uncertainty. Our theory uses easily obtainable statistical data to provide expectations concerning how long propositions are likely to persist in the absence of specific knowledge to the contrary. We consider a number of issues that arise in combining evidence, and describe an approach to computing probabilistic assessments of the sort licensed by our theory.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-093.pdf,
93,1988,Learning,The Utility of Difference-Based Reasoning,Brian Falkenhainer,"The traditional approach to problem solving examines a current situation in isolation, ignoring the existence of previous experience. More recent analogical approaches look for previous, similar cases and attempt to infer further similarity from existing similarity. What has been overlooked is the power that identifying a disanalogy provides. Identifying disanalogies enables one to learn and reason by focusing on what is different between two similar situations, rather than on what is the same. This paper describes a technique called difference-based reasoning which exploits differences found between two otherwise identical situations to focus search and generate plausible hypotheses. The technique’s power and diversity is demonstrated with implemented examples from theory formation, diagnosis, and failure explanation in planning.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-094.pdf,
94,1988,Learning,Learning from Opportunities: Storing and Re-using Execution Time Optimizations,"Krlstian Hammond, Tim Converse, Mitchell Marks","In earlier work (Hammond 1986), we proposed a mechanism for learning from execution-time plan failure. In this paper, we suggest a corollary notion of learning from execution-time planning opportunities. We argue that both are special cases of learning from expectation failure (Schank 1982). The result of this type of learning is a set of plans for frequently occurring conjuncts of goals, indexed by the features in the world that predict their usefulness. We discuss this notion, using examples from the University of Chicago planner TRUCKER, an implementation of case-based planning in the domain of a UPS-like pick-up and delivery service.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-095.pdf,
95,1988,Learning,Explanation-Based Indexing of Cases,"Ralph Barletta, William Mark","Proper indexing of cases is critically important to the functioning of a case-based reasoner. In real domains such as fault recovery, a body of domain knowledge exists that can be captured and brought to bear on the indexing problem-even though the knowledge is incomplete. Modified explanation-based learning techniques allow the use of the incomplete domain theory to justify the actions of a case with respect to the facts known when the case was originally executed. Demonstrably relevant facts are generalized to form primary indices for the case. Inconsistencies between the domain theory and the actual case can also be used to determine facts that are demonstrably irrelevant to the case. The remaining facts are treated as secondary indices, subject to refinement via similarity based inductive techniques.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-096.pdf,
96,1988,Learning,Parsing to Learn Fine Grained Rules,"Subrata Roy, Jack Mostow","The grain size of rules acquired by explanation-based learning may vary widely depending on the size of the training examples. Such variation can cause redundancy among the learned rules and limit their range of applicability. In this paper, we study this problem in the context of LEAP, the ""learning apprentice"" component of the VEXED circuit design system. LEAP acquires circuit design rules by analyzing and generalizing design steps performed by the user. We show how to reduce the grain size of rules learned by LEAP by using ""synthetic parhzg"" to extract parts of the manual design step not covered by existing design rules and then using LEAP to generalize the extracted parts. A prototype implementation of this technique yields finer grained rules with more coverage. We examine its effects on some problems associated with the explanation-based learning technique used in LEAP.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-097.pdf,
97,1988,Learning,Simulation-Assisted Inductive Learning,"Bruce G. Buchanan, John Sullivan, Tze-Pin Cheng, Scott H. Clearwater","Learning by induction can require a large number of training examples. We show the power of using a simulator to generate training data and test data in learning rules for an expert system. The induction program is RL, a simplified version of Meta-DENDRAL. The expert system is ABLE, a rule-based system that identifies and locates errors in particle beam lines used in high energy physics. A simulator of beam lines allowed forming and testing rules on sufficient numbers of cases that ABLE’s performance is demonstrably accurate and precise.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-098.pdf,
98,1988,Learning,The Automatic Acquisition of Proof Methods,Kurt Ammon,"The SHUNYATA program constructs proof methods by analyzing proofs of simple theorems in mathematical theories such as group theory and uses these methods to form prooh of new theorems in the same or in other theories. Such methods are capable of generating proof of theorems whose complexity represents the state of the art in automated theorem proving. They are composed of elementary functions such as the union of sets and the subset relation. Elementary knowledge about these functions such as descriptions of their domains and their ranges forms the basis of the method acquisition processes. These processes are controlled genetically, which means that SHUNYATA, starting from scratch, constructs a sequence of more and more powerful partial methods each of which forms the basis for the construction of its successor until a complete method is generated.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-099.pdf,
99,1988,"AnalyticLearning",Quantitative Results Concerning the Utility of Explanation-Based Learning,Steven Minton,"Although Previous research has demonstrated that EBL is a viable approach for acquiring search control knowledge, in practice the control knowledge learned via EBL may not be useful. To be useful, the cumulative benefits of applying the knowledge must outweigh the cumulative costs of testing whether the knowledge is applicable. Unlike most previous EBL systems, ODIGY/EBL system evaluates the costs and benefits of the control knowledge it learns. The system produces useful control knowledge by actively searching for ""good"" explanations --explanations that can be profitably employed to control problem solving. This paper summarizes a set of experiments measuring the effectiveness of PRODIGY’s EBL method (and its components) in several different domains.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-100.pdf,
100,1988,"AnalyticLearning",Approximate Theory Formation: An Explanation-Based Approach,Thomas Ellman,"Existing machine learning techniques have only limited capabilities of handling computationally intractable domains. This research extends explanation-based learning techniques in order to overcome such limitations. It is based on a strategy of sacrificing theory accuracy in order to gain tractability. Intractable theories are approximated by incorporating simplifying assumptions. Explanations of teacher-provided examples are used to guide a search for accurate approximate theories. The paper begins with an overview of this learning technique. Then a typology of simplifying assumptions is presented along with a technique for representing such assumptions in terms of generic functions. Methods for generating and searching a space of approximate theories are discussed. Empirical results from a testbed domain are presented. Finally, some implications of this research for the field of explanation-based learning are also discussed.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-101.pdf,
101,1988,"AnalyticLearning",IMEX: Overcoming Intractability in Explanation Based Learning,"Michael S. Braverman, Stuart J. Russell","Compiled knowledge, which allows macro inference steps through an explanation space, can enable explanation-based learning (EBL) systems to reason efficiently in complex domains. Without this knowledge, the explanation of goal concepts is not generally feasible; moreover, the problem of finding the most general operational concept definition is intractable. Unfortunately, the use of compiled knowledge leads to explanations which yield overly specific concept definitions. These concept definitions may be overly specific in one of two ways: either a similar concept definition with one or more constants changed to variables is operational, or a concept definition which is more general, according to the implication rules of the domain theory, is operational. This paper introduces a method (ME%) for modifying, in a directed manner, the explanation structures of goal concepts that have been derived using compiled knowledge. In this way, more general operational concept definitions may be obtained.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-102.pdf,
102,1988,"AnalyticLearning",Some Philosophical Problems with Formal Learning Theory,Jonathan Amsterdam,Recent work in formal learning theory has attempted to capture the essentials of the concept-learning task in a formal framework. This paper evaluates the potential contributions of certain kinds of models to the study of learning by exploring the philosophical implications of these models. Some of my remarks bear on mainstream AI learning techniques such as version spaces and explanation-based learning.,https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-103.pdf,
103,1988,"AnalyticLearning",Knowledge-Base Reduction: A New Approach to Checking Knowledge Bases for Inconsistency and Redundancy,Allen Ginsberg,"This paper presents a new approach, called knowledge-base reduction, to the problem of checking knowledge bases for inconsistency and redundancy. The algorithm presented here makes use of concepts and techniques that have recently been advocated by de Kleer [deKleer, 1986] in conjunction with an assumption-based truth maintenance system. Knowledge-base reduction is more comprehensive than previous approaches to this problem in that it can in principle detect all potential contradictions and redundancies that exist in knowledge bases (having expressive power equivalent to propositional logic). While any approach that makes such a guarantee must be computationally intractable in the worst case, experience with KB-Reducer -- a system that implements a specialized version of knowledge-base reduction and is described in this paper -- has demonstrated that this technique is feasible and effective for fairly complex ""real world"" knowledge bases. Although KB-Reducer is currently intended for use by expert system developers, it is also a first step in the direction of providing safe ""local end-user modifiability"" for distant ""sites"" in a nationwide network of expert systems.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-104.pdf,
104,1988,"AnalyticLearning",Theory Revision via Prior Operationalization,Allen Ginsberg,"Research in machine learning often focuses either on inductive learning - learning from experience with minimal reliance on prior theory - or, more recently, on explanation-based learning - deducing general descriptions from theories with minimal reliance on experience. Theory revision unites these two concerns: one must revise one' s theory in the light of experience, but one must simultaneously use information implicit in the theory in order to guide the revision process. This paper focuses on the second step of a unified three-step method for solving theory revision problems for certain classes of empirical theories. Test results for the first two phases of this approach are reported.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-105.pdf,
105,1988,"EmpiricalLearning",Credit Assignment in Genetic Learning Systems,John J. Grefenstette,"Credit assignment problems arise when long sequences of rules fire between successive external rewards. Two distinct approaches to rule learning with genetic algorithms have been developed, each approach offering a useful solution to a different level of the credit assignment problem. We present a system, called RUDI, that combines features from both approaches. Experimental results are presented that support the hypothesis that multiple levels of credit assignment can improve the performance of rule learning systems based on genetic algorithms.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-106.pdf,
106,1988,"EmpiricalLearning",Perceptron Trees: A Case Study in Hybrid Concept Representations,Paul E. Utgoff,"The paper presents a case study in examining the bias of two particular formalisms: decision trees and linear threshold units. The immediate result is a new hybrid representation, called a perceptron tree, and an associated learning algorithm called the perceptron tree error correction procedure. The longer term result is a model for exploring issues related to understanding representational bias and constructing other useful hybrid representations.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-107.pdf,
107,1988,"EmpiricalLearning",Bayesian Classification,"Peter Cheeseman, Matthew Self, Jim Kelly, Will Taylor, Don Freeman","This paper describes a Bayesian technique for unsupervised classification of data and its computer implementation, AutoClass. Given real valued or discrete data, AutoClass determines the most probable number of classes present in the data, the most probable descriptions of those classes, and each object’s probability of membership in each class. The program performs as well as or better than other automatic classification systems when run on the same data and contains no ad hoc similarity measures or stopping criteria. AutoClass has been applied to several databases in which it has discovered classes representing previously unsuspected phenomena.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-108.pdf,
108,1988,"EmpiricalLearning",Combining Symbolic Learning Techniques and Statistical Regression Analysis,Carlo Berzuini,"This paper discusses relationships between statistical modeling techniques and symbolic learning from examples, and indicates types of learnine problem where a combined viewpoint may be very helpful. A novel computational approach is proposed which combines statistical modeling with a transformation procedure which maps the statistical model onto logical decision rules for the sake of domain experts’ intuitions. The proposed algorithm is illustrated by working through a simple but challenging case-study on learning prognostic rules from clinical observational data.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-109.pdf,
109,1988,"EmpiricalLearning",Recovery from Incorrect Knowledge in Soar,John Laird,"Incorrect knowledge can be a problem for any intelligent system. Soar is a proposal for the underlying architecture that supports intelligence. It has a single representation of long-term memory and a single learning mechanism called chunking. This paper investigates the problem of recovery from incorrect knowledge in Soar. Recovery is problematic in Soar because of the simplicity of chunking: it does not modify existing productions, nor does it analyze the long-term memory during learning. In spite of these limitations, we demonstrate a domain-independent approach to recovery from incorrect control knowledge and present extensions to this approach for recovering from all types of incorrect knowledge. The key idea is to correct decisions instead of long-term knowledge. Soar’s architecture allows this corrections to occur in parallel with normal processing. This approach does not require any changes to the Soar architecture and because of Soar’s uniform representations for tasks and knowledge, this approach can be used for all tasks and subtasks in Soar.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-110.pdf,
110,1988,"FormalResults in Learning",Inferring Probabilistic Theories from Data,Edwin P. D. Pednault,"When formulating a theory based on observations influenced by noise or other sources of uncertainty, it becomes necessary to decide whether the proposed theory agrees with the data ""well enough."" This paper presents a criterion for making this judgement. The criterion is based on a gambling scenario involving an infinite sequence of observations. In addition, a rule derived from the idea of minimal-length representations is presented for selecting an appropriate theory based on a finite set of observations. A proof is briefly outlined demonstrating that the theories selected by the rule obey the success criterion given a sufficient number of observations.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-111.pdf,
111,1988,"FormalResults in Learning",Functionality in Neural Nets,L. G. Valiant,"We investigate the functional capabilities of sparse networks of computing elements in accumulating knowledge through successive learning experiences . As experiences, we consider various combinations of episodic and concept learning, in supervised or unsupervised mode, of conjunctions and of disjunctions. For these we exhibit algorithms for learning in well defined senses. Each concept or episode is expressible in terms of concepts or episodes already known, and is thus learned hierarchically, without disturbing previous knowledge. Minimal assumptions are made about the computing elements, which are assumed to be classical threshold elements with states. Also we adhere to severe resource constraints. Each new concept or episode requires storage linear in the relevant parameters, and the algorithms take very few steps. We hypothesise that in our context functionality is limited more by the communication bottlenecks in the networks than by the computing capabilities of the elements and hence that this approach may prove useful in understanding biological systems even in the absence of accurate neurophysiological models.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-112.pdf,
112,1988,"FormalResults in Learning",Learning Complicated Concepts Reliably and Usefully,"Ronald L. Rivest, Robert Sloan","We show how to learn from examples (Valiant style) any concept representable as a boolean function or circuit, with the help of a teacher who breaks the concept into subconcepts and teaches one subconcept per lesson. Each subconcept corresponds to a gate in the boolean circuit. The learner learns each subconcept from examples which have been randomly drawn according to an arbitrary probability distribution, and labeled as positive or negative instances of the subconcept by the teacher. The learning procedure runs in time polynomial in the size of the circuit. The learner outputs not the unknown boolean circuit, but rather a program which, for any input, either produces the same answer as the unknown boolean circuit, or else says ""I don’t know."" Thus the output of this learning procedure is reliable. Furthermore, with high probability the output program is nearly always useful in that it says ""I don’t know"" very rarely. A key technique is to maintain a hierarchy of explicit ""version spaces."" Our main contribution is thus a learning procedure whose output is reliable and nearly always useful; this has not been previously accomplished within Valiant’s model of learnability.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-113.pdf,
113,1988,"FormalResults in Learning",Tree-Structured Bias,Stuart J. Russell,"This paper reports on recent progress in the study of autonomous concept learning systems. In such systems, the initial space of hypotheses is considered as a first-order sentence, the declarative bias, and can thus be derived from background knowledge concerning the goal concept. It is easy to show that a simple derivation process generates a concept language corresponding to an unbiased version space defined on a restricted instance description language. However, the structure of a typical derivation corresponds to a stronger restriction still. It is shown that this semantically-motivated, tree-structured bias can in fact reduce the size of the concept language from doubly-exponential to singly-exponential in the number of features. This allows effective learning from a small number of examples.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-114.pdf,
114,1988,"KnowledgeAcquisition",Knowledge Base Refinement Using Apprenticeship Learning Techniques,David C. Wilkins,"This paper describes how apprenticeship learning techniques can be used to refine the knowledge base of an expert system for heuristic classification problems. The described method is an alternative to the long-standing practice of creating such knowledge bases via induction from examples. The form of apprenticeship learning discussed in this paper is a form of learning by watching, in which learning occurs by completing failed explanations of human problem-solving actions. An apprenticeship is the most powerful method that human experts use to refine their expertise in knowledge-intensive domains such as medicine; this motivates giving such capabilities to an expert system. A major accomplishment in this work is showing how an explicit representation of the strategy knowledge to solve a general problem class, such as diagnosis, can provide a basis for learning the knowledge that is specific to a particular domain, such as medicine.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-115.pdf,
115,1988,"MachineArchitectures and Computer Languages for AI",Towards a Virtual Parallel Inference Engine,"Howard E. Shrobe, John G. Aspinall, Neil L. Mayle","Parallel processing systems offer a major improvement in capabilities to AI programmers. However, at the moment, all such systems require the programmer to manage the control of parallelism explicitly, leading to an unfortunate intermixing of knowledge-level and control-level information. Furthermore, parallel processing systems differ radically, making a control regime that is effective in one environment less so in another. We present a means for overcoming these problems within a unifying framework in which 1) Knowledge level information can be expressed effectively 2) Information regarding the control of parallelism can be factored out and 3) Different regimes of parallelism can be efficiently supported without modification of the knowledge-level information. The Protocol of Inference introduced in [Rowley et al., 1987] forms the basis for our approach.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-116.pdf,
116,1988,"MachineArchitectures and Computer Languages for AI",Generality versus Specificity: An Experience with AI and OR Techniques,"Pascal Van Hentenryck, Jean-Philippe Carillon","This paper contains an in-depth study of a particular problem in order to evaluate several approaches to the solving of discrete combinatorial problems. We take a warehouse location problem as a case study and present solutions to it by using Integer Programming, a specialized program based on A* and the constraint logic programming CHIP. The merits of each approach are discussed and compared in the light of the problem. Finally, we conclude by arguing that CHIP provides a valuable addition to the current set of tools for solving discrete combinatorial problems.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-117.pdf,
117,1988,"MachineArchitectures and Computer Languages for AI",Knowledge-Based Real-Time Control: A Parallel Processing Perspective,"D. D. Sharma, N. S. Sridharan","Knowledge-based real-time control problems can be usefully viewed as dynamic resource allocation problems. Analysis of various real-time applications and real-time AI models reveals that real-time control problems require the problem solving capability of knowledge intensive methods coupled with the control mechanisms of operating systems. Moreover, there is an opportunity and need to exploit parallelism inherent in real-time control problems. We describe a user-programmable concurrent computation model which blends the capabilities of knowledge-based systems and operating systems. We also propose a novel set of performance measures useful for real-time AI systems.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-118.pdf,
118,1988,"Architecturesand Languages for Problem Solving",Tuning a Blackboard-Based Application: A Case Study Using GBB,"Daniel D. Corkill, Kevin Q. Gallagher","The run-time performance of a blackboard-based application can be significantly improved by selecting an appropriate blackboard database representation. We present empirical validation of this statement by tuning the representation used in a large, blackboard-based AI application. Dramatic performance gains were obtained without changing my problem solving or control activities. The results underscore the importance of efficient blackboard database operations and the benefits of a flexible, instrumented blackboard development environment when tuning the blackboard representation. This investigation was facilitated by use of the Generic Blackboard Development system (GBB) to construct the application. GBB provides the flexibility to quickly change the database implementation without recoding. Similar performance tuning capabilities are available to any application written using GBB.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-119.pdf,
119,1988,"Architecturesand Languages for Problem Solving",A Tree Representation for Parallel Problem Solving,L. V. Kalé,"A tree-representation for problem-solving suited for parallel processing is proposed. We give a formal definition of REDUCE-OR trees and illustrate it with a detailed example. Each node of the proposed tree denotes a completely described subproblem. When literals share variables, it permits solutions from one literal to prune the search space for the other literals. Attempts to get such pruning with AND-OR trees lose a significant form of ""OR parallelism."" An alternative strategy for searching AND-OR trees leads to the SLD trees, which miss the ""AND-parallelism."" The REDUCE-OR trees are especially useful for problems with a generate-and-test flavor.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-120.pdf,
120,1988,"Architecturesand Languages for Problem Solving",Parallel Hardware for Constraint Satisfaction,"Michael J. Swain, Paul R. Cooper","A parallel implementation of constraint satisfaction by arc consistency is presented. The implementation is constructed of standard digital hardware elements, used in a very fine-grained, massively parallel style. As an example of how to specialize the design, a parallel implementation for solving graph isomorphism with arc consistency is also given. Complexity analyses are given for both circuits. Worst case running time for the algorithms turns out to be linear in the number of variables n and labels a, O(an), and if the I/O must be serial, it will dominate the computation time. Fine-grained parallelism trades off time complexity for space complexity, but the number of gates required is only O(a2n2).",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-121.pdf,
121,1988,"ProductionSystems",Suitability of Message Passing Computers for Implementing Production Systems,"Anoop Gupta, Milind Tambe","Two important parallel architecture types are the shared-memory architectures and the message-passing architectures. In the past researchers working on the parallel implementations of production systems have focussed either on shared-memory multiprocessors or on special purpose architectures. Message-passing computers have not been studied. The main reasons have been the large message-passing latency (as large as a few milliseconds) and high message reception overheads (several hundred microseconds) exhibited by the first generation message-passing computers. These overheads are too large for the parallel implementation of production systems, where it is necessary to exploit parallelism at a very fine granularity to obtain significant speed-up (subtasks execute about 100 machine instructions). However, recent advances in interconnection network technology and processing node design have cut the network latency and message reception overhead by 2-3 orders of magnitude, making these computers much more interesting. In this paper we present techniques for mapping production systems onto message-passing computers. We show that using a concurrent distributed hash table data structure, it is possible to exploit parallelism at a very fine granularity and to obtain significant speed-ups from paralIelism.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-122.pdf,
122,1988,"ProductionSystems",Comparison of the Rete and Treat Production Matchers for Soar,"Pandurang Nayak, Anoop Gupta, Paul Rosenbloom","RETE and TREAT are two well known algorithms used for performing match in production systems (rule-based systems). In this paper, we compare the performance of these two algorithms in the context of Soar programs. Using the number of tokens processed by each algorithm as the performance metric, we show that the RETE algorithm performs better than the TREAT algorithm in most cases. Our results are different than the ones shown by Miranker for OPS5. The main reasons for this difference are related to the following: (i) fraction of times no joins need to be done; (ii) the long chain effect; (iii) matching of static structures; and (iv) handling of combinatorial joins. These reasons go beyond Soar in their applicability, and are relevant to other OPS5-based production systems that share some of Soar’s properties. We also discuss several implementation issues for the two algorithms.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-123.pdf,
123,1988,"ProductionSystems",Optimizing Rules in Production System Programs,Toru Ishida,"Recently developed production systems enable users to specify an appropriate ordering or a clustering of join operations. Various efficiency heuristics have been used to optimize production rules manually. The problem addressed in this paper is how to automatically determine the best join structure for production system programs. Our algorithm is not to directly apply the efficiency heuristics to programs, but rather to enumerate possible join structures under various constraints. Evaluation results demonstrate this algorithm generates a more efficient program than the one obtained by manual optimization.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-124.pdf,
124,1988,"ProductionSystems",The Challenge of Real-Time Process Control for Production Systems,"Franz Barachini, Norbert Theuretzbacher","Although the technology of expert systems has been developed substantially during the past decade, there still seems to be relatively little application to time-critical problems because of their extensive computational requirements. One application area of particular interest is that of process control. Because this area requires real-time operation, the expert system must operate within the time scale of the process involved. Here we present techniques which have been used to implement PAMELA, a language suitable for building time-critical expert systems. We discuss substantial optimizations of the well known RETE algorithm and present run-time measurements based on these optimizations. Despite the critics on RETE' s real-time behaviour we show that the presented optimizations and extensions will cover the demands of many real-time applications. In order to eflciently support process control applications, some useful language constructs concerning interrupt handling and rule interruption are discussed.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-125.pdf,
125,1988,Syntax,Representing Pronouns in Logical Form: Computational Constraints and Linguistic Evidence,Mary P. Harper,"In this paper, we discuss the representation of pronouns in logical form for the purpose of handling verb phrase ellipsis. In particular, we discuss two factors which influence the representation of pronouns in a computational model. The first is computational, the other linguistic. Both factors must be attended to in order to construct a good representation for pronouns in logical form. We review past attempts to represent pronouns in logical form for the purpose of handling verb phrase ellipsis, and show how these approaches do not meet the computational constraints outlined in this paper. We also show that, they do not, handle a rather simple example of verb phrase ellipsis. We develop a representation for pronouns in logical form which both meets the computational criteria outlined in this paper and handles the verb phrase ellipsis example.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-126.pdf,
126,1988,Syntax,Principle-Based Interpretation of Natural Language Quantifiers,Samuel S. Epstein,"This paper describes a working prototype that determines possible relative quantifier scopes and pronoun bindings for natural language sentences, with coverage of a variety of problematic cases. The prototype parses a significant fragment of English, positing empty categories and deriving various relationships among constituents in addition to dominance. It applies cross-linguistically valid principles of Government-Binding theory to compute a set of ""Logical Forms"" for each sentence it parses, and to derive possible relative quantifier scopes from these Logical Forms. It then translates sentences into an enriched predicate logic. Simple principles apply to these translations to determine possibilities for interpretation of pronouns as bound variables. The prototype’s scope and binding modules correspond transparently to elements of a principle-based grammar. Principles apply as filters. All processing is nevertheless highly efficient. The computational techniques employed in the prototype may find wider application in principle-based language processing.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-127.pdf,
127,1988,Speech,The Integration of Unification-Based Syntax/Semantics and Memory-Based Pragmatics for Real-Time Understanding of Noisy Continuous Speech Input,"Hideto Tomabechi, Masaru Tomita",Real-time understanding of speech input is difficult especially because the input is often noisy and elliptic. Multiple morphophonemic and lexical hypotheses generated for a single input sentence cannot be resolved by local semantics alone. We have developed a system in which unilication-based parsing of speech input is integrated with thematic memory-based spreading activation that supplies extra-sentential knowledge that can help to disambiguate noisy and elliptic real-time speaker-independent continuous speech input.,https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-128.pdf,
128,1988,Speech,Using Dialog-Level Knowledge Sources to Improve Speech Recognition,"Alexander G. Hauptmann, Sheryl R. Young, Wayne H. Ward","We motivate and describe an implementation of the MINDS* speech recognition system. MINDS uses knowledge of dialog structures, user goals and focus in a problem solving situation. The knowledge is combined to form predictions which translate into dynamically generated semantic network grammars. An experiment evaluated recognition accuracy given different levels of knowledge as constraints. Our results show that speech recognition accuracy improves dramatically, when the maximally constrained dynamic network grammar is used to process the speech input signal.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-129.pdf,
129,1988,Speech,Data-Driven Execution of Multi-Layered Networks for Automatic Speech Recognition,"Renato De Mori, Yoshua Bengio, Régis Cardin",A set of Multi-Layered Networks (MLN) for Automatic Speech Recognition (ASR) is proposed. Such a set allows the integration of information extracted with variable resolution in the time and frequency domains and to keep the number of links between nodes of the networks small in order to allow significant generalization during learning with a reasonable training set size. Subsets of networks can be executed depending on preconditions based on descriptions of the time evolution of signal energies allowing spectral properties that are significant in different acoustic situations to be learned. Preliminary experiments on speaker-independent recognition of the letters of the E-set are reported. Voices from 70 speakers were used for learning. Voices of 10 new speakers were used for test. An overall error rate of 9.5% was obtained in the test showing that results better than those previously reported can be achieved.,https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-130.pdf,
130,1988,Dialogue,Acquiring Lexical Knowledge from Text: A Case Study,"Paul Jacobs, Uri Zemik","Language acquisition addresses two important text processing issues. The immediate problem is understanding a text in spite of the existence of lexical gaps. The long term issue is that the understander must incorporate new words into its lexicon for future use. This paper describes an approach to constructing new lexical entries in a gradual process by analyzing a sequence of example texts. This approach permits the graceful tolerance of new words while enabling the automated extension of the lexicon. Each new acquired lexeme starts as a set of assumptions derived from the analysis of each word in a textual context. A variety of knowledge sources, including morphological, syntactic, semantic, and contextual knowledge, determine the assumptions. These assumptions, along with justifications and dependencies, are interpreted and refined by a learning program that ultimately updates the system’s lexicon. This approach uses existing linguistic knowledge, and generalization of multiple occurrences, to create new operational lexical entries.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-131.pdf,
131,1988,Dialogue,The Interpretation of Temporal Relations in Narrative,"Fei Song, Robin Cohen","This paper describes an algorithm for the interpretation of temporal relations between events mentioned in narrative (such as which event occurs before another). These relations are decided through three different levels of linguistic concepts: aspectual information for verbs, time relations for tenses, and time relations between clauses and/or sentences. One contribution of this paper is to present a more rigorous description for time relations of tenses, which is able to express all the 16 tenses in English and incorporate the interval properties of events from the aspectual analysis into the tense relations. For interpreting time relations between clauses, we emphasize the use of anaphoric references to events and introduce the concept of a situational description for an event (including the participants, place, time duration, etc.), used to make the interpreting algorithm deterministic, i.e. the set of interpreting rules are applied in a fixed order rather than in parallel. Last, we suggest a tree-like structure for the representation of temporal relations between events, which allows us to include vaguely specified relations (which may be clarified later), to facilitate the interpretation of subsequent temporal relations.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-132.pdf,
132,1988,Dialogue,Beyond Semantic Ambiguity,"Galina Datskovsky Moerdler, Kathleen R. McKeown","An advice giving system, such as an expert system gathers information from a user in order to provide advice. In this type of dialogue a single user statement or question may map into several facts of an underlying system, while several non consecutive statements may derive only one such fact. To support this type of interaction, a truly flexible natural language interface must be able to handle an extended notion of semantic ambiguity; it must avoid failure on producing partial semantic interpretations and be able to gather additional information for the interpretations from subsequent input. In this paper we describe a semantic mechanism that is able to handle this type of semantic ambiguity, while retaining other desirable properties of a general semantic interpreter.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-133.pdf,
133,1988,Dialogue,Exploiting User Expertise in Answer Expression,David N. Chin,"Previous natural language help systems have not taken into account the user’s knowledge when formulating answers. Such pragmatic information is needed to formulate more concise and helpful answers. By not repeating things that the user already knows, a system can provide more succinct answers that, because they focus on pertinent new facts, are easier to understand. A users’s prior knowledge also allows a system to utilize special teaching formats such as similes. This process of refining answers using pragmatic information is called answer expression. It has been implemented in the UCExpress component of UC (UNIX Consultant), a natural language system that helps users solve problems in using UNIX. UCExpress separates answer expression into two phases: pruning and formatting. During pruning, subconcepts of the answer are marked as not needing generation when they are already known by the user, or marked as candidates for generating anaphora or ellipsis when they are part of the conversational context. During formatting, UCExpress uses information about the user’s prior domain knowledge to select among specialized expository formats, such as similes and examples, for expressing information to the user. These formats allow UCExpress to present different types of information clearly and concisely.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-134.pdf,
134,1988,Robotics,Compliance Viewed as Programming a Damped Spring,Stephen J. Buckley,"Parts mating often requires the use of compliant motions, which cause an object in the grasp of a robot to slide along obstacles in its environment. This paper is about the interface between a compliant motion programming system and a compliant motion control system. We propose that in this interface the robot can be modeled as a damped spring. This model allows the programming system to specify and reason about compliant motions without worrying about low-level control details. The utility of the damped spring model is demonstrated by applications in teaching and planning of compliant motion strategies.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-135.pdf,
135,1988,Robotics,Reasoning About Grasping,S. A. Stansfield,"The promise of robots for the future is that of intelligent, autonomous machines functioning in a variety of tasks and situations. If this promise is to be met, then it is vital that robots be capable of grasping and manipulating a wide range of objects in the execution of highly variable tasks. A current model of human grasping divides the grasp into two stages, a precontact stage and a postcontact stage. In this paper, we present a rule-based reasoning system and an object representation paradigm for a robotic system which utilizes this model to reason about grasping during the precontact stage. Sensed object features and their spatial relations are used to invoke a set of hand preshapes and reach parameters for the robot arm/hand. The system has been implemented in PROLOG and results are presented to illustrate how the system functions.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-136.pdf,
136,1988,Robotics,"A Robust, Qualitative Method for Robot Spatial Learning","Benjamin J. Kuipers, Yung-Tai Byun","We present a qualitative method for a mobile robot to explore an unknown environment and learn a map, which can be robust in the face of various possible errors in the real world. Procedural knowledge for the movement, topological modeI for the structure of the environment, and metrical information for geometrical accuracy are separately represented in our method, whereas traditional methods describe the environment mainly by metrical information. The topological model consists of distinctive places and local travel edges linking nearby distinctive places. A distinctive place is defined as the local maximum of some measure of distinctiveness appropriate to its immediate neighborhood, and is found by a hill-climbing search. Local travel edges are defined in terms of local control strategies required for travel. How to find distinctive places and follow edges is the procedural knowledge which the robot learns dynamically during exploration stage and guides the robot in the navigation stage. An accurate topological model is created by linking places and edges, and allows metrical information to be accumulated with reduced vulnerability to metrical errors. We describe a working simulation in which a robot, NX, with range sensors explores a variety of 2-D environments and we give its successful results under varying levels of random sensor error.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-137.pdf,
137,1988,Robotics,Subassembly Stability,"Nico Boneschanscher, Hans van der Drift, Stephen J. Buckley, Russell H. Taylor","Planning a product assembly requires that we determine the order in which the product subparts are to be assembled. One constraint on this ordering is that the subassembly must be stable at each stage under the gravitational force and the insertion force of the next part to be assembled. In this paper, we discuss the stability problem for the case where the subassembly sits on a table. A program has been written to solve this problem for a class of subassemblies. The input to the program consists of a model of the subparts and their interconnections, and a set of external insertion forces. The program tests whether the total disturbance force is contained in the set of all stable forces between each subpart and the table. A linearized model of friction in six dimensions is used in the computation.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-138.pdf,
138,1988,"IntegratedRobotics",Inferring Ignorance from the Locality of Visual Perception,Ernest Davis,"This paper presents a logical theory that supports high-level reasoning about knowledge and perception. We construct a formal language in which perception can be described. Using this language, we state some fundamental axioms, and we show that these are sufficient to justify some elementary but interesting inferences about perception. In particular, our axioms make it possible in some cases to infer that an agent does not know about a particular event, because he has had no way to find out about it.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-139.pdf,
139,1988,"IntegratedRobotics",Connectionist Networks for Learning Coordinated Motion in Autonomous Systems,"Jahir Pabon, David Gossard","A central problem inherent to autonomous systems is the absence of an external reference frame in which sensory inputs can be interpreted. It is hypothesized that, in natural systems, sensory information is transformed into a consistent internal representation that serves as an internal invariant reference frame. This paper presents a hierarchical connectionist network for learning coordinated motion in an autonomous robot. The robot model used in the adaptation studies consists of three subsystems: an eye-like visual receptor, a head, and an arm. The network contains a hierarchy of adaptive subnetworks for processing sensory information. The performance of the hierarchical system was observed to improve towards an asymptotic value. The performance was found to be one order of magnitude better than that of non-hierarchical systems. This suggests that the intermediate layers may be serving as an internal invariant reference frame for the robot.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-140.pdf,
140,1988,"IntegratedRobotics",Situated Vision in a Dynamic World: Chasing Objects,"Ian Douglas Horswill, Rodney Allen Brooks","We describe a system that approaches and follows arbitrary moving objects in real time using vision as its only sense. The system uses multiple simple vision computations which, although individually unreliable, complement each other in a manner mediated by a situated control network. The objects can move over a wide variety of backgrounds including those with strong secondary reflections from light sources. Previously unseen objects can be tracked against backgrounds that include other moving objects. Computations are carried out in image coordinates at roughly 5 frames per second on a Lisp machine. The camera need not be calibrated or aligned well, and the system can tolerate a wide range of dynamically changing actuator response characteristics.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-141.pdf,
141,1988,"UserInterfaces",Knowledge-Based Spreadsheets,"Gerhard Fischer, Christian Rathke","Spreadsheet systems have changed the way the world perceives and deals with computers. In an attempt at maintaining the positive elements of spreadsheets while overcoming some of their limitations, we have developed FINANZ, a computational environment for developing financial planning systems. FINANZ contains a form-based user interface construction system, which allows the creation of advanced user interfaces without the need for conventional programming. It uses constraint based programming for the representation of knowledge about the application domain. Its layered architecture (based on object-oriented knowledge representation) supports the modification and extension of the system and the dynamic generation of explanations.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-142.pdf,
142,1988,"UserInterfaces",Automatic Construction of User-lnterface Displays,"Yigal Arens, Lawrence Miller, Stuart C. Shapiro, Norman K. Sondheimer","Construction of user interfaces for most computer applications remains time consuming and difficult. This is particularly true when the user interface system must dynamically create displays integrating the use of several interface modes. This paper shows how Artificial Intelligence knowledge base and rule technology can be used to address this problem. NIKL is used to model the entities of the application domain and the facilities of the user interface. Rules are written connecting the two models. These rules range from application specific to general rules of presentation. The situation to be displayed is asserted into a PENN1 database. A Presentation Designer interprets this data using the domain model, chooses the appropriate rules to use in creating the display, and creates a description of the desired display in terms of the interface model. A system, Integrated Interfaces, using this design for an integrated multi-modal map graphics, natural language, menu, and form interface has been created and applied to a database reporting application.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-143.pdf,
143,1988,"UserInterfaces",Specification by Reformulation: A Paradigm for Building Integrated User Support Environments,"John Yen, Robert Neches, Michael DeBellis","Specification by reformulation is a general interface paradigm. It is an abstraction of retrieval by reformulation, a paradigm used in previous systems for assisting users in formulating database queries. Specification by reformulation serves as a general foundation upon which domain specific applications can be built. To illustrate its usage, we describe three services built within it: a database retrieval aid, a notecards facility, and an electronic-mail interface to an on-line procurement system. Building systems in this way illustrates the concept of an integrated user support environment - a set of cooperating tools for end users that can be extended by application builders.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-144.pdf,
144,1988,"UserInterfaces",Multi-Modal References in Human-Computer Dialogue,"Jeannette G. Neal, Zuzana Dobes, Keith E. Bettinger, Jong S. Byoun","Multi-modal communication is common among humans. People frequently supplement natural language (NL) communication with simultaneous coordinated pointing gestures and drawing on ancillary visual aids. Similar multi-modal communication can facilitate human interaction with modern sophisticated information processing and decision-aiding computer systems. In this paper, we focus on the use of deictic pointing gestures with simultaneous coordinated NL in both user input and system-generated output. Key knowledge sources and methodology for referent resolution are presented. The synergistic mutual disambiguation of simultaneous NL and pointing is discussed as well as a methodology for handling inconsistent NL/pointing expressions and expressions that have an apparent null referent. This work is part of the Intelligent Multi-Media Interface Project (Neal and Shapiro, 1988) which is devoted to the development of intelligent interface technology that integrates speech, NL text, graphics, and pointing gestures for human-computer dialogues.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-145.pdf,
145,1988,Vision,On the Extraction of Shape Information from Shading,Alex Pentland,"We present a closed-form solution to the problem extracting shape information from image shading, given standard assumptions and oblique illumination. Neither integration nor iterative propagation of information is required. An improved method for estimating the illuminant direction is also presented.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-146.pdf,
146,1988,Vision,Feature Recognition Using Correlated Information Contained in Multiple Neighborhoods,Andrea Califano,"Parameter transforms play a very important role in the recognition of geometric features in image data. Local operators devised to compute parametric descriptions of geometric entities using a small neighborhood p(x,y) about points of interest have been succesfully employed. These operators fail to exploit the long distance correlations present in the image (distant points belonging to the same feature). Thus, their accuracy decreases with the order of the parametric properties (e.g., position, direction, curvature, torsion, etc.) and they are very sensitive to noise. This paper presents a generalized neighborhood concept that allows parameter-extraction operators to use the joint information of different portions of the same feature. This produces up to a few orders of magnitude improvement in accuracy (signal/noise ratio) and a smoother response of the transform. A general framework, based on a connectionist approach, is presented to deal with the complex response in parameter space generated by such operators. A layered and concurrent scheme to extract 3D surfaces intersection curves is presented which, exploiting the properties of these operators, is able to reconstruct lines and conic sections in three-space.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-147.pdf,
147,1988,Vision,Performance of a System to Locate Address Blocks on Mail Pieces,"Ching-Huei Wang, Paul W. Palumbo, Sargur N. Srihari","The objective of an Address Block Location System (ABLS) is to determine the position and orientation of a destination address block in a mail piece image of either a letter, magazine, or parcel. The corresponding sub-image can then be presented to either a human or machine reader (OCR) to direct the mail piece to the appropriate sort category based on the ZIP code. ABLS is capable of dealing with a wide range of environments from those having a high degree of global spatial structure to those with no structure. The system consists of several specialized tools and a control structure so that the tools are opportunistically invoked and coordinated. Its performance on a training and testing image database of difficult cases is described.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-148.pdf,
148,1988,"InvitedTalks and Panels",Future Intelligent Information Systems: AI and Database Technologies Working Together,Michael L. Brodie,The effective application of AI Technology and the development of future computing systems require the integration of AI and Database Technologies. The integration will benefit both AI and Databases and will substantially advance the state of computing.,https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-149.pdf,
149,1988,"InvitedTalks and Panels",How to Draw Three People in a Botanical Garden,Harold Cohen,"AARON is a program designed to investigate the cognitive principles underlying visual representation. Under continuous development for fifteen years, it is now able autonomously to make ""freehand"" drawings of people in garden-like settings. This has required a complex interplay between two bodies of knowledge: object-specific knowledge of how people are constructed and how they move, together with morphological knowledge of plant growth: and procedural knowledge of representational strategy. AARON' s development through the events leading up to this recently-implemented knowledge-based form is discussed as an example of an ""expert' s system"" as opposed to an ""expert system."" AARON demonstrates that, given appropriate interaction between domain knowledge and knowledge of representational strategy, relatively rich representations may result from sparse information.",https://aaai.org/Library/AAAI/1988/../../../Papers/AAAI/1988/AAAI88-150.pdf,
