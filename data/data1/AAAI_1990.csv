,conference_year,category,title,author,abstract,download_url,keywords
0,1990,"ConstraintSatisfaction Problems",Complexity of K-Tree Structured Constraint Satisfaction Problems,Eugene C. Freuder,"Trees have played a key role in the study of constraint satisfaction problems because problems with tree structure can be solved efficiently. It is shown here that a family of generalized trees, k-trees, can offer increasing representational complexity for constraint satisfaction problems, while maintaining a bound on computational complexity linear in the number of variables and exponential in k. Additional results are obtained for larger classes of graphs known as partial k-trees. These methods may be helpful even when the original problem does not have k-tree or partial k-tree structure. Specific tradeoffs are suggested between representational power and computational complexity.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-001.pdf,
1,1990,"ConstraintSatisfaction Problems",Tree Decomposition with Applications to Constraint Processing,"Itay Meiri, Rina Dechter, Judea Pearl","This paper concerns the task of removing redundant information from a given knowledge base, and restructuring it in the form of a tree, so as to admit efficient problem solving routines. We offer a novel approach which guarantees the removal of all redundancies that hide a tree structure. We develop a polynomial time algorithm that, given an arbitrary constraint network, generates a precise tree representation whenever such a tree can be extracted from the input network, otherwise, the fact that no tree representation exists is acknowledged, and the tree generated may serve as a good approximation to the original network.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-002.pdf,
2,1990,"ConstraintSatisfaction Problems",Solving Large-Scale Constraint Satisfaction and Scheduling Problems Using a Heuristic Repair Method,"Steven Minton, Mark D. Johnston, Andrew B. Philips, Philip Laird","This paper describes a simple heuristic method for solving large-scale constraint satisfaction and scheduling problems. Given an initial assignment for the variables in a problem, the method operates by searching though the space of possible repairs. The search is guided by an ordering heuristic, the min-conflicts heuristic, that attempts to minimize the number of constraint violations after each step. We demonstrate empirically that the method performs orders of magnitude better than traditional backtracking techniques on certain standard problems. For example, the one million queens problem can be solved rapidly using our approach. We also describe practical scheduling applications where the method has been successfully applied. A theoretical analysis is presented to explain why the method works so well on certain types of problems and to predict when it is likely to be most effective.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-003.pdf,
3,1990,"ConstraintSatisfaction Problems",Dynamic Constraint Satisfaction Problems,"Sanjay Mittal, Brian Falkenhainer","Constraint satisfaction (CSP) is a powerful and extensively used framework for describing search problems. A CSP is typically defined as the problem of finding consistent assignment of values to a fixed set of variables given some constraints over these variables. However, for many synthesis tasks such as configuration and model composition, the set of variables that are relevant to a solution and must be assigned values changes dynamically in response to decisions made during the course of problem solving. In this paper, we formalize this notion as a dynamic constraint satisfaction problem that uses two types of constraints. Compatibility constraints correspond to those traditionally found in CSPs, namely, constraints over the values of variables. Activity constraints describe conditions under which a variable may or may not be actively considered as a part of a final solution. We present a language for expressing four types of activity constraints in terms of variable values and variables being considered. We then describe an implemented algorithm that enables tight interaction between constraints about variable activity and constraints about variable values. The utility of this approach is demonstrated for configuration and model composition tasks.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-004.pdf,
4,1990,"ConstraintSatisfaction Problems",The Complexity of Constraint Satisfaction in Prolog,Bernard A. Nadel,"We obtain here the complexity of solving a type of Prolog problem which Genesereth and Nilsson have called sequential constraint satisfactrlon. Such problems are of direct relevance to relational database retrieval as well as providing a tractable first step in analyzing Prolog problem-solving in the general case. The present paper provides the first analytic expressions for the expected complexity of solving sequential constraint satisfaction problems. These expressions provide a basis for the formal derivation of heuristics for such problems, analogous to the theory-based heuristics obtained by the author for traditional constraint satisfaction problem-solving. A first application has been in providing a formal basis for Warren’s heuristic for optimally ordering the goals in a conjunctive query. Due to the incorporation of ""constraint looseness"" into the analysis, the expected complexity obtained here has the useful property that it is usually quite accurate even for individual problem instances, rather than only for the assumed underlying problem class as a whole. Heuristics based on these results can be expected to be equally instance-specific. Preliminary results for Warren’s heuristic have shown this to be the case.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-005.pdf,
5,1990,"ConstraintSatisfaction Problems",An Algebraic Approach to Conflict Resolution in Planning,Qiang Yang,"This paper presents an algebra for conflict resolution in nonlinear planning. A set of conflicts in a plan is considered as a constraint network. Each node in the network represents a conflict, and is associated with a set of alternative ways for resolving it. Thus, resolving conflicts in a plan corresponds to selecting a set of consistent resolution methods so that, after they are applied to the plan, every conflict can be removed. The paper discusses the representional issues related to the conflict resolution, presents an algebra for resolving conflicts, and illustrates that some modified algorithms for preprocessing networks of constraints can greatly enhance the efficiency of conflict resolution.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-006.pdf,
6,1990,"ConstraintSatisfaction Problems",Some Applications of Graph Bandwidth to Constraint Satisfaction Problems,Ramin Zabih,"Bandwidth is a fundamental concept in graph theory which has some surprising applications to a class of AI search problems. Graph bandwidth provides a link between the syntactic structure of a constraint satisfaction problem (CSP) and the complexity of the underlying search task. Bandwidth can be used to define a new class of easy CSP’s, namely those that have limited constraint graph bandwidth. These CSP’s can be solved in polynomial time, essentially by divide and conquer. This in turn suggests that bandwidth provides a mathematical measure of the decomposability of a search problem. In addition, bandwidth supplies a measure for comparing different search orderings for a given CSP. Statistical analysis suggests that backtracking with small bandwidth orderings leads to a more efficient search than that obtained under orderings with larger bandwidths. Small bandwidth orderings also limit the pruning that can be done by intelligent backtracking. If small bandwidth orderings are indeed advantageous, then a large number of heuristics developed in numerical analysis to find such orderings may find applicability to solving constraint satisfaction problems.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-007.pdf,
7,1990,"Distributedand Parallel Systems",An Organizational Approach to Adaptive Production Systems,"Toru Ishida, Makoto Yokoo, Les Gasser","Recently-developed techniques have improved the performance of production systems several times over. However, these techniques are not yet adequate for continuous problem solving in a dynamically changing environment. To achieve adaptive real-time performance in such environments, we use an organization of distributed production system agents, rather than a single monolithic production system, to solve problems. Organization self-design is performed to satisfy real-time constraints and to adapt to changing resource requirements. When overloaded, individual agents decompose themselves to increase parallelism, and when the load lightens the agents compose with each other to free hardware resources. In addition to increased performance, generalizations of our composition/decomposition approach provide several new directions for organization self-design, a pressing concern in Distributed AI.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-008.pdf,
8,1990,"Distributedand Parallel Systems",The Design of a Marker Passing Architecture for Knowledge Processing,"Wing Lee, Dan Moldovan","Knowledge processing is very demanding on computer architectures. Knowledge processing generates subcomputation paths at an exponential rate. It is memory intensive and has high communication requirements. Marker passing architectures are good candidates to solve knowledge processing problems. In this paper, we justify the design decisions made for the Semantic Network Array Processor (SNAP). Important aspects of SNAP are: the instruction set, markers, relations, propagation rules, interconnection network, and granularity. These features are compared to those in NETL and the Connection Machine.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-009.pdf,
9,1990,"Distributedand Parallel Systems",A Parallel Asynchronous Distributed Production System,"James G. Schmolze, Suraj Goel","To speed up production systems, many researchers have turned to parallel implementations. We describe a system called PARS that executes production rules in parallel. PARS is novel because it (1) executes many rules simultaneously, (2) runs in a highly asynchronous fashion, and (3) runs on a distributed memory machine. Item (1) improves available concurrency over systems that only perform the MATCH step in parallel. Item (2) reduces bottlenecks over synchronous parallel production systems. Item (3) makes the techniques more available given the lower cost of distributed versus shared memory machines. The two main problems regarding correctness, namely serialization and the maintenance of consistent distributed databases, are addressed and solved. Estimates of the effectiveness of this approach are also given.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-010.pdf,
10,1990,"DistributedArtificial Intelligence",Distributed Truth Maintenance,"David Murray Bridgeland, Michael N. Huhns","In this paper we define the concept of logical consistency of belief among a group of computational agents that are able to reason nonmonotonically. We then provide an algorithm for truth maintenance that guarantees local consistency for each agent and global consistency for data shared by the agents. Furthermore, we show the algorithm to be complete, in the sense that if a consistent state exists, the algorithm will either find it or report failure. The algorithm has been implemented in the RAD distributed expert system shell.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-011.pdf,
11,1990,"DistributedArtificial Intelligence",DARES: A Distributed Automated Reasoning System,"S.E. Conry, D.J. MacIntosh, R.A. Meyer","In many domains of interest to distributed artificial intelligence, the problem of solving environment may be viewed as a collection of loosely coupled intelligent agents, each of which reasons based on its own incomplete knowledge of the state of the world. No agent has sufficient knowledge to solve the problem at hand so that coordinated cooperative problem solving is required to satisfy system goals. In this paper, we present DARES, a distributed reasoning system in which agents have the ability to focus their attention on selective information interchange to facilitate cooperative problem solving. The experimental results we present demonstrate that agents in a loosely coupled network of problem solvers can work semi-independently, yet focus their attention with the aid of relatively simple heuristics when cooperation is appropriate. These results suggest that we have developed an effective cooperation strategy which is largely independant of initial knowledge distribution.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-012.pdf,
12,1990,"DistributedArtificial Intelligence",A Hierarchical Protocol for Coordinating Multiagent Behaviors,"Edmund H. Durfee, Thomas A. Montgomery","We describe how a behavior hierarchy can be used in a protocol that allows AI agents to discover and resolve interactions flexibly. Agents that initially do not know with whom they might interact use this hierarchy to exchange abstractions of their anticipated behaviors. By comparing behaviors, agents iteratively investigate interactions through more focused exchanges of successively detailed information. They can also modify their behaviors along different dimensions to either avoid conflicts or promote cooperation. We explain why our protocol gives agents a richer language for coordination than they get through exchanging plans or goals, and we use a prototype implementation to illustrate our protocol. We argue that our hierarchical protocol for coordinating behaviors provides a powerful representation for negotiation and can act as a common foundation for integrating theories about plans and organizations.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-013.pdf,
13,1990,"DistributedArtificial Intelligence",On Acting Together,"Hector J. Levesque, Philip R. Cohen, José H.T. Nunes","Joint action by a team does not consist merely of simultaneous and coordinated individual actions; to act together, a team must be aware of and care about the status of the group effort as a whole. We present a formal definition of what it could mean for a group to jointly commit to a common goal, and explore how these joint commitments relate to the individual commitments of the team members. We then consider the case of joint intention, where the goal in question involves the team performing some action. In both cases, the theory is formulated in a logical language of belief, action, and time previously used to characterize individual commitment and intention. An important consequence of the theory is the types of communication among the team members that it predicts will often be necessary.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-014.pdf,
14,1990,"DistributedArtificial Intelligence",Negotiation and Conflict Resolution in Non-Cooperative Domains,"Gilad Zlotkin, Jeffrey S. Rosenschein","In previous work [Zlotkin and Rosenschein, 1989a], we have developed a negotiation protocol and offered some negotiation strategies that are in equilibrium. This negotiation process can be used only when the ""negotiation set"" (NS) is not empty. Domains in which the negotiation sets are never empty are called cooperative domains; in general non-cooperative domains, the negotiation set is sometimes empty. In this paper, we present a theoretical negotiation model for rational agents in general non-cooperative domains. Necessary and sufficient conditions for cooperation are outlined. By redefining the concept of utility, we are able to enlarge the number of situations that have a cooperative solution. An approach is offered for conflict resolution, and it is shown that even in a conflict situation, partial cooperative steps can be taken by interacting agents (that is, agents in fundamental conflict might still agree to cooperate up to a certain point). A Unified Negotiation Protocol is developed that can be used in all cases. It is shown that in certain borderline cooperative situations, a partial cooperative agreement (i.e., one that does not achieve all agents’ goals) might be preferred by all agents, even though there exists a rational agreement that would achieve all their goals.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-015.pdf,
15,1990,"EvidentialReasoning",Probabilistic Semantics for Cost Based Abduction,"Eugene Charniak, Solomon E. Shimony","Cost-based abduction attempts to find the best explanation for a set of facts by finding a minimal cost proof for the facts. The costs are computed by summing the costs of the assumptions necessary for the proof plus the cost of the rules. We examine existing methods for constructing explanations (proofs), as a minimization problem on a DAG. We then define a probabilistic semantics for the costs, and prove the equivalence of the cost minimization problem to the Bayesian network MAP solution of the system.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-016.pdf,
16,1990,"EvidentialReasoning",Two Views of Belief: Belief as Generalized Probability and Belief as Evidence,"Joseph Y. Halpern, Ronald Fagin","Belief functions are mathematical objects defined to satisfy three axioms that look somewhat similar to the axioms defining probability functions. We argue that there are (at least) two useful and quite different ways of understanding belief functions. The first is as a generalized probability function (which technically corresponds to the lower envelope or intimum of a family of probability functions). The second is as a way of representing evidence. Evidence, in turn, can be understood as a mapping from probability functions to probability functions. It makes sense to think of tipdating a belief if we think of it as a generalized probability. On the other hand, it makes sense to combine two beliefs (using, say, Dempster’s rule of combination) only if we think of the belief functions as representing evidence. Many previous papers have pointed out problems with the belief function approach; the claim of this paper is that these problems can be explained as a consequence of confounding these two views of belief functions.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-017.pdf,
17,1990,"EvidentialReasoning",The Belief Calculus and Uncertain Reasoning,Yen-Teh Hsia,"We formulate the Dempster-Shafer formalism of belief functions [Shafer 76] in the spirit of logical inference systems. Our formulation (called the belief calculus) explicitly avoids the use of set-theoretic notations. As such, it serves as an alternative for the use of the Dempster-Shafer formalism for uncertain reasoning.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-018.pdf,
18,1990,"EvidentialReasoning",Symbolic Probabilistic Inference in Belief Networks,"Ross D. Shachter, Bruce D'Ambrosio, Brendan A. Del Favero","The Symbolic Probabilistic Inference (SPI) Algorithm [D'Ambrosio, 1989] provides an efficient framework for resolving general queries on a belief network. It applies the concept of dependency-directed backward search to probabilistic inference, and is incremental with respect to both queries and observations. Unlike most belief network algorithms, SPI is goal directed, performing only those calculations that are required to respond to queries. The directed graph of the underlying belief network is used to develop a tree structure for recursive query processing. This allows effective caching of intermediate results and significant opportunities for parallel computation. A simple preprocessing step ensures that, given the search tree, the algorithm will include no unnecessary distributions. The preprocessing step eliminates dimensions from the intermediate results and prunes the search path.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-019.pdf,
19,1990,Planning,An Approach to Reasoning About Continuous Change for Applications in Planning,"Thomas Dean, Greg Siegle","There are many planning applications that require an agent to coordinate its activities with processes that change continuously over time. Several proposals have been made for combining a temporal logic of time with the differential and integral calculus to provide a hybrid calculus suitable for planning applications. We take one proposal and explore some of the issues involved in implementing a practical system that derives conclusions consistent with such a hybrid calculus. Models for real-valued parameters are specified as systems of ordinary differential equations, and constructs are provided for reasoning about how these models change over time. For planning problems that require projecting the consequences of a set of events from a set of initial conditions and causal rules, a combination of numerical approximation and symbolic math routines and a simple default reasoning strategy provide for an efficient inference engine.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-020.pdf,
20,1990,Planning,Anytime Synthetic Projection: Maximizing the Probability of Goal Satisfaction,"Mark Drummond, John Bresinai","This paper presents a projection algorithm for incremental control rule synthesis. The algorithm synthesizes an initial set of goal-achieving control rules using a combination of situation probability and estimated remaining work as a search heuristic. This set of control rules has a certain probability of satisfying the given goal. The probability is incrementally increased by synthesizing additional control rules to handle ""error"" situations the execution system is likely to encounter when following the initial control rules. By using situation probabilities the algorithm achieves a computationally effective balance between the limited robustness of triangle tables and the absolute robustness of universal plans.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-021.pdf,
21,1990,Planning,"Incremental, Approzimate Planning",Charles Elkan,"This paper shows how using a nonmonotonic logic to describe the effects of actions enables plausible plans to be discovered quickly, and then refined if time permits. Candidate plans are found by allowing them to depend on unproved assumptions. The nonmonotonic logic makes explicit which antecedents of rules have the status of default conditions, and they are the only ones that may be left unproved, so only plausible candidate plans are produced. These are refined incrementally by trying to justify the assumptions on which they depend. The new planning strategy has been implemented, with good experimental results.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-022.pdf,
22,1990,Planning,Admissible Criteria for Loop Control in Planning,"Roy Feldman, Paul Morris","We introduce methods for identifying operator preconditions that need not be expanded further. The methods are proved to be admissible, that is, they will not cause a solution to be missed when one exists. In certain cases, the methods also identify operator reformulations that increase the number of nonexpandable preconditions. This approach provides effective loop control in common situations. Moreover, the computation required can be performed during a precompilation of the operators in a domain; thus, there is no significant additional run-time overhead during planning.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-023.pdf,
23,1990,Planning,Practical Temporal Projection,Steve Hanks,"Temporal projection-predicting future states of a changing world-has been studied mainly as a formal problem. Researchers have been concerned with getting the concepts of causality and change right, and have ignored the practical issues surrounding projection. In planning, for example, when the effects of a plan’s actions depend on the prevailing state of the world and that state of the world is not known with certainty, projecting the plan may generate an exponential number of possible outcomes. This problem has traditionally been eliminated by (1) restricting the domain so the world state is always known, and (2) by restricting the action representation so that either the action’s intended effect is realized or the action cannot be projected at all. We argue against these restrictions and instead present a system that (1) represents and reasons about an uncertain world, (2) supports a representation that allows context-sensitive action effects, and (3) generates projections that reflect only the significant or relevant outcomes of the plans, where relevance is determined by the planner’s queries about the resulting world state.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-024.pdf,
24,1990,Planning,Synthesis of Reactive Plans for Multi-Path Environments,F. Kabanza,We describe a planner that works on the description of a multi-path environment and generates a conditional plan. The resulting plan is guaranteed to fulfill its goal whatever path of the description the environment follows during the plan execution.,https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-025.pdf,
25,1990,Planning,Mapping and Retrieval During Plan Reuse: A Validation Structure Based Approach,Subbarao Kambhampati,"Effective mapping and retrieval are important issues in successful deployment of plan reuse strategies. In this paper we present a domain independent strategy for ranking a set of plausible reuse candidates in the order of cost of modifying them to solve a new planning problem. The cost of modification is estimated by measuring the amount of disturbance caused to the validation structure of a reuse candidate if it were to be reused in the new problem situation. This strategy is more informed than the typical feature based retrieval strategies, and is more efficient than the methods which require partial knowledge of the nature of the plan for the new problem situation to guide the retrieval process. We discuss the implementation of this retrieval strategy in PRIAR, a framework for flexible reuse and modification in hierarchical planning.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-026.pdf,
26,1990,Planning,A Theory of Plan Modification,Subbarao Kambhampati,"We present a theory of plan modification applicable to hierarchical nonlinear planning. Our theory utilizes the validation structure of the stored plans to yield a flexible and conservative plan modification framework The validation structure, which constitutes a hierarchical explanation of correctness of the plan with respect to the planner’s own knowledge of the domain, is annotated on the plan as a byproduct of initial planning. Plan modification is characterized as a process of removing inconsistencies in the validation structure of a plan when it is being reused in a new (changed) planning situation. The repair of these inconsistencies involves removing unnecessary parts of the plan and adding new non-primitive tasks to the plan to establish missing or failing validations. The resultant partially reduced plan (with a consistent validation structure) is sent to the planner for complete reduction. We discuss the development of this theory in PMAR system, and characterize its completeness, coverage, efficiency and limitations.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-027.pdf,
27,1990,Planning,Introducing the Tileworld: Experimentally Evaluating Agent Architectures,"Martha E. Pollack, Marc Ringuette","We describe a system called Tileworld, which consists of a simulated robot agent and a simulated environment which is both dynamic and unpredictable. Both the agent and the environment are highly parameterized, enabling one to control certain characteristics of each. We can thus experimentally investigate the behavior of various meta-level reasoning strategies by tuning the parameters of the agent, and can assess the success of alternative strategies in different environments by tuning the environmental parameters. Our hypothesis is that the appropriateness of a particular meta-level reasoning strategy will depend in large part upon the characteristics of the environment in which the agent incorporating that strategy is situated. We describe our initial experiments using Tileworld, in which we have been evaluating a version of the meta-level reasoning strategy proposed in earlier work by one of the authors [Bratman et al., 1988].",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-028.pdf,
28,1990,Planning,Getting Serious about Parsing Plans: A Grammatical Analysis of Plan Recognition,Marc Vilain,"This paper is concerned with making precise the notion that recognizing plans is much like parsing text. To this end, it establishes a correspondence between Kautz' plan recognition formalism and existing grammatical frameworks. This mapping helps isolate subsets of Kautz' formalism in which plan recognition can be efficiently performed by parsing.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-029.pdf,
29,1990,Planning,The STRIPS Assumption for Planning Under Uncertainty,Michael P. Wellman,"The virtue of the STRIPS assumption for planning is that it bounds the information relevant to determining the effects of actions. Viewing the ""assumption"" as a statement about beliefs, we find that it does not actually assume anything about the world itself. We can characterize the assertion about beliefs in terms of probabilistic independence, thereby facilitating analysis of representations for planning under uncertainty. This interpretation separates the STRIPS assumption from other necessary features of a planning architecture, such as its model of persistence and its inferential policies. By isolating these factors, we can understand the role of dependence across a wide range of planners and action representations. Graphical models of dependence developed for probabilistic analysis provide a convenient tool for verifying the STRIPS assumption for a variety of planning systems. Investigation of a few representative systems reveals a Markovian event structure common to these planning models.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-030.pdf,
30,1990,Planning,"ABTWEAK: Abstracting a Nonlinear, Least Commitment Planner","Qiang Yang, Josh D. Tenenberg","We present the system ABTWEAK, which extends the precondition-elimination abstraction of ABSTRIPS to hierarchical planners using the non-linear plan representation as defined in TWEAK. We show that ABTWEAK satisfies the monotonic property, whereby the existence of a lowest level solution II implies the existence of a highest level solution that is structurally similar to II. This property enables one to prune a considerable amount of the search space without loss of completeness.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-031.pdf,
31,1990,Search,Search Lessons Learned from Crossword Puzzles,"Matthew L. Ginsberg, Michael Frank, Michael P. Halpin, Mark C. Torrance","The construction of a program that generates crossword puzzles is discussed. As in a recent paper by Dechter and Meiri, we make an experimental comparison of various search techniques. The conclusions to which we come differ from theirs in some areas - although we agree that directional arc consistency is better than path-consistency or other forms of lookahead, and that backjumping is to be preferred to backtracking, we disagree in that we believe dynamic ordering of the constraints to be necessary in the solution of more difficult problems.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-032.pdf,
32,1990,Search,Iterative Broadening,"Matthew L. Ginsberg, William D. Harvey","Conventional blind search techniques generally assume that the goal nodes for a given problem are distributed randomly along the fringe of the search tree. We argue that this is often invalid in practice, suggest that a more reasonable assumption is that decisions made at each point in the search carry equal weight, and show that a new search technique that we call iterative broadening leads to orders-of-magnitude savings in the time needed to search a space satisfying this assumption. Both theoretical and experimental results are presented .",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-033.pdf,
33,1990,Search,Path-Focused Duplication: A Search Procedure for General Matings,Sunil Issar,"The mating paradigm for automated theorem provers was proposed by Andrews to avoid some of the shortcomings in resolution. It facilitates automated deduction in higher-order and non-classical logics. Moreover, there are procedures which translate back and forth between refutations by the mating method and proofs in a natural deduction system. We describe a search procedure, called path-focused duplication, for finding refutations by the mating method. This procedure, which is a complete strategy for the mating method, addresses two crucial issues (inadequately handled in current implementations) that arise in the search for refutations: when and how to expand the search space. It focuses on a particular path that seems to cause an impasse in the search and expands the search space relative to this path in a way that allows the search to immediately resolve the impasse. The search space grows and shrinks dynamically to respond to the requirements that have arisen or have been met in the search process, thus avoiding an explosion in the size of the search space. We have implemented a prototype of this procedure and have been able to easily solve many problems that an earlier program found difficult.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-034.pdf,
34,1990,Search,Consistent Linear Speedups to a First Solution in Parallel State-Space Search,"Vikram A. Saletore, L.V. Kalé","Consider the problem of exploring a large state-space for a goal state. Although many such states may exist, finding any one state satisfying the requirements is sufficient. All methods known until now for conducting such search in parallel fail to provide consistent linear speedups over sequential execution. The speedups vary between sublinear to superlinear and from run to run. Further, adding processors may sometimes lead to a slow-down rather than speedup, giving rise to speedup anomalies. We present prioritizing strategies which yield consistent linear speedups and requires substantially smaller memory over other methods. The performance of these strategies is demonstrated on a multiprocessor.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-035.pdf,
35,1990,"TheoremProving and Program Synthesis",Inductive Synthesis of Equational Programs,"Nachum Dershowitz, Eli Pinchover","An equational approach to the synthesis of functional and logic programs is taken. Typically, a target program contains equations that are only true in the standard model of the given domain rules. To synthesize such programs, induction is necessary. We propose heuristics for generalizing from a sequence of deductive consequences. These are combined with rewrite-based methods of inductive proof to derive provably correct programs.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-036.pdf,
36,1990,"TheoremProving and Program Synthesis",Mechanizing Inductive Reasoning,"Emmanuel Kounalis, Michael Rusinowitch","Automating proofs by induction is important in many computer science and artificial intelligence applications, in particular in program verification and specification systems. We present a new method to prove (and disprove) automatically inductive properties. Given a set of axioms, a well-suited induction scheme is constructed automatically. We call such a scheme a test-set. Then, for proving a property, we just instantiate it with terms from the test-set and apply pure algebraic simplification to the result. This method avoids completion and explicit induction. However it retains their positive features, namely the completeness of the former and the robustness of the latter.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-037.pdf,
37,1990,"TheoremProving and Program Synthesis",Skolem Functions and Equality in Automated Deduction,William McCune,"We present a strategy for restricting the application of the inference rule paramodulation. The strategy applies to problems in first-order logic with equality and is designed to prevent paramodulation into subterms of Skolem expressions. A weak completeness result is presented (the functional reflexive axioms are assumed). Experimental results on problems in set theory, combinatory logic, Tarski geometry, and algebra show that the strategy can be useful when searching for refutations and when applying Knuth-Bendix completion. The emphasis of the paper is on the effectiveness of the strategy rather than on its completeness.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-038.pdf,
38,1990,"TheoremProving and Program Synthesis",Automatically Generating Universal Attachments Through Compilation,Karen L. Myers,"Universal attachment is a general-purpose mechanism for integrating diverse representation structures and their associated inference programs into a framework built on logical representations and theorem proving. The integration is achieved by links, referred to as universal attachments, that connect logical expressions to these structures and programs. In this paper, we describe a compilation-based method for automatically generating new programs and new universal attachments to those programs given a base set of existing programs and universal attachments. The generation method provides the means to obtain large collections of attachments and attached programs without the traditional specification overhead. As well, the method simplifies the task of validating that a collection of attachments is correct.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-039.pdf,
39,1990,"TheoremProving and Program Synthesis",Solving Term Inequalities,Gerald E Peterson,"This work pertains to the Knuth-Bendix (KB) algorithm which tries to find a complete set of reductions from a given set of equations. In the KB algorithm a term ordering is employed and it is required that every equation be orientable in the sense that the left-hand side be greater than the right. The KB algorithm halts if a non-orientable equation is produced. A generalization of the KB algorithm has recently been developed in which every equation is orientable and which halts only when a complete set is generated. In the generalization a constraint is added to each equation. The constraint governs when the equation can be used as a reduction. The constraint is obtained from the equation by ""solving"" the term inequality left-hand side > right-hand side. To understand what it means to solve a term inequality, consider the analogy with algebra in which solving term equalities, i.e. unification, is analogous to solving algebraic equalities. Then solving term inequalities is analogous to solving algebraic inequalities. Thus, the solution of term inequalities relates to unification as the solution of algebraic inequalities relates to the solution of algebraic equalities. We show how to solve term inequalities when using the lexicographic path ordering.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-040.pdf,
40,1990,"TruthMaintenance Systems",Exploiting Locality in a TMS,Johan de Kleer,"This paper presents a new approach for exploiting Truth Maintenance Systems(TMSs) which makes them simpler to use without necessarily incurring a substantial performance penalty. The basic intuition behind this approach is to convey the locality of the knowledge representation of the problem solver to the TMS. The TMS then uses this locality information to control and restrict its inferences. The new TMSs accept arbitrary propositional formulae as input and use general Boolean Constraint Propagation(BCP) t o answer queries about whether a particular literal follows from the formulae. Our TMS exploits the observation that if the set of propositional formulae are converted to their prime implicates, then BCP is both efficient and logically complete. This observation allows the problem solver to influence the degree of completeness of the TMS by controlling how many implicates are constructed. This control is exerted by using the locality in the original task to guide which combinations of formulae should be reduced to their prime implicates. This approach has been implemented and tested both within Assumption-Based Truth Maintenance Systems and Logic-Based Truth Maintenance Systems.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-041.pdf,
41,1990,"TruthMaintenance Systems",Computing Stable Models By Using the ATMS,Kave Eshghi,"An algorithm is described which computes stable models of propositional logic programs with negation as failure using the Assumption Based Truth Maintenance mechanism. Since stable models of logic programs are closely connected to stable expansions of a class of autoepistemic theories, this algorithm points to a link between stable expansions of a class of autoepistemic theories and ATMS structures.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-042.pdf,
42,1990,"TruthMaintenance Systems",Computing the Extensions of Autoepistemic and Default Logics with a Truth Maintenance System,"Ulrich Junker, Kurt Konolige","In this paper we develop a proof procedure for autoepistemic (AEL) and default logics (DL), based on translating them into a Truth Maintenance System (TMS). The translation is decidable if the theory consists of a finite number of defaults and premises and classical derivability for the base language is decidable. To determine all extensions of a network, we develop variants of Doyle’s labelling algorithms.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-043.pdf,
43,1990,"TruthMaintenance Systems",Maintaining Consistency in a Stratified Production System Program,Louiqa Raschid,"We present our research on defining a correct semantics for forward chaining production systems (PS) programs. A correct semantics ensures that the execution of the program will not produce incorrect answers and execution will terminate; it also ensures that the answers are consistent. We define a class of stratified PS programs, and propose an operational semantics for these programs. We define an operator TPs, which computes the operational fixpoint for the productions of the stratified PS program; the fixpoint captures the meaning of the PS program. The theory that can be derived from the productions of the PS program may be inconsistent with the constraints that are also derived from the PS program. We can then view the constraints as modifying the theory so that the modified theory PS is consistent with the constraints. However, the same answers are obtained in the operational semantics of the stratified PS program or from the modified theory E.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-044.pdf,
44,1990,"Case-BasedReasoning",Integrating Planning and Acting in a Case-Based Framework,"Kristian Hammond, Timothy Converse, Charles Martin","This paper presents an outline of a theory of agency that seeks to integrate ongoing understanding, planning and activity into a single model of representation and processing. Our model of agency rises out of three basic pieces of work: Schank’s structural model of memory organization (Schank, 1982), Hammond’s work in case-based planning and dependency directed repair (Hammond, 1989d), and Martin’s work in Direct Memory Access Parsing (Martin 1990). We see this paper as a first step in the production of a memory-based theory of agency: the active pursuit of goals in the face of a changing environment, that can exist within the computational constraints of a computer model.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-045.pdf,
45,1990,"Case-BasedReasoning",A Method of Calculating the Measure of Salience in Understanding Metaphors,"Iwayama Makoto, Tokunaga Takenobu, Tanaka Hozumi","This paper presents a computaional method of calculating the measure of salience in understanding metaphors. We mainly treat metaphors in the form of ""A is (like) B,"" in which ""A"" is called target concept, and ""B"" is called source concept. In understanding a metaphor, some properties of the source concept are transferred to the target concept. In the transfer process, we first have to select the properties of the source concept that can be more preferably transferred to the target concept. The measure of salience represents how typical or prominent the property is and is used to measure the transferability of the property. By introducing the measure of salience, we have to consider only the high salient properties after the selection. The measure of salience was calculated from Smith and Medin’s probabilistic concept[l2, 13] according to Tversky’s two factors[l4]. One is intensity which refers to signal-to-noise ratio; this is calculated from the entropy of properties. The other is diagnostic factor which refers to the classificatory significance of properties; this is calculated from the distribution of the property’s intensity among similar concepts. Finally we briefly outline the whole process of understanding metaphors using the measure of salience.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-046.pdf,
46,1990,"Case-BasedReasoning",Distributed Cases for Case-Based Reasoning; Facilitating Use of Multiple Cases,Michael Redmond,"A case-based reasoner can frequently benefit from using pieces of multiple previous cases in the course of solving a single problem. In our model, case pieces, called snippets, are organized around the pursuit of a goal, and there are links between the pieces that preserve the structure of reasoning. The advantages of our representational approach include: 1) The steps taken in a previous case can be followed as long as they are relevant, since the connections between steps are preserved. 2) There is easy access to all parts of previous cases, so they can be directly accessed when appropriate.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-047.pdf,
47,1990,"Case-BasedReasoning",Validated Retrieval in Case-Based Reasoning,"Evangelos Simoudis, James Miller","We combine simple retrieval with domain-specific validation of retrieved cases to produce a useful practical tool for case-based reasoning. Based on 200 real-world cases, we retrieve between three and six cases over a wide range of new problems. This represents a selectivity ranging from 1.5% to 3%, compared to an average selectivity of only 11% from simple retrieval alone.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-048.pdf,
48,1990,"Model-BasedDiagnosis and Design",Model-Based Diagnosis of Planning Failures,"Lawrence Birnbaum, Gregg Collins, Michael Freed, Bruce Krulwich","We propose that a planner should be provided with an explicit model of its own planning mechanism, and show that linking a planner’s expectations about the performance of its plans to such a model, by means of explicit justification structures, enables the planner to determine which aspects of its planning are responsible for observed performance failures. We have implemented the ideas presented in this paper in a computer model. Applied to the game of chess, the model is capable of diagnosing planning failures due to incomplete knowledge of the rules, improper or overly optimistic focus of attention, faulty projection, and insufficient lead time for warning about threats, and is therefore able to learn such concepts as discovered attack and the fork.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-049.pdf,
49,1990,"Model-BasedDiagnosis and Design",Characterizing Diagnoses,"Johan de Kleer, Alan K. Mackworth, Raymond Reiter","Most approaches to model-based diagnosis describe a diagnosis for a system as a set of failing components that explains the symptoms. In order to characterize the typically very large number of diagnoses, usually only the minimal such sets of failing components are represented. This method of characterizing all diagnoses is inadequate in general, in part because not every superset of the faulty components of a diagnosis necessarily provides a diagnosis. In this paper we analyze the notion of diagnosis in depth exploiting the notions of implicate/implicant and prime implicate/implicant. We use these notions to propose two alternative approaches for addressing the inadequacy of the concept of minimal diagnosis. First, we propose a new concept, that of kernel diagnosis, which is free of the problems of minimal diagnosis. Second, we propose to restrict the axioms used to describe the system to ensure that the concept of minimal diagnosis is adequate.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-050.pdf,
50,1990,"Model-BasedDiagnosis and Design",Physical Impossibility Instead of Fault Models,"Gerhard Friedrich, Georg Gottlob, Wolfgang Nejdl","In this paper we describe the concept of physical impossibility as an alternative to the specification of fault models. These axioms can be used to exclude impossible diagnoses similar to fault models. We show for Horn clause theories while the complexity of finding a first diagnosis is worst-case exponential for fault models, it is polynomial for physical impossibility axioms. Even for the case of finding all diagnoses using physical impossibility axioms instead of fault models is more efficient, although both are exponential in the worst case. These results are used for a polynomial diagnosis and measurement strategy which finds a final sufficient diagnosis.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-051.pdf,
51,1990,"Model-BasedDiagnosis and Design",On the Role of Coherence in Abductive Explanation,"Hwee Tou Ng, Raymond J. Mooney","Abduction is an important inference process underlying much of human intelligent activities, including text understanding, plan recognition, disease diagnosis, and physical device diagnosis. In this paper, we describe some problems encountered using abduction to understand text, and present some solutions to overcome these problems. The solutions we propose center around the use of a different criterion, called explanatory coherence, as the primary measure to evaluate the quality of an explanation. In addition, explanatory coherence plays an important role in the construction of explanations, both in determining the appropriate level of specificity of a preferred explanation, and in guiding the heuristic search to efficiently compute explanations of sufficiently high quality.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-052.pdf,
52,1990,"Model-BasedDiagnosis and Design",Abductive and Default Reasoning: A Computational Core,"Bart Selman, Hector J. Levesque","Of all the possible ways of computing abductive explanations, the ATMS procedure is one of the most popular. While this procedure is known to run in exponential time in the worst case, the proof actually depends on the existence of queries with an exponential number of answers. But how much of the difficulty stems from having to return these large sets of explanations? Here we explore abduction tasks similar to that of the ATMS, but which return relatively small answers. The main result is that although it is possible to generate some non-trivial explanations quickly, deciding if there is an explanation containing a given hypothesis is NP-hard, as is the task of generating even one explanation expressed in terms of a given set of assumption letters. Thus, the method of simply listing all explanations, as employed by the ATMS, probably cannot be improved upon. An interesting result of our analysis is the discovery of a subtask that is at the core of generating explanations, and is also at the core of generating extensions in Reiter’s default logic. Moreover, it is this subtask that accounts for the computational difficulty of both forms of reasoning. This establishes for the first time a strong connection between computing abductive explanations and computing extensions in default logic.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-053.pdf,
53,1990,"Model-BasedDiagnosis and Design",Interaction-Based Invention: Designing Novel Devices from First Principles,Brian C. Williams,"An inventor who is skilled at constructing innovative designs is distinguished, not just by the first principles he knows, but by the way he uses these principles and how he focusses the search for novel devices among an overwhelming space of possibilities. We propose that an appropriate focus for design is the network of qualitative interactions between quantities, (called an interaction topology), used by a device to achieve its desired behavior. We present an approach, called interaction-based invention, which views design as a process of building interaction topologies - in this paper directly from first principles. The program Ibis, which embodies this approach, designs simple hydro-mechanical regulators, analogous to devices that were fundamental to the development of feedback control theory.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-054.pdf,
54,1990,"Model-BasedDiagnosis and Design",Efficient Diagnosis of Multiple Disorders Based on a Symptom Clustering Approach,Thomas D. Wu,"Diagnosis of multiple disorders can be made efficient using a new representation and algorithm based on symptom clustering. The symptom clustering approach partitions symptoms into causal groups, in contrast to the existing candidate generation approach, which assembles disorders, or candidates. Symptom clustering achieves efficiency by generating aggregates of candidates rather than individual candidates and by representing them implicitly in a Cartesian product form. Search criteria of parsimony, subsumption, and spanning narrow the symptom clustering search space, and a problem-reduction search algorithm explores this space efficiently. Experimental results on a large knowledge base indicate that symptom clustering yields a near-exponential increase in performance compared to candidate generation.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-055.pdf,
55,1990,"QualitativeModeling of Physical Systems",QPC: A Compiler from Physical Models into Qualitative Differential Equations,"James Crawford, Adam Farquhar, Benjamin Kuipers","Qualitative reasoning can, and should, be decomposed into a model-building task, which creates a qualitative differential equation (QDE) as a model of a physical situation, and a qualitative simulation task, which starts with a QDE, and predicts the possible behaviors following from the model. In support of this claim, we present QPC, a model builder that takes the general approach of Qualitative Process Theory [Forbus, 1984], describing a scenario in terms of views, processes, and influences. However, QPC builds QDE s for simulation by QSIM, which gives it access to a variety of mathematical advances in qualitative simulation incorporated in QSIM. We present QPC and its approach to Qualitative Process Theory, provide an example of building and simulating a model of a non-trivial mechanism, and compare the representation and implementation decisions underlying QPC with those of QPE [Falkenhainer and Forbus, 1988; Forbus, 1990].",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-056.pdf,
56,1990,"QualitativeModeling of Physical Systems",Dynamic Across-Time Measurement Interpretation,Dennis DeCoste,"Incrementally maintaining a qualitative understanding of physical system behavior based on observations is crucial to real-time process monitoring, diagnosis, and control. This paper describes the DATMI theory for dynamically maintaining a pinterp-space, a concise representation of the local and global interpretations consistent with observations over time. Each interpretation signifies an alternative path of states in a qualitative envisionment. DATMI can use domain-specific knowledge about state and transition probabilities to maintain the best working interpretation. By maintaining the space of alternative interpretations as well, DATMI avoids the need for extensive backtracking to handle incomplete or faulty data.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-057.pdf,
57,1990,"QualitativeModeling of Physical Systems",Self-Explanatory Simulations: An Integration of Qualitative and Quantitative Knowledge,"Kenneth D. Forbus, Brian Falkenhainer","A central goal of qualitative physics is to provide a framework for organizing and using quantitative knowledge. One important use of quantitative knowledge is numerical simulation. While current numerical simulators are powerful, they are often hard to construct, do not reveal the assumptions underlying their construction, and do not produce explanations of the behaviors they predict. This paper shows how to combine qualitative and quantitative models to produce a new class of self-explanatory simulations which combine the advantages of both kinds of reasoning. Self-explanatory simulations provide the accuracy of numerical models and the interpretive power of qualitative reasoning. We define what self-explanatory simulations are and show how to construct them automatically. We illustrate their power with some examples generated with an implemented system, SIMGEN. We analyze the limitations of our techniques, and discuss plans for future work.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-058.pdf,
58,1990,"QualitativeModeling of Physical Systems",Obtaining Quatitative Predictions From Monotone Relationships,Joseph Hellerstein,"Quantitative predictions are typically obtained by characterizing a system in terms of algebraic relationships and then using these relationships to compute quantitative predictions from numerical data. For real-life systems, such as mainframe operating systems, an algebraic characterization is often difficult, if not intractable. This paper proposes a statistical approach to obtaining quantitative predictions from monotone relationships -- non-parametric interpolative-prediction for monotone functions (NIMF). NIMF uses monotone relationships to search historical data for bounds that provide a desired level of statistical confidence. We evaluate NIMF by comparing its predictions to those of linear least-squares regression (a widely-used statistical technique that requires specifying algebraic relationships) for memory contention in an IBM computer system. Our results suggest that using an accurate monotone relationship better quantitative predictions than proximate algebraic relationship. can produce using an approximate algebraic relationship.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-059.pdf,
59,1990,"Reasoningwith Multiple Models",Shifting Ontological Perspectives in Reasoning about Physical Systems,"Zheng-Yang Liu, Arthur M. Farley","Commitment to an ontological perspective is a primary aspect of reasoning about the physical world. For complex analytic tasks, the ability to switch between different ontologies to represent the same target system can be critical. Supplementing the standard device ontology for electronic circuits, we outline elements of a charge-carrier (CC) ontology for reasoning about electronics. Having two ontologies extends our range of reasoning, but raises the issue of how to control their application. We propose a set of ontological-choice rules to govern the process of ontological shift and demonstrate its effectiveness with examples involving the two ontologies in reasoning about electronic circuits.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-060.pdf,
60,1990,"Reasoningwith Multiple Models",Qualitative Reasoning with Microscopic Theories,Shankas A. Rajamoney and Sang Hoe Koo,"A model of the elementary particles of a domain and their rudimentary interactions is essential for sophisticated reasoning about the macroscopic behavior of physical system. A microscopic theory can make explicit the deeper mechanisms underlying causal models, collapse a great variety of macroscopic phenomena into a few rudimentary interactions, elaborate upon or validate macroscopic explanations, and so forth. This paper describes a qualitative representation for microscopic theories and, a method for reasoning with microscopic particles to obtain the macroscopic behavior. The representation and reasoning are illustrated using implemented examples from the fluids domain.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-061.pdf,
61,1990,"Reasoningwith Multiple Models",Approximation Reformulations,Daniel S. Weld,"Although computers are widely used to simulate complex physical systems, crafting the underlying models that enable computer analysis remains difficult. When a model is created for one task, it is often impossible to reuse the model for another purpose because each task requires a different set of simplifying assumptions. By representing modeling assumptions explicitly as approximation reformulations, we have developed qualitative techniques for switching between models. We assume that automated reasoning proceeds in three phases: 1) model selection, 2) quantitative analysis using the model, and 3) validation that the assumptions underlying the model were appropriate for the task at hand. If validation discovers a serious discrepancy between predicted and observed behavior, a new model must be chosen. We present a domain independent method for performing this model shift when the models are related by an approximation reformulation and describe a Common Lisp implementation of the theory.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-062.pdf,
62,1990,"Reasoningwith Multiple Models",Finding the Average Rates of Change in Repetitive Behavior,Alexander Yeh,"The repetitive behavior of a device or system can be described in two ways: a detailed description of one iteration of the behavior, or a summary description of the behavior over many repetitions. This paper describes an implemented program called AIS that transforms the first type of description into the second type. AIS deals only with behavior where each repetition changes parameters by the same amounts. At present, the summary consists of the symbolic average rates of change in parameter values and information on how those rates would be different if various constants and functions had been different. Unlike some other approaches, AIS does not require that a repeating behavior be described in terms of a set of differential equations. Two examples of running AIS are given: one concerns the human heart, the other a steam engine.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-063.pdf,
63,1990,"Educationand AI",Towards a System Architecture Supporting Contextualized Learning,"Gerhasd Fischer, Andreas C. Lemke, Raymond McCall","We have developed a conceptual framework and a demonstration system that contextualize (or situate) learning in the context of real-world work situations. The conceptual framework is based on the following requirements: the choice of tasks and goals must be under the control of the user, not the system. The environment must be able to situate learning, allow situations to ""talk back,"" support reflection-in-action, identify the instructional information relevant for tasks at hand, and turn breakdowns from disasters into opportunities for learning. Learning must not disrupt or interfere with solving a problem, and new information to be learned must help to accomplish the task at hand. Our demonstration system JANUS (developed for the domain of architectural design) is built on an integrated architecture: a knowledge-based construction component, a hypermedia-based argumentation component, a set of critics, and a catalog of precedent solutions. Contextualized learning is supported by the critics that link construction and argumentation, and precedent solutions from the catalog that situate argumentation. Evaluation of JANUS and the underlying conceptual framework have shown that this approach combines some of the best features of open-ended learning environments and tutoring systems.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-064.pdf,
64,1990,"Educationand AI",Backward Model Tracing: An Explanation-Based Approach for Reconstructing Student Reasoning,"Danilo Fum, Paolo Giangrandi, Carlo Tasso","An original methodology, called backward model tracing to model student performance which features a profitable integration of the bug collection and bug construction techniques is presented. This methodology has been used for building the modelling module of a new version of ET (English Tutor), an ITS aimed at supporting the learning of the English verb system. Backward model tracing is based on the idea of analyzing the reasoning process of the student by reconstructing, step by step and in reverse order, the chain of reasoning (s)he has followed in giving his/her answer. In order to do this, both correct domain specific knowledge and a catalogue of stereotyped errors (mahules) are utilized. When the system is unable to explain the student behavior by exploiting its previous knowledge, new malrules are generated dynamically, by utilizing explanation-based learning techniques. The overall process is based on a deep modelling of the student problem solving and the discrimination among possible explicative hypotheses about the reasons underlying the student behavior is carried on non- monotonically through a truth maintenance system. The proposed approach has been fully implemented in a student modelling module developed in PROLOG.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-065.pdf,
65,1990,"Educationand AI",A Blackboard-based Dynamic Instructional Planner,William R. Murray,"The Blackboard Instructional Planner is a blackboard-based dynamic planner for intelligent tutoring systems. It generates a sequence of lesson plans customized to a student’s background, and adaptively replan, to handle student requests and unexpected changes to the student model or time remaining. The planner is designed to be generic to tutors that teach troubleshooting for complex physical devices. It controls the Lower Hoist Tutor, a prototype tutor for the Mark-45 naval gun mount. This tutor teaches troubleshooting of the lower hoist, a complex hydraulic-electronic-mechanical assembly of the Mark-45 The tutor implementation demonstrates the planner’s operation and means of integration. This research contributes to an understanding of dynamic instructional planners, planner-controlled tutors, and ITS control architectures. The planner implementation shows precisely how a blackboard architecture can be used to realize a dynamic instructional planner. Although experimental, the tutor implementation demonstrates how such a planner can be embedded in an intelligent tutoring system and what the respective roles of the different components of a pianner-controlled tutor are. Finally, the analysis of the planner’s use of the blackboard architecture clarifies requirements for control architectures in intelligent tutoring systems and trade-offs made in choosing alternatives.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-066.pdf,
66,1990,Multimedia,Coordinating Text and Graphics in Explanation Generation,"Steven K. Feiner, Kathleen R. McKeown","To generate multimedia explanations, a system must be able to coordinate the use of different media in a single explanation. In this paper, we present the architecture that we have developed for COMET (Coordinated Multimedia Explanation Testbed), a system that generates directions for equipment maintenance and repair, and we show how it addresses the coordination problem. COMET includes a single content planner that produces a common content description used by multiple media-specific generators, and a media coordinator that performs a fine-grained division of information among media. Bidirectional interaction between media-specific generators allows influence across media. We describe COMET’s current capabilities and provide an overview of our plans for extending the system.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-067.pdf,
67,1990,Multimedia,Avoiding Unwanted Conversational Implicatures in Text and Graphics,Joseph Marks and Ehud Reiter,"We have developed two systems, FN and ANDD, that use natural language and graphical displays, respectively, to communicate information about objects to human users. Both systems must deal with the fundamental problem of ensuring that their output does not carry unwanted and inappropriate conversational implicatures. We describe the types of conversational implicatures that FN and ANDD can avoid, and the computational strategies the two systems use to generate output that is free of unwanted implicatures.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-068.pdf,
68,1990,Multimedia,Pointing: A Way Toward Explanation Dialogue,"Johanna D. Moore, William R. Swartout","Explanation requires a dialogue. Users must be allowed to ask questions about previously given explanations. However, building an interface that allows users to ask follow-up questions poses a difficult challenge for natural language understanding because such questions often intermix meta-level references to the discourse with object-level references to the domain. We propose a hypertext-like interface that allows users to point to the portion of the system’s explanation they would like clarified. By allowing users to point, many of the difficult referential problems in natural language analysis can be avoided. However, the feasibility of such an interface rests on the system’s ability to understand what the user is pointing at; i.e., the system must understand its own explanations. To solve this problem, we employ a planning approach to explanation generation which records the design process that produced an explanation so that it can be used in later reasoning. In this paper, we show how synergy arises from combining a ""pointing-style"" interface with a text planning generation system, making explanation dialogues more feasible.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-069.pdf,
69,1990,Multimedia,Understanding Natural Language with Diagrams,"Gordon S. Novak Jr., William C. Bulko","We describe a program, BEATRIX, that can understand textbook physics problems specified by a combination of English text and a diagram. The result of the understanding process is a unified internal model that represents the problem, including information derived from both the English text and the diagram. The system is implemented as two opportunistic coparsers, one for English and one for diagrams, within a blackboard architecture. A central problem is establishing coreference, that is, determining when parts of the text and diagram refer to the same object. Constraints supplied by the text and diagram mutually reduce ambiguity in interpretation of the other modality.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-070.pdf,
70,1990,"PlanRecognition",Incorporating Default Inferences into Plan Recognition,Sandra Carberry,"This paper presents a process model of plan inference for use in natural language consultation systems. It includes a strategy that can both defer unwarranted decisions about the relationship of a new action to the user’s overall plan and sanction rational default inferences. The paper describes an implementation of this strategy using the Dempster-Shafer theory of evidential reasoning. Our process model overcomes a limitation of previous plan recognition systems and produces a richer model of the user’s plans and goals, yet one that can be explained and justified to the user when discrepancies arise between it and what the user is actually trying to accomplish.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-071.pdf,
71,1990,"PlanRecognition",A Cooperative Problem Solving System for User Interface Design,"Andreas C. Lemke, Gerhard Fischer","Designing a user interface is an ill-defined problem making cooperative problem solving systems a promising approach to support user interface designers. Cooperative problem solving systems are modular systems that support the human designer with multiple, independent system components. We present a system architecture and an implemented system, FRAMER, that demonstrate the cooperative problem solving approach. FRAMER represents design knowledge in formal, machine-interpretable knowledge sources such as critics and dynamic specification sheets, and in semi-formal knowledge sources such as a palette of user interface building blocks and a checklist. Each of these components contributes significantly to the overall usefulness of the system while requiring only limited resources to be designed and implemented.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-072.pdf,
72,1990,"PlanRecognition",Models of Plans to Support Communication: An Initial Report,"Karen E. Lochbaum, Barbara J. Grosz, Candace L. Sidner","Agents collaborating to achieve a goal bring to their joint activity different beliefs about ways in which to achieve the goal and the actions necessary for doing so. Thus, a model of collaboration must provide a way of representing and distinguishing among agents’ beliefs and of stating the ways in which the intentions of different agents contribute to achieving their goal. Furthermore, in collaborative activity, collaboration occurs in the planning process itself. Thus, rather than modelling plan recognition, per se, what must be modelled is the augmentation of beliefs about the actions of multiple agents and their intentions. In this paper, we modify and expand the SharedPlan model of collaborative behavior (Grosz and Sidner 1990). We present an algorithm for updating an agent’s beliefs about a partial SharedPlan and describe an initial implementation of this algorithm in the domain of network management.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-073.pdf,
73,1990,"PlanRecognition",A Collaborative Interface for Editing Large Knowledge Bases,"Loren G. Terveen, David A. Wroblewski","A new generation of knowledge/databases is emerging. These systems contain thousands of objects, densely interconnected and heterogeneously organized, entered from many sources, both human and automated. Such systems present tremendous challenges to their users, who must locate relevant information quickly and add new information effectively. Our research aims to understand and support the knowledge editing task. The HITS Knowledge Editor (HKE) is an interface that supports browsing and modifying the CYC knowledge base (Guha and Lenat 1990). HKE has been designed to be a collaborative interface, following a set of principles for sharing tasks between system and user. We describe these principles and illustrate how HKE provides resources built according to those principles that collaborate with its users on a variety of knowledge editing tasks.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-074.pdf,
74,1990,"ExpertSystem Design Methodologies",An Experiment in Direct Knowledge Acquisition,Peter W. Mullarkey,"LQMS is a knowledge-based system that identifies and explains anomalies in data acquired from multiple sensors. The knowledge base was built by a sequence of domain experts. Its prototype performed with a high level of accuracy and that performance has been incrementally and significantly improved during development and field testing. Several points are developed in this paper. (1) The combination of an intuitive model (sufficient for the task) and powerful, graphical development tools allowed the domain experts to build a large, high performance system. (2) The Observation-Situation-Blation representation illustrates an intermediate point on the simplicity-expressiveness spectrum, which is understandable to the domain experts, while being expressive enough for the diagnostic task. (3) The system was designed as a workbench for the domain experts. This enticed them to become more directly involved, and, resulted in a better system. (4) The use of an integrated knowledge base edit-tracking system was important to the project in several ways: it reassured computer-naive experts that they could not damage the overall system, which increased their productivity; and, it also allowed experts located in various places around the world to compare, contrast, and integrate changes in a structured way.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-075.pdf,
75,1990,"ExpertSystem Design Methodologies",Parametric Engineering Design Using Constraint-Based Reasoning,"Niall Murtagh, Masarnichi Shimura","Conventional methods for the parametric design of engineering structures rely on the iterative re-use of analysis programs in order to converge on a satisfactory solution. Since finite element and other analysis programs require considerable computer resources, this research proposes a general method to minimize their use, by utilizing constraint-based reasoning to carry out redesign. A problem-solver, consisting of constraint networks which express basic relationships between individual design parameters and variables, is attached to the analysis programs. Once an initial design description has been set out using the conventional analysis programs, the networks can then reason about required adjustments in order to find a consistent set of parameter values. We describe how global constraints representing standard design behavioral equations are decomposed to form binary constraint networks. The networks use approximate reasoning to determine dependencies between key parameters, and after an adjustment has been made, use exact relationship information to update only those parts of the design description that are affected by the adjustment. We illustrate the ideas by taking as an example the design of a continuous prestressed concrete beam.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-076.pdf,
76,1990,"ExpertSystem Design Methodologies",Establishing the Coherence of an Explanation to Improve Refinement of an Incomplete Knowledge Base,"Young-Tack Park, David C. Wilkins","The power of knowledge acquisition systems that employ failure-driven learning derives from two main sources: an effective global credit assignment process that determines when to acquire new knowledge by watching an expert’s behavior, and an efficient local credit assignment process that determines what new knowledge will be created for completing a failed explanation of an expert’s action. Because an input (e.g., observed action) to a failure-driven learning system can generate multiple explanations, a learning opportunity to extend the incomplete domain theory can go unobserved. This paper describes a failure-driven learning with a context analysis mechanism as a method to constrain explanations and thereby increase the number of learning opportunities. Experimentation using a synthetic expert system as the observed expert shows that the use of context analysis increases the number of learning opportunities by about 47%, and increases the overall amount of improvement to the expert system by around 10%.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-077.pdf,
77,1990,"ExpertSystem Design Methodologies",A Design Based Approach to Constructing Computational Solutions to Diagnostic Problems,"D. Volovik, I. A. Zualkernan, P. E. Johnson, C. E. Matthews","Troubleshooting problems in real manufacturing environments impose constraints on admissible solutions that make the computational solutions offered by ""troubleshooting from first principles"" and the conventional experience based expert systems approaches infeasible. In this paper we present a computational theory for a solution to these problems that is based on the Principle of Locality and exploits the domain specific weak methods of troubleshooters and debugging knowledge of the designers. The computational theory is evaluated by generating focus of attention heuristics for a moderately complex digital device.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-078.pdf,
78,1990,"Causalityand Introspection",Causal Theories for Nonmonotonic Reasoning,Hector Geffner,"Causal theories are default theories which explicitly accommodate a distinction between ""explained"" and ""unexplained ""propositions. This is accomplished by means of an operator ""C"" in the language for which propositions a are assumed explained when literals of the form Ca hold. The behavior of causal theories is determined by a preference relation on models based on the minimization of unexplained abnormality. We show that causal networks, general logic programs and theories for reasoning about change can be all naturally expressed as causal theories. We also develop a proof-theory for causal theories and discuss how they relate to autoepistemic theories, prioritized circumscription, and Pearl’s C-E calculus.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-079.pdf,
79,1990,"Causalityand Introspection",Decidable Reasoning in First-Order Knowledge Bases with Perfect Introspection,Gerhard Lakemeyer,"Since knowledge bases (KBs) are usually incomplete, they should be able to provide information regarding their own incompleteness, which requires them to introspect on what they know and do not know. An important area of research is to devise models of introspective reasoning that take into account resource limitations. Under the view that a KB is completely characterized by the set of beliefs it represents (its epistemic state), it seems natural to model KBs in terms of belief. Reasoning can then be understood as the problem of computing membership in the epistemic state of a KB. The best understood models of belief are based on possible-world semantics. However, their computational properties are unacceptable. In particular, they render reasoning in first-order KBs undecidable. In this paper, we propose a novel model of belief, which preserves many of the advantages of possible-world semantics yet, at the same time, guarantees reasoning to be decidable, where a KB may contain sentences in full first-order logic. Moreover, such KBs have perfect knowledge about their own beliefs even though their beliefs about the world are limited.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-080.pdf,
80,1990,"Causalityand Introspection",A Formal Theory of Multiple Agent Nomonotonic Reasoning,Leora Morgenstern,"This paper presents a formal theory of multiple agent non-monotonic reasoning. We introduce the subject of multiple agent non-monotonic reasoning for inquiry and motivate the field in terms of its applications for con-n-nonsense reasoning. We extend Moore’s [1985] Autoepistemic Logic to the multiple agent case, and show that the resulting logic is too weak for most applications of commonsense reasoning. We then suggest some possible sets of principles for a logic of multiple-agent non-monotonic reasoning, based on the concept of an agent’s arrogance towards his knowledge of another agent’s ignorance. While the principles of arrogance are in general too strong, we demonstrate that restricted versions of these principles can work quite well for commonsense reasoning. In particular, we show that a restricted form of the principle of arrogance yields results that are equivalent to EMAT [Morgenstern, 1989], a non-monotonic logic which was designed to reason about temporal projection in epistemic contexts.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-081.pdf,
81,1990,"Causalityand Introspection",A Circumscriptive Theory for Causal and Evidential Support,Eunok Paek,"Reasoning about causality is an interesting application area of formal nonmonotonic theories. Here we focus our attention on a certain aspect of causal reasoning, namely causal asymmetry. In order to provide a qualitative account of causal asymmetry, we present a justification-based approach that uses circurnscription to obtain the minimality of causes. We define the notion of causal and evidential support in terms of a justification change with respect to a circumscriptive theory and show how the definition provides desirable interactions between causal and evidential support.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-082.pdf,
82,1990,"Complexityand Expressiveness",The Complexity of Closed World Reasoning and Circumscription,"Marco Cadoli, Maurizio Lenzerini","Closed world reasoning is a common nonmonotonic technique that allows for dealing with negative information in knowledge and data bases. We present a detailed analysis of the computational complexity of the different forms of closed world reasoning for various fragments of propositional logic. The analysis allows us to draw a complete picture of the tractability/intractability frontier for such a form of nonmonotonic reasoning. We also discuss how to use our results in order to characterize the computational complexity of other problems related to nonmonotonic inheritance, diagnosis, and default reasoning.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-083.pdf,
83,1990,"Complexityand Expressiveness",On the Expressiveness of Networks with Hidden Variables,Rina Dechter,"This paper investigates design issues associated with representing relations in binary networks augmented with hidden variables. The trade-off between the number of variables required and the size of their domains is discussed. We show that if the number of values available to each variable is just two, then hidden variables cannot improve the expressional power of the network, regardless of their number. However, for k greater than or equal to 3, we can always find a layered network using k-valued hidden variables that represent an arbitrary relation. We then provide a scheme for decomposing an arbitrary relation, p, using |p|-2/k-2 hidden variables, each having k values (k >2).",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-084.pdf,
84,1990,"Complexityand Expressiveness",An Optimally Efficient Limited Inference System,"Lokendra Shastri, Venkat Ajjanagadde",This paper describes a knowledge representation and reasoning system that performs a limited but interesting class of inferences over a restricted class of first-order sentences with optimal efficiency. The proposed system can answer yes-no as well as wh-queries in time that is only proportional to the length of the shortest derivation of the query and is independent of the size of the knowledge base. This work suggests that the expressiveness and the inferential ability of a representation and reasoning systems may be limited in unusual ways to arrive at extremely efficient yet fairly powerful knowledge based systems.,https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-085.pdf,
85,1990,"Complexityand Expressiveness",It’s Not My Default: The Complexity of Membership Problems in Restricted Propositional Default Logics,Jonathan Stillman,"We investigate the computational complexity of membership problems in a number of propositional default logics. We introduce a hierarchy of classes of propositional default rules that extends that described in [Kautz and Selman 1989], and characterize the complexity of membership problems in these classes under various simplifying assumptions about the underlying propositional theory. Our work significantly extends both that presented in [Kautz and Selman 1989] and in [Stillman 199Oa].",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-086.pdf,
86,1990,Connectionism,"Connectionism, Rule Following, and Symbolic Manipulation",Robert F. Wadley,"At present, the prevailing Connectionist methodology for representing rules is to implicitly embody rules in ""neurally-wired"" networks. That is, the methodology adopts the stance that rules must either be hard-wired or ""trained into"" neural structures, rather than represented via explicit symbolic structures. Even recent attempts to implement production systems within connectionist networks have assumed that condition-action rules (or rule schema) are to be embodied in the structure of individual networks. Such networks must be grown or trained over a significant span of time. However, arguments are presented herein that humans sometimes follow rules which are very rapidly assigned explicit internal representations, and that humans possess general mechanisms capable of interpreting and following such rules. In particular, arguments are presented that the speed with which humans are able to follow rules of novel structure demonstrates the existence of general-purpose rule following mechanisms. It is further argued that the existence of general-purpose rule following mechanisms strongly indicates that explicit rule following is not an isolated phenomenon, but may well be a pervasive aspect of cognition. The arguments presented here are pragmatic in nature, and are contrasted with the kind of arguments developed by Fodor and Pylyshyn in their recent, influential paper.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-087.pdf,
87,1990,Connectionism,A Structured Connectionist Unification Algorithm,Steffen Hölldobler,"A connectionist unification algorithm is presented. It utilizes the fact that the most general unifier of two terms corresponds to a finest valid equivalence relation defined on a occurrence-label representation of the unification problem. The algorithm exploits the maximal parallelism inherent in the computation of such a finest valid equivalence relation while using only computational features of connectionism. It can easily be restricted to solve special forms of the unification problem such as the word problem, the matching problem, or the unification problem over infinite trees.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-088.pdf,
88,1990,"DefaultRepresentations",Conditional Logics of Normality as Modal Systems,Craig Boutilier,"Recently, conditional logics have been developed for application to problems in default reasoning. We present a uniform framework for the development and investigation of conditional logics to represent and reason with ""normality"", and demonstrate these logics to be equivalent to extensions of the modal system S4. We also show that two conditional logics, recently proposed to reason with default knowledge, are equivalent to fragments of two logics developed in this framework.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-089.pdf,
89,1990,"DefaultRepresentations",Nonmonotonicity and the Scope of Reasoning: Preliminary Report,David W. Etherington,"Existing formalisms for default reasoning capture some aspects of the nonmonotonicity of human commonsense reasoning. However, Perlis has shown that one of these formalisms, circumscription, is subject to certain counterintuitive limitations. Kraus and Perlis suggested a partial solution, but significant problems remain. In this paper, we observe that the unfortunate limitations of circumscription are even broader than Perlis originally pointed out. Moreover, these problems are not confined to circumscription; they appear to be endemic in current nonmonotonic reasoning formalisms. We develop a much more general solution than that of Kraus and Perlis, involving restricting the scope of nonmonotonic reasoning, and show that it remedies these problems in a variety of formalisms.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-090.pdf,
90,1990,"DefaultRepresentations",The Representation of Defaults in Cyc,Ramanathan V. Guha,"This paper provides an account of the representation of defaults in Cyc and their semantics in terms of first order logic with reification. Default reasoning is a complex thing, and we have found it beneficial to separate various complex issues whose ""current best solution"" is likely to change now and then - such as deciding between extensions, preferring one default to another, etc. - and deal with them explicitly in the knowledge base, thus allowing us to adopt a simple (and hopefully fixed) logical mechanism to handle the basic non-monotonicity itself. We also briefly describe how this default reasoning scheme is implemented in Cyc.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-091.pdf,
91,1990,"DefaultRepresentations",The Generalized Theory of Model Preference (Preliminary Report),Piotr Rychlik,In this paper we present a purely semantic view on non-monotonic reasoning. We follow the direction pointed in [16] and claim that any non-monotonic logic can be viewed as a result of transforming some base standard logic by a selection strategy defined on models. The generalized theory of model preference is shortly outlined here together with its use in modeling non-monotonic beliefs.,https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-092.pdf,
92,1990,Inheritance,Terminological Cycles in KL-ONE-based Knowledge Representation Languages,Franz Baader,"Cyclic definitions are often prohibited in terminological knowledge representation languages because, from a theoretical point of view, their semantics is not clear and, from a practical point of view, existing inference algorithms may go astray in the presence of cycles. In this paper, we shall consider terminological cycles in a very small KL-ONE-based language. For this language, the effect of the three types of semantics introduced by (Nebel 1987,1989,1989a) can be completely described with the help of finite automata. These descriptions provide a rather intuitive understanding of terminologies with cyclic definitions and give insight into the essential features of the respective semantics. In addition, one obtains algorithms and complexity results for subsumption determination. As it stands, the greatest fixed-point semantics comes off best. The characterization of this semantics is easy and has an obvious intuitive interpretation. Furthermore, important constructs - such as value-restriction with respect to the transitive or reflexive-transitive closure of a role - can easily be expressed.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-093.pdf,
93,1990,Inheritance,On the Complexity of Monotonic Inheritance with Roles,"Ramiro A. de T. Guerreiro, Andrea S. Hemerly, Yoav Shoham","We investigate the complexity of reasoning with monotonic inheritance hierarchies that contain, beside ISA edges, also ROLE (or FUNCTION) edges. A ROLE edge is an edge labelled with a name such as spouse-of or brother-of. We call such networks ISAR networks. Given a network with n vertices and m edges, we consider two problems: (P1) determining whether the network implies an isa relation between two particular nodes, and (P2) determining all isa relations implied by the network. As is well known, without ROLE edges the time complexity of P1 is O(m), and the time complexity of P2 is O(n3). Unfortunately, the results do not extend naturally to ISAR networks, except in a very restricted case. For general ISAR network we frost give an polynomial algorithm by an easy reduction to proposional Horn theory. As the degree of the polynomial is quite high (O(mn4) for P1, O(mn6 ) for P2), we then develop a more direct algorithm. For both P1 and P2 its complexity is O(n3 + m2). Actually, a finer analysis of the algorithm reveals a complexity of O(nr(log r) + n2r + n3), where r is the number of different ROLE labels. One corolary is that if we fix the number of ROLE labels, the complexity of our algorithm drops back to O(n3).",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-094.pdf,
94,1990,Inheritance,Boolean Extensions of Inheritance Networks,"John F. Horty, Richmond H. Thomason","Much of the theoretical research on nonmonotonic inheritance has concentrated on formalisms involving only IS-A links between primitive nodes. However, it is hard to imagine a useful network representation of commonsense or expert knowledge that would not involve nodes representing negative, conjunctive, or disjunctive properties. Certain nodes of this kind were included in some of the earliest formalisms for defeasible inheritance, but were omitted in later work, either to secure tractability or to simplify the task of theoretical analysis. The purpose of the present paper is to extend the theoretical analysis of defeasible inheritance to networks incorporating these expressive enhancements.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-095.pdf,
95,1990,Inheritance,A Temporal Terminological Logic,Albrecht Schmiedel,"An attempt is made to integrate three well-known formalisms of knowledge representation: terminological logic in the tradition of KL-ONE, the temporal logic of Shoham, and Allen’s interval calculus. Drawing on each of these sources, a temporal terminological logic is proposed which combines structural with temporal abstraction. A formal semantics is provided, and some hints are given for exploring the computational properties of reasoning in the formalism.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-096.pdf,
96,1990,"Representationand Uncertainty",A Maximum Entropy Approach to Nonmonotonic Reasoning,"Moisés Goldszmidt, Paul Morris, Judea Pearl","This paper describes a probabilistic approach to nonmonotonic reasoning which combines the principle of infinitesimal probabilities with that of maximum entropy, and which sanctions inferences similar to those produced by the principle of minimizing abnormalities. The paper provides a precise formalization of the consequences entailed by a defeasible knowledge base, develops the computational machinery necessary for deriving these consequences, and compares the behavior of the maximum entropy approach to those of e-semantics ([Pearl 89a]) and rational closure ([Lehmann 89]).",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-097.pdf,
97,1990,"Representationand Uncertainty",A Hybrid Framework for Representing Uncertain Knowledge,Alessandro Saffiotti,"This paper addresses the problem of bridging the gap between the fields of Knowledge Renresentation OCR) and Uncertain Reasoning (UR). The prohosed solution consists of a framework for representing uncertain knowledge in which two components, one dealing with (categorical) knowledge and one dealing with uncertainty about this knowledge, are singled out. In this sense, the framework is ""hybrid"". This framework is characterized in both model-theoretic and proof-theoretic terms. State of belief is represented by ""belief sets"", defined in terms of the ""functional approach to knowledge representation"" suggested by Levesque. Examples are given, using first order logic and (a minimal subset of) M-Krypton for the KR side, and a yes/no trivial case and Dempster-Shafer theory for the UR side.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-098.pdf,
98,1990,"Representationand Uncertainty",A Probabilistic Interpretation for Lazy Nonmonotonic Reasoning,Ken Satoh,"This paper presents a formal relationship for . probability theory and a class of nonmonotonic reasoning which we call laxy nonmonotonic reasoning. In lazy nonmonotonic reasoning, nonmonotonicity emerges only when new added knowledge is contradictory to the previous belief. In this paper, we consider nonmonotonic reasoning in terms of consequence relation. A consequence relation is a binary relation over formulas which expresses that a formula is derivable from another formula under inference rules of a considered system. A consequence relation which has lazy nonmonotonicity is called a rational consequence relation studied by Lehmann and Magidor (1988). We provide a probabilistic semantics which characterizes a rational consequence relation exactly. Then, we show a relationship between propositional circumscription and consequence relation, and apply this semantics to a consequence relation defined by propositional circumscription which has lazy nonmonotonicity.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-099.pdf,
99,1990,"Representationand Uncertainty",Probabilities That Imply Certainties,Haim Shvaytser (Schweitzer),A method is described for deriving rules of inference from relations between probabilities of sentences in Nilsson’s probabilistic logic.,https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-100.pdf,
100,1990,Architectures,Very Fast Decision Table Execution of Propositional Expert Systems,"Robert M. Colomb, Charles Y.C. Chung","A formal equivalence between propositional expert systems and decision tables is proved, and a practicable procedure given to perform the transformation between propositional expert systems and decision tables. The method gave an order of magnitude speed increase for a well-known expert system in routine use. The method is very general: adaptations are shown for forward and backward chaining inferencing engines, inexact reasoning, and systems where some facts have a high cost and must be determined only if necessary. A particular application for the decision table representation is in real-time expert systems, since a simple hardware implementation is available which gives further orders of magnitude increase in performance. Finally, the decision table representation greatly simplifies the problem of completeness and consistency checking.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-101.pdf,
101,1990,Architectures,The Intelligent Database Interface: Integrating AI and Database Systems,"Donald P. McKay, Timothy W. Finin, Anthony O'Hare",The Intelligent Database Interface (IDI) is a cache-based interface that is designed to provide Artificial Intelligence systems with efficient access to one or more databases on one or more remote database management systems (DBMSs). It can be used to interface with a wide variety of different DBMSs with little or no modification since SQL is used to communicate with remote DBMSs and the implementation of the IDI provides a high degree of portability. The query language of the IDI is a restricted subset of function-free Horn clauses which is translated into SQL. Results from the ID1 are returned one tuple at a time and the IDI manages a cache of result relations to improve efficiency. The IDI is one of the key components of the Intelligent System Server (ISS) knowledge representation and reasoning system and is also being used to provide database services for the Unisys spoken language systems program.,https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-102.pdf,
102,1990,Architectures,On the Performance of Lazy Matching in Production Systems,"Daniel P. Miranker, David A. Brant, Bernie Lofaso, David Gadbois","Production systems are an established method for encoding knowledge in an expert system. The semantics of production system languages and the concomitant algorithms for their evaluation, RETE and TREAT, enumerate the set of rule instantiations and then apply a strategy that selects a single instantiation for firing. Often rule instantiations are calculated and never fired. In a sense, the time and space required to eagerly compute these unfired instantiations is wasted. This paper presents preliminary results about a new match technique, lazy matching. The lazy match algorithm folds the selection strategy into the search for instantiations, such that only one instantiation is computed per cycle. The algorithm improves the worst-case asymptotic space complexity of incremental matching. Moreover, empirical and analytic results demonstrate that lazy matching can substantially improve the execution time of production system programs.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-103.pdf,
103,1990,Architectures,A Framework for Investigating Production System Formulations with Polynomially Bounded Match,"Milind Tarnbe, Paul S. Rosenbloom","Real time constraints on AI systems require guaranteeing bounds on these systems’ performance. However, in the presence of sources of uncontrolled combinatorics, it is extremely difficult to guarantee such bounds on their performance. In production systems, the primary source of uncontrolled combinatorics is the production match. To eliminate these combinatorics, the unique-attribute formulation was introduced in (Tambe and Rosenbloom, 1989). which achieved a linear bound on the production match. This formulation leads to several questions: is this unique-attributes formulation the best conceivable production system formulation? In fact, are there other alternative production system formulations? If there are other formulations, how should these alternatives be compared with the unique-attribute formulation? This paper attempts to address these questions in the context of Soar. It identifies independent dimensions along which alternative production system formulations can be specified. These dimensions are based on the fiied class of match algorithms currently employed in production systems. These dimensions create a framework for systematically generating alternative formulations. Using this framework we show that the unique-attribute formulation is the best one within the dimensions investigated. However, if a new class of match algorithms is admitted, by relaxing certain constraints, other competitor fonnulations emerge. The paper indicates which competitor formulations are promising and why. Although some of the concepts, such as unique-attributes, are introduced in the context of Soar, they should also be relevant to other rule-based systems.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-104.pdf,
104,1990,Architectures,A Principled Approach to Reasoning about the Specificity of Rules,John Yen,"Even though specificity has been one of the most useful conflict resolution strategies for selecting productions, most existing rule-based systems use heuristic approximation such as the number of clauses to measure a rule’s specificity. This paper describes an approach for computing a principled specificity relation between rules whose conditions are constructed using predicates defined in a terminological knowledge base. Based on a formal definition about pattern subsumption relation, we first show that a subsumption test between two conjunctive patterns can be viewed as a search problem. Then we describe an implemented pattern classification algorithm that improves the efficiency of the search process by deducing implicit conditions logically implied by a pattern and by reducing the search space using subsumption relationships between predicates. Our approach enhances the maintainability of rule-based systems and the reusability of definitional knowledge.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-105.pdf,
105,1990,"Temporaland Spatial Reasoning",Solving Geometric Constraint Systems,Glenn A. Kramer,"Finding the configurations of a set of rigid bodies that satisfy a set of geometric constraints is a problem traditionally solved by reformulating the geometry and constraints as algebraic equations which are solved symbolically or numerically. But many such problems can be solved by reasoning symbolically about the geometric bodies themselves using a new technique called degrees of freedom analysis. In this approach, a sequence of actions is devised to satisfy each constraint incrementally, thus monotonically decreasing the system’s remaining degrees of freedom. This sequence of actions is used metaphorically to solve, in a maximally decoupled form, the equations resulting from an algebraic representation of the problem. Degrees of freedom analysis has significant computational advantages over conventional algebraic approaches. The utility of the technique is demonstrated with a program that assembles and kinematically simulates mechanical linkages.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-106.pdf,
106,1990,"Temporaland Spatial Reasoning",Weak Representations of Interval Algebras,Gérard Ligozat,"Ladkin and Maddux [LaMa87] showed how to interpret the calculus of time intervals defined by Allen [All83] in terms of representations of a particular relation algebra, and proved that this algebra has a unique countable representation up to isomorphism. In this paper, we consider the algebra An of n-intervals, which coincides with Allen’s algebra for n=2, and prove that An has a unique countable representation up to isomorphism for all n > = 1. We get this result, which implies that the first order theory of An is decidable, by introducing the notion of a weak representation of an interval algebra, and by giving a full classification of the connected weak representations of A n. We also show how the topological properties of the set of atoms of An can be represented by a n-dimensional polytope.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-107.pdf,
107,1990,"Temporaland Spatial Reasoning",A Qualitative Model for Space,"Amitabha Mukerjee, Gene Joe","Most geometric models are quantitative, making it diffiiult to abstract the underlying spatial information needed for tasks such as planning, learning or vision. Furthermore, the precision used in a typical quantitative system often exceeds the actual accuracy of the data. In this work we describe a systematic representation that builds spatial maps based on local qualitative relations between objects. It derives relations that are more ""functionally relevant"" - i.e. those that involve accidental alignments, or can be described based on such alignments. In one dimension, interval logic (Allen 83) provides a mechanism for representing these type of relations; in this work we propose a formalism that enables us to perform alignment-based reasoning in two and higher dimensions with objects at angles. The principal advanmges of this representation is that a) it is free of subjective bias, and b) it is complete in the qualitative sense of distinguishing all overlap/ tangency/no-contact geometries. In addition, the model is capable of handling uncertainty in the initial system (e.g. ""the fuse box is somewhere behind the compressor) by constructing bounded inferences from disjunctive input dam. Two kinds of uncertainty can be handled those arising from deliberate imprecision in the interest of compactness (""down the road from""), or those caused by an inadequacy of data (sensors, spatial descriptions, or maps).",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-108.pdf,
108,1990,"Temporaland Spatial Reasoning",Reasoning about Qualitative Temporal Information,Peter van Beek,"Interval and point algebras have been proposed for representing qualitative temporal information about the relationships between pairs of intervals and pairs of points, respectively. In this paper, we address two related reasoning tasks that arise in these algebras: Given (possibly indefinite) knowledge of the relationships between some intervals or points, (1) find one or more scenarios that are consistent with the information provided, and (2) find all the feasible relations between every pair of intervals or points. Solutions to these problems have applications in natural language processing, planning, and a knowledge representation language. We define computationally efficient procedures for solving these tasks for the point algebra and for a corresponding subset of the interval algebra. Our algorithms are marked improvements over the previously known algorithms. We also show how the results for the point algebra aid in the design of a backtracking algorithm for the full interval algebra that is useful in practice.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-109.pdf,
109,1990,"InductiveLearning",Myths and Legends in Learning Classification Rules,Wray Buntine,"This paper is a discussion of machine learning theory on empirically learning classification rules. The paper proposes six myths in the machine learning community that address issues of bias, learning as search, computational learning theory, Occam’s razor, ""universal"" learning algorithms, and interactive learning. Some of the problems raised are also nddrcssed from a Bayesian perspective. The paper concludes by suggesting questions that machine learning researchers should be addressing both theoretically and experimentally.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-110.pdf,
110,1990,"InductiveLearning",Learning from Textbook Knowledge: A Case Study,William W. Cohen,"One of the ""grand challenges for machine learning"" is the problem of learning from textbooks. This paper addresses the problem of learning from texts including omissions and inconsistencies that are clarified by illustrative examples. To avoid problems in natural language understanding, we consider a simplification of this problem in which the text has been manually translated into a logical theory. This learning problem is solvable by a technique that we call analogical abductive explanation based learning (ANA-EBL). Formal evidence and experimental results in the domain of contract bridge show that the learning technique is both efficient and effective.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-111.pdf,
111,1990,"InductiveLearning",What Should Be Minimized in a Decision Tree?,"Usama M. Fayyad, Keki B. Irani","In this paper, we address the issue of evaluating decision trees generated from training examples by a learning algorithm. We give a set of performance measures and show how some of them relate to others. We derive results suggesting that the number of leaves in a decision tree is the important measure to minimize. Minimizing this measure will, in a probabilistic sense, improve performance along the other measures. Notably it is expected to produce trees whose error rates are less likely to exceed some acceptable limit. The motivation for deriving such results is two-fold: 1. To better understand what constitutes a good measure of performance, and 2. To provide guidance when deciding which aspects of a decision tree generation algorithm should be changed in order to improve the quality of the decision trees it generates. The results presented in this paper can be used as a basis for a methodology for formally proving that one decision tree generation algorithm is better than another. This would provide a more satisfactory alternative to the current empirical evaluation method for comparing algorithms.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-112.pdf,
112,1990,"InductiveLearning",Generalization with Taxonomic Information,"Alan M. Frisch, C. David Page Jr.","This paper studies sorted generalization-the generalization, with respect to an arbitrary taxonomic theory, of atomic formulas containing sorted variables. It develops an algorithm for the task, discusses the algorithm and task complexity, and presents semantic properties of sorted generalization. Based on its semantic properties, we show how sorted generalization is applicable to such problems as abduction, induction, knowledge base vivification, and analogical reasoning. Significant distinctions between this work and related work with taxonomic information arise from the generality of the taxonomic theories we allow, which may be any first-order taxonomic theories, and the semantic completeness properties of sorted generalization.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-113.pdf,
113,1990,"InductiveLearning",Constructor: A System for the Induction of Probabilistic Models,"Robert M. Fung, Stuart L. Crawford","The probabilistic network technology is a knowledge-based technique which focuses on reasoning under uncertainty. Because of its well defined semantics and solid theoretical foundations, the technology is finding increasing application in fields such as medical diagnosis, machine vision, military situation assessment , petroleum exploration, and information retrieval. However, like other knowledge-based techniques, acquiring the qualitative and quantitative information needed to build these networks can be highly labor-intensive. CONSTRUCTQR integrates techniques and concepts from probabilistic networks, artificial intelligence, and statistics in order to induce Markov networks (i.e., undirected probabilistic networks). The resulting networks are useful both qualitatively for concept organization and quantitatively for the assessment of new data. The primary goal of CONSTRUCTOR is to find qualitative structure from data. CONSTRUCTOR finds structure by first, modeling each feature in a data set as a node in a Markov network and secondly, by finding the neighbors of each node in the network. In Markov networks, the neighbors of a node have the property of being the smallest set of nodes which ""shield"" the node from being affected by other nodes in the graph. This property is used in a heuristic search to identify each node’s neighbors. The traditional x2 test for independence is used to test if a set of nodes ""shield"" another node. Cross-validation is used to estimate the quality of alternative structures.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-114.pdf,
114,1990,"InductiveLearning",Learning Causal Trees from Dependence Information,"Dan Geiger, Azaria Paz, Judea Pearl","In constructing probabilistic networks from human judgments, we use causal relationships to convey useful patterns of dependencies. The converse task, that of inferring causal relationships from patterns of dependencies, is far less understood. This paper establishes conditions under which the directionality of some interactions can be determined from non-temporal probabilistic information - an essential prerequisite for attributing a causal interpretation to these interactions. An efficient algorithm is developed that, given data generated by an undisclosed causal polytree, recovers the structure of the underlying polytree, as well as the directionality of all its identifiable links.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-115.pdf,
115,1990,"InductiveLearning","Theory Reduction, Theory Revision, and Retranslation",Allen Ginsberg,"This paper presents an approach to retranslation, the third and final step of the theory reduction approach to solving theory revision problems [3,4]. Retranslation involves putting a modified ""operationalized,"" or ""reduced,"" version of the desired revised theory back into the entire language of the original theory. This step is desirable for a number of reasons, not least of which is the need to ""compress"" what are generally very large reduced theories into much smaller, and thus, more efficiently evaluated, unreduced theories. Empirical results for the retranslation method are presented.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-116.pdf,
116,1990,"InductiveLearning","A Hybrid Connectionist, Symbolic Learning System","Allen Lawrence O. Hall, Steve G. Romaniuk","This paper describes the learning part of a system which has been developed to provide expert systems capability augmented with learning. The learning scheme is a hybrid connectionist, symbolic one. A network representation is used. Learning may be done incrementally and requires only one pass through the data set to be learned. Attribute, value pairs are supported as a variable implementation. Variables are represented by groups of connected cells in the network. The learning algorithm is described and an example given. Current results are discussed, which include learning the well-known Iris data set. The results show that the system has promise.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-117.pdf,
117,1990,"InductiveLearning",Empirical Studies on the Speed of Convergence of Neural Network Training using Genetic Algorithms,Hiroaki Kitano,"This paper reports several experimental results on the speed of convergence of neural network training using genetic algorithms and back propagation. Recent excitement regarding genetic search lead some researchers to apply it to training neural networks. There are reports on both successful and faulty results, and, unfortunately, no systematic evaluation has been made. This paper reports results of systematic experiments designed to judge whether use of genetic algorithms provides any gain in neural network training over existing methods. Experimental results indicate that genetic search is, at best, equally efficient to faster variants of back propagation in very small scale networks, but far less efficient in larger networks.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-118.pdf,
118,1990,"InductiveLearning",Learning to Coordinate Behaviors,"Pattie Maes, Rodney A. Brooks","We describe an algorithm which allows a behavior-based robot to learn on the basis of positive and negative feedback when to activate its behaviors. In accordance with the philosophy of behavior-based robots, the algorithm is completely distributed: each of the behaviors independently tries to find out (i) whether it is relevant (i.e. whether it is at all correlated to positive feedback) and (ii) what the conditions are under which it becomes reliable (i.e. the conditions under which it maximises the probability of receiving positive feedback and minimises the probability of receiving negative feedback). The algorithm has been tested successfully on an autonomous 6-legged robot which had to learn how to coordinate its lens so as to walk forward.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-119.pdf,
119,1990,"InductiveLearning",Adding Domain Knowledge to SBL through Feature Construction,Christopher John Matheus,"This paper presents two methods for adding domain knowledge to similarity-based learning through feature construction, a form of representation change in which new features are constructed from relationships detected among existing features. In the first method, domain-knowledge constraints are used to eliminate less desirable new features before they are constructed. In the second method, domain-dependent transformations generalize new features in ways meaningful to the current problem. These two uses of domain knowledge are illustrated in CITRE where they are shown to improve hypothesis accuracy and conciseness on a tic-tat-toe classification problem.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-120.pdf,
120,1990,"InductiveLearning",Inductive Learning in Probabilistic Domain,"Yoichiro Nakakuki, Yoshiyuki Koseki, Midori Tanaka","This paper describes an inductive learning method in probabilistic domain. It acquires an appropriate probabilistic model from a small amount of observation data. In order to derive an appropriate probabilistic model, a presumption tree with least description length is constructed. Description length of a presumption tree is defined as the sum of its code length and log-likelihood. Using a constructed presumption tree, the probabilistic distribution of future events can be presumed appropriately from observations of occurrences in the past. This capability enables the efficiency of certain kinds of performance systems, such as diagnostic system, that deal with probabilistic problems. The experimental results show that a model-based diagnostic system performs efficiently by making good use of the learning mechanism. In comparison with a simple probability estimation method, it is shown that the proposed approach requires fewer observations, to acquire an appropriate probabilistic model.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-121.pdf,
121,1990,"InductiveLearning",Changing the Rules: A Comprehensive Approach to Theory Refinement,"Dirk Ourston, Raymond J. Mooney","This paper presents a comprehensive approach to automatic theory refinement. In contrast to other systems, the approach is capable of modifying a theory which contains multiple faults and faults which occur at intermediate points in the theory. The approach uses explanations to focus the corrections to the theory, with the corrections being supplied by an inductive component. In this way, an attempt is made to preserve the structure of the original theory as much as possible. Because the approach begins with an approximate theory, learning an accurate theory takes fewer examples than a purely inductive system. The approach has application in expert system development, where an initial, approximate theory must be refined. The approach also applies at any point in the expert system lifecycle when the expert system generates incorrect results. The approach has been applied to the domain of molecular biology and shows significantly better results then a purely inductive learner.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-122.pdf,
122,1990,"InductiveLearning",Knowledge Level and Inductive Uses of Chunking (EBL),"Paul S. Rosenbloom, Jans Aasman","When explanation-based learning (EBL) is used for knowledge level learning (KLL), training examples are essential, and EBL is not simply reducible to partial evaluation. A key enabling factor in this behavior is the use of domain theories in which not every element is believed a priori. When used with such domain theories EBL provides a basis for rote learning (deductive KLL) and induction from multiple examples (nondeductive KLL). This article lays the groundwork for using EBL in KLL, by describing how EBL can lead to increased belief, and describes new results from using Soar’s chunking mechanism - a variation on EBL - as the basis for a task-independent rote learning capability and a version-space-based inductive capability. This latter provides a compelling demonstration of nondeductive KLL in Soar, and provides the basis for an integration of conventional EBL with induction. However, it also reveals how one of Soar’s key assumptions - the non-penetrable memory assumption - makes this more complicated than it would otherwise be. This complexity may turn out to be appropriate, or it may point to where modifications of Soar are needed.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-123.pdf,
123,1990,"InductiveLearning",A Proven Domain-Independent Scientific Function-Finding Algorithm,Cullen Schaffer,"Programs such as Bacon, Abacus, Coper, Kepler and others are designed to find functional relationships of scientific significance in numerical data without relying on the deep domain knowledge scientists normally bring to bear in analytic work. Whether these systems actually perform as intended is an open question, however. To date, they have been supported only by anecdotal evidence-reports that a desirable answer has been found in one or more hand-selected and often artificial cases. In this paper, I describe a function-finding algorithm which differs radically from previous candidates in three respects. First, it concentrates rather on reliable identification of a few functional forms than on heuristic search of an infinite space of potential relations. Second, it introduces the use of distinction, significance and lack of fit -- three general concepts of value in evaluating apparent functional relationships. Finally, and crucially, the algorithm has been tested prospectively on an extensive collection of real scientific data sets. Though I claim much less than previous investigators about the power of my approach, these claims may be considered-to a degree quite unfamiliar in function-finding research-as conclusively proven.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-124.pdf,
124,1990,"InductiveLearning",Complementary Discrimination Learning: A Duality between Generalization and Discrimination,Wei-Min Shen,"Although generalization and discrimination are commonly used together in machine learning, little has been understood about how these two methods are intrinsically related. This paper describes the idea of complementary discrimination, which exploits semantically the syntactic duality between the two approaches: discriminating a concept is equivalent to generalizing the complement of the concept, and vice versa. This relation brings together naturally generalization and discrimination so that learning programs may utilize freely the advantages of both approaches, such as learning by analogy and learning from mistakes. We will give a detailed description of the complementary discrimination learning (CDL) algorithm and extend the previous results by considering the effect of noise and analyzing the complexity of the algorithm. CDL’s performance on both perfect and noisy data and its ability to manage the tradeoff between simplicity and accuracy of concepts have provided some evidence that complementary discrimination is a useful and intrinsic relation between generalization and discrimination.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-125.pdf,
125,1990,"InductiveLearning",Inductive Learning in a Mixed Paradigm Setting,"David B. Skalak, Edwina E. Rissland","In a precedent-bused domain one appeals to previous cases to support a solution, decision, explanation, or an argument. Experts typically use care in choosing cases in precedent-based domains, and apply such criteria as case relevance, prototypicality, and importance. In domains where both cases and rules are used, experts use an additional case selection criterion: the generalizations that a particular group of cases support. Domain experts use their knowledge of cases to forge the rules learned from those cases. In this paper, we explore inductive learning in a ""mixed paradigm"" setting, where both rule-based and case-based reasoning methods are used. In particular, we consider how the techniques of case-based reasoning in an adversarial, precedent-based domain can be used to aid a decision-tree based classification algorithm for (1) training set selection, (2) branching feature choice, and (3) induction policy preference and deliberate exploitation of inductive bias. We focus on how precedent-based argumentation may inform the selection of training examples used to build classification trees. The resulting decision trees may then be reexpressed as rules and incorporated into the mixed paradigm system. We discuss the heuristic control problems involved in incorporating an inductive learner into CABARET, a mixed paradigm reasoner. Finally, we present an empirical study in a legal domain of the classification trees generated by various training sets constructed by a case-based reasoning module.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-126.pdf,
126,1990,"InductiveLearning",Incremental Non-Backtracking Focusing: A Polynomially Bounded Generalization Algorithm for Version Spaces,"Benjamin D. Smith, Paul S. Rosenbloom","The candidate elimination algorithm for inductive learning with version spaces can require both exponential time and space. This article describes the Incremental Non-Backtracking Focusing (INBF) algorithm which learns strictly tree-structured concepts in polynomial space and time. Specifically, learns in time O(pnk) and space O(nk) where p the number of positives, n the number of negatives, and k the number of features. INBF is an extension of an existing batch algorithm, Avoidance Focusing (AF). Although AF also learns in polynomial time, it assumes a convergent set of positive examples, and handles additional examples inefficiently; INBF has neither of these restrictions. Both the AF and INBF algorithms assume that the positive examples plus the near misses will be sufficient for convergence if the initial set of examples is convergent. This article formally proves that for tree-structured concepts this assumption does in fact hold.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-127.pdf,
127,1990,"InductiveLearning",Two Case Studies in Cost-Sensitive Concept Acquisition,"Ming Tan, Jeffrey C. Schlimmer","This paper explores the problem of learning from examples when feature measurement costs are significant. It then extends two effective and familiar learning methods, ID3 and IBL, to address this problem. The extensions, CS-ID3 and CS-IBL, are described in detail and are tested in a natural robot domain and a synthetic domain. Empirical studies support the hypothesis that the extended methods are indeed sensitive to feature costs: they deal effectively with varying cost distributions and with irrelevant features.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-128.pdf,
128,1990,"InductiveLearning",Refinement of Approximate Domain Theories by Knowledge-Based Neural Networks,"Geoffrey G. Towell, Jude W. Shavlik, Michiel O. Noordewier","Standard algorithms for explanation-based learning require complete and correct knowledge bases. The KBANN system relaxes this constraint through the use of empirical learning methods to refine approximately correct knowledge. This knowledge is used to determine the structure of an artificial neural network and the weights on its links, thereby making the knowledge accessible for modification by neural learning. KBANN is evaluated by empirical tests in the domain of molecular biology. Networks created by KBANN are shown to be superior, in terms of their ability to correctly classify unseen examples, to randomly initialized neural networks, decision trees, ""nearest neighbor"" matching, and standard techniques reported in the biological literature. In addition, KBANN'S networks improve the initial knowledge in biologically interesting ways.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-129.pdf,
129,1990,"InductiveLearning",On Analytical and Similarity-Based Classification,"Marc Vilain, Phyllis Koton, Melissa P. Chase","This paper is concerned with knowledge representation issues in machine learning. In particular, it presents a representation language that supports a hybrid analytical and similarity-based classification scheme. Analytical classification is produced using a KL-ONE-like term-subsumption strategy, while similarity-based classification is driven by generalizations induced from a training set by an unsupervised learning procedure. This approach can be seen as providing an inductive bias to the learning procedure, thereby shortening the required training phase, and reducing the brittleness of the induced generalizations.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-130.pdf,
130,1990,"InductiveLearning",Effective Generalization of Relational Descriptions,Larry Watanabe and Larry Rendell,"The problem of computing maximally-specific generalizations (MSCG~) of relational descriptions can be modelled as tree search. We describe several transformations and pruning methods for reducing the complexity of the problem. Based on this analysis, we have implemented a search program (X-search) for finding the MSCG’s. Experiments compare the separate and combined effects of pruning methods on search efficiency. With effective pruning methods, full-width search appears feasible for moderately sized relational descriptions.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-131.pdf,
131,1990,"InductiveLearning",Explaining Temporal Differences to Create Useful Concepts for Evaluating States,"Richard C. Yee, Sharad Saxena, Paul E. Utgoff, Andrew G. Barto","We describe a technique for improving problem-solving performance by creating concepts that allow problem states to be evaluated through an efficient recognition process. A temporal-difference (td) method is used to bootstrap a collection of useful concepts by backing up evaluations from recognized states to their predecessors. This procedure is combined with explanation- based generalization (EBG) and goal regression to use knowledge of the problem domain to help generalize the new concept definitions. This maintains the efficiency of using the concepts and accelerates the learning process in comparison to knowledge-free approaches. Also, because the learned definitions may describe negative conditions, it becomes possible to use EBG to explain why some instance is not an example of a concept. The learning technique has been elaborated for minimax game-playing and tested on a Tic-Tat-Toe system, T2. Given only concepts defining the end-game states and constrained to a two-ply search bound, experiments show that T2 learns concepts for achieving near-perfect play. T2’s total searching time, including concept recognition, is within acceptable performance limits while perfect play without the concepts requires searches taking well over 100 times longer than T2’s.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-132.pdf,
132,1990,"InductiveLearning",Automated Discovery in a Chemistry Laboratory,"Jan M. Zytkow, Jieming Zhu, Abul Hussam","We describe an application of the discovery system FAHRENHEIT in a chemistry laboratory. Our emphasis is on automation of the discovery process as opposed to human intervention and on computer control over real experiments and data collection as opposed to the use of simulation. FAHRENHEIT performs automatically many cycles of experimentation, data collection and theory formation. We report on electrochemistry experiments of several hour duration, in which FAHRENHEIT has developed empirical equations (quantitative regularities) equivalent to those developed by an analytical chemist working on the same problem. The theoretical capabilities of FAHRENHEIT have been expanded, allowing the system to find maxima in a dataset, evaluate error for all concepts, and determine reproducibility of results. After minor adjustments FAHRENHEIT has been able to discover regularities in maxima locations and heights, and to analyse repeatability of measurements by the same mechanism, adapted from BACON, by which all numerical regularities are detected.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-133.pdf,
133,1990,"Learningand Problem Solving",Adaptive Search by Explanation-Based Learning of Heuristic Censors,"Neeraj Bhatnagar, Jack Mostow","We introduce an adaptive search technique that speeds up state space search by learning heuristic censors while searching. The censors speed up search by pruning away more and more of the space until a solution is found in the pruned space. Censors are learned by explaining dead ends and other search failures. To learn quickly, the technique over-generalizes by assuming that certain constraints are preservable, i.e., remain true on at least one solution path. A recovery mechanism detects violations of this assumption and selectively relaxes learned censors. The technique, implemented in an adaptive problem solver named FAILSAFE-2, learns useful heuristics that cannot be learned by other reported methods. Its effectiveness is indicated by a preliminary complexity analysis and by experimental results in three domains, including one in which PRODIGY failed to learn effective search control rules.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-134.pdf,
134,1990,"Learningand Problem Solving",Empirical Comparisons of Some Design Replay Algorithms,Brad Blumenthal,"Although most design replay techniques have been empirically tested against some performance program, there has been very little empirical evidence published that compares various approaches on the same problems to determine the source of power. Six different design replay algorithms based on approaches in the literature are implemented and tested on 20 different design replay problems. The resulting data indicate that there is a trade-off between efficiency and autonomy for certain types of adaptation strategies. Based on some of the lessons drawn from this data, a new algorithm, REMAID, has been developed. This algorithm recognizes two different types of mis-matches between previous experience and current problems: detours and pretours. The REMAID strategy takes advantage of its knowledge of m and matches to improve replay autonomy without sacrificing efficiency. The success of the REMAID algorithm is empirically verified.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-135.pdf,
135,1990,"Learningand Problem Solving",Learning Search Control for Constraint-Based Scheduling,Megan Eskey and Monte Zweben,"This paper describes an application of an analytical learning technique, Plausible Explanation-Based Learning (PEBL), that dynamically acquires search control knowledge for a constraint-based scheduling system. In general, the efficiency of a scheduling system suffers because of resource contention among activities. Our system learns the general conditions under which chronic contention occurs and uses search control to avoid repeating mistakes. Because it is impossible to prove that a chronic contention will occur with only one example, traditional EBL techniques are insufficient. We extend classical EBL by adding an empirical component that creates search control rules only when the system gains enough confidence in the plausible explanations. This extension to EBL was driven by our observations about the behavior of our scheduling system when applied to the real-world problem of scheduling tasks for NASA Space Shuttle payload processing. We demonstrate the utility of this approach and provide experimental results.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-136.pdf,
136,1990,"Learningand Problem Solving",Why PRODIGY/EBL Works,Oren Etzioni,"Explanation-Based Learning (EBL) fails to accelerate problem solving in some problem spaces. How do these problem spaces differ from the ones in Minton’s experiments [1988b]? Can minute modifications to problem space encoding drastically alter EBL’s performance? Will PRODIGY/EBL'S success scale to real-world domains? This paper presents a formal theory of problem space structure that answers these questions. The central observation is that PRODIGY/EBL relies on finding nonrecursive explanations of PRODIGY'S problem-solving behavior. The theory explains and predicts PRODIGY/EBL'S performance in a wide range of problem spaces. The theory also predicts that a static program transformer, called STATIC, can match PRODIGY/EBL'S performance in some cases. The paper reports on an array of experiments that confirms this prediction. STATIC matches PRODIGY/EBL'S performance in each of Minton’s problem spaces.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-137.pdf,
137,1990,"Learningand Problem Solving",Learning Abstraction Hierarchies for Problem Solving,Craig A. Knoblock,"The use of abstraction in problem solving is an effective approach to reducing search, but finding good abstractions is a difficult problem, even for people. This paper identifies a criterion for selecting useful abstractions, describes a tractable algorithm for generating them, and empirically demonstrates that the abstractions reduce search. The abstraction learner, called ALPINE, is integrated with the PRODIGY problem solver [Minton et ab., 1989b, Carbonell et al., 1990] and has been tested on large problem sets in multiple domains.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-138.pdf,
138,1990,"Learningand Problem Solving",Extending EBG to Term-Rewriting Systems,"Philip Laird, Evan Gamble","We show that the familiar explanation-based generalization (EBG) procedure is applicable to a large family of programming languages, including three families of importance to AI: logic programming (such as Prolog); lambda calculus (such as LISP); and combinator languages (such as FP). The main application of this result is to extend the algorithm to domains for which predicate calculus is a poor representation. In addition, many issues in analytical learning become clearer and easier to reason about.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-139.pdf,
139,1990,"Learningand Problem Solving",Operationality Criteria for Recursive Predicates,Stanley Letovsky,"Current explanation-based generalization (EBG) techniques can perform badly when the problem being solved involves recursion. Often an infinite series of learned concepts are generated that correspond to the expansion of recursive solutions over every finite depth. Previous attempts to address the problem, such as Shavlik’s generalization-to-N EBG method, are overly reluctant to expand recursions; this reluctance can lead to inefficient rules. In this paper EBG is viewed as a program transformation technique on logic programs. Within that framework an improved operationality criterion for controlling the expansion of recursions is presented. This criterion prevents certain infinite and combinatorially explosive rule classes from being generated, yet permits expansion in some useful circumstances, allowing more efficient rules to be learned.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-140.pdf,
140,1990,"Learningand Problem Solving",The Utility of EBL in Recursive Domain Theories,Devika Subramanian and Ronen Feldman,"We investigate the utility of explanation-based learning in recursive domain theories and examine the cost of using macro-rules in these theories. The compilation options in a recursive domain theory range from constructing partial unwindings of the recursive rules to converting recursive rules into iterative ones. We compare these options against using appropriately ordered rules in the original domain theory and demonstrate that unless we make very strong assumptions about the nature of the distribution of future problems, it is not profitable to form recursive macro-rules via explanation-based learning in these domains.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-141.pdf,
141,1990,Discourse,Accent and Discourse Context: Assigning Pitch Accent in Synthetic Speech,Julia Hirschberg,"Identifying the regularities underlying speaker decisions to emphasize or de-emphasize an item intonationally has long been the subject of speculation and controversy. This paper describes a study of accent assignment based upon the analysis of natural recorded (read) speech. Results are being incorporated in NewSpeak, an interface to the Bell Laboratories Text-to-Speech System, which varies intonational features based upon syntactic structure and higher-level discourse information inferred from unrestricted text, in order to generate more natural synthetic speech. Implications of the work for the evaluation of discourse models, for automatic labeling of prosodic features, and for speech synthesis are discussed.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-142.pdf,
142,1990,Discourse,"Structure of Perspectivity: A Case of Japanese Reflexive Pronoun ""zibun""",Yasuhiro Katagiri,"A theory of perspectivity is proposed to establish the foundation of the theory of situated agents. An account is then given, based on the theory of perspectivity, of the use of a seemingly perspectivity related expression, Japanese long-range reflexive ""zibun. "" The theory we proposed for perspectival mental states incorporates two independent notions, indexicality and world view. The first captures the situatedness of agents within physical environments, and the second captures the mode of reasoning adopted by agents in interacting with other agents. The relationship between these two notions were also discussed. Based on the proposed theory of perspectivity, we argued that, contrary to wide-spread beliefs, the usage of ""zibun"" is not directly related to perspectivity. We gave an alternative explication for the interaction of the usage of ""zibun"" with perspectivity sensitive expressions and the indexical pronoun ""watashi (I),"" in terms of the coreference rule for ""zibun,"" the constraint on the two components of perspectivity, and the agent awareness default principle for the world view.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-143.pdf,
143,1990,Discourse,PRAGMA - A Flexible Bidirectional Dialogue System,John M. Levine,"This paper gives an overview of a natural language dialogue system called PRAGMA. This system contains a number of novel and important features, as well as integrating previous work into a unified mechanism. The most important advance that PRAGMA represents compared with previous systems is the high degree of bidirectional@ employed in its design. A single grammar is used for interpretation and generation, and the same knowledge sources are used for plan recognition and response generation. The system is also flexible, in that it generates useful extended responses, not only to queries which allow the user’s plan to be inferred, but also to queries which do not allow this.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-144.pdf,
144,1990,Discourse,Logical Task Modelling for Man-Machine Dialogue,M.D. Sadek,"To design a task-independent dialogue system, we present a task-oriented dialogue analysis in terms of finding the referents of definite descriptions and we show how this analysis leads to a goal-oriented inferential representation of the task. This representation provides a logical generic model of the task, which is compatible with a belief system. Then, we show how this task model, jointly used with the domain-specific user model for which we propose a formalization, enables a dialogue system to plan request negotiation dialogues.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-145.pdf,
145,1990,Interpretation,Integrating Natural Language Processing and Knowledge Based Processing,"Rebecca Passonneau, Carl Weir, Tim Finin, Martha Palmer","A central problem in text-understanding research is the indeterminacy of natural language. Two related issues that arise in confronting this problem are the need to make complex interactions possible among the system components that search for cues, and the need to control the amount of reasoning that is done once cues have been discovered. We identify a key difficulty in enabling true interaction among system components and we propose an architectural framework that minimizes this difficulty. A concrete example of a reasoning task encountered iu an actual text-understanding application is used to motivate the design principles of our framework.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-146.pdf,
146,1990,Interpretation,Parsing a Natural Language Using Mutual Information Statistics,David M. Magerman and Mitchell P. Marcus,"The purpose of this paper is to characterize a constituent boundary parsing algorithm, using an information-theoretic measure called generalized mutual information, which serves as an alternative to traditional grammar-based parsing methods. This method is based on the hypothesis that constituent boundaries can be extracted from a given sentence (or word sequence) by analyzing the mutual information values of the part-of-speech n-grams within the sentence. This hypothesis is supported by the performance of an implementation of this parsing algorithm which determines a recursive unlabeled bracketing of unrestricted English text with a relatively low error rate. This paper derives the generalized mutual information statistic, describes the parsing algorithm, and presents results and sample output from the parser.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-147.pdf,
147,1990,Interpretation,Towards Incremental Disambiguation with a Generalized Discrimination Network,"Manabu Okumura, Hozumi Tanaka","Semantic disambiguation is a difficult problem in natural language analysis. A better strategy for semantic disambiguation is to accumulate constraints obtained during the analytical process of a sentence, and disambiguate as early as possible the meaning incrementally using the constraints. We propose such a computational model of natural language analysis, and call it the ""incremental disambiguation model."" The semantic disambiguation process can be equated with the downward traversal of a discrimination network. However, the discrimination network has a problem in that it cannot be traversed unless constraints are entered in an a priori-fixed order. In general, the order in which constraints are obtained cannot be a priori fixed, so it is not always possible to traverse the network downward during the analytical process. In this paper, we propose a method which can traverse the discrimination network according to the order in which constraints a.re obtained incrementally during the analytical process. This order is independent of the a priori-fixed order of the network.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-148.pdf,
148,1990,Interpretation,Truly Parallel Understanding of Text,Yeong-Ho Yu and Robert F. Simmons,"Understanding a text requires two basic tasks: making inferences at several levels of knowledge and composing a global interpretation of the given text from those various types of inferences. Since making inferences at each level demands an extensive computations, there have been several attempts to use parallel inference mechanisms such as parallel marker passing (PMP) to increase the productivity of the inference mechanism. Such a mechanism, when used with many local processors, is capable of making inferences in parallel. However, it often poses a large burden on the task of composing the global interpretation by producing a number of meaningless inferences which should be filtered out. Therefore, the increased productivity of the inference mechanism causes the slow down of the task of forming the global interpretation and makes it the bottleneck of the whole system. Our system, TRUE, effectively solves this problem with the Constrained Marker Passing mechanism. The new mechanism not only allows the system to make necessary inferences in parallel, but also provides a way to compose the global interpretation in parallel. Therefore, the system is truly parallel, and does not suffer from any single bottleneck.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-149.pdf,
149,1990,Robotics,A Hierarchical Planner that Generates its Own Hierarchies,Jens Christensen,"PABLO is a nonlinear planner that reasons hierarchically by generating abstract predicates. PABLO’s abstract search spaces are generated automatically using predicate relaxation, a new technique for defining hierarchies of abstract predicates. For some domains, this mechanism generates hierarchies that are more useful than those created by previous techniques. Using abstractions can lead to substantial savings in computation time. Furthermore, PABLO can achieve a limited form of reactivity when reasoning with relaxed predicates. These abstractions can be viewed as small reactive plans, and our method as an approach to dynamically combining these into useful nonlinear plans.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-150.pdf,
150,1990,Robotics,Coping with Uncertainty in a Control System for Navigation and Exploration,"Thomas Dean, Kenneth Basye, Robert Chekaluk, Seungseok Hyun, Moises Lejter, Margaret Randazza","A significant problem in designing mobile robot control systems involves coping with the uncertainty that arises in moving about in an unknown or partially unknown environment and relying on noisy or ambiguous sensor data to acquire knowledge about that environment. We describe a control system that chooses what activity to engage in next on the basis of expectations about how the information returned as a result of a given activity will improve its knowledge about the spatial layout of its environment. Certain of the higher-level components of the control system are specified in terms of probabilistic decision models whose output is used to mediate the behavior of lower-level control components responsible for movement and sensing. The control system is capable of directing the behavior of the robot in the exploration and mapping of its environment, while attending to the real-time requirements of navigation and obstacle avoidance.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-151.pdf,
151,1990,Robotics,Learning General Completable Reactive Plans,Melinda T. Gervasio,"This paper presents an explanation-based learning strategy for learning general plans for use in an integrated approach to planning. The integrated approach augments a classical planner with the ability to defer achievable goals, thus preserving the construction of provably-correct plans while gaining the ability to utilize runtime information in planning. Proving achievability is shown to be possible without having to determine the actions to achieve the associated goals. A learning strategy called contingent explanation-based learning uses conjectured variables to represent the eventual values of plan parameters with unknown values a priori, and completers to determine these values during execution. An implemented system demonstrates the use of contingent EBL in learning a general completable reactive plan for spaceship acceleration.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-152.pdf,
152,1990,Robotics,"Integrating Execution, Planning, and Learning in Soar for External Environments","John E. Laird, Paul S. Rosenbloom","Three key components of an autonomous intelligent system are planning, execution, and learning. This paper describes how the Soar architecture supports planning, execution, and learning in unpredictable and dynamic environments. The tight integration of these components provides reactive execution, hierarchical execution, interruption, on demand planning, and the conversion of deliberate planning to reaction. These capabilities are demonstrated on two robotic systems controlled by Soar, one using a Puma robot arm and an overhead camera, the second using a small mobile robot with an arm.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-153.pdf,
153,1990,Robotics,Indexical Knowledge in Robot Plans,"Yves Lesphance, Hector J. Levesque","Robots act upon and perceive the world from a particular perspective. It is important to recognize this relativity to perspective if one is not to be overly demanding in specifying what they need to know in order to be able to achieve goals through action. In this paper, we show how a formal theory of knowledge and action proposed in (Lesperance 1989) can be used to formalize several kinds of situations drawn from a robotics domain, where indexical knowledge is involved. Several examples treated deal with the fact that ability to act upon an object does not require de re knowledge of the object or its absolute position; knowledge of its relative position is sufficient. It is shown how the fact that perception yields indexical knowledge can be captured. We also point out the value of being able to relate indexical knowledge and objective knowledge within the same formalism through an example involving the use of a map for navigation. Finally, we discuss a problem raised by some higher-level parametrized actions and propose a solution.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-154.pdf,
154,1990,Robotics,Symmetry Constraint Inference in Assembly Planning - Automatic Assembly Configuration Specification,"Yanxi Liu, Robin J. Popplestone","In this paper we shall discuss how to treat the automatic generation of assembly task specifications as a constraint satisfaction problem (CSP) over finite and infinite domains. Conceptually it is straightforward to formulate assembly planning in terms of CSP, however the choice of constraint representation and of the order in which the constraints are applied is nontrivial if a computationally tractable system design is to be achieved. This work investigates a subtle interaction between a pair of interleaving constraints, namely the kinematic and the spatial occupancy constraints. While finding one consistent solution to a general CSP is NP-complete, our work shows how to reduce the combinatorics in problems arising in assembly using the symmetries of assembly components. Group theory, being the standard mathematical theory of symmetry, is used extensively in this work since both robots and assembly components are three-dimensional rigid bodies whose features have certain symmetries. This forms part of our high-level robot assembly task planner in which geometric solid modelling, group theory and CSP are combined into one computationally effective framework.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-155.pdf,
155,1990,Robotics,LOGnets: A Hybrid Graph Spatial Representation for Robot Navigation,"Peter K. Malkin, Sanjaya Addanki","In this article we present a novel, hybrid graph spatial representation for robot navigation. This representation enables our mobile robot to build a model of its surroundings which it can then use for navigation. The models or maps that use this representation are hybrid graphs, the nodes being analogical local maps of landmark locations in the robot’s environment, the arcs being the actions the robot executes to travel between the locations. This representation yields a reliable navigation tool, one which ensures that the robot can re-orient itself to recover from errors in path execution and encounters with unexpected obstacles. The LOGnet approach also meshes with human’s natural approach of mapping with landmarks, instead of using angular and translational data.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-156.pdf,
156,1990,Robotics,Becoming Increasingly Reactive,Tom M. Mitchell,"We describe a robot control architecture which combines a stimulus-response subsystem for rapid reaction, with a search-based planner for handling unanticipated situations. The robot agent continually chooses which action it is to perform, using the stimulus-response subsystem when possible, and falling back on the planning subsystem when necessary. Whenever it is forced to plan, it applies an explanation-based learning mechanism to formulate a new stimulus-response rule to cover this new situation and others similar to it. With experience, the agent becomes increasingly reactive as its learning component acquires new stimulus-response rules that eliminate the need for planning in similar subsequent situations. This Theo-Agent architecture is described, and results are presented demonstrating its ability to reduce routine reaction time for a simple mobile robot from minutes to under a second.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-157.pdf,
157,1990,Vision,Constraints for the Early Detection of Discontinuity from Motion,Michael J. Black and P. Anandan,"Surface discontinuities are detected in a sequence of images by exploiting physical constraints at early stages in the processing of visual motion. To achieve accurate early discontinuity detection we exploit five physical constraints on the presence of discontinuities: i) the shape of the sum of squared differences (SSD) error surface in the presence of surface discontinuities; ii) the change in the shape of the SSD surface due to relative surface motion; iii) distribution of optic flow in a neighborhood of a discontinuity; iv) spatial consistency of discontinuities; v) temporal consistency of discontinuities. The constraints are described, and experimental results on sequences of real and synthetic images are presented. The work has applications in the recovery of environmental structure from motion and in the generation of dense optic flow fields.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-158.pdf,
158,1990,Vision,Generalized Shape Autocorrelation,"Andrea Califano, Rakesh Mohan","This paper presents an efficient and homogeneous paradigm for automatic acquisition and recognition of nonparametric shapes. Acquisition time varies from linear to cubic in the number of object features. Recognition time is linear to cubic in the number of features in the image and grows slowly with the number of stored models. Nonparametric shape representation is achieved by spatial autocorrelation transforms. Both acquisition and recognition are two-step processes. In the first phase, spatial autocorrelation operators are applied to the image data to perform local shape analysis. Then, spatial autocorrelation operators are applied to the local shape descriptors to either create entries (acquisition) or index (recognition) into a table containing the distributed shape information. The output of the table is used to generate a density function on the space of possible shapes with peaks corresponding to high confidence in the presence of a particular shape instance. The behavior of the system on a set of complex shapes is shown with respect to occlusion, geometric transformation, and cluttered scenes.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-159.pdf,
159,1990,Vision,Computing Exact Aspect Graphs of Curved Objects: Parametric Surfaces,"Jean Ponce, David J. Kriegman","This paper introduces a new approach for computing the exact aspect graph of curved objects observed under orthographic projection. Curves corresponding to various visual events partition the Gaussian sphere into regions where the image structure is stable. A catalogue of these events for piecewise-smooth objects is available from singularity theory. For a solid bounded by rational parametric patches and their intersection curves, it is shown that each visual event is characterized by a system of n polynomials in n + 1 variables whose solutions can be found by numerical curve tracing methods. Combining these methods with ray tracing, it is possible to characterize the stable image structure within each region. Results from a preliminary implementation are presented.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-160.pdf,
160,1990,"InvitedTalks & Panels",The Future of Knowledge Representation (Extended Abstract),Ronald J. Brachman,"Knowledge representation (KR) has traditionally been thought of as the heart of artificial intelligence. Anyone who has ever built an expert system, a natural language system-almost any AI system at all-has had to tackle the problem of representing its knowledge of the world. Despite it ubiquity, for most of AI’s history KR has been a backstage activity. But in the 1980’s it emerged as a field unto itself, with its own burgeoning literature. Along with this growth, the last decade has seen major changes in KR methodology, important technical contributions, and challenges to the basic assumptions of the field. I survey some of these developments, and then speculate about some of the equally interesting changes that appear on the horizon. I also look at some of the critical problems facing KR research in the near future, both technical and sociological.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-161.pdf,
161,1990,"InvitedTalks & Panels",Rationality and its Roles in Reasoning (Extended Abstract),Jon Doyle,"The economic theory of rationality promises to equal mathematical logic in its importance for the mechanization of reasoning. We survey the growing literature on how the basic notions of probability, utility, and rational choice, coupled with practical limitations on information and resources, influence the design and analysis of reasoning and representation systems.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-162.pdf,
162,1990,"InvitedTalks & Panels",Probably Approximately Correct Learning,David Haussler,"This paper surveys some recent theoretical results on the efficiency of machine learning algorithms. The main tool described is the notion of Probably Approximately Correct (PAC) 1 earning, introduced by Valiant. We define this learning model and then look at sorne of the results obtained in it. We then consider some criticisms of the PAC model and the extensions proposed to address these criticisms. Finally, we look briefly at other models recently proposed in computational learning theory.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-163.pdf,
163,1990,"InvitedTalks & Panels",Truth Maintenance,David McAllester,General purpose truth maintenance systems have received considerable attention in the past few years. This paper discusses the functionality of truth maintenance systems and compares various existing algorithms. Applications and directions for future research are also discussed.,https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-164.pdf,
164,1990,"InvitedTalks & Panels",Massively Parallel AI,David L. Waltz,"Most AI researchers would, I believe, agree that truly intelligent machines (i.e. machines on a par with humans) will require at least four orders of magnitude more power and memory than are available on any machine today [Schwartz 1988, Waltz 1988]. There is now widespread agreement in the supercomputing community that by the year 2000 all supercomputers (defined as the most powerful machines available at a given time) will be massively parallel [Fox 1990]. Yet relatively little thought has been given in AI as to how to utilize such machines. With few exceptions, AI’s attention has been limited to workstations, minicomputers and PCs. Today’s massively parallel machines present AI with a golden opportunity to make an impact, especially in the world of commercial applications. The most striking near-term opportunity is in the marriage of research on very large databases with case-based and memory-based AI. Moreover, such applications are steps on a path that can lead eventually to a class of truly intelligent systems.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-165.pdf,
165,1990,"InvitedTalks & Panels",AI and Software Engineering Will the Twain Ever Meet?,"Robert Balzer, Richard Fikes, Mark Fox, John McDermott, Elliot Soloway","This session will explore the reasons for the lack of impact in four important areas in which AI has been expected to significantly affect real world Software Engineering. The panelists, each representing one of these areas, will respond to the conjecture that these failures rest upon a common cause - reliance on isolationist technology and approaches, rather than upon creating additive technology and approaches that can be integrated with other existing capabilities.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-166.pdf,
166,1990,"InvitedTalks & Panels",AI and Software Engineering -- Managing Exploratory Programming,Richard Fikes,"The AI community at large has been a major contributor to software engineering by playing a pioneering role in the development of symbolic, interactive, and exploratory programming. The building of knowledge-based application systems has typically required all three of these forms of programming and has recently been particularly responsible for motivating the development of techniques for effectively managing exploratory programming activities. (See, for example, (Walters and Nielsen 1988), (Schoen and Sykes 1988), and (Fikes and Jacobstein 1989).) Since exploratory programming can be a useful methodology in a broad range of both AI and non-AI system building projects (Sheil 1983), I consider techniques for managing its use to be a significant contribution to software engineering. It is those techniques that I wish to address briefly in this note.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-167.pdf,
167,1990,"InvitedTalks & Panels",Looking for the AI in Software Engineering An Applications Perspective,Mark S. Fox,"I have been asked to address the reasons why Artificial Intelligence has not had a significant affect on real world Software Engineering, from the perspective of the development of smart application components. In the following I will explore twhat it means to be a ""smart application component"", then explore why they have not had the impact Robert Balzer thinks they should deserve.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-168.pdf,
168,1990,"InvitedTalks & Panels",Developing Software is like Talking to Eskimos about Snow,John McDermott,"It’s not clear to me exactly what people in AI do, but one thing that is talked about a lot is how important knowledge is. One of the things knowledge is supposed to be good for is solving hard problems. The idea, as I understand it, is that one characteristic of large set of problems is that their solutions are radically contingent on the peculiarities of the various situations in which the problems are instantiated. To anyone who is ignorant of most, or even many, of the peculiarities, those problems appear hard. It is only to an agent who has knowledge of almost all of the relevant peculiarities that the problems appear straightforward.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-169.pdf,
169,1990,"InvitedTalks & Panels",The Techies vs. the Non-techies: Today’s Two Cultures,Elliot Soloway,"During the postwar heyday of physics, C.P. Snow, wrote a short article entitled ""The Two Cultures"" (1954). There he pointed out the growing division between the ""science culture"" and the ""non-science (literary) culture."" He observed that scientists basically had no understanding of -- nay, even any concern for -- literary culture, and vice versa. He pointed out the profound loss to society that was resulting from this dichotomy. Namely, creativity often arises in the interchange of ideas. Sadly, the two cultures were so polarized, even then, that Snow felt that little real dialogue took place between members of the two cultures.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-170.pdf,
170,1990,"InvitedTalks & Panels",Panel: User Modeling and User Interfaces,Susan T. Dumais,"I will consider the problem of helping user’s retrieve information from large diverse collections of textual materials such as bibliographic databases, electronic bulletin boards and online manuals, although I believe many of my arguments apply to other interactive environments as well. Information retrieval systems this must handle a heterogeneous population of users, even a single user will have a range of information needs including very specific requests as well as more general and often ill-defined topical searches.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-171.pdf,
171,1990,"InvitedTalks & Panels",User Modeling and User Interfaces,Kathleen R. McKeown,"In the past several years, it has become increasingly apparent that there is strong disagreement between researchers within and outside of AI on how to build systems for man-machine communication. Within AI, and specifically within natural language, researchers have noted that human speakers and hearers draw on their knowledge about each other when communicating. This knowledge is used both in understanding, and responding to, a speaker’s utterances. User models are a means for representing various types of information about speakers and hearers so that systems are able to reason about their users when interpreting input and producing responses. While there is disagreement in the AI community about the form of user model that should be used, there is an implicit assumption that some form of knowledge about users is essential for successful man-machine communication.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-172.pdf,
172,1990,"InvitedTalks & Panels","User Models and User Interfaces: A Case for Domain Models, Task Models, and Tailorability",James D. Hollan,"My thesis is that the case for user models has little empirical support, necessarily must confront currently intractable problems, and is motivated by aspects of metaphor that does not provide, at least presently, an effective base for the design of user interfaces. More positively, I argue that some of what is sought in the name of user modeling can be accomplished by basing interface design on models of application domains, user tasks, and by providing tailorable systems.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-173.pdf,
173,1990,"InvitedTalks & Panels",What' s in a User?,Karen Sparck Jones,"I take user modelling to imply reasoning, explicitly or quasi-explicitly, about the user. Thus while systems or interfaces usually embed assumptions about their users (eg they know English or find some icon mnemonic), and defacto exploit these assumptions, this is not real user modelling.",https://aaai.org/Library/AAAI/1990/../../../Papers/AAAI/1990/AAAI90-174.pdf,
