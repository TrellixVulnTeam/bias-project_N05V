,conference_year,category,title,author,abstract,download_url,keywords
0,1987,AI Architectures,Recent Developments in Butterfly(TM) Lisp,"Donald C. Allen, Seth A. Steinberg, and Lawrence A. Stabile","This paper describes recent enhancements to the Common Lisp system that BBN is developing for its Butterfly multiprocessor. The BBN Butterfly is a shared memory multiprocessor that contains up to 256 processor nodes. The system provides a shared heap, parallel garbage collector, and window based I/Q system. The ""future"" construct is used to specify parallelism.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-001.pdf,
1,1987,AI Architectures,CCLISPTM on the iPSC(TM) Concurrent Computer,"David Billstrom, Joseph Brandenburg, John Teeter","Concurrent Common LISPTM (CCLISP) is the LISP environment for the iPSCm system, the Intel Personal SuperComputer. CCLISP adds message-passing communication and other constructs to the Common LISP environment on each processor node. The iPSC system is configured with Intel 80286 processor nodes, in systems ranging from 8 to 128 nodes. Performance on a per node basis roughly equivalent to AI workstation LISP performance is discussed, as are the implementation details of the CCLISP language constructs.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-002.pdf,
2,1987,AI Architectures,A Parallel Resolution Procedure Based on Connection Graph,"P. Daniel Cheng, J. Y. Juang","In this paper, we present a new approach towards a parallel resolution procedure which explores another dimension of parallelism in addition to the AND/OR formulation and special hardware constructs. The approach organizes the input clauses of a problem domain into a connection graph. The connection graph is then partitioned and each partition is worked on by a different processor of a multiprocessor system. These processors execute the resolution procedure independently on its partition, and exchange intermediate results via clause migrations. Preliminary test results and qualitative assessments of this procedure are also given.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-003.pdf,
3,1987,AI Architectures,"Achieving Flexibility, Efficiency, and Generality in Blackboard Architectures","Daniel D. Corkill, Kevin Q. Gallagher, Philip M. Johnson","Achieving flexibility and efficiency in blackboard-based AI applications are often conflicting goals. Flexibility, the ability to easily change the blackboard representation and retrieval machinery, can be achieved by using a general purpose blackboard database implementation, at the cost of efficient performance for a particular application. Conversely, a customized blackboard database implementation, while efficient, leads to strong interdependencies between the application code (knowledge sources) and the blackboard database implementation. Both flexibility and efficiency can be achieved by maintaining a sufficient level of data abstraction between the application code and the blackboard implementation. The abstraction techniques we present are a crucial aspect of the generic blackboard development system GBB. Applied in concert, these techniques simultaneously provide flexibility, efficiency, and sufficient generality to make GBB an appropriate blackboard development tool for a wide range of applications.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-004.pdf,
4,1987,AI Architectures,Forward Chaining Logic Programming with the ATMS,"Nicholas S. Flann, Thomas G. Dietterich, Dan R. Corpron","Two powerful reasoning tools have recently appeared, logic programming and assumption-based truth maintenance systems (ATMS). An ATMS offers significant advantages to a problem solver: assumptions are easily managed and the search for solutions can be carried out in the most general context first and in any order. Logic programming allows us to program a problem solver declaratively-describe what the problem is, rather than describe how to solve the problem. However, we are currently limited when using an ATMS with our problem solvers, because we are forced to describe the problem in terms of a simple language of forward implications. In this paper we present a logic programming language, called FORLOG, that raises the level of programming the ATMS to that of a powerful logic programming language. FORLOG supports the use of ""logical variables"" and both forward and backward reasoning. FORLOG programs are compiled into a data-flow language (similar to the RETE network) that efficiently implements deKleer’s consumer architecture. FORLOG has been implemented in Interlisp-D.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-005.pdf,
5,1987,AI Architectures,Integrating Diverse Reasoning Methods in the BB1 Blackboard Control Architecture,"M. Vaughan Johnson, Jr., Barbara Hayes-Roth",The BB1 blackboard control architecture has been proposed to enable systems to integrate diverse reasoning methods to control their own actions. Previous work has shown BB1’s ability to integrate hierarchical planning and opportunistic focusing. We show how it can integrate goal-directed reasoning as well and demonstrate these capabilities in the PROTEAN system. We also compare BB1 with alternative control architectures.,https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-006.pdf,
6,1987,AI Architectures,A Multiprocessor Architecture for Production System Matching,"Michael A. Kelly, Rudolph E. Seviora","This paper presents a new, highly parallel algorithm for OPS5 production system matching, and a multiprocessor architecture to support it. The algorithm is based on a partitioning of the Rete algorithm at the comparison level, suitable for execution on an array of several hundred processing elements. The architecture provides an execution environment which optimizes the algorithm’s performance. Analysis of existing production systems and results of simulations indicate that an increase in match speed of two orders of magnitude or more over current implementations is possible.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-007.pdf,
7,1987,AI Architectures,TREAT: A Better Match Algorithm for AI Production Systems,Daniel P. Miranker,"This paper presents the TREAT match algorithm for AI production systems. The TREAT algorithm introduces a new method of state saving in production system interpreters called conflict-set support. Also presented are the results of an empirical study comparing the performance of the TREAT match with the commonly assumed best algorithm for this problem, the RETE match. On five different OPS5 production system programs TREAT outperformed RETE, often by more than fifty percent. This supports an unsubstantiated conjecture made by McDermott, Newell and Moore, that the state saving mechanism employed in the RETE match, condition-element support, may not be worthwhile.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-008.pdf,
8,1987,AI Architectures,"Joshua: Uniform Access to Heterogeneous Knowledge Structures, or, Why Joshing Is Better than Conniving or Planning","Steve Rowley, Howard Shrobe, Robert Cassels, Walter Hamscher","This paper presents Joshua, a system which provides syntactically uniform access to heterogeneously implemented knowledge bases. Its power comes from the observation that there is a Protocol of Inference consisting of a small set of abstract actions, each of which can be implemented in many ways. We use the object-oriented programming facilities of Flavors to control the choice of implementation. A statement is an instance of a class identified with its predicate. The steps of the protocol are implemented by methods inherited from the classes. Inheritance of protocol methods is a compile-time operation, leading to very fine-grained control with little run-time cost. Joshua has two major advantages: First, a Joshua programmer can easily change his program to use more efficient data structures without changing the rule set or other knowledge-level structures. We show how we thus sped up one application by a factor of 3. Second, it is straightforward to build an interface which incorporates an existing tool into Joshua, without modifying the tool. We show how a different TMS, implemented for another system, was thus interfaced to Joshua.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-009.pdf,
9,1987,AI Architectures,CP as a General-Purpose Constraint-Language,Vijay A. Saraswat,"In this paper we present the notion of concurrent, controllable constraint systems. We argue that purely declarative search formalisms, whether they are based on dependency-directed backtracking (as in Steele [Steele, 1980] or Bruynooghe [Bruynooghe and Pereira, 1985]) or bottom-up breadth-first (albeit incremental) definite clause theorem provers (as in deKleer’s ATM approach [deKleer, 1986]) or built-in general purpose heuristics (as in Laurier’s work [Lauriere, 1978]) are unlikely to be efficient enough to serve as the basis of a general purpose programming formalism which supports the notion of constraint-based computation. To that end we propose the programming language CP[!,1,&], based on the concurrent interpretation of definite clauses, which allows the user to express domain-specific heuristics and control the forward search process based on eager propogation of constraints and early detection of determinacy and contradiction. This control follows naturally from the alternate metaphor of viewing constraints as processes that communicate by exchanging messages. The language, in addition, allows for the dynamic generation and hierarchical specification of constraints, for concurrent exploration of alternate solutions, for pruning and merging sub-spaces and for expressing preferences over which portions of the search space to explore next.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-010.pdf,
10,1987,AI Architectures,Non-Deterministic Lisp with Dependency-directed Backtracking,"Ramin Zabih, David McAllester, David Chapman",Extending functional Lisp with McCarthy’s non-deterministic operator AMB yields a language which can concisely express search problems. Dependency-directed backtracking is a powerful search strategy. We describe a non-deterministic Lisp dialect called SCHEMER and show that it can provide automatic dependency-directed backtracking. The resulting language provides a convenient interface to this efficient backtracking strategy.,https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-011.pdf,
11,1987,"AIand Education",From Intelligent Tutoring to Computerized Psychotherapy,David Servan-Schreiber,"Building on the successes and shortcomings of previous experiences with computerized psychotherapy, we have attempted to extend the paradigm of intelligent tutoring systems to the domain of therapeutic interaction. Based on canonical examples, I present three dimensions of the task of tutoring systems: teaching problem-solving vs. domain knowledge; teaching isolated domains vs domains where students have prior misconceptions; teaching with the use of functional models of the domain vs no functional models. I then show how implications of these dimensions have helped us determine the specifications of a tutoring system for sexual therapy. Our approach has consisted of engaging patients in a tutoring dialogue driven by the identification of problem areas and their associated misconceptions. A diagnostic module, implemented as a traditional expert system, uses an extensive bug library to derive an internal model of patients. A dialogue driver relies on a hierarchy of dialogue plans and demons in order to preserve a logical grouping of related topics while remaining flexible to adapt itself, at each level of the dialogue hierarchy, to the unfolding case.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-012.pdf,
12,1987,"AIand Education",An Intelligent Tutoring System for Interpreting Ground Tracks,"Kathleen Swigger, Hugh Burns, Harry Loveland, Terresa Jackson","This paper describes an intelligent tutoring system for the space domain. The system was developed on a Xerox 1108 using LOOPS and provides an environment for discovering principles of ground tracks as a direct function of the orbital elements. The system was designed to teach students how to ""deduce"" a satellite’s orbital elements by looking at a graphic display of a satellite’s ground track. The system also teaches students how to use more systematic behaviors to explore this domain. Since the system is equipped with a number of online tools that were specially designed to help students better understand facts, principles and relationships, the student is free to investigate different options and learn at his own pace.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-013.pdf,
13,1987,"AIand Education",Plan Inference and Student Modeling in ICAI,"Y. M. Visetti, P. Dague","This paper adresses the problem of building user models within the framework of Computer Assisted Instruction (ICAI), and more particularly for systems teaching elementary arithmetic or algebra. By ""model building"" we mean the understanding of the student’s performances, as well as a global description and evaluation of his/her ability (competence), including a representation of some errors. As an application domain we have here retained the learning of ""calculus"" in the field of rational numbers, as an intermediate area between arithmetic and algebra. The aim of our system is to control the way in which the pupil solves exercises. In the light of the particular nature of the chosen application, the main points to be stressed are the following : - calculations are described as plan generation and execution ; 	consequently the student’s modelling consists primarily in plan inferencing - the system takes into account the non deterministic nature of the task, and recognizes valid variants of expert calculation plans - numerous errors are detected and categorized - the system accepts that the student write the calculations in a more or less elliptic manner ; whenever ambiguities occur, the student is precisely asked about implicit steps of his calculations, and the system uses the answers given to reduce the uncertainties - a global model of the student is generated, which incorporates observations and appreciations ; this model, in turn, determines the subsequent interpretations. All these questions are discussed both at the fundamental and the methodological levels.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-014.pdf,
14,1987,"AIand Education",Building a Community Memory for Intelligent Tutoring Systems,"Beverly Woolf, Pat Cunningham",This article discusses the need for multiple experts to work together to develop knowledge representation systems for intelligent tutors. Three case studies are examined in which the need for a pragmatic approach to the problem of knowledge acquisition has become apparent. Example methodologies for building tools for the knowledge acquisition phase are described including specific tasks and criteria that might be used to transfer expertise from several experts to an intelligent tutoring system.,https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-015.pdf,
15,1987,"AutomatedReasoning",A Model of Two-Player Evaluation Functions,"Bruce Abramson, Richard E. Korf","We present a model of heuristic evaluation functions for two-player games. The basis of the proposal is that an estimate of the expected-outcome of a game situation, assuming random play from that point on, is an effective heuristic function. The model is supported by three distinct sets of experiments. The first set, run on small, exhaustively searched game-trees, shows that the quality of decisions made on the basis of exact values for the expected-outcome is quite good. The second set shows that in large games, estimates of the expected-outcome derived by randomly sampling terminal positions produce reasonable play. Finally, the third set shows that the model can be used to automatically learn efficient and effective evaluation functions in a game-independent manner.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-016.pdf,
16,1987,"AutomatedReasoning",Proof Analogy in Interactive Theorem Proving: A Method to Express and Use It Via Second Order Pattern Matching,"Thierry Boy de la Tour, Ricardo Caferra","A method is presented to express and use syntactic analogies between proofs in interactive theorem proving and proof checking. Up to now, very few papers have addressed instances of this problem. The paradigm of ""proposition as types"" is adopted and proofs are represented as terms. The proposed method is to transform a known proof of a theorem into what might become a proof of an ""analogous"" -according to the user-proposition, namely the one to be proved. This transformation is expressed by means of second order pattern matching (this may be seen as a generalisation of rewriting rules), thus allowing the use of variable function symbols. For the moment, it is up to the user to discover the transformation rule, and the paper deals only with the problem of managing it. We explain the proposed analogy treatment with a fully developed running example.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-017.pdf,
17,1987,"AutomatedReasoning",Comparing Minimax and Product in a Variety of Games,"Ping-Ching Chi, Dana S. Nau","This paper describes comparisons of the minimax back-up rule and the product back-up rule on a wide variety of games, including P-games, G-games, three-hole kalah, Othello, and Ballard’s incremental game. In three-hole kalah, the product rule plays better than a minimax search to the same depth. This is a remarkable result, since it is the first widely known game in which product has been found to yield better play than minimax. Furthermore, the relative performance of minimax and product is related to a parameter called the rate of heuristic flaw (rhf). Thus, rhf has potential use in predicting when to use a back-up rule other than minimax.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-018.pdf,
18,1987,"AutomatedReasoning",Removing Redundancies in Constraint Networks,"Avi Dechter, Rina Dechter","The removal of inconsistencies from the problem’s representation, which has been emphasized as a means of improving the performance of backhacking algorithms in solving constraint satisfaction problems, increases the amount of redundancy in the problem. In this paper we argue that some solution methods might actually benefit from using an opposing strategy, namely, the removal of redundancies from the representation. We present various ways in which redundancies may be identified. In particular, we show how the path-consistency method, developed for removing inconsistencies can be reversed for the purpose of identifying redundancies, and discuss the ways in which redundancy removal can be beneficial in solving constraint satisfaction problems.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-019.pdf,
19,1987,"AutomatedReasoning",Computational Costs versus Benefits of Control Reasoning,"Alan Garvey, Craig Cornelius, Barbara Hayes-Roth","We assess the computational costs and benefits of control reasoning in the PROTEAN system, which is built in the BB environment. We experimentally manipulate PROTEAN’s control knowledge and analyze differences in total problem-solving time as a function of several component times. Our results demonstrate and explain the utility of control reasoning. They also illustrate the importance of experimental investigation and utility of the BB environment for conducting such investigations.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-020.pdf,
20,1987,"AutomatedReasoning",Filming a Terrain under Uncertainty Using Temporal and Probabilistic Reasoning,Raymond D. Gumb,"We address the problem of interpreting sensor data under uncertainty, using temporal and spatial context to facilitate the identification of objects. We seek to identify the type of an object presented in an ambiguous image by reasoning about conditional probabilities and the possible movements objects can make. A conditional probability (that an object is of a certain type given that some of its properties have been recognized) is used in conflict resolution, and an object is assigned an alternative type when an impossible movement is detected. Think of a map as being a frame and a sequence of frames as being a film. The idea is to construct a consistent and plausible (coherent and highly probable) film in which an object of one type does not mysteriously change into an object of another type.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-021.pdf,
21,1987,"AutomatedReasoning",On the Expressiveness of Rule-based Systems for Reasoning with Uncertainty,"David E. Heckerman, Eric J. Horvitz","We demonstrate that classes of dependencies among beliefs held with uncertainty cannot be represented in rule-based systems in a natural or efficient manner. We trace these limitations to a fundamental difference between certain and uncertain reasoning. In particular, we show that beliefs held with certainty are more modular than uncertain beliefs. We argue that the limitations of the rule-based approach for expressing dependencies are a consequence of forcing non-modular knowledge into a representation scheme originally designed to represent modular beliefs. Finally, we describe a representation technique that is related to the rule-based framework yet is not limited in the types of dependencies that it can represent.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-022.pdf,
22,1987,"AutomatedReasoning",Inferring Formal Software Specifications from Episodic Descriptions,"Van E. Kelly, Uwe Nonnenmann","The WATSON automatic programming system computes formal behavior specifications for process-control software from informal ""scenarios"": traces of typical system operation. It first generalizes scenarios into stimulus-response rules, then modifies and augments these rules to repair inconsistency and incompleteness. It finally produces a formal specification for the class of computations which implement that scenario and which are also compatible with a set of ""domain axioms"". A particular automaton from that class is constructed as an executable prototype for the specification. WATSON’s inference engine combines theorem proving in a very weak temporal logic with faster and stronger, but approximate, model-based reasoning. The use of models and of closed-world reasoning over ""snapshots"" of an evolving knowledge base leads to an interesting special case of non-monotonic reasoning.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-023.pdf,
23,1987,"AutomatedReasoning",Real-Time Heuristic Search: First Results,Richard E. Korf,"Existing heuristic search algorithms are not applicable to real-time applications because they cannot commit to a move before an entire solution is found. We present a special case of minimax lookahead search to handle this problem, and an analog of alpha-beta pruning that significantly improves the efficiency of the algorithm. In addition, we present a new algorithm, called Real-Time-A*, for searching when actions must actually be executed, as opposed to merely simulated. Finally, we examine the nature of the tradeoff between computation and execution cost.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-024.pdf,
24,1987,"AutomatedReasoning",Reasoning in the Presence of Inconsistency,Fangzhen Lin,"In this paper, we propose a logic which is nontrivial in the presence of inconsistency. The logic is based on the resolution principle and coincides with the classical logic when premises are consistent. Some results of interesting to Automated Theorem Proving are a sound and sometimes complete three-valued semantics for the resolution rule and a refutation process which is much in the spirit of the problem reduction format.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-025.pdf,
25,1987,"AutomatedReasoning",A New Structural Induction Scheme for Proving Properties of Mutually Recursive Concepts,"Peiya Liu, Ruey-Juin Chang","Structural induction schemes have been used for mechanically proving properties of self-recursive concepts in previous research. However, based on those schemes, it becomes very difficult to automatically generate the right induction hypotheses whenever the conjectures are involved with mutually recursive concepts. This paper will show that the difficulties come mainly from the weak induction schemes provided in the past, and a strong induction scheme is needed for the mutually defined concepts. Furthermore, a generalized induction principle is provided to smoothly integrate both schemes. Thus, in this mechanical induction, hypotheses are generated by mixing strong induction schemes with weak inductions schemes. While the weak induction schemes are suggested by self- recursive concepts, the strong induction schemes are suggested by mutually recursive concepts.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-026.pdf,
26,1987,"AutomatedReasoning",Synthesizing Algorithms with Performance Constraints,Robert D. McCartney,"This paper describes MEDUSA, an experimental algorithm synthesizer. MEDUSA is characterized by its top-down approach, its use of cost-constraints, and its restricted number of synthesis methods. Given this model, we discuss heuristics used to keep this process from being unbounded search through the solution space. The results indicate that the performance criteria can be used effectively to help avoid combinatorial explosion. The system has synthesized a number of algorithms in its test domain (geometric intersection problems) without operator intervention.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-027.pdf,
27,1987,"AutomatedReasoning",The Deductive Synthesis of Imperative LISP Programs,"Zohar Manna, Richard Waldinger","A framework is described for the automatic synthesis of imperative programs, which may alter data structures and produce destructive side effects as part of their intended behavior. A program meeting a given specification is extracted from the proof of a theorem in a variant of situational logic, in which the states of a computation are explicit objects. As an example, an in-place reverse program has been derived in an imperative LISP, which in cludes assignment and destructive list operations (rplaca and rplacd) .",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-028.pdf,
28,1987,"AutomatedReasoning",Path Dissolution: A Strongly Complete Rule of Inference,"Neil V. Murray, Erik Rosenthal","We introduce path dissolution, a rule of inference that operates on formulas in negation normal form. Path dissolution is strongly complete; i.e., it has the property that, given an unsatisfiable ground formula, any sequence of dissolution steps will produce the empty graph. This is accomplished by strictly reducing (at each step) the number of c-paths in the formula. Dissolution, unlike most resolution-based inference rules, does not directly lift into first-order logic; techniques for employing dissolution at the first order level are discussed.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-029.pdf,
29,1987,"AutomatedReasoning",Revised Dependency-Directed Backtracking for Default Reasoning,"Charles J. Petrie, Jr.","Default reasoning is a useful inference technique which involves choosing a single context in which further inferences are to be made. If this choice is incorrect, the context may need to be switched. Dependency-directed backtracking provides a method for such context switching. Doyle’s algorithm for dependency-directed backtracking is revised to allow context switching to be guided by the calling inference system using domain knowledge. This new backtracking mechanism has been implemented as part of software for developing expert systems.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-030.pdf,
30,1987,"AutomatedReasoning",Efficiency Analysis of Multiple-Context TMSs in Scene Representation,Gregory M. Provan,"Multiple possible solutions can arise in many domains, such as scene interpretation and speech recognition. This paper examines the eficiency of multiple-context TMSs, such as the ATMS, in solving a scene representation problem which we call the Vision Constraint Recognition problem. The ATMS has been claimed to be quite eficient for solving problems with multiple possible solutions, even for problems with large databases. However, we present evidence that for large databases with multiple possible solutions (which we argue occur frequently in practice), such multiple-context TMSs can be very inefficient. We present a class of problems for which using a multiple-context TMS is both intrinsically interesting and ideal, but which will be computationally infeasible because of the exponential size of the database which the TMS must explore. To circumvent such infeasiblity, appropriate control must be exerted by the problem solver.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-031.pdf,
31,1987,"AutomatedReasoning",A Parallel Implementation of Iterative-Deepening-A*,"V. Nageshwara Rao, Vipin Kumar, K. Ramesh","This paper presents a parallel version of the Iterative-Deepening-A* (IDA*) algorithm. Iterative-Deepening-A* is an important admissible algorithm for state-space search which has been shown to be optimal both in time and space for a wide variety of state-space search problems. Our parallel version retatins all the nice properties of the sequential IDA* and yet does not appear to be limited in the amount of parallelism. To test its effectiveness, we have implemented this algorithm on Sequent Balance 21000 parallel processor to solve the 15-puzzle problem, and have been able to obtain almost linear speedups on the 30 processors that are available on the machine. On machines where larger number of processors are available, we expect that the speedup will still grow linearly. The parallel version seems suitable even for loosely coupled architectures such as the Hypercube.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-032.pdf,
32,1987,"AutomatedReasoning",Foundations of Assumption-based Truth Maintenance Systems: Preliminary Report,"Raymond Reiter, Johan de Kleer","In this paper we (1) define the concept of a Clause Management System (CMS) - a generalization of de Kleer’s ATMS, (2) motivate such systems in terms of efficiency of search and abductive reasoning, <and (3) characterize the computation affected by a CMS in terms of the concept of prime implicants.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-033.pdf,
33,1987,Planning,Reasoning about Exceptions during Plan Execution Monitoring,"Carol A. Broverman, W. Bruce Croft","In a cooperative problem-solving environment; such as an office, a hierarchical planner can be incorporated into an intelligent interface to accomplish tasks. During plan execution monitoring, user actions may be inconsistent with system expectations. In this paper, we present an approach towards reasoning about these exceptions in an attempt to accommodate them into an evolving plan. We propose a representation for plans and domain objects that facilitates reasoning about exceptions.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-034.pdf,
34,1987,Planning,Incremental Causal Reasoning,"Thomas Dean, Mark Boddy","Causal reasoning comprises a large portion of the inference performed by automatic planners. In this paper, we consider a class of inference systems that are said to be predictive in that they derive certain causal consequences of a base set of premises corresponding to a set of events and constraints on their occurrence. The inference system is provided with a set of rules, referred to as a causal theory, that specifies, with some limited accuracy, the cause and effect relationships between objects and processes in a given domain. As modifications are made to the base set of premises, the inference system is responsible for accounting for all and only those inferences licensed by the premises and current causal theory. Unfortunately, the general decision problem for nontrivial causal theories involving partially ordered events is NP-complete. As an alternative to a complete but potentially exponential-time inference procedure, we describe a limited-inference polynomial-time algorithm capable of dealing with partially ordered events. This algorithm generates a useful subset of those inferences that will be true in all total orders consistent with some specified partial order. The algorithm is incremental and, while it is not complete, it is provably sound.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-035.pdf,
35,1987,Planning,An Investigation into Reactive Planning in Complex Domains,R. James Firby,"A model of purely reactive planning is proposed based on the concept of reactive action packages. A reactive action package, or RAP, can be thought of as an independent entity pursuing some goal in competition with many others at execution time. The RAP processing algorithm addresses the problems of execution monitoring and replanning in uncertain domains with a single, uniform representation and control structure. Use of the RAP model as a basis for adaptive strategic planning is also discussed.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-036.pdf,
36,1987,Planning,On Stratified Autoepistemic Theories,Michael Gelfond,"In this paper we investigate some properties of ""autoepistemic logic"" approach to the formalization of common sense reasoning suggested by R. Moore in [Moore, 1985]. In particular we present a class of autoepistemic theories (called stratified autoepistemic theories) and prove that theories from this class have unique stable autoepistemic expansions and hence a clear notion of ""theoremhood"". These results are used to establish the relationship of Autoepistemic Logic with other formalizations of non-monotonic reasoning, such as negation as failure rule and circumscription. It is also shown that ""classical"" SLDNF resolution of Prolog can be used as a deductive mechanism for a rather broad class of autoepistemic theories. Key words and phrases: common sense reasoning, autoepistemic logic, negation as failure rule, non-monotonic reasoning.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-037.pdf,
37,1987,Planning,Possible Worlds and the Qualification Problem,"Matthew L. Ginsberg, David E. Smith","In this paper, we propose a solution to McCarthy’s qualification problem [10] based on the notion of possible worlds [3,6]. We begin by noting that existing formal solutions to qualification seem to us to suffer from serious epistemological and computational difficulties. We present a formalization of action based on the notion of possible worlds, and show that our solution to the qualification problem avoids the difficulties encountered by earlier ones by associating to each action a set of domain constraints that can potentially block it. We also compare the computational resources needed by our approach with those required by other formulations.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-038.pdf,
38,1987,Planning,Simple Causal Minimizations for Temporal Persistence and Projection,Brian A. Haugh,"Formalizing temporal persistence and solving the temporal projection problem within traditional non-monotonic logics is shown possible through two different approaches, neither of which requires any special minimization techniques. Minimizing potential causes is shown to yield a type of temporal persistence that is useful for the temporal projection problem, although it differs significantly from the ordinary conception of temporal persistence. A conception of determined causes is then developed whose minimization does yield the results preferred by ordinary temporal persistence. Finally, previous approaches to formalizing temporal persistence using chronological minimizations are shown inadequate for certain classes of scenarios, which causal minimizations formalize correctly.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-039.pdf,
39,1987,Planning,Using Goal Interactions to Guide Planning,Caroline Hayes,"The Machinist program extends domain dependent planning technology. It is modeled after the behavior of human machinists, and makes plans for fabricating metal parts using machine tools. Many existing plannmg programs rely on a problem solving strategy that involves fixing problems in plans only after they occur. The result is that planning time may be wasted when a bad plan is unnecessarily generated and must be thrown out or modified. The machinist program improves on these methods by looking for cues in the problem specification that may indicate potential difficulties or conflicting goal interactions, before generating any plans. It plans around those difficulties, greatly increasing the probability of producing a good plan on the first try. Planning efficiency is greatly increased when false starts can be eliminated. The machinist program contains about I80 UPS5 rules, and has been judged by experienced machinists to make plans that, are on the average, better than those of a 5 year journeyman. The knowledge that makes the technique eflective is domain dependent, but the technique itself can be used in other domains.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-040.pdf,
40,1987,Planning,Compiling Plan Operators from Domains Expressed in Qualitative Process Theory,John C. Hogge,"The study of Qualitative Physics has concentrated on expressing qualitatively how the physical world behaves. Qualitative Physics systems accept partial descriptions of the world and output the possible changes that can occur. These systems currently assume that the world is left untouched by human or robot agents, limiting them to certain types of problem solving. For instance, a state-of-the-art qualitative physics system can diagnose faulty electrical circuits but can not construct plans to rewire circuits to change their behavior. This paper describes an approach to planning in physical domains and a working implementation which integrates Forbus’ Qualitative Process Engine (QPE) with a temporal interval-based planner. The approach involves compiling QPE expressions describing a physical domain into a set of operators and rules of the planner. The planner can then construct plans involving processes, existence of individuals, and changes in quantities. We describe how the compilation is performed, the types derivable plans, and current limitations in our approach.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-041.pdf,
41,1987,Planning,Models of Axioms for Time Intervals,Peter Ladkin,"James Allen and Pat Hayes have considered axioms expressed in first-order logic for relations between time intervals [AllHay85, AllHay87.1, AllHay87.2]. One important consequence of the results in this paper is that their theory is decidable [Lad87.4]. In this paper, we characterise all the models of the theory, and of an important subtheory. A model is isomorphic to an interval structure INT(S) over some unbounded linear order S, and conversely, INT(S), for or an arbitrary unbounded linear order S, is a model. The models of the subtheory are similar, but with an arbitrary number of copies of each interval (conversely, all structures of this form are models). We also show that one of the original axioms is redundant, and we exhibit an additional axiom which makes the Allen-Hayes theory complete and countably categorical, with all countable models isomorphic to INT(Q), the theory of intervals with rational endpoints, if this is desired. These results enable us to directly compare the Allen-Hayes theory with the theory of Ladkin and Maddux [LadMad87.1], and of van Benthem [vBen83].",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-042.pdf,
42,1987,Planning,Localized Representation and Planning Methods for Parallel Domains,"Amy L. Lansky, David S. Fogelsong","This paper presents a general method for structuring domains that is based on the notion of locality. We consider a localized domain description to be one that is partitioned into regions of activity, each of which has some independent significance. The use of locality can be very beneficial for domain representation and reasoning, especially for parallel, multiagent domains. We show how localized domain descriptions can alleviate aspects of the frame problem and serve as the foundation of a planning technique based on localized planning spaces. Because domain constraints and properties are localized, potential interactions among these search spaces are fewer and more easily identified.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-043.pdf,
43,1987,Planning,A Model for Concurrent Actions Having Temporal Extent,"Richard N. Pelavin, James F. Allen","In this paper we present a semantic model that is used to interpret a logic that represents concurrent actions having temporal extent. In an earlier paper [Pelavin and Allen, 1986] we described how this logic is used to formulate planning problems that involve concurrent actions and external events. In this paper we focus on the semantic structure. This structure provides a basis for describing the interaction between actions, both concurrent and sequential, and for composing simple actions to form complex ones. This model can also treat actions that are influenced by properties that hold and events that occur during the time that the action is to be executed. Each model includes a set of world-histories, which are complete worlds over time, and a function that relates world-histories that differ solely on the account of an action executed at a particular time. This treatment derives from the semantic theories of conditionals developed by Stalnaker [Stalnaker, 1968] and Lewis [Lewis, 1973].",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-044.pdf,
44,1987,Planning,The Consistent Labeling Problem in Temporal Reasoning,Edward P. K. Tsang,"Temporal reasoning can be performed by maintaining a temporal relation network, a complete network in which the nodes are time intervals and each arc is the temporal relation between the two intervals which it connects. In this paper, we point out that the task of detecting inconsistency of the network and mapping the intervals onto a date line is a Consistent Labeling problem (CLP). The problem is formalized and analyzed. The significance of identifying and analyzing the CLP in temporal reasoning is that CLPs have certain features which allow us to apply certain techniques to our problem. We also point out that the CLP exists when we reason with disjunctive temporal relations. Therefore, the intractability of the constraint propagation mechanism in temporal reasoning is inherent in the problem, not caused by the representation that we choose for time, as [Vilain $ Kautz 86] claims.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-045.pdf,
45,1987,Planning,The Satisfiability of Temporal Constraint Networks,Raúl E. Valdés-Pérez,"A popular representation of events and their relative alignment in time is James Allen’s intervals and algebra. Networks of disjunctive interval constraints have served both to assimilate knowledge from ambiguous sentences, and to hold partial solutions in a planner. The satisfiability of these networks is of practical concern, and little has been achieved beyond proving that determining satisfiability is NP-hard. This paper scrutinizes the interval representation and its mechanisms. We make explicit the unstated assumptions of the mechanisms, introduce several useful theorems regarding interval networks, distinguish three types of inconsistency exhibited by these networks, and point out under what conditions these inconsistencies are detected. Finally the theorems, observations, and distinctions regarding inconsistency are exploited to design a practical algorithm to determine the satisfiability of an interval network. The extension of our results to two-dimensional spatial reasoning is under investigation.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-046.pdf,
46,1987,Planning,Validating Generalized Plans in the Presence of Incomplete Information,Marianne Winslett,"Let Robbie be an agent possessing a generalized plan for accomplishing a goal. Can Robbie use his plan to accomplish the goal without passing through any of a set of forbidden world states en route to the goal? This situation arises if, for example, Robbie must accomplish the goal with some additional constraints (""Can I get to the airport in time without speeding?""). There are two poles in the spectrum of methods Robbie can use to test his plan in the new world situation, each with its own advantages and disadvantages. At one extreme, Robbie can choose to express the new world constraints as additional preconditions on all the operators used for planning. At the other extreme, Robbie can attempt to prove that the new constraints are satisfied in every possible world that could arise during execution of the plan, from any initial world state that is consistent with his axioms. In this paper we examine the tradeoffs between these two opposing approaches, and show that the approaches are in fact very similar from a computational complexity point of view.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-047.pdf,
47,1987,Cognitive Modeling,Pengi: An Implementation of a Theory of Activity,"Philip E. Agre, David Chapman","AI has generally interpreted the organized nature of everyday activity in terms of plan-following. Nobody could doubt that people often make and follow plans. But the complexity, uncertainty, and immediacy of the real world require a central role for moment-to-moment improvisation. But before and beneath any planning ahead, one continually decides what to do now. Investigation of the dynamics of everyday routine activity reveals important regularities in the interaction of very simple machinery with its environment. We have used our dynamic theories to design a program, called Pengi, that engages in complex, apparently planful activity without requiring explicit models of the world.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-048.pdf,
48,1987,Cognitive Modeling,Compare and Contrast: A Test of Expertise,"Kevin D. Ashley, Edwina L. Rissland","In this paper we present three key elements of case-based reasoning (""CBR"") and describe how these are realized in our HYPO program which performs legal reasoning in the domain of trade secret law by comparing and contrasting cases. More specifically, the key elements involve how prior cases are used for: (1) Credit assignment of factual features; (2) Justification; and (3) Argument in domains that do not necessarily have strong causal theories or well-understood empirical regularities. We show how HYPO uses ""dimensions"", ""case-analysis-record"" and ""claim lattice"" mechanisms to perform indexing and relevancy assessment of past cases dynamically and how it compares and contrasts cases to come up with the best cases pro and con a decision.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-049.pdf,
49,1987,Cognitive Modeling,Modular Learning in Neural Networks,Dana H. Ballard,"In the development of large-scale knowledge networks, much recent progress has been inspired by connections to neurobiology. An important component of any ""neural"" network is an accompanying learning algorithm. Such an algorithm, to be biologically plausible, must work for very large numbers of units. Studies of large-scale systems have so far been restricted to systems without internal units (units with no direct connections to the input or output). Internal units are crucial to such systems as they are the means by which a system can encode high-order regularities (or invariants) that are implicit in its inputs and outputs. Computer simulations of learning using internal units have been restricted to small-scale systems. This paper describes a way of coupling autoassociative learning modules into hierarchies that should greatly improve the performance of learning algorithms in large-scale systems. The idea has been tested experimentally with positive results.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-050.pdf,
50,1987,Cognitive Modeling,Reducing Indeterminism in Consultation: A Cognitive Model of User/Librarian Interactions,"Hsinchun Chen, Vasant Dhar","In information facilities such as libraries, finding documents that are relevant to a user query is difficult because of the indeterminism involved in the process by which documents are indexed, and the latitude users have in choosing terms to express a query on a particular topic. Reference librarians play an important support role in coping with this indeterminism, focusing user queries through an interactive dialog. Based on thirty detailed observations of user/librarian interactions obtained through a field experiment, we have developed a computational model designed to simulate the reference librarian. The consultation includes two phases. The first is handle search, where the user’s rough problem statement and a user stereotyping imposed by the librarian are used in determining the appropriate tools (handles). The second phase is document search, involving the search for documents within a chosen handle. We are collaborating with the university library for putting our model to use as an intelligent assistant for an online retrieval system.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-051.pdf,
51,1987,Cognitive Modeling,A Mechanism for Early Piagetian Learning,Gary L. Drescher,"I propose a mechanism to model aspects of Piagetian development, in infants. The mechanism combines a powerful empirical learning technique with an unusual facility for constructing novel elements of representation-elements designating states that are not, mere logical combinations of other represented states. I sketch how this mechanism might recapitulate the infant’s gradual recognition that there exist physical objects that persist even when the infant does not perceive them. I also report results of a preliminary, partial implementation.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-052.pdf,
52,1987,Cognitive Modeling,Rules for the Implicit Acquisition of Knowledge about the User,"Robert Kass, Tim Finin","A major problem with incorporating a user model into an application has been the difficulty of acquiring the information for the user model. To make the user model effective, past approaches have relied heavily upon the explicit encoding of a large amount of information about potential system users. This paper discusses techniques for acquiring knowledge about the user implicitly (as the interaction with the user proceeds) in interactions between users and cooperative advisory systems. These techniques were obtained by analyzing transcripts of a large number of interactions between advice-seekers and a human expert, and have been encoded as a set of user model acquisition rules. Furthermore, the rules are domain independent, supporting the feasibility of building a general user modelling module.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-053.pdf,
53,1987,Cognitive Modeling,Case-based Problem Solving with a Large Knowledge Base of Learned Cases,Wendy G. Lehnert,"Recent experiments indicate that a case-based approach to the problem of word pronunciation is effective as the basis for a system that learns to pronounce English words. More generally, the approach taken here illustrates how a case-based reasoner can access a large knowledge base containing hundreds of potentially relevant cases and consolidate these multiple knowledge sources using numerical relaxation over a structured net-work. In response to a test item, a search space is first generated and structured as a lateral inhibition network. Then a spreading activation algorithm is applied to this search space using activation levels derived from the case base. In this paper we describe the general design of our model and report preliminary test results based on a training vocabulary of 750 words. Our approach combines traditional heuristic methods for memory organization with connectionist-inspired techniques for network manipulation in an effort to exploit the best of both information-processing methodologies.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-054.pdf,
54,1987,Cognitive Modeling,Material Handling: A Conservative Domain for Neural Connectivity and Propagation,"H. Van Dyke Parunak, James Kindrick, Bruce Irish","Two important components of connectionist models are the connectivity between units and the propagation rule for mapping outputs of units to inputs of units. The biological domains where these models are usually applied are nonconservative, in that a single output signal produced by one unit can become the input to zero, one, or many subsequent units. The connectivity matrices and propagation rules common in these domains reflect this nonconservativism in both learning and performance. CASCADE is a connectionist system for performing material handling in a discrete parts manufacturing environment. We have described elsewhere the architecture and implementation of CASCADE [PARU86a] and its formal correspondence [PARU86c], ] [PARU87a] with the PDP model [RUME86]. The signals that CASCADE passes between units correpond to discrete physical objects, and thus must obey certain conservation laws not observed by conventional neural architectures. This paper briefly reviews the problem domain and the connectionist structure of CASCADE, describes CASCADE’s scheme for maintaining connectivity information and propagating signals, and reports some experiments with the system.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-055.pdf,
55,1987,Cognitive Modeling,AQUA: Asking Questions and Understanding Answers,Ashwin Ram,"Story understanding programs are often designed to answer questions to demonstrate that they have adequately understood a story (e.g., [Leh78]). In contrast, we claim that asking questions is central to understanding. Reading a story involves the generation of questions, which in turn focus the understander on the relevant aspects of the story as it reads further. We are interested in the kinds of questions that people ask as they read. In this paper, we talk about the origin of these questions in the basic cycle of understanding, and their effect on processing. We present an understanding algorithm based on our theory of questions, which we have implemented in a computer program called AQUA (Asking Questions and Understanding Answers).",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-056.pdf,
56,1987,Cognitive Modeling,Information Retrieval from Never-Ending Stories,Lisa F. Rau,"The System for Conceptual Information Summarization, Organization, and Retrieval (SCISOR) is a research system that consists of a set of programs to parse short newspaper texts in the domain of corporate takeovers and finance. The conceptual information extracted from these stories may then be accessed through a natural language interface. Events in the world of corporate takeovers unfold slowly over time. As a result of this, the input to SCISOR consists of multiple short articles, most of which add a new piece of information to an ongoing story. This motivates a natural language, knowledge-based approach to information retrieval, as traditional methods of document retrieval are inappropriate for retrieving multiple short articles describing events that take place over time. A natural language, knowledge-based approach facilitates obtaining both concise answers to straightforward questions and summaries or updates of the events that take place. The predictable events that take place in the domain make expectation-driven, partial parsing feasible.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-057.pdf,
57,1987,Cognitive Modeling,Analogical Processing: A Simulation and Empirical Corroboration,"Janice Skorstad, Brian Falkenhainer, Dedre Gentner","This paper compares the performance of the Structure-Mapping Engine (SME), a cognitive simulation of analogy, with two aspects of human performance. Gentner’s Structure-Mapping theory predicts that soundness is highest for relational matches, while accessibility is highest for surface matches. These predictions have been borne out in psychological studies, and here we demonstrate that SME replicates these results. In particular, we ran SME on the same stories used in the psychological studies with two different kinds of match rules. In analogy mode, SME closely captures the human soundness ordering. In mere-appearance mode, SME captures the accessibility ordering. We briefly review the psychological studies, describe our computational experiments, and discuss the utility of SME as a cognitive modeling tool.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-058.pdf,
58,1987,Cognitive Modeling,Goal-based Generation of Motivational Expressions in a Learning Environment,Ingrid Zukerman,"In a tutorial setting, we often hear expressions such as ""The method we are about to discuss will help you solve . . . "" or ""Let us consider a subject which demands some more practice,"" which are issued by a tutor to motivate a student to attend to forthcoming discourse. In this paper we model the meaning of these expressions in terms of their anticipated influence on the status of a listener’s goals, and use these predictions to produce motivational expressions and embed them in computer generated discourse. In particular, we have recognized relations which are instrumental in determining a listener’s motivational requirements in a hierarchical problem-solving domain. These ideas have been incorporated into a system called FIGMENT which generates commentaries on the solution of algebraic equations.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-059.pdf,
59,1987,Default Reasoning,Incremental Inference: Getting Multiple Agents to Agree on What to Do Next,Gary C. Borchardt,"This paper presents a symbolic reasoning algorithm for use in the construction of mixed-initiative interfaces; that is, interfaces allowing several human or machine agents to share collectively the control of an ongoing, real-time activity. The algorithm, called Incremental Inference, is based on propositional logic and is related in structure to the Truth Maintenance System; however, the notion of justifications in the Truth Maintenance System is replaced with a simpler notion of recency. Basic properties of the Incremental Inference mechanism are described and compared with those of the Truth Maintenance System, and an example is provided drawn from the domain of SPECTRUM, a knowledge-based system for the geological interpretation of imaging spectrometer data.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-060.pdf,
60,1987,Default Reasoning,An Approach to Default Reasoning Based on a First-Order Conditional Logic,James P. Delgrande,"This paper presents an approach to default reasoning based on an extension to classical first-order logic. In this approach, first-order logic is augmented with a ""variable conditional"" operator for representing default statements. Truth in the resulting logic is based on a possible worlds semantics: the default statement C-P is true just when p is true in the least exceptional worlds in which 01 is true. This system provides a basis for representing and reasoning about default statements. Inferences of default properties of individuals rely on two assumptions: first that the world being modelled by a set of sentences is as uniform as consistently possible and, second, that sentences that may consistently be assumed to be irrelevant to a default inference are, in fact, irrelevant to the inference. Two formulations of default inferencing are proposed. The first involves extending the set of defaults to include all combinations of irrelevant properties. The second involves assuming that the world being modelled is among the simplest worlds consistent with the defaults and with what is contingently known. In the end, the second approach is argued to be superior to the first.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-061.pdf,
61,1987,Default Reasoning,Counterfactual Reasoning with Direct Models,Mark Derthick,"Most of the effort AI has put into common sense reasoning has involved inference by sequential rule application. This approach is most effective in well characterized domains where any valid chain of inference from a set of observations leads to an acceptable interpretation. In more realistic cases where there are multiple consistent interpretations that are not equally good, or where there are no consistent interpretations, it seems more natural to choose the best alternative based on the interpretations themselves rather than the chains of inference used to derive them. ,uKLONE is a connectionist network which uses simulated annealing to search the space of interpretations, or models. Inconsistent theories lead to generation of models which come as close as possible to satisfying all of the axioms, so counterfactual reasoning can be accomplished by the same mechanism as factual reasoning. An example involving conflicting information is presented for which uKLONE finds an intuitively plausible interpretation.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-062.pdf,
62,1987,Default Reasoning,More on Inheritance Hierarchies with Exceptions: Default Theories and Inferential Distance,David W. Etherington,"In Artificial Intelligence, well-understood reasoning systems and tractable reasoning systems have often seemed mutually exclusive. This has been exemplified by nonmonotonic reasoning formalisms and inheritance-with-exceptions reasoners. These have epitomized the two extremes: the former not even semidecidable, the latter completely ad hoc. We previously presented a formal mechanism for specifying inheritance systems, and minimal criteria for acceptable inheritance reasoning. This left open the problem of realizing an acceptable reasoner. Since then, Touretzky has developed a reasoner that appears to meet our criteria. We show that his reasoner is formally adequate, and explore some of the implications of this result vis-a-vis the study of nonmonotonic reasoning.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-063.pdf,
63,1987,Default Reasoning,A Skeptical Theory of Inheritance in Nonmonotonic Semantic Networks,"John F. Horty, Richmond H. Thomason, David S. Touretzky","This paper describes a new approach to inheritance reasoning in semantic networks allowing for multiple inheritance with exceptions. The approach leads to a definition of inheritance that is both theoretically sound and intuitively attractive: it yields unambiguous results applied to any acyclic semantic net, and these results conform to our own intuitions in the cases in which the intuitions themselves are firm and unambiguous. Since, however, the definition provided here is based on an alternative, skeptical view of inheritance reasoning, it does not always agree with previous definitions when it is applied to nets about which our intuitions are unsettled, or in which different reasoning strategies could naturally be expected to yield distinct results.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-064.pdf,
64,1987,Default Reasoning,"Circumscriptive Theories: A Logic-based Framework for Knowledge Representation, Preliminary Report",Vladimir Lifschitz,"The use of circumscription for formalizing commonsense knowledge and reasoning requires that a circumscription policy be selected for each particular application: we should specify which predicates are circumscribed, which predicates and functions are allowed to vary, what priorities between the circumscribed predicates are established, etc. The circumscription policy is usually described either informally or using suitable metamathematical notation. In this paper we propose a simple and general formalism which permits describing circumscription policies by axioms, included in the knowledge base along with the axioms describing the objects of reasoning. This method allows us to formalize some important forms of metalevel reasoning in the circumscriptive theory itself.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-065.pdf,
65,1987,Default Reasoning,Embracing Causality in Formal Reasoning,Judea Pearl,"The purpose of this note is to draw attention to certain aspects of causal reasoning which are pervasive in ordinary discourse yet, based on the author’s scan of the literature, have not received due treatment by logical formalisms of common-sense reasoning. In a nutshell, it appears that almost every default rule falls into one of two categories: expectation-evoking or explanation-evoking. The former describes association among events in the outside world (e.g., Fire is typically accompanied by smoke.); the latter describes how we reason about the world (e.g., Smoke normally suggests fire.). This distinction is consistently recognized by people and serves as a tool for controlling the invocation of new default rules. This note questions the ability of formal systems to reflect common-sense inferences without acknowledging such distinction and outlines a way in which the flow of causation can be summoned within the formal framework of default logic.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-066.pdf,
66,1987,Default Reasoning,The Logic of Representing Dependencies by Directed Graphs,"Judea Pearl, Thomas Verma","Data-dependencies of the type ""x can tell us more about y given that we already know z"" can be represented in various formalisms: Probabilistic Dependencies, Embedded-Multi-Valued Dependencies, Undirected Graphs and Directed-Acyclic Graphs (DAGs). This paper provides an axiomatic basis, called a semi-graphoid which captures the structure common to all four types of dependencies and explores the expressive power of DAGs in representing various types of data dependencies. It is shown that DAGs can represent a richer set of dependencies than undirected graphs, that DAGs completely represent the closure of their specification bases, and that they offer an effective computational device for testing membership in that closure as well as inferring new dependencies from given inputs. These properties might explain the prevailing use of DAGs in causal reasoning and semantic nets.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-067.pdf,
67,1987,Default Reasoning,Default Reasoning through Belief Revision Strategy,Chern H. Seet,"The thesis of this paper is that default reasoning can be accomplished rather naturally if an appropriate strategy of belief revision is employed. The idea is based on the premise that new beliefs introduced into a situation change the structure of current beliefs to accomodate the new beliefs as exceptions. It is easy to characterise these exceptions in beliefs if we extend the belief language to include some modal operator and prefix the exceptions with the operator. This serves to make the exceptions syntactically explicit, which can then be processed in a routine way by a default reasoning theorem prover.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-068.pdf,
68,1987,Default Reasoning,A Theory of Default Reasoning,Wlodek W. Zadrozny,"We propose a theory of default reasoning satisfying a list of natural postulates. These postulates imply that knowledge bases containing defaults should be understood not as sets of formulas (rules and facts) but as collections of partially ordered theories. As a result of this shift of perspective we obtain a rather natural theory of default reasoning in which priorities in interpretation of predicates are the source of nonmonotonicity in reasoning. We also prove that our theory shares a number of desirable properties (completeness, soundness etc.) with the theory of normal defaults of R. Reiter. We limit our discussion to logical properties of the proposed system and prove some theorems about it. Modal operators or second order formulas do not appear in our formalization. Instead, we augment the usual, two-part logical structures consisting of a metalevel and an object level, with a third level - a referential level. The referential level is a partially ordered collection of defaults; it contains a more permanent part of a knowledge base. Current situations are described on the object level. The metalevel is a place for rules that can eliminate some of the models permitted by the object level and the referential level.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-069.pdf,
69,1987,"KnowledgeRepresentation",The Goal/Subgoal Knowledge Representation for Real-Time Process Monitoring,"James R. Allard, William F. Kaemmerer","We have developed and implemented a plan representation system which has been used as the knowledge representation for COOKER, a real-time process monitoring and operator advisory system for batch manufacturing processes. This representation (called ""Goal/Subgoal"" or ""GSG"") associates two hierarchies of subgoals with each goal: a sequence of subgoals which need to be satisfied to satisfy the superior goal, and a set of requisite subgoals which must remain satisfied throughout the process of satisfying the superior goal. By explicitly representing correct process operating behavior instead of the infinite space of problem behaviors, a broad range of process operation anomalies can be recognized and diagnosed in terms of a single, simple description of the system. In this paper we compare GSG to our first approach at representation, describe the GSG representation, show how goals are used to monitor processes, and describe some results of our installation of COOKER in a manufacturing plant.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-070.pdf,
70,1987,"KnowledgeRepresentation",Partial Compilation of Strategic Knowledge,"Russ B. Altman, Bruce G. Buchanan","Many system building efforts in artificial intelligence intentionally begin with expressively rich and flexible declarative structures for the control of problem solving-especially when the best problem solving strategies are not known. However, as experience with a system increases, it sometimes becomes desirable to compile declarative knowledge into procedures for purposes of efficiency. We present a paradigm for compilation which begins with declarative opportunism, moves to a phase of heuristic implementation of a partial plan and finally evolves into a fully elaborated procedure. We use the PROTEAN geometric constraint satisfaction system as an example. Using results from a purely declarative structure, we were able to compile strategic knowledge into a procedure for planning a solution. The problem solving behavior of the new system is reported.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-071.pdf,
71,1987,"KnowledgeRepresentation",Representing Databases in Frames,Ey-Chih Chow,"Three methods for representing data in a relational storage system with an in-core frame-based system are experimented with and reported upon. Tradeoffs among these three representational methods are sizes of databases, times for loading data, and performance of queries. Essentially, these methods differ in ways of capturing relationships among frames. The three different ways of capturing such relationships are via links (pointers), symbolic names (keys), or both. Results of the experiments shed light on efficient interfacing of databases with frame-based systems.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-072.pdf,
72,1987,"KnowledgeRepresentation",Intention = Choice + Commitment,"Philip R. Cohen, Hector J. Levesque","This paper provides a logical analysis of the concept of intention as composed of two more basic concepts, choice (or goal) and commitment. By making explicit the conditions under which an agent can drop her goals, i.e., by specifying how the agent is committed to her goals, the formalism provides analyses for Bratman’s three characteristic functional roles played by intentions [Bratman, 1986], and shows how agents can avoid intending all the foreseen side-effects of what they actually intend. Finally, the analysis shows how intentions can be adopted relative to a background of relevant beliefs and other intentions or goals. By relativizing one agent’s intentions in terms of beliefs about another agent’s intentions (or beliefs), we derive a preliminary account of interpersonal commitments.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-073.pdf,
73,1987,"KnowledgeRepresentation",TAXI: A Taxonomic Assistant,Thomas Y. Galloway,"The task of constructing knowledge bases is a difficult one due to their size and complexity. A useful aid for this task would be a system which has both knowledge about a particular knowledge representation scheme and tools with which to manipulate the representation’s components. Such a system would be a knowledge maniupulation system (KMS). This paper describes a KMS called TAXI which is used to manipulate knowledge in the form of a taxonomic knowledge representation scheme. The particular taxonomic representation used is discussed, along with support for the usefulness of a KMS for this particular representation scheme. Tools provided in the TAXI system are described, as are possible applications for the system.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-074.pdf,
74,1987,"KnowledgeRepresentation",Complexity in Classificatory Reasoning,"Ashok Goel, N. Soundararajan, B. Chandrasekaran","Classificatory reasoning involves the tasks of concept evaluation and classification, which may be performed with use of the strategies of concept matching and concept activation, respectively. Different implementations of the strategies of concept matching and concept activation are possible, where an implementation is characterized by the organization of knowledge and the control of information processing it uses. In this paper we define the tasks of concept evaluation and classification, and describe the strategies of concept matching and concept activation. We then derive the computational complexity of the tasks using different implementations of the task-specific strategies. We show that the complexity of performing a task is determined by the organization of knowledge used in performing it. Further, we suggest that the implementation that is computationally the most efficient for performing a task may be cognitively the most plausible as well.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-075.pdf,
75,1987,"KnowledgeRepresentation",All I Know: An Abridged Report,Hector J. Levesque,"Current approaches to formalizing non-monotonic reasoning using logics of belief require new metalogical properties over sets of sentences to be defined. This research attempts to show how some of these patterns of reasoning can be captured using only the classical notions of logic (satisfiability, validity, implication). This is done by extending a logic of belief so that it is possible to say that only a certain proposition (or finite set of them) is believed. This research also extends previous approaches to handle quantifiers and equality, provides a semantic account of certain types of non-monotonicity, and through a simple proof theory, allows formal derivations to be generated.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-076.pdf,
76,1987,"KnowledgeRepresentation",Algorithm Synthesis through Problem Reformulation,Michael R. Lowry,"AI has been successful in producing expert systems for diagnosis, qualitative simulation, configuration and tutoring-e.g. classification problem solving. It has been less successful in producing expert systems that design artifacts, including computer programs. Deductive synthesis of a design from first principles is combinatorially explosive, yet libraries of design schemas do not have sufficient flexibility for application to novel problems. This paper proposes that the major factor in applying design knowledge is reformulating a problem in terms of the parameters of generic designs. This paper shows how to represent knowledge of generic designs as parameterized theories. This facilitates problem reformulation, making it a well defined search for appropriate parameter instantiations. The representation of design knowledge with parameterized theories is illustrated with generic local search algorithms. The utility of parameterized theories is shown by deriving the simplex algorithm for linear optimization from specification.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-077.pdf,
77,1987,"KnowledgeRepresentation",Curing Anomalous Extensions,Paul Morris,"In a recent paper, Hanks and McDermott presented a simple problem in temporal reasoning which showed that a seemingly natural representation of a frame axiom in nonmonotonic logic can give rise to an anomalous extension, i.e., one which is counter-intuitive in that it does not appear to be supported by the known facts. An alternative, less formal approach to nonmonotonic reasoning uses the mechanism of a truth maintenance system (TMS). Surprisingly, when reformulated in terms of a TMS, the anomalous extension noted by Hanks and McDermott disappears. We analyze the reasons for this. First it is seen that anomalous extensions are not limited to temporal reasoning, but can occur in simple non-temporal default reasoning as well. In these cases also, the natural TMS representation avoids the problem. Exploring further, it is observed that the form of the TMS justifications resembles that of nonnormal default rules. Nonnormal rules have already been proposed as a means of avoiding anomalous extensions in some non-temporal reasoning situations. It appears that, suitably formulated, they can exclude the anomalous extension in the Hanks-McDermott case also, although the representation does not adjust smoothly to fresh information, as does the TMS. Some variant of nonnormal default appears to be required to provide a correct semantic basis for truth maintenance systems.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-078.pdf,
78,1987,"KnowledgeRepresentation",Semantically Sound Inheritance for a Formally Defined Frame Language with Defaults,"Robert Nado, Richard Fikes","Most frame languages either are glaringly deficient in their treatment of default information or do not represent it at all. This paper presents a formal description of a frame language that provides semantically sound facilities for representing default information and an efficient serial algorithm for inheriting default information down class-subclass and class-member hierarchies constructed in that language. We present the inheritance algorithm in two forms. In the first form, the algorithm provides justifications to a TMS, which then manages the inherited information. In the second form, the algorithm performs its own, special-purpose truth maintenance and therefore is useable in a system that does not, include a general-purpose TMS.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-079.pdf,
79,1987,"KnowledgeRepresentation",Assimilation: A Strategy for Implementing Self-Reorganizing Knowledge Bases,Jane Terry Nutter,Assimilation is a process by which a knowledge base restructures itself to improve the organization of and access to information in the base. This paper presents a strategy for implementing assimilation in propositional knowledge bases which distinguish between the axioms of the system’s knowledge (called the context) and the derived consequences of those axioms (called the belief space). The strategy in question takes advantage of housekeeping phases in which the system discards accumulated clutter to discover useful patterns of access on the basis of which the context can be reorganized. Unused axioms are replaced by their more useful consequences; derivable generalizations that shorten common inference paths are added to the belief space.,https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-080.pdf,
80,1987,"MachineLearning and Knowledge Acquisition",Learning to Control a Dynamic Physical System,"Margaret E. Connell, Paul E. Utgoff","This paper presents an approach to learning to control a dynamic physical system. The approach has been implemented in a program named CART, and applied to a simple physical system studied previously by several researchers. Experiments illustrate that a control method is learned in about 16 trials, an improvement over previous learning programs.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-081.pdf,
81,1987,"MachineLearning and Knowledge Acquisition",Improving Inference through Conceptual Clustering,Douglas Fisher,"Conceptual clustering is an important way to summarize data in an understandable manner. However, the recency of the conceptual clustering paradigm has allowed little exploration of conceptual clustering as a means of improving performance. This paper presents COBWEB, a conceptual clustering system that organizes data to maximize inference abilities. It does this by capturing attribute intercorrelations at classification tree nodes and generating inferences as a by-product of classification. Results from the domains of soybean and thyroid disease diagnosis support the success of this approach.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-082.pdf,
82,1987,"MachineLearning and Knowledge Acquisition",Learning Conjunctive Concepts in Structural Domains,David Haussler,"We study the problem of learning conjunctive concepts from examples on structural domains like the blocks world. This class of concepts is formally defined and it is shown that even for samples in which each example (positive or negative) is a two-object scene it is NF-complete to determine if there is any concept in this class that is consistent with the sample. We demonstrate how this result affects the feasibility of Mitchell’s version space approach and how it shows that it is unlikely that this class of concepts is polynomially learnable from random examples in the sense of Valiant. On the other hand, we show that this class is polynomially learnable if we allow a larger hypothesis space. This result holds for any fixed number of objects per scene, but the algorithm is not practical unless the number of objects per scene is very small. We also show that heuristic methods for learning from larger scenes are likely to give an accurate hypothesis if they produce a simple hypothesis consistent with a large enough random sample.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-083.pdf,
83,1987,"MachineLearning and Knowledge Acquisition",An Experimental Comparison of Knowledge Engineering for Expert Systems and for Decision Analysis,"Max Henrion, Daniel R. Cooley","Decision analysis provides a set of techniques for structuring and encoding expert knowledge, comparable with knowledge engineering techniques for rule-based expert systems. In order to compare the expert systems and decision analysis approach, each was applied to the same task, namely the diagnosis and treatment of root disorders in apple trees. This experiment illustrates a variety of theoretical and practical differences between them, including the semantics of the network representations (inference net vs. influence diagram or Bayes’ belief net), approaches to modelling uncertainty and preferences, the relative effort required, and their attitudes to human reasoning under uncertainty, as the ideal to be emulated or as unreliable and to be improved upon?",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-084.pdf,
84,1987,"MachineLearning and Knowledge Acquisition",Formulating Concepts According to Purpose,Smadar T. Kedar-Cabelli,"Explanation-Based Generalization (EBG) has been recently a much-explored method of generalization. By utilizing domain knowledge, and knowledge of the concept being learned, EBG produced a valid generalization from a single example. Most EBG systems are currently provided with the concept being learned-or target concept-as a fixed input. A more robust generalization mechanism needs the ability to automatically formulate appropriate target concepts based on the purpose of the learning, since concepts learned for one purpose may not be appropriate for another. This paper introduced a technique and an implemented system that automatically formulate target concepts and their specialized definitions. In particular, the technique derives definitions of everyday artifacts (e.g. CUP), from information about the purpose for which agents intend to use them (e.g. to satisfy their thirst). Given two different purposes for which an agent might use a cup (e.g. as an ornament, versus to satisfy thirst), two different definitions can be derived.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-085.pdf,
85,1987,"MachineLearning and Knowledge Acquisition",Defining Operationality for Explanation-based Learning,Richard M. Keller,"Operationality is the key property that distinguishes the final description learned in an explanation-based system from the initial concept description input to the system. Yet most existing systems fail to define operationality with necessary precision. In particular, attempts to define operationality in terms of ""efficient instance recognition"" tacitly incorporate several unrealistic, simplifying assumptions about the learner’s performance task and the type of performance improvement desired. Over time, these assumptions are likely to be violated, and the learning system’s effectiveness will deteriorate. We survey how operationality is defined and assessed in several explanation-based systems, and then present a more comprehensive definition of operationality. We also describe an implemented system that incorporates our new definition and overcomes some of the limitations exhibited by current operationality assessment schemes.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-086.pdf,
86,1987,"MachineLearning and Knowledge Acquisition",A KNACK for Knowledge Acquisition,"Georg Klinker, Casey Boyd, Serge Genetet, John McDermott","KNACK is a knowledge acquisition tool that generates expert systems for evaluating designs of electromechanical systems. An important feature of KNACK is that it acquires knowledge from domain experts without presupposing knowledge engineering skills on their part. This is achieved by incorporating general knowledge about evaluation tasks in KNACK. Using that knowledge, KNACK builds a conceptual model of the domain through an interview process with the expert. KNACK expects the expert to communicate a portion of his knowledge as a sample report and divides the report into small fragments. It asks the expert for strategies of how to customize the fragments for different applications. KNACK generalizes the fragments and strategies, displays several instantiations of them, and the expert edits any of these that need it. The corrections motivate and guide KNACK in refining the knowledge base. Finally, KNACK examines the acquired knowledge for incompleteness and inconsistency. This process of abstraction and completion results in a knowledge base containing a large collection of generalized report fragments more broadly applicable than the sample report.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-087.pdf,
87,1987,"MachineLearning and Knowledge Acquisition",PROLEARN: Towards a Prolog Interpreter that Learns,"Armand E. Prieditis, Jack Mostow","An adaptive interpreter for a programming language adapts to particular applications by learning from execution experience. This paper describes PROLEARN, a prototype adaptive interpreter for a subset of Prolog. It uses two methods to speed up a given program: explanation-based generalization and partial evaluation. The generalization of computed results differentiates PROLEARN from programs that cache and reuse specific values. We illustrate PROLEARN on several simple programs and evaluate its capabilities and limitations. The effects of adding a learning component to Prolog can be summarized as follows: the more search and subroutine calling in the original query, the more speedup after learning; a learned subroutine may slow down queries that match its head but fail its body.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-088.pdf,
88,1987,"MachineLearning and Knowledge Acquisition",Knowledge Level Learning in Soar,"Paul S. Rosenbloom, John E. Laird, Allen Newell","In this article we demonstrate how knowledge level learning can be performed within the Soar architecture. That is, we demonstrate how Soar can acquire new knowledge that is not deductively implied by its existing knowledge. This demonstration employs Soar’s chunking mechanism-a mechanism which acquires new productions from goal-baaed experience-as its only learning mechanism. Chunking has previously been demonstrated to be a useful symbol level learning mechanism, able to speed up the performance of existing systems, but this is the first demonstration of its ability to perform knowledge level learning. Two simple declarative-memory tasks are employed for this demonstration: recognition and recall.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-089.pdf,
89,1987,"MachineLearning and Knowledge Acquisition",A Declarative Approach to Bias in Concept Learning,"Stuart J. Russell, Benjamin N. Grosof","We give a declarative formulation of the biases used in inductive concept learning, particularly the Version-Space approach. We then show how the process of learning a concept from examples can be implemented as a first-order deduction from the bias and the facts describing the instances. This has the following advantages: 1) multiple sources and forms of knowledge can be incorporated into the learning process; 2) the learning system can be more fully integrated with the rest of the beliefs and reasoning of a complete intelligent agent. Without a semantics for the bias, we cannot generally and practically build machines that generate inductive biases automatically and hence are able to learn independently. With this in mind, we show how one part of the bias for Meta-DENDRAL, its instance description language, can be represented using first-order axioms called determinations, and can be derived from basic background knowledge about chemistry. The second part of the paper shows how bias can be represented as defaults, allowing shift of bias to be accommodated in a nonmonotonic framework.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-090.pdf,
90,1987,"MachineLearning and Knowledge Acquisition",Learning and Representation Change,Jeffrey C. Schlimmer,"To remain effective without human interaction, intelligent systems must be able to adapt to their environment. One useful form of adaptation is to incrementally form concepts from examples for the purposes of inference and problem-solving. A number of systems have been constructed for this task, yet their capability is limited by the language used to represent concepts. This paper presents an extension to the concept acquisition system STAGGER that allows it to utilize continuously valued attributes. The combination of methods employed is able to dynamically acquire appropriate representations, thereby minimizing the impact of initial representational bias decisions. Of additional interest is the distinction between the computational flavor of the learning methods, for one is similar to connectionist approaches while the other two are of a more symbolic nature.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-091.pdf,
91,1987,"MachineLearning and Knowledge Acquisition",BAGGER: An EBL System that Extends and Generalizes Explanations,"Jude W. Shavlik, Gerald F. DeJong","This paper addresses the important issue in explanation-based learning of generalizing number. Most research in explanation-based learning involves relaxing constraints on the variables in an explanation, rather than generalizing the number of inference rules used. However, many concepts require generalizing the structure of the explanation. An explanation-based approach to the problem of generalizing to N is presented. The fully-implemented BAGGER system analyzes explanation structures and detects extendible repeated, inter-dependent applications of rules. When any are found, the explanation is extended is extended so that an arbitrary number of repeated applications of the original rule are supported. The final structure is then generalized and a new rule produced. An important property of the extended rules is that their preconditions are expressed in terms of the initial state - they do not depend on the results of intermediate applications of the original rule. To illustrate the approach, portions of several situation calculus examples from the blocks world are analyzed. The approach presented leads to the acquisition of efficient plans that can be used to clear an object directly supporting an arbitrary number of other objects, build towers of arbitrary height, and unstack towers containing any number of blocks.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-092.pdf,
92,1987,"MachineLearning and Knowledge Acquisition",Optimizing the Predictive Value of Diagnostic Decision Rules,"Sholom M. Weiss, Robert S. Galen, Prasad V. Tadepalli","An approach to finding an optimal solution for an important diagnostic problem is described. Examples are taken from laboratory medicine, where the problem can be stated as finding the best combination of tests for making a diagnosis. These tests are typically numerical with unknown decision thresholds. Because of uncertainty in classification, the solution is described in terms of maximizing measures of decision rule performance on a data base of cases, for example maximizing positive predictive value, subject to a constraint of a minimum sensitivity. The resultant rules are quite similar to classification production rules, and the procedures described should be valid for many knowledge acquisition and refinement tasks. The solution is found by a heuristic search procedure, and empirical results for several data bases and published studies are described.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-093.pdf,
93,1987,"NaturalLanguage",Interpreting Clues in Conjunction with Processing Restrictions in Arguments and Discourse,Robin Cohen,"This paper extends previous work which provided a theory for the interpretation of and necessity for clue words in a particular kind of discourse - namely, one-way arguments. Previous work described a taxonomy of connective clues (words such as ""hence"" or phrases such as ""as a result""), where each clue, classified according to the taxonomy, would set in place a default interpretation of its containing proposition, with respect to the representation for the argument so far. In this paper, we examine how to combine the restrictions for clues with a basic processor for the discourse, offering a integrated processing algorithm, which takes advantage of clues to reduce processing and to detect incoherent arguments, and can still produce an analysis in the absence of clues. We conclude with some suggestions for incorporating clues of re-direction and clues that signal exceptional transmissions. We also demonstrate the implications of our results for discourse in general.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-094.pdf,
94,1987,"NaturalLanguage",UNITRAN: An Interlingual Approach to Machine Translation,Bonnie Dorr,"Machine translation has been a particularly difficult problem in the area of Natural Language Processing for over two decades. Early approaches to translation failed in part because interaction effects of complex phenomena made translation appear to be unmanageable. Later approaches to the problem have succeeded but are based on many language-specific rules. To capture all natural language phenomena, rule-based systems require an overwhelming number of rules; thus, such translation systems either have limited coverage, or poor performance due to formidable grammar size. This paper presents an implementation of an ""interlingual"" approach to natural language translation. The UNITRAN system relies on principle-based descriptions of grammar rather than rule-oriented descriptions. The model is based on linguistically motivated principles and their associated parameters of variation. Because a few principles cover all languages, the unmanageable grammar size of alternative approaches is no longer a problem.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-095.pdf,
95,1987,"NaturalLanguage",Recovering from Erroneous Inferences,Kurt P. Eiselt,"Many models of natural language understanding make inference decisions as they process a text, but few models can correct their interpretation when later text reveals that earlier inference decisions are wrong. This paper describes how ATLAST, a marker-passing model of text understanding, addresses this problem. The keys to ATLAST’s error recovery capability are a means for remembering the choices it could have made but did not, and a means for initiating the re-evaluation of those previously rejected choices at the appropriate times. This paper also discusses some of the arguments for and against the psychological validity of a theory of inference retention in human text understanding.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-096.pdf,
96,1987,"NaturalLanguage",Interpretation in Generation,Eduard H. Hovy,"The computer maxim garbage in, garbage out is especially true of language generation. When a generator slavishly follows its input topics, it usually produces bad text. In order to find more appropriate forms of expression, generators must be given the ability to interpret their input topics. Often, newly formed interpretations can help generators achieve their pragmatic goals with respect to the hearer. Since interpretation requires inference, generators must exercise some control over the inference process. Some general strategies of control, and some specific techniques geared toward achieving pragmatic goals, are described here.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-097.pdf,
97,1987,"NaturalLanguage",Word-Order Variation in Natural Language Generation,Aravind K. Joshi,"In natural language generation the grammatical component has to be systematically interfaced to the other components of the system, for example, the planning component. Grammatical formalisms can be studied with respect to their suitability for generation. The tree adjoining grammar (TAG) formalism has been previously studied in terms of incremental generation. In this paper, the TAG formalism has been investigated from the point of view of its ability to handle word-order variation in the context of generation. Word-order cannot be treated as a last minute adjustment of a structure; this position is not satisfactory cognitively or computationally. The grammatical framework has to be able to deal with the word-order phenomena in a way such that it can be systematically interfaced to the other components of the generation system.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-098.pdf,
98,1987,"NaturalLanguage",Porting and Extensible Natural Language Interface: A Case History,"Candace E. Kalish, Matthew B. Cox","The KING KONG linguistic interface was developed at MITRE to be a portable natural Ianguage interface for expert systems. It is possible to port KING KONG from one expert system to another without writing more than a modest amount of code, regardless of backend architecture. We describe porting it from its original expert system backend to another expert system which was radically different in domain and representation.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-099.pdf,
99,1987,"NaturalLanguage",Inference in Text Understanding,Peter Norvig,"The problem of deciding what was implied by a written text, of ""reading between the lines"" is the problem of inference. To extract proper inferences from a text requires a great deal of general knowledge on the part of the reader. Past approaches have often postulated an algorithm tuned to process a particular kind of knowledge structure (such as a script, or a plan). An alternative, unified approach is proposed. The algorithm recognizes six very general classes of inference, classes that are not dependent on individual knowledge structures, but instead rely on patterns of connectivity between concepts. The complexity has been effectively shifted from the algorithm to the knowledge base; new kinds of knowledge structures can be added without modifying the algorithm.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-100.pdf,
100,1987,"NaturalLanguage",The Acquisition of Conceptual Structure for the Lexicon,"James Pustejovsky, Sabine Bergler","There has recently been a great deal of interest in the structure of the lexicon for natural language understanding and generation. One of the major problems encountered has been the optimal organization of the enormous amounts of lexical knowledge necessary for robust NLP systems. Modifying machine readable dictionaries into semantically organized networks, therefore, has become a major research interest. In this paper we propose a representation language for lexical information in dictionaries, and describe an interactive learning approach to this problem, making use of extensive knowledge of the domain being learned. We compare our model to existing systems designed for automatic classification of lexical knowledge.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-101.pdf,
101,1987,"NaturalLanguage",Ambiguity Procrastination,"Elaine Rich, Jim Barnett, Kent Wittenburg, David Wroblewski","In this paper we present the procrastination approach to the treatment of ambiguity, particularly in the context of natural language interfaces. In this approach, we try not to notice ambiguity unless we have the knowledge to resolve it available. In order to support this approach, we have developed a collection of structures that describe sets of possible interpretations efficiently. We have implemented this approach using these structures in Lucy, an English interface program for knowledge-based systems.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-102.pdf,
102,1987,"NaturalLanguage",Memory-based Reasoning Applied to English Pronunciation,Craig W. Stanfill,"Memory-based Reasoning is a paradigm for AI in which best-match recall from memory is the primary inference mechanism. In its simplest form, it is a method of solving the inductive inference (learning) problem. The primary topics of this paper are a simple memory-based reasoning algorithm, the problem of pronouncing english words, and MBRtalk, a program which uses memory-based reasoning to solve the pronunciation problem. Experimental results demonstrate the properties of the algorithm as training-set size is varied, as distracting information is added, and as noise is added to the data.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-103.pdf,
103,1987,"NaturalLanguage",Nondestructive Graph Unification,David A. Wroblewski,"Graph unification is sometimes implemented as a destructive operation, making it neccesary to copy the argument graphs before beginning the actual unification. Previous research on graph unification claimed that this copying is a computation sink, and has sought to correct this. In this paper I claim that the fundamental problem is in designing graph unification as a destructive operation. This forces it to both over copy and early copy. I present a nondestructive graph unification algorithm that minimizes over copying and eliminates early copying. This algorithm is significantly simpler than recently published solutions to copying problems, but maintains the essential efficiency gains of older techniques.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-104.pdf,
104,1987,"EngineeringProblem Solving",Reasoning about Fluids via Molecular Collections,"John W. Collins, Kenneth D. Forbus","Hayes has identified two distinct ontologies for reasoning about liquids. Most qualitative physics research has focused on applying and generalizing his contained-liquid ontology. This paper presents a technique for generating descriptions using the molecular collection (MC) ontology, a specialization of his alternate ontology which represents liquids in terms of little ""pieces of stuff"" traveling through a system. We claim that MC descriptions are parasitic on the Contained-Stuff ontology, and present rules for generating MC descriptions given a Qualitative Process theory model using contained stuffs. We illustrate these rules using several implemented examples and discuss how this representation can be used to draw complex conclusions.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-105.pdf,
105,1987,"EngineeringProblem Solving",Extending the Mathematics in Qualitative Process Theory,Bruce D'Ambrosio,"We present a semi-quantitative extension to the qualitative value and relationship representations in Qualitative Process (QP) theory. Examination of a detailed example reveals a number of limitations in the current ability of QP theory to analyze physical situations. The source of those limitations is traced in part to the qualitative mathematics used in QP theory. An extension to this mathematics is then presented and shown capable of eliminating many of these limitations, at the price of requiring additional system specific information about the system being modelled.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-106.pdf,
106,1987,"EngineeringProblem Solving",Troubleshooting: When Modeling Is the Trouble,"Philippe Dague, Olivier Raiman, Philippe Devès","This paper shows how order of magnitude reasoning has been successfully used for troubleshooting complex analog circuits. The originality of this approach was to be able to remove the gap between the information required to apply a general theory of diagnosis and the limited information actually available. The expert’s ability to detect a defect by reasoning about the significant changes in behavior it induces is extensively exploited here: as a kind of reasoning that justifies the qualitative modeling, as a heuristic that defines a strategy and as a working hypothesis that makes clear the scope of this approach.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-107.pdf,
107,1987,"EngineeringProblem Solving",Explanation-based Failure Recovery,Ajay Gupta,Interactions are inherent in design-type problem-solving tasks where only partially compiled operators are available. Failures arising from such interactions can best be recovered by explaining them in the underlying domain models. In this paper we explain how Explanation-Based Learning provides a framework for recovering in this manner. This approach also alleviates some of the problems associated with the least-commitment approach to design-type problem-solving.,https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-108.pdf,
108,1987,"EngineeringProblem Solving",Shape and Function in Mechanical Devices,Leo Joskowicz,"This paper describes a two-step algorithm for the qualitative analysis of mechanical devices. The first step takes the geometrical description of the parts and their initial position and produces a description of the possible relative motions of pairs in contact by computing the configuration space of those pairs with respect to selected motions. Given the possible relative motions and an input motion, the second step computes the actual motion of each object for fixed axis mechanisms using a constraint propagation, label inferencing technique. The output is a state diagram describing the motion of each part in the mechanism.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-109.pdf,
109,1987,"EngineeringProblem Solving",Critical Hypersurfaces and the Quantity Space,Mieczyslaw M. Kokar,"Qualitative reasoning about physical processes is based on the notion of ""quantity space"" [Forbus, 1984a, 1984b]. The question is how to construct a quantity space for a particular physical process. One line of research is to establish a set of so called ""landmark points"" by selecting some values of the continuous physical parameters characterizing the physical process under consideration [Kuipers, 1985a, 1985b]. The landmark points are to delimit operating regions of qualitative processes. In most practical situations it is impossible to find a finite set of such points. This is because the operating regions of physical processes are delimited not by some specific values of physical parameters but by some hypersurfaces in the cross-product of the parameters, they are called here ""critical hypersurfaces"". The paper presents a relatively complete methodology for establishing critical hypersurfaces.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-110.pdf,
110,1987,"EngineeringProblem Solving",Abstraction by Time-Scale in Qualitative Simulation,Benjamin Kuipers,"Qualitative simulation faces an intrinsic problem of scale: the number of limit hypotheses grows exponentially with the number of parameters approaching limits. We present a method called Time-Scale Abstraction for structuring a complex system as a hierarchy of smaller, interacting equilibrium mechanisms. Within this hierarchy, a given mechanism views a slower one as being constant, and a faster one as being instantaneous. A perturbation to a fast mechanism may be seen by a slower mechanism as a displacement of a monotonic function constraint. We demonstrate the time-scale abstraction hierarchy using the interaction between the water and sodium balance mechanisms in medical physiology, an example drawn from a larger, fully implemented, program. Where the structure of a large system permits decomposition by time-scale, this abstraction method permits qualitative simulation of otherwise intractibly complex systems.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-111.pdf,
111,1987,"EngineeringProblem Solving",Reasoning with Orders of Magnitude and Approximate Relations,"Michael L. Mavrovouniotis, George Stephanopoulos","The O[M] formalism for representing orders of magnitude and approximate relations is described, based on seven primitive relations among quantities. Along with 21 compound relations, they permit expression and solution of engineering problems without explicit disjunction or negation. In the semantics of the relations, strict interpretation allows exact inferences, while heuristic interpretation allows inferences more aggressive and human-like but not necessarily error-free. Inference strategies within O[M] are based on propagation of order of magnitude relations through properties of the relations, solved or unsolved algebraic constraints, and rules. Assumption-based truth- maintenance is used, and the physical dimensions of quantities efficiently constrain the inferences. Statement of goals allows more effective employment of the constraints and focuses the system’s opportunistic forward reasoning. Examples on the analysis of biochemical pathways are presented.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-112.pdf,
112,1987,"EngineeringProblem Solving",Making Partial Choices in Constraint Reasoning Problems,"Sanjay Mittal, Felix Frayman","Constraint problems derived from design and configurations tasks often use components (structured values) as domains of constrained variables. Most existing methods are forced into unnecessary search because they assign complete components to variables. A notion of partial choice is introduced as a way to assign a part of a component. The basic idea is to work with descriptions of classes of solutions as opposed to the actual solutions. It is shown how this idea can reduce search and in the best case eliminate search. A distinction is made between a partial commitment (a partial choice that would not be retracted) and a partial guess. A particular way to implement partial choice problem solvers is discussed. This method organizes choices in a taxonomic classification. Use of taxonomies not only helps in pruning the search space but also provides a compact language for describing solutions, no-goods, and representing constraints. It is also shown how multiple hierarchies can be used to avoid some of the problems associated with using a single hierarchy.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-113.pdf,
113,1987,"EngineeringProblem Solving",PROMPT: An Innovative Design Tool,"Seshashayee S. Murthy, Sanjaya Addanki","We describe a system, Prompt, used to design physical systems. Prompt employs a multi-level approach to design. When simple constraint propagation over prototypes [Adda85] fails, Prompt can significantly modify prototypes by reasoning about their structure and physics. Prompt derives the behavior of a prototype from its structure using knowledge of physics stored in a Graph of Models. It then uses heuristics called Modification operators to control the process of modifying the prototypes. In this paper we describe how our system works in the domain of structural design. We describe the kinds of analysis Prompt performs on beams and how it makes innovative changes to prototypes.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-114.pdf,
114,1987,"EngineeringProblem Solving",Reasoning about Discontinuous Change,"Toyoaki Nishida, Shuji Doshita","Intuitively, discontinuous changes can be seen as very rapid continuous changes. A couple of alternative methods based on this ontology are presented and compared. One, called the approximation method, approximates discontinuous change by continuous function and then calculates a limit. The other, called the direct method, directly creates a chain of hypothetical intermediate states (mythical instants) which a given circuit is supposed to go through during a discontinuous change. Although the direct method may fail to predict certain properties of discontinuity and its applicability is limited, it is more efficient than the approximation method. The direct method has been fully implemented and incorporated into an existing qualitative reasoning program.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-115.pdf,
115,1987,"EngineeringProblem Solving",Hierarchical Reasoning about Inequalities,Elisha Sacks,"This paper describes a program called BOUNDER that proves inequalities between functions over finite sets of constraints. Previous inequality algorithms perform well on some subset of the elementary functions, but poorly elsewhere. To overcome this problem, BOUNDER maintains a hierarchy of increasingly complex algorithms. When one fails to resolve an inequality, it tries the next. This strategy resolves more inequalities than any single algorithm. It also performs well on hard problems without wasting time on easy ones. The current hierarchy consists of four algorithms: bounds propagation, substitution, derivative inspection, and iterative approximation. Propagation is an extension of interval arithmetic that takes linear time, but ignores constraints between variables and multiple occurrences of variables. The remaining algorithms consider these factors, but require exponential time. Substitution is a new, provably correct, algorithm for utilizing constraints between variables. The final two algorithms analyze constraints between variables. Inspection examines the signs of partial derivatives. Iteration is based on several earlier algorithms from interval arithmetic.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-116.pdf,
116,1987,"EngineeringProblem Solving",Piecewise Linear Reasoning,Elisha Sacks,"This paper describes a new technique called piecewise linear reasoning (PLR) for analyzing dynamic systems describable by finite sets of ordinary differential equations. Current qualitative reasoning programs derive the abstract behavior of a system by simulating handcrafted ""qualitative"" versions of the differential equations that characterize it and summarizing the results. PLR infers more detailed information by constructing and examining piecewise linear approximations of the original equations. As evidence that PLR can provide useful information to engineers, its analyses of the Lienard and van der Pol equations are presented.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-117.pdf,
117,1987,"EngineeringProblem Solving",Probabilistic Semantics for Qualitative Influences,Michael P. Wellman,"What’s in an infiuence link? To answer this foundational question, I propose a semantics for qualitative influences: a positively influences b if and only if the posterior distribution for b given o increases with o in the sense of first-order stochastic dominance. By requiring that this condition hold in all contexts, we gain the ability to perform inference across chains of qualitative influences. Under sets of basic desiderata, the proposed definition is necessary as well as sufficient for this desirable computational property.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-118.pdf,
118,1987,"EngineeringProblem Solving",Extracting Qualitative Dynamics from Numerical Experiments,Kenneth Man-kam Yip,"The Phase Space is a powerful tool for representing and reasoning about the qualitative behavior of non-linear dynamical systems. Significant physical phenomena of the dynamical system - periodicity, recurrence, stability and the like - are reflected by outstanding geometric features of the trajectories in the phase space. Successful use of numerical computations to completely explore the dynamics of the phase space depends on the ability to (1) interpret the numerical results, and (2) control the numerical experiments. This paper presents an approach for the automatic reconstruction of the full dynamical behavior from the numerical results. The approach exploits knowledge of Dynamical Systems Theory which, for certain classes of dynamical systems, gives a complete classification of all the possible types of trajectories, and a list of bifurcation rules which govern the way trajectories can fit together in the phase space. These bifurcation rules are analogous to Waltz’s consistency rules used in labeling of line drawings. The approach is applied to an important class of dynamical system: the area-preserving maps, which often arise from the study of Hamiltonian systems. Finally, the paper describes an implemented program which solves the interpretation problem by using techniques from computational geometry and computer vision.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-119.pdf,
119,1987,Robotics,An Architecture for Intelligent Task Automation,"Jeffrey M. Becker, Fred L. Garrett","This report discusses the Martin Marietta Intelligent Task Automation Project (ITA). The purpose of the ITA project is to integrate Artificial Intelligence (AI) task planning, path planning, vision, and robotics technologies into a system designed to autonomously perform manufacturing tasks in dynamic or unstructured environments. The application domain chosen for primary demonstrations is dimensional measurement of an F-15 bulkhead. The overall goal is to be able to perform the inspection an order of magnitude faster than the current manual method, which takes about 24 hours for about 1000 inspection points. The project was conducted in two phases. Phase I, completed in December 1984, demonstrated the readiness of the technologies in each of the areas making up the ITA system. Phase II, which was mostly complete in June 1987, demonstrated that the technologies can be integrated into a working system and that the system can be transferred to other applications. The architecture of the ITA system is discussed with an emphasis on the AI components making up the system. The strengths and weaknesses of the architecture and AI techniques applied are discussed.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-120.pdf,
120,1987,Robotics,Reactive Reasoning and Planning,"Michael P. Georgeff, Amy L. Lansky","In this paper, the reasoning and planning capabilities of an autonomous mobile robot are described; The reasoning system that controls the robot is designed to exhibit the kind of behavior expected of a rational agent, and is endowed with the psychological attitudes of belief, desire, and intention. Because these attitudes are explicitly represented, they can be manipulated and reasoned about, resulting in complex goal-directed and reflective behaviors. Unlike most planning systems, the plans or intentions formed by the robot need only be partly elaborated before it decides to act. This allows the robot to avoid overly strong expectations about the environment, overly constrained plans of action, and other forms of overcommitment common to previous planners. In addition, the robot is continuously reactive and has the ability to change its goals and intentions as situations warrant. The system has been tested with SRI’s autonomous robot (Flakey) in a space station scenario involving navigation and the performance of emergency tasks.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-121.pdf,
121,1987,Robotics,Visual Grammars for Visual Languages,Fred Lakin,"In modern user interfaces, graphics play an important role in the communication between human and computer. When a person employs text and graphic objects in communication, those objects have meaning under a system of interpretation, or ""visual language."" Formal visual languages are ones which have been explicitly designed to be syntactically and semantically unambiguous. The research described in this paper aims at spatially parsing expressions in formal visual languages to recover their underlying syntactic structure. Such ""spatial parsing"" allows a general purpose graphics editor to be used as a visual language interface, giving the user the freedom to first simply create some text and graphics, and later have the system process those objects under a particular system of interpretation. The task of spatial parsing can be simplified for the interface designer/implementer through the use of visual grammars. For each of the four formal visual languages described in this paper, there is a specifiable set of spatial arrangements of elements for well-formed visual expressions in that language. Visual Grammar Notation is a way to describe those sets of spatial arrangements; the context-free grammars expressed in this notation are not only visual, but also machine-readable, and are used directly to guide the parsing.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-122.pdf,
122,1987,Robotics,Qualitative Landmark-based Path Planning and Following,"Tod S. Levitt, Daryl T. Lawton, David M. Chelberg, Philip C. Nelson","This paper develops a theory for path planning and following using visual landmark recognition for the representation of environmental locations. It encodes local perceptual knowledge in structures called viewframes and orientation regions. Rigorous representations of places as visual events are developed in a uniform framework that smoothly integrates a qualitative version of path planning with inference over traditional metric representations. Paths in the world are represented as sequences of sets of landmarks, viewframes, orientation boundary crossings, and other distinctive visual events. Approximate headings are computed between viewframes that have lines of sight to common landmarks. Orientation regions are range-free, topological descriptions of place that are rigorously abstracted from viewframes. They yield a coordinate-free model of visual landmark memory that can also be used for path planning and following. With this approach, a robot can opportunistically observe and execute visually cued ""shortcuts"".",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-123.pdf,
123,1987,Robotics,Insertions Using Geometric Analysis and Hybrid Force-Position Control on a PUMA 560 with VAL II,David R. Strip,Automatic programming of insertions is an essential step in achieving a truly flexible manufacturing environment. We present techniques based on active compliance implemented with hybrid force-position control capable of inserting a wide variety of shaped pegs. These techniques provide a significant step towards an automatically programmed flexible manufacturing environment.,https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-124.pdf,
124,1987,Vision,The Sensitivity of Motion and Structure Computations,"John L. Barron, Allan D. Jepson, John K. Tsotsos","We address the problem of interpreting image velocity fields generated by a moving monocular observer viewing a stationary environment under perspective projection to obtain 3-D information about the relative motion of the observer (egomotion) and the relative depth of environmental surface points (environmental layout). The algorithm presented in this paper involves computing motion and structure from a spatio-temporal distribution of image velocities that are hypothesized to belong to the same 3-D planar surface. However, the main result of this paper is not just another motion and structure algorithm that exhibits some novel features but rather an extensive error analysis of the algorithm’s preformance for various types of noise in the image velocities. Waxman and Ullman [83] have devised an algorithm for computing motion and structure using image velocity and its 1st and 2d order spatial derivatives at one image point. We generalize this result to include derivative information in time as well. Further, we show the approximate equivalence of reconstruction algorithms that use only image velocities and those that use one image velocity and its 1st and/or 2nd spatio-temporal derivatives at one image point. The main question addressed in this paper is: ""How accurate do the input image velocities have to be?"" or equivalently, ""How accurate does the input image velocity and its Ist and 2nd order derivatives have to be?"". The answer to this question involves worst case error analysis. We end the paper by drawing some conclusions about the feasibility of motion and structure calculations in general.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-125.pdf,
125,1987,Vision,Using Generic Geometric Models for Intelligent Shape Extraction,"Pascal Fua, Andrew J. Hanson","Object delineation that is based only on low-level segmentation or edge-finding algorithms is difficult because typical edge maps have either too few object edges or too many irrelevant edges, while object-containing regions are generally oversegmented or undersegmented. We correct these shortcomings by using model-based geometric constraints to produce delineations belonging to generic shape classes. Our work thus supplies an essential link between low-level and high-level image-understanding techniques. We show representative results achieved when our models for buildings, roads, and trees are applied to aerial images.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-126.pdf,
126,1987,Vision,Detecting Runways in Aerial Images,"A. Huertas, W. Cole, R. Nevatia","We are pursuing the detection of runways in aerial images as part of a project to automatically map complex cultural areas such as a major commercial airport complex. This task is much more difficult that appears at first. We use a hypothesize and test paradigm. Hypotheses are formed by looking for instances of long rectangular shapes, possibly interrupted by other long rectangles. We use runway markings, mandated by standards for runway construction, to verify our hypotheses.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-127.pdf,
127,1987,Vision,Hypothesis Testing in a Computational Theory of Visual Word Recognition,Jonathan J . Hull,A computational theory of reading and an algorithmic realization of the theory is presented that illustrates the application of the methodology of a computational theory to an engineering problem. The theory is based on past studies of how people read that show there are two steps of visual processing in reading and that these steps are influenced by cognitive processes. This paper discusses the development of a similar set of algorithms. A gross visual description of a word is used to suggest a set of hypotheses about its identity. These then drive further selective analysis of the image that can be altered by knowledge of language characteristics such as syntax. This is not a character recognition algorithm since an explicit segmentation of a word and a recognition of its isolated characters is avoided. This paper presents a unified discussion of this methodology with a concentration on the second stage of selective image analysis. An algorithm is presented that determines the minimum number of tests that have to be programmed under the constraint that the minimum number of tests are to be executed. This is used to compare the proposed technique to a similar character recognition algorithm.,https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-128.pdf,
128,1987,Vision,An Integrated System that Unifies Multiple Shape from Texture Algorithms,"Mark L. Moerdler, Mark L. Moerdler","This paper describes an approach which integrates several conflicting and corroborating shape-from-texture methods in a single system. The system uses a new data structure, the augmented texel, which combines multiple constraints on orientation in a compact notation for a single surface patch. The augmented texels initially store weighted orientation constraints that are generated by the system’s several independent shape-from-texture components. These texture components, which run autonomously and may run in parallel, derive constraints by any of the currently existing shape-from-texture approaches e.g. shape-from-uniform-texel-spacing. For each surface patch the augmented texel then combines the potentially inconsistent orientation data, using a Hough transform-like method on a tesselated gaussian spheres, restthing in an estimate of the most likely orientation for the patch. The system then defines which patches are part of the same surface, simplifing surface reconstruction. This knowledge fusion approach is illustrated by a system that integrates information from two different shape-from-texture methods, shape-from-uniform-texel-spacing and shape-from-uniform-tel-size. The system is demonstrated on camera images of artificial and natural textures.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-129.pdf,
129,1987,Vision,Similitude-lnvariant Pattern Recognition Using Parallel Distributed Processing,K. Prazdny,"Translation-, rotation-, and scale-invariant recognition of multiple, superimposed, partially specified or occluded objects can be accomplished in a fast, simple, distributed and parallel fashion using localizable features with intrinsic orientation. All known objects are recognized, localized, and segmented simultaneously. The method is robust and efficient.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-130.pdf,
130,1987,Vision,Range Image Interpretation of Mail Pieces with Superquadrics,"Franc Solina, Ruzena Bajcsy","Although mail pieces can be classified by shape into parallelopipeds and cylinders, they do not conform exactly to these perfect geometrical shapes due to rounded edges, distorted comers, and bulging sides. Segmentation and classification of mail pieces hence cannot rely on a limited set of specific models. Variations and deformations of shape can be conveniently expressed when using superquadrics. We show how to recover superquadric models for mail pieces and segment the range image at the same time.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-131.pdf,
131,1987,Vision,Closed Form Solution to the Structure from Motion Problem from Line Correspondences,"Minas E. Spetsakis, John (Yiannis) Aloimonos","A theory is presented for the computation of three dimensional motion and structure from dynamic imagery, using only line correspondences. The traditional approach of corresponding microfeatures (interesting points-highlights, corners, high curvature points, etc.) is reviewed and its shortcomings are discussed. Then, a theory is presented that describes a closed form solution to the motion and structure determination problem from line correspondences in three views. The theory is compared with previous ones that are based on nonlinear equations and iterative methods.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-132.pdf,
132,1987,Vision,Bounds on Translational and Angular Velocity Components from First Order Derivatives of Image Flow,Muralidhara Subbarao,"A moving rigid object produces a moving image on the retina of an observer. It is shown that only the first order spatial derivatives of image motion are sufficient to determine (i) the maximum and minimum velocities of the object towards the observer, and (ii) the maximum and minimum angular velocities of the object along the direction of view. The second or higher order derivatives whose estimation is expensive and unreliable are not necessary. (The second order derivatives are necessary to determine the actual motion of the object; many researchers have worked on this problem.) These results are interpreted in the image domain in terms of three differential invariants of the image flow field: divergence, curl, and shear magnitude. In the world domain, the above results are interpreted in terms of the motion and local surface orientation of the object. In particular, the result that the maximum velocity of approach of an object can be determined from only the first order derivatives has a fundamental significance to both biological and machine vision systems. It implies that an organism (or a robot) can quickly respond to avoid collision with a moving object from only coarse information. This capability exists irrespective of the shape or motion of the object. The only restriction is that motion should be rigid.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-133.pdf,
133,1987,Vision,Regularization Uses Fractal Priors,Richard Szeliski,"Many of the processing tasks arising in early vision involve the solution of ill-posed inverse problems. Two techniques that are often used to solve these inverse problems are regularization and Bayesian modeling. Regularization is used to find a solution that both fits the data and is also sufficiently smooth. Bayesian modeling uses a statistical prior model of the field being estimated to determine an optimal solution. One convenient way of specifying the prior model is to associate an energy function with each possible solution, and to use a Boltzmann distribution to relate the solution energy to its probability. This paper shows that regularization is an example of Bayesian modeling, and that using the regularization energy function for the surface interpolation problem results in a prior model that is fractal (self-affine over a range of scales). We derive an algorithm for generating typical (fractal) estimates from the posterior distribution. We also show how this algorithm can be used to estimate the uncertainty associated with a regularized solution, and how this uncertainty can be used at later stages of processing.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-134.pdf,
134,1987,Vision,Energy Constraints on Deformable Models: Recovering Shape and Non-Rigid Motion,"Demetri Terzopoulos, Andrew Witkin, Michael Kass","We propose a paradigm for shape and motion reconstruction based on dynamic energy constraints. Objects are modeled as deformable elastic bodies and constraints derived from image data are modeled as external forces applied to these bodies. The external constraint forces are designed to mold a deformable body into a configuration that satisfies the constraints, making the model consistent with the images. We present a particular shape model whose internal forces induce a preference for surface continuity and axial symmetry. We develop a constraint force for dynamic stereo images and present results for the recovery of shape and non-rigid motion from natural imagery.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-135.pdf,
135,1987,Vision,Shadow Stereo--Locating Object Boundaries Using Shadows,"William B. Thompson, Michael T. Checky, William F. Kaemmerer",Shadows are a useful source of information about object structure. Shadows cast under oblique lighting often indicate the location of the silhouette of an object. This paper describes a method for reliably detecting shadow edges corresponding to object edges. It is able to distinguish between detected edges due to shadows and those due to surface markings. The basis of the technique is to observe the differences in shadows due to changes in the direction of illumination. Analysis is further aided by a simple stereo technique that does not require a solution to the general correspondence problem. Both the multi-light source and multi-camera methods can be implemented in an extremely efficient manner.,https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-136.pdf,
136,1987,Vision,Perceptual Significance Hierarchy: A Computer Vision Theory for Color Separation,"Deborah Walters, Ganapathy Krishnan",A Perceptual Significance Hierarchy (PSH) for line art images is developed which represents the relative perceptual significance of each image component. This is possible through the use of a set of image-features which are used by the human visual system. The PSH and related rho-space computer vision algorithms can be used to automate the fake color separation process used by the printing industry. This is accomplished by adding rudimentary visual processing capabiliities to a computer graphics system.,https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-137.pdf,
137,1987,Vision,Visual Estimation of 3-D Line Segments from Motion--A Mobile Robot Vision System,William M. Wells III,"An efficient technique is presented for detecting, tracking and locating three-dimensional (3-D) line segments. The utility of this technique has been demonstrated by the SRI mobile robot, which uses it to locate features in an office environment in real time (one Hz frame rate). A formulation of Structure-from- Motion using line segments is described. The formulation uses longitudinal as well as transverse information about the endpoints of image line segments. Although two images suffice to form an estimate of a world line segment, more images are used here to obtain a better estimate. The system operates in a sequential fashion, using prediction-based feature detection to eliminate the need for global image processing.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-138.pdf,
138,1987,"ExpertSystems",Data Validation during Diagnosis: A Step beyond Traditional Sensor Validation,"B. Chandrasekaran, W. F. Punch III","A well known problem in diagnosis is the difficulty of providing correct diagnostic conclusions in light incorrect or missing data. Traditional approaches to solving this problem, as typified in the domains of various complex mechanical systems, validate data by using various kinds of redundancy in sensor hardware. While such techniques are useful, we propose that another level of redundancy exists beyond the hardware level, the redundancy provided by expectations derived during diagnosis. That is, in the process of exploring the space of possible malfunctions, initial data and intermediate conclusions set up expectations of the characteristics of the final answer. These expectations then provide a basis for judging the validity of the derived answer. We will show how such expectation- based data validation is a natural part of diagnosis as performed by hierarchical classification expert systems.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-139.pdf,
139,1987,"ExpertSystems",MU: A Development Environment for Prospective Reasoning Systems,"Paul R. Cohen, Michael Greenberg, Jefferson DeLisio","We describe a style of problem solving, prospective reasoning, and a development environment, MU, for building prospective reasoning systems. Prospective reasoning is a form of planning in which knowledge of the state of the world and the effects of actions is incomplete. We illustrate one implementation of prospective reasoning in MU with examples from medical diagnosis.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-140.pdf,
140,1987,"ExpertSystems",Diagnostic Improvement through Qualitative Sensitivity Analysis and Aggregation,Keith L. Downing,"This paper lays the foundation for a diagnostic system that improves its performance by deriving symptom-fault associations from an underlying causal model and then utilizes those relationships to impose further structure upon the ""deep"" model. A qualitative version of sensitivity analysis is introduced to extract the implicit symptom-fault information from a set of local constraints. Parameter aggregation triggered by this new information then simplifies diagnosis by forming a more abstract causal representation. The resulting diagnostician thus employs both an experiential and a first-principle approach, where in this case ""experiences"" are compiled directly from first-principles. Key issues include the roles of knowledge compilation and abstraction in refining qualitative models of physical systems.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-141.pdf,
141,1987,"ExpertSystems",CAMEX--An Expert System for Process Planning on CNC Machines,"O. Eliyahu, L. Zaidenberg, M. Ben-Bassat","CAMEX is an expert system designed to plan machining processes for CNC (Computerized Numerical Control) cutting machines. At the present state of development it is constrained to parts for which 2 l/2 D description is sufficient. For this kinds of parts, CAMEX is able to read a drawing of a workpiece from an ordinary CAD file, to understand its 3-dimensional structure and generate a plan for producing the workpiece. CAMEX is implemented in FRANZ LISP on an APOLLO workstation.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-142.pdf,
142,1987,"ExpertSystems",A Multiple Representation Approach to Understanding the Time Behavior of Digital Circuits,"Robert J. Hall, Richard H. Lathrop, Robert S. Kirk","We put forth a multiple representation approach to deriving the behavioral model of a digital circuit automatically from its structure and the behavioral simulation models of its components. One representation supports temporal reasoning for composition and simplification, another supports simulation, and a third helps to partition the translation problem. A working prototype, FUNSTRUX, is described.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-143.pdf,
143,1987,"ExpertSystems",KADBASE--A Prototype Expert System-Database Interface for Integrated CAE Environments,"H. Craig Howard, Daniel R. Rehak","Database management systems (DBMSs) are important components of existing integrated computer-aided engineering (CAE) systems. Expert systems (ESs) are being applied to a broad range of engineering problems. However, most of the prototype expert system applications have been restricted to limited amounts of data and have no facility for sophisticated data management. KADBASE is a flexible, knowledge-based interface in which multiple expert systems and multiple databases can communicate as independent, self-descriptive components within an integrated, distributed engineering computing environment.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-144.pdf,
144,1987,"ExpertSystems",An Automated Reasoning Technique for Providing Moment-by-Moment Advice Concerning the Operation of a Process,"William F. Kaemmerer, James R. Allard","A system for continuously providing advice about the operation of some other device or process, rather than just problem diagnoses, must not only function in real time, but also cope with dynamic problem courses. The reasoning technique underlying such a system must not assume that faults have single causes, that queries to the user will be answered and advice to the user will be followed, nor that aspects of a problem, once resolved, will not reoccur. This paper presents a reasoning technique that can be used in conjunction with an inference engine to model the state of a problem situation throughout the entire problem-handling process, from discovery to final resolution. The technique has been implemented and installed on-line in a factory control room, as part of a real time expert system for advising the operators of a manufacturing process.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-145.pdf,
145,1987,"ExpertSystems",TEST: A Model-driven Application Shell,"Gary S. Kahn, AI Kepner, Jeff Pepper","TEST (Troubleshooting Expert System Tool) is an application shell that provides a domain-independent diagnostic problem solver together with a library of schematic prototypes. TEST fills a design niche halfway between rule-based and causal-model approaches. This approach has resulted in a design that meets several functional requirements for an effective troubleshooting shell. Most critically, TEST can represent both the impact of failure-modes on a machine or system of interest, as well as the heurist!c problem-solving behavior which can lead to rapid conclusions. This paper provides an overview of TEST’s approach to diagnosis. As a special purpose application shell, TEST provides considerably more leverage to developers than can be gained through the use of general purpose heuristic classification systems.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-146.pdf,
146,1987,"ExpertSystems",Script-based Reasoning for Situation Monitoring,"Sharon J. Laskowski, Emily J. Hofmann","An expert system that monitors complex activity requires knowledge that is difficult to capture with standard rule-based representations. The focus of this research has been to design and implement script-based reasoning techniques integrated into a rule-based expert system for situation monitoring to address this problem. The resulting expert system, SCripted ANalyst (SCAN), for battlefield monitoring has the capability of reasoning about tactical situations as they develop and providing plausible explanations of activities as inferred from intelligence reports. Sequences of events are monitored through the use of script templates which are matched against events and the time relations between events. SCAN detects causal relations between events, generates multiple hypotheses, fills in information gaps, and sets up expectations about time-dependent events--all features a simple rule-based expert system cannot easily provide.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-147.pdf,
147,1987,"ExpertSystems",Assessing the Maintainability of XCON-in-RlME: Coping with the Problems of a VERY Large Rule-Base,"Elliot Soloway, Judy Bachant, Keith Jensen","XCON is a rule-based expert system that configures computer systems. Over 7 years, XCON has grown to 6,200 rules, of which approximately 50% change every year. While the performance of XCON is satisfactory, it is increasingly becoming more difficult to change. With the goal of facilitating maintenance, DEC has developed a new rule-based language, RIME, in which the successor to XCON, XCON-in-RIME, is being written. This paper evaluates the potential for enhanced maintainability of XCON-in-RIME over XCON.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-148.pdf,
148,1987,"ExpertSystems",Design as Refinement Plus Constraint Propagation: The VEXED Experience,Louis I. Steinberg,"Underlying any system that does design is a model of the design process and a division of labor between the system and the user. We are just beginning to understand what the main alternative models are, what their strengths and weaknesses are, and for which domains and tasks each is appropriate. The research reported here is an attempt to further that understanding by studying a particular model, the model of design as top down refinement plus constraint propagation, with the user making control decisions and the system carrying them out. We have studied this model by embodying it in VEXED, a design aid for NMOS digital circuits, and by experimenting with this system. Our primary conclusion is that this model needs further elaboration, but seems like a good basic model on which to build such systems.",https://aaai.org/Library/AAAI/1987/../../../Papers/AAAI/1987/AAAI87-149.pdf,
