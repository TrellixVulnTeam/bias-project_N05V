,conference_year,category,title,author,abstract,download_url,keywords
0,1991,"Case-BasedReasoning",Rules and Precedents as Complementary Warrants,"L. Karl Branting, Bruce W. Porter","This paper describes a model of the complementarity of rules and precedents in the classification task. Under this model, precedents assist rule-based reasoning by operationalizing abstract rule antecedents. Conversely, rules assist case-based reasoning through case elaboration, the process of inferring case facts in order to increase the similarity between cases, and term reformulation, the process of replacing a term whose precedents only weakly match a case with terms whose precedents strongly match the case. Fully exploiting this complementarity requires a control strategy characterized by impartiality, the absence of arbitrary ordering restrictions on the use of rules and precedents. An impartial control strategy was implemented in GREBE in the domain of Texas worker’s compensation law. In a preliminary evaluation, GREBE’s performance was found to be as good or slightly better than the performance of law students on the same task.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-001.pdf,
1,1991,"Case-BasedReasoning",An Indexing Vocabulary for Case-Based Explanation,David B. Leake,"The success of case-based reasoning depends on effective retrieval of relevant prior cases. If retrieval is expensive, or if the cases retrieved are inappropriate, retrieval and adaptation costs will nullify many of the advantages of reasoning from prior experience. We propose an indexing vocabulary to facilitate retrieval of explanations in a case-based explanation system. The explanations we consider are explanations of anomalies (conflicts between new situations and prior expectations or beliefs). Our vocabulary groups anomalies according to the type of information used to generate the expectations or beliefs that failed, and according to how the expectations failed. We argue that by using this vocabulary to characterize anomalies, and retrieving explanations that were built to account for similarly-characterized past anomalies, a case-based explanation system can restrict retrieval to explanations likely to be relevant. In addition, the vocabulary can be used to organize general explanation strategies that suggest paths for explanation in novel situations.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-002.pdf,
2,1991,"Case-BasedReasoning",Indexing Stories as Social Advice,Eric A. Domeshek,"This paper reports on an indexing system supporting retrieval of past cases as advice about everyday social problems; it has been implemented in the Abby lovelorn advising system. Two points are emphasized: (1) indices are descriptions of problems and their causes, couched in a vocabulary centered on intentional causality, and (2) indices fit a fixed format that allows reification of identity and thematic relationships as features. Abby answers several of the central questions that any indexing system must address, and has advantages over less restrictive systems.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-003.pdf,
3,1991,"Case-BasedReasoning",Improving Rule-Based Systems Through Case-Based Reasoning,"Andrew R. Golding, Paul S. Rosenbloom","A novel architecture is presented for combining rule-based and case-based reasoning. The central idea is to apply the rules to a target problem to get a first approximation to the answer; but if the problem is judged to be compellingly similar to a known exception of the rules in any aspect of its behavior, then that aspect is modelled after the exception rather than the rules. The architecture is implemented for the full-scale task of pronouncing surnames. Preliminary results suggest that the system performs almost as well as the best commercial systems. However, of more interest than the absolute performance of the system is the result that this performance was better than what could have been achieved with the rules alone. This illustrates the capacity of the architecture to improve on the rule-based system it starts with. The results also demonstrate a beneficial interaction in the system, in that improving the rules speeds up the case-based component.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-004.pdf,
4,1991,"Case-BasedReasoning",The Roles of Adaptation in Case-Based Design,"Thomas R. Hinrichs, Janet L. Kolodner","Many design tasks have search spaces that are vague and evaluation criteria that are subjective. We present a model of design that can solve such problems using a method of plausible design adaptation. In our approach, adaptation transformations are used to modify the components and structure of a design and constraints on the design problem. This adaptation process plays multiple roles in design: 1) It is used as part of case-based reasoning to modify previous design cases. 2) It accommodates constraints that arrive late in the design process by adapting previous decisions rather than by retracting them. 3) It resolves impasses in the design process by weakening preference constraints. This model of design has been implemented in a computer program called JULIA that designs the presentation and menu of a meal to satisfy multiple, interacting constraints.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-005.pdf,
5,1991,"Case-BasedReasoning",Prototype-Based Reasoning: An Integrated Approach to Solving Large Novel Problems,"Shankar A. Rajamoney, Hee-Youn Lee","Two important computational approaches to problem solving are model-based reasoning (MBR) and case-based reasoning (CBR). MBR, since it reasons from first principles, is especially suited for solving novel problems. CBR, since it reasons from previous experience, is especially suited for solving frequently encountered problems. However, large novel problems pose difficulties for both approaches-MBR rapidly grows intractable and CBR fails to find a relevant previous case. In this paper we describe an approach called prototype- based reasoning that integrates both approaches to solve such problems. Prototype-based reasoning treats a large novel problem as a novel combination of several familiar subproblems. It uses CBR to find and solve the subproblems, formulates a new problem by combining these individual solutions, and uses MBR to solve this new problem. We demonstrate the effectiveness of this method on several examples involving the causal simulation of complex electronic circuits.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-006.pdf,
6,1991,"Communicationand Cooperation",Student Modelling with Confluences,"Daniel Baril, Jim E. Greer, Gordon I. McCalla","Student modelling is not typically concerned with representing the deep mental models a student employs in dealing with the world around him/her. In this research we discuss an intelligent tutoring system, PRESTO, whose goal is to understand the mental model a student has of a physical device, and then use this mental model in providing help to overcome misunderstandings related to the functioning of the device. The mental model is extracted from the student by asking questions about the relationships of variables affecting the physics of the device. The mental model is represented using deKleer and Brown’s qualitative confluence equations. The mental model can then be compared to a set of confluences representing a correct perspective on how the device functions. A variety of pedagogical choices can be made: to explain contradictions implicit in the student’s understanding of the device, to show the student a simpler physical device that by analogy illustrates anomalies in the student’s understanding, or to let the student witness his/her version of the device in operation so the misunderstandings become obvious. Experiments in running PRESTO with a number of students shows this approach to mental modelling to be promising.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-007.pdf,
7,1991,"Communicationand Cooperation",FITS: A Fraction Intelligent Tutoring System,Hyacinth S. Nwana,"This paper presents FITS - an Intelligent Tutoring System (ITS) for the domain of addition of fractions. It was developed with the aim of improving on many of the shortcomings of existing tutors in the mathematical domain. The paper largely describes its functioning. In order to give the reader a better 'feel' of the tutor’s capabilities than obtained from its description, an actual student-tutor protocol extract is given. More significantly the tutor has also been evaluated in several ways with seemingly very encouraging results so far; however, due to length restrictions they are not reported in this paper. The paper concludes by briefly highlighting some of FITS’s improved features over other existing tutors in the domain as well as some of its shortcomings.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-008.pdf,
8,1991,"Communicationand Cooperation",Teaching Diagnostic Skills Using AI: An Architecture Suitable for Students and Teachers,Joël Courtois,"This paper shows how a new approach in the use of AI techniques has been successfully used for the design of an effective ITS in the domain of diagnosis training. The originality of this approach was to take into account three complex problems simultaneously: teaching diagnosis methods to students, giving the means to the teachers of maintaining the system by themselves and providing a tool easy to insert in the context of university laboratories. The architecture of the system is based on a distinct use of two kinds of knowledge representation. All the knowledge liable to modifications is gathered within libraries under descriptive forms easily maintained by the educational staff. General diagnosis knowledge independent of hardware, circuits and even application fields, is described with basic production rules and control metarules. The development of the system was based on the precise analysis of the expert’s behaviour and of the user’s needs, with the aim of making extensive use of the descriptive forms in order to minimize the static knowledge embedded in the rules. The system can work on a microcomputer and is used in an engineering school.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-009.pdf,
9,1991,"Communicationand Cooperation",Planning Multimedia Explanations Using Communicative Acts,Mark T. Maybury,"A number of researchers have investigated the use of plan-based approaches to generate textual explanations (e.g., Appelt 1985; Hovy 1988; Moore 1989; Maybury 1990b). This paper extends this approach to generate multimedia explanations by defining three types of communicative acts: linguistic acts (illocutionary and locutionary speech acts), visual acts (e.g., deictic acts), and media-independent rhetorical acts (e.g., identify, describe). This paper formalizes several of these communicative acts as operators in the library of a hierarchical planner. A computational implementation is described which uses these plan operators to compose route plans in coordinated natural language and graphics in the context of a cartographic information system.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-010.pdf,
10,1991,"Communicationand Cooperation",Making Design Objects Relevant to the Task at Hand,"Gerhard Fischer, Kumiyo Nakakoji","Many problem-solving approaches are based on the assumption that a problem can be precisely defined before it is solved. These approaches are inadequate for dealing with ill-defined problems, which require the coevolution of problem setting and problem solving. In this paper, we describe integrated, domain-oriented, knowledge-based design environments and their underlying multifaceted architecture. The environments empower humans to cope with ill-defined problems, such as design, by supporting an incremental approach to problem setting and problem solving. We focus on the integration of specification, construction, and a catalog of prestored design objects in those environments. The synergy of integration enables the environments to make those objects relevant to the task at hand. Taking architectural design as a domain to illustrate our approach, we describe an operational, prototype system (CATALOGEXPLORER) that assists designers in locating examples in the catalog that are relevant to the task at hand as articulated by a partial specification and a partial construction. Users are thereby relieved of the task of forming queries and navigating in information spaces.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-011.pdf,
11,1991,"Communicationand Cooperation",A Tool for Achieving Consensus in Knowledge Representation,"Loren G. Terveen, David A. Wroblewski","We develop the notion that knowledge editing is a cooperative activity that requires knowledge editors to reach consensus as they represent information in a knowledge base. We describe an intelligent knowledge editing tool, the HITS Knowledge Editor, and illustrate how it assists knowledge editors in reaching consensus.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-012.pdf,
12,1991,"Communicationand Cooperation",Piction: A System That Uses Captions to Label Human Faces in Newspaper Photographs,Rohini K. Srihari,"It is often the case that linguistic and pictorial information are jointly provided to communicate information. In situations where the text describes salient aspects of the picture, it is possible to use the text to direct the interpretation (i.e., labelling objects) in the accompanying picture. This paper focuses on the implementation of a multi-stage system PICTION that uses captions to identify humans in an accompanying photograph. This provides a computationally less expensive alternative to traditional methods of face recognition. It does not require a pre-stored database of face models for all people to be identified. A key component of the system is the utilisation of spatial constraints (derived from the caption )in order to reduce the number of possible labels that could be associated with face candidates (generated by a face locator). A rule-based system is used to further reduce this number and arrive at a unique labelling. The rules employ spatial heuristics as well as distinguishing characteristics of faces (e.g., male versus female). The system is noteworthy since a broad range of AI techniques are brought to bear (ranging from natural-language parsing to constraint satisfaction and computer vision).",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-013.pdf,
13,1991,"Communicationand Cooperation",Generating Interactive Explanations,Alison Cawsey,"Existing approaches to text generation fail to consider how interactions with the user may be managed within a coherent explanation or description. This paper presents an approach to generating such interactive explanations based on two levels of discourse planning - content planning and dialogue planning. The system developed allows aspects of the changing context to be monitored with an explanation, and the developing explanation to depend on this changing context. Interruptions from the user are allowed and dealt with (and resumed from) within the context of that explanation.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-014.pdf,
14,1991,"Communicationand Cooperation",Automatic Generation of Formatted Text,"Eduard H. Hovy, Yigal Arens","Few texts longer than a paragraph are written without appropriate formatting. To ensure readability, automated text generation programs must not only plan and generate their texts but be able to format them as well. We describe how work on the automated planning of multisentence text and on the display of information in a multimedia system led to the insight that text formatting devices such as footnotes, italicized regions, enumerations, etc., can be planned automatically by a text structure planning process. This is achieved by recognizing that each formatting device fulfills a specific communicative function in a text, and that such functions can be defined in terms of the text structure relations used as plans in a text planning system. An example is presented in which a text is planned from a semantic representation to a final form that includes English sentences and LATEX formatting commands, intermingled as appropriate.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-015.pdf,
15,1991,"Communicationand Cooperation",Generating Adjectives to Express the Speaker’s Argumentative Intent,Michael Elhadad,"We address the problem of generating adjectives in a text generation system. We distinguish between usages of adjectives informing the hearer of a property of an object and usages expressing an intention of the speaker, or an argumentative orientation. For such argumentative usages, we claim that a generator cannot simply map from information in the knowledge base to adjectives. Instead, we identify various knowledge sources necessary to decide whether to use an adjective, what adjective should be selected and what syntactic function it should have. We show how these decisions interact with lexical properties of adjectives and the syntax of the clause. We propose a mechanism for adjective selection and illustrate it in the context of the eXpktnatiOn component of the ADVISOR expert system. We describe an implementation of adjective selection using a version of Functional Unification Grammars.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-016.pdf,
16,1991,"Communicationand Cooperation",Interpreting Prepositions Physically,"J. K. Kalita, N. I. Badler","We develop representations for locative and path specifying prepositions emphasizing the implementability of the underlying semantic primitives. Our primitives pertain to mechanical characteristics such as geometric relationships among objects, kinematic or motional characteristics implied by prepositions. The representation along with representation for action verbs along similar lines, have been used to successfully animate the performance of tasks underlying natural language imperatives by ""human"" agents.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-017.pdf,
17,1991,"Communicationand Cooperation",Semantics-First Natural Language Processing,Steven L. Lytinen,"There is no consensus on how syntax and semantic/pragmatics should interact in natural language processing. This paper focuses on one issue concerning interaction: order of processing. Two approaches are compared empirically: an interleaved syntax-first approach, in which semantic interpretations is performed at intermediate points during parsing; and a semantics-first approach, in which semantic considerations drive the rule selection process during parsing. The study provides empirical evidence that the semantics-first approach is more efficient than the syntax-first approach in processing texts in narrow domains.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-018.pdf,
18,1991,"Communicationand Cooperation",A Cognitively Plausible Approach to Understanding Complex Syntax,"Claire Cardie, Wendy Lehnert","This paper describes a cognitively plausible mechanism for systematically handling complex syntactic constructions within a semantic parser. More specifically, we show how these constructions are handled without a global syntactic grammar or syntactic parse tree representations and without sacrificing the benefits of semantically-oriented parsing. We evaluate the psychological validity of our architecture and conclude that it is a plausible computational model of human processing for an important class of embedded clause constructions. As a result, we achieve robust sentence processing capabilities not found in other parsers of its class.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-019.pdf,
19,1991,"Communicationand Cooperation",From Syntax to Meaning in Natural Language Processing,Alexander G. Hauptmann,"The development of larger scale natural language systems has been hampered by the need to manually create mappings from syntactic structures into meaning representations. A new approach to semantic interpretation is proposed, which uses partial syntactic structures as the main unit of abstraction for interpretation rules. This approach can work for a variety of syntactic representations corresponding to directed acyclic graphs. It is designed to map into meaning representations based on frame hierarchies with inheritance. We define semantic interpretation rules in a compact format. The format is suitable for automatic rule extension or rule generalization, when existing hand-coded rules do not cover the current input. Furthermore, automatic discovery of semantic interpretation rules from input/output examples is made possible by this new rule format. The principles of the approach are validated in a comparison to other methods on a separately developed domain. Instead of relying purely on painstaking human effort, this paper combines human expertise with computer learning strategies to successfully overcome the bottleneck of semantic interpretation.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-020.pdf,
20,1991,"Communicationand Cooperation",Tense Interpretation in the Context of Narrative,"Fei Song, Robin Cohen","This paper presents an algorithm which makes use of tense interpretation to determine the intended temporal ordering between the states and events mentioned in a narrative. This is done by maintaining a temporal focus and interpreting the tense of each new statement of the narrative with respect to this focus. In particular, we propose heuristics for determining the temporal ordering and constraints for characterizing coherent tense sequences. The algorithm is further defended through experiments with naturally occurring examples.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-021.pdf,
21,1991,"Communicationand Cooperation",ULINK: A Semantics-Driven Approach to Understanding Ungrammatical Input,"Jeffrey D. Kirtner, Steven L. Lytinen","This paper describes ULINK, a program designed to understand ungrammatical input. While most previous work in the field has relied on syntactic techniques or sublanguage analysis to parse grammatical errors, ULINK uses a semantics-driven algorithm to process such input. The paper gives a brief overview of LINK, the unification-based system upon which ULINK is built; special attention is given to those aspects of link which allow ULINK to use semantics to process ill-formed input. The details of ULINK’s algorithm are then discussed by considering two examples. The paper concludes with a discussion of related research and problems which remain to be solved.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-022.pdf,
22,1991,"Communicationand Cooperation",A Tabular Method for Island-Driven Context-Free Grammar Parsing,"Giorgio Satta, Oliviero Stock","Island-driven parsing is of great relevance for speech recognition/understanding and other natural language processing applications. A bidirectional algorithm is presented that efficiently solves this problem, allowing both any possible determination of the starting words in the input sentence and flexible control. In particular, a mixed bottom-to-top and top-down approach is followed, without leading to redundant partial analyses. The algorithm performance is discussed.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-023.pdf,
23,1991,"Communicationand Cooperation",High Performance Memory-Based Translation on IXM2 Massively Parallel Associative Memory Processor,"Hiroaki Kitano, Tetsuya Higuchi","This paper reports experimental results of a high performance (real-time) memory-based translation. Memory-based translation is a new approach to machine translation which uses examples, or cases, of past translations to carry out translation of sentences. This idea is counter to traditional machine translation systems which rely on extensive use of rules in parsing, transfer and generation. Although, there are some preliminary reports on the superiority of the memory-based translation in terms of its scalability, quality of translation, and easiness of grammar writing, we have not seen any reports on its performance. This is perhaps, the first report discussing the feasibility and problems of the approach based on actual massively parallel implementation using real data. We also claim that the architecture of the IXM2 associative processor is highly suitable for memory-based translation tasks. Parsing performance of the memory-based translation system attained a few milliseconds per sentence.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-024.pdf,
24,1991,"Communicationand Cooperation",Disambiguation of Prepositional Phrases in Automatically Labelled Technical Text,"Lois Boggess, Rajeev Agarwal, Ron Davis","A system is described for semi-automatically tagging a large body of technical English with domain-specific syntactic/semantic labels. These labels have been used to disambiguate prepositional phrase attachments for a lO,OOO-word body of text containing more than 1,000 prepositions, and to proxide case role information for about half of the phrases.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-025.pdf,
25,1991,"Communicationand Cooperation",A Probabilistic Model of Plan Recognition,"Eugene Charniak, Robert Goldman","Plan-recognition requires the construction of possible plans which could explain a set of observed actions, and then selecting one or more of them as providing the best explanation. In this paper we present a formal model of the latter process based upon probability theory. Our model consists of a knowledge-base of facts about the world expressed in a first-order language, and rules for using that knowledge-base to construct a Bayesian network. The network is then evaluated to find the plans with the highest probability.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-026.pdf,
26,1991,"Communicationand Cooperation",The Utility of Communication in Coordinating Intelligent Agents,"Piotr J. Gmytrasiewicz, Edmund H. Durfee, David K. Wehe","When intelligent agents who have different knowledge and capabilities must work together, they must communicate the right information to coordinate their actions. Developing techniques for deciding what to communicate, however, is problematic, because it requires an agent to have a model of a message recipient and to infer the impact of a message on the recipient based on that model. We have developed a method by which agents build recursive models of each other, where the models are probabilistic and decision-theoretic. In this paper, we show how an agent can compute the impact of a message in terms of how it increases (or decreases) its expected utility. By treating the imperfect communication channel probabilistically, our method allows agents to account for risk in committing to nonintuitive courses of action, and to compute the utility of acknowledging messages.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-027.pdf,
27,1991,"Communicationand Cooperation",The Clarke Tax as a Consensus Mechanism Among Automated Agents,"Eithan Ephrati, Jeffrey S. Rosenschein","When autonomous agents attempt to coordinate action, it is often necessary that they reach some kind of consensus. Reaching such a consensus has traditionally been dealt with in the Distributed Artificial Intelligence literature via the mechanism of negotiation. Another alternative is to have agents bypass negotiation by using a voting mechanism; each agent expresses its preferences, and a group choice mechanism is used to select the result. Some choice mechanisms are better than others, and ideally we would like one that cannot be manipulated by an untruthful agent. One such non-manipulable choice mechanism is the Clarke tax [Clarke, 1971]. Though theoretically attractive, the Clarke tax presents a number of difficulties when one attempts to use it in a practical implementation. This paper examines how the Clarke tax could be used as an effective ""preference revealer"" in the domain of automated agents, reducing the need for explicit negotiation.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-028.pdf,
28,1991,"Communicationand Cooperation",The Function of Time in Cooperative Negotiations,"Sarit Kraus, Jonathan Wilkenfeld","Work in distributed artificial intelligence (DAI) has, since its earliest years, been concerned with negotiation strategies. which can be used in building agents that are able to communicate to reach mutually beneficial agreements. In this paper we suggest a strategic model of negotiation that takes the passage of time during the negotiation process itself into consideration. Changes in the agent’s preferences over time will change their strategies in the negotiation and, as a result the agreements they are willing to reach. We will show that in this model the delay in reaching agreements can be avoided.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-029.pdf,
29,1991,"Communicationand Cooperation",A Dynamic Organizational Architecture for Adaptive Problem Solving,"Les Gasser, Toru Ishida","There is an essential correspondence between the architecture of a distributed problem-solving system, the structure of the problems it solves, and the environmental conditions under which it solves them. In a dynamic world, such as one populated by multiple agents in a changing environment, this correspondence must be maintained by dynamic adaptation. There are four ways to disrupt or to maintain this correspondence: alter the structure of problems, the environmental conditions, the problem-solving architecture, or the goal-knowledge-action relationships (e.g., task and skill allocations or types of knowledge).",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-030.pdf,
30,1991,"Communicationand Cooperation",Sophisticated Cooperation in FA/C Distributed Problem Solving Systems,"Norman Carver, Zarko Cvetanovic, Victor Lesser","In the functionally accurate, cooperative (FA/C) distributed problem solving paradigm, agents exchange tentative and partial results in order to converge on correct solutions. The key questions for FA/C problem solving are: how should cooperation among agents be structured and what capabilities are required in the agents to support the desired cooperation. To date, the FA/C paradigm has been explored with agents that did not have sophisticated evidential reasoning capabilities. We have implemented a new framework in which agents maintain explicit representations of the reasons why their hypotheses are uncertain and explicit representations of the state of the actions being taken to meet their goals. In this paper, we will show that agents with more sophisticated models of their evidence and their problem solving states can support the complex, dynamic interactions between agents that are necessary to fully implement the FA/C paradigm. Our framework makes it possible for agents to have directed dialogues among agents for distributed differential diagnosis, make use of a variety of problem solving methods in response to changing situations, transmit information at different levels of detail, and drive local and global problem solving using the notion of the global consistency of local solutions. These capabilities have not been part of previous implementations of the FA/C paradigm.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-031.pdf,
31,1991,"Communicationand Cooperation",Combining Specialized Reasoners and General Purpose Planners: A Case Study,"Subbarao Kambhampati, Mark Cutkosky, Marty Tenenbaum, Soo Hong Lee","Many real-world planning problems involve substantial amounts of domain-specific reasoning that is either awkward or inefficient to encode in a general purpose planner. Previous approaches for planning in such domains have either been largely domain specific or have employed shallow models of the domain-specific considerations. In this paper we investigate a hybrid planning model that utilizes a set of specialists to complement both the overall expressiveness and the reasoning power of a traditional hierarchical planner. Such a model retains the flexibility and generality of classical planning framework while allowing deeper and more efficient domain-specific reasoning through specialists. We describe a preliminary implementation of a planning architecture based on this model in a manufacturing planning domain, and use it to explore issues regarding the effect of the specialists on the planning, and the interactions and interfaces between them and the planner.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-032.pdf,
32,1991,"Communicationand Cooperation",Toward an Intelligent Agent Framework for Enterprise Integration,"Jeff Y-C Pan, Jay M. Tenenbaum","We propose a software framework for integrating people and computer systems in large, geographically dispersed manufacturing enterprises. Underlying the framework is an enterprise model that is built by dividing complex business processes into elementary tasks or activities. Each such task is then modeled in cognitive terms (e.g., what to look for, what to do, who to tell), and entrusted to an Intelligent Agent (IA) for execution. The IAs interact with each other directly via a message bus, or through a shared, distributed knowledge base. They can also interact with humans through personal assistants (PAS), a special type of IA that knows how to communicate with people through multi-media interfaces. Preliminary experimental results suggest that this model-based, man-machine approach provides a viable path for applying DAI to realworld enterprises.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-033.pdf,
33,1991,"ConstraintReasoning and Component Technologies",Conditional Existence of Variables in Generalized Constraint Networks,"James Bowen, Dennis Bahler","Classical constraint systems require that the set of variables which exist in a problem be known ab initio. However, there are some applications in which the existence of certain variables is dependent on conditions whose truth or falsity can only be determined dynamically. In this paper, we show how this conditional existence of variables can be handled in a mathematically well-founded fashion by viewing a constraint network as a set of sentences in free logic. Based on these ideas, we have developed, implemented and applied, a constraint language in which any sentence in full first-order free logic, about a many-sorted universe of discourse which subsumes R, is a well-formed constraint.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-034.pdf,
34,1991,"ConstraintReasoning and Component Technologies",Arc-Consistency in Dynamic Constraint Satisfaction Problems,Christian Bessiere,"Constraint satisfaction problems (CSPs) provide a model often used in Artificial Intelligence. Since the problem of the existence of a solution in a CSP is an NP-complete task, many filtering techniques have been developed for CSPs. The most used filtering techniques are those achieving arc-consistency. Nevertheless, many reasoning problems in AI need to be expressed in a dynamic environment and almost all the techniques already developed to solve CSPs deal only with static CSPs. So, in this paper, we first define what we call a dynamic CSP, and then, give an algorithm achieving arc-consistency in a dynamic CSP. The performances of the algorithm proposed here and of the best algorithm achieving arc-consistency in static CSPs are compared on randomly generated dynamic CSPs. The results show there is an advantage to use our specific algorithm for dynamic CSPs in almost all the cases tested.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-035.pdf,
35,1991,"ConstraintReasoning and Component Technologies",Eliminating Interchangeable Values in Constraint Satisfaction Problems,Eugene C. Freuder,"Constraint satisfaction problems (CSPs) involve finding values for variables subject to constraints on which combinations of values are permitted. This paper develops a concept of interchangeability of CSP values. Fully interchangeable values can be substituted for one another in solutions to the problem. Removing all but one of a set of fully interchangeable values can simplify the search space for the problem without effectively losing solutions. Refinements of the interchangeability concept extend its applicability. Basic properties of interchangeablity and complexity parameters are established. A hierarchy of local interchangeability is defined that permits recognition of some interchangeable values with polynomial time local computation. Computing local interchangeability at any level in this hierarchy to remove values before backtrack search is guaranteed to be cost effective for some CSPs. Several forms of weak interchangeability are defined that permit eliminating values without losing all solutions. Interchangeability can be introduced by grouping values or variables, and can be recalculated dynamically during search. The idea of interchangeability can be abstracted to encompass any means of recovering the solutions involving one value from the solutions involving another.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-036.pdf,
36,1991,"ConstraintReasoning and Component Technologies",On Generalized Interval Calculi,Gérard Ligozat,"The calculus of time intervals defined by Allen has been extended in various ways in order to accomodate the need for considering other time objects than convex intervals (eg.time points and intervals, non convex intervals). This paper introduces and investigates the calculus of generalized intervals, which subsumes these extensions, in an algebraic setting. The set of (p,q)-relations, which generalizes the set of relations in the sense of Allen, has both an order structure and an algebraic structure. We show that, as an order, it is a distributive lattice whose properties express the topological properties of the set of (p,q)-relations. We also determine in what sense the algebraic operations of transposition and composition act continuously on this set. In Allen’s algebra, the subset of relations which can be translated into conjunctive constraints on the endpoints using only <, >,=, <_,>_ has special computational significance (the constraint propagation algorithm is complete when restricted to such relations). We give a geometric characterization of a similar subset in the general case, and prove that it is stable under composition. As a consequence of this last fact, we get a very simple explicit formula for the composition of two elements in this subset.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-037.pdf,
37,1991,"ConstraintReasoning and Component Technologies",Integrating Metric and Qualitative Temporal Reasoning,"Henry A. Kautz, Peter B. Ladkin","Research in Artificial Intelligence on constraint-based representations for temporal reasoning has largely concentrated on two kinds of formalisms: systems of simple linear inequalities to encode metric relations between time points, and systems of binary constraints in Allen’s temporal calculus to encode qualitative relations between time intervals. Each formalism has certain advantages. Linear inequalities can represent dates, durations, and other quantitive information; Allen’s qualitative calculus can express relations between time intervals, such as disjointedness, that are useful for constraint-based approaches to planning. In this paper we demonstrate how metric and Allen-style constraint networks can be integrated in a constraint-based reasoning system. The highlights of the work include a simple but powerful logical language for expressing both quantitative and qualitative information; translation algorithms between the metric and Allen sublanguages that entail minimal loss of information; and a constraint-propagation procedure for problems expressed in a combination of metric and Allen constraints.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-038.pdf,
38,1991,"ConstraintReasoning and Component Technologies",Temporal Reasoning During Plan Recognition,"Fei Song, Robin Cohen","This paper presents a strengthened algorithm for temporal reasoning during plan recognition, which improves on a straightforward application of Allen’s reasoning algorithm. This is made possible by viewing plans as both hierarchical structures and temporal networks. As a result, we can show how to use as constraints the temporal relations explicitly given in input to improve the results of plan recognition. We also discuss how to combine the given constraints with those prestored in the system’s plan library to make more specific the temporal constraints indicated in the plans being recognised.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-039.pdf,
39,1991,"ConstraintReasoning and Component Technologies",Metric Constraints for Maintaining Appointments: Dates and Repeated Activities,"Massimo Poesio, Ronald J. Brachman","Reasoning about one’s personal schedule of appointments is a common but surprisingly complex activity. Motivated by the novel application of planning and temporal reasoning techniques to this problem, we have extended the formalization of the temporal distance model of Dechter, Meiri, and Pearl. We have developed methods for using dates as reference intervals and for meeting the challenge of repeated activities, such as weekly recurring appointments.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-040.pdf,
40,1991,"ConstraintReasoning and Component Technologies",Combining Qualitative and Quantitative Constraints in Temporal Reasoning,Itay Meiri,"This paper presents a general model for temporal reasoning, capable of handling both qualitative and quantitative information. This model allows the representation and processing of all types of constraints considered in the literature so far, -including metric constraints (restricting the distance between time points), and qualitative, disjunctive, constraints (specifying the relative position between temporal objects). Reasoning tasks in this unified framework are formulated as constraint satisfaction problems, and are solved by traditional constraint satisfaction techniques, such as backtracking and path consistency. A new class of tractable problems is characterized, involving qualitative networks augmented by quantitative domain constraints, some of which can be solved in polynomial time using arc and path consistency.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-041.pdf,
41,1991,"ConstraintReasoning and Component Technologies",Efficiency of Production Systems When Coupled with an Assumption Based Truth Maintenance System,"Geneviève Morgue, Thomas Chehire","Expert systems in complex domains require rich knowledge representation formalisms and problem solving paradigms. A typical framework may involve a blackboard architecture and a Reason Maintenance System (RMS) to guarantee the consistency of the links between the blackboard nodes. However, in order to satisfy computational feasibility and become operational, the resulting expert system must often be rewritten using less expressive tools. We propose an architecture integrating efficiently an OPS-like inference engine and an Assumption based Truth Maintenance System (ATMS). These paradigms have been separately investigated and extended. Roles distribution between an ATMS and an inference engine integrated in a single framework is one of the major issues to obtain good overall performance. Two architectures will be studied : loose coupling, where the ATMS and the inference engine are clearly separated, and tight coupling where the ATMS is intimately integrated with the match phase of a RETE-based inference engine. The advantages and drawbacks of both solutions are described in details. Finally, future work is discussed.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-042.pdf,
42,1991,"ConstraintReasoning and Component Technologies",Using Attention in Belief Revision,"Xueming Huang, Gordon I. McCalla, Eric Neufeld","Belief revision for an intelligent system is usually computationally expensive. Here we tackle this problem by using focus in belief revision: that is, revision occurs only in a subset of beliefs under attention (or in focus). Attention can be shifted within the belief base, thus allowing use and revision of other subsets of beliefs. This attention-shifting belief revision architecture shows promise to allow efficient and natural revision of belief bases.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-043.pdf,
43,1991,"ConstraintReasoning and Component Technologies",CATMS: An ATMS Which Avoids Label Explosions,"John W. Collins, Dennis DeCoste","Assumption-based truth maintenance systems have developed into powerful and popular means for considering multiple contexts simultaneously during problem solving. Unfortunately, increasing problem complexity can lead to explosive growth of node labels. In this paper, we present a new ATMS algorithm (CATMS) which avoids the problem of label explosions, while preserving most of the query-time efficiencies resulting from label compilations. CATMS generalizes the standard ATMS subsumption relation, allowing it to compress an entire label into a single assumption. This compression of labels is balanced by an expansion of environments to include any implied assumptions. The result is a new dimension of flexibility, allowing CATMS to trade-off the query-time efficiency of uncompressed labels against the costs of computing them. To demonstrate the significant computational gains of CATMS over de Kleer’s ATMS, we compare the performance of the ATMS-based QPE [9] problem-solver using each.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-044.pdf,
44,1991,"ConstraintReasoning and Component Technologies",Context Maintenance,"Charles J. Petrie, Jr.","Traditional applications of ""Truth Maintenance Systems""(TMSs) fail to adequately represent heuristic search in which some paths are initially preferred. What they miss is the idea of switching contexts rationally based on heuristic preferences. We show that it is useful, especially for plans with contingencies, to maintain the validity of the reason for context choices and rejections. We demonstrate how to do so with a problem solver/TMS architecture called REDUX.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-045.pdf,
45,1991,"ConstraintReasoning and Component Technologies",IXM2: A Parallel Associative Processor for Knowledge Processing,"Tetsuya Higuchi, Hiroaki Kitano, Tatsumi Furuya, Ken-ichi Handa, Akio Kokubu, Naoto Takahashi","This paper describes a parallel associative processor, IXM2, developed mainly for semantic network processing. IXM2 consists of 64 associative processors and 9 network processors, having a total of 256K words of associative memory. The large associative memory enables 65,536 semantic network nodes to be processed in parallel and reduces the order of algorithmic complexity to O(1) in basic semantic net operations. We claim that intensive use of associative memory provides far superior performance in carrying out the basic operations necessary for semantic network processing: intersection, marker-propagation, and arithmetic operations.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-046.pdf,
46,1991,"ConstraintReasoning and Component Technologies",Implementation of Multiple Rule Firing Production Systems on Hypercube,"Steve Kuo, Dan Moldovan","The performance of production programs can be improved by firing multiple rules in a production cycle. In this paper, we present the multiple-contexts-multiple-rules (MCMR) model which speeds up production program execution by firing multiple rule concurrently and guarantees the correctness of the solution. The MCMR model is implemented using the RUBIC parallel inference model on the Intel iPSC/2 hypercube. The Intel iPSC/2 hypercube is chosen because it is a cost-effective solution to large-scale application. To avoid unnecessary synchronization and improve performance, rules are executed asynchronously and messages are used to update the database. Preliminary implementation results for the RUBIC parallel inference environment on the Intel iPSC/2 hypercube are reported.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-047.pdf,
47,1991,"ConstraintReasoning and Component Technologies",Control Issues in Parallel Rule-Firing Production Systems,Daniel E. Neiman,"When rules are executed in a parallel production system, the goal of control is to ensure both that a high-quality solution is achieved and that processing resources are used effectively. We argue that the conventional conflict resolution algorithm is not suitable as a control mechanism for parallel rule-firing systems. The necessity for examining all eligible rules within a system imposes a synchronization delay which limits processor utilization. Rather than perform conflict resolution, we propose that rules should be executed asynchronously as soon as they become enabled, however, this approach leaves the problem of controlling the computation unsolved. We have identified three distinct types of control, program sequencing, heuristic control, and dynamic scheduling, which are required for efficient and correct parallel execution of rules. We discuss the issues involved in implementing each type of control without undue overhead within the context of our system, a parallel rule-firing system with an augmented agenda manager.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-048.pdf,
48,1991,"ConstraintReasoning and Component Technologies",Using Abstraction to Automate Program Improvement by Transformation,Ian Green,"The problem of automatically improving functional programs using Darlington’s unfold/fold technique is addressed. Transformation tactics are formalized as methods consisting of pre- and post-conditions, expressed within a sorted meta-logic. Predicates and functions of this logic induce an abstract program property space within which conventional monotonic planning techniques are used to automatically compose methods (hence tactics) into a program improving strategy. This meta-program reasoning casts the undirected search of the transformation space as a goal-directed search of the more abstract space. Tactics are only weakly specified by methods. This flexibility is required if they are to be applicable to the class of generalized programs that satisfy the pre-conditions of their methods. This is achieved by allowing the tactics to generate degenerate scripts that may require refinement. Examples of tactics and methods are given, with illustrations of their use in automatic program improvement.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-049.pdf,
49,1991,"ConstraintReasoning and Component Technologies",Verification of Multi-Level Rule-Based Expert Systems,Pedro Meseguer,"Verification methods and tools developed so far have assumed a very simple model of rule-based expert system (RBES). Current RBES often do not comply this model and require more sophisticated verification techniques. A RBES model including uncertainty and control has been used to analyze four verification issues (inconsistency, redundancy, circularity and useless RB objects), identifying a number of new verification problems. The concepts of labels and environments (deKleer 1986) have been extended to incorporate uncertainty and control information, obtaining the constructs extended-labels and extended-environments. They have been used to express and solve these new verification problems.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-050.pdf,
50,1991,"ConstraintReasoning and Component Technologies",Formal Verification of Pure Production System Programs,"Rose F. Gamble, Gruia-Catalin Roman, William E. Ball","Reliability, defined as the guarantee that a program satisfies its specifications, is an important aspect of many applications for which rule-based programs are suited. Executing rule-based programs on a series of test cases does not guarantee correct behavior in all possible test cases. To show a program is reliable, it is desirable to construct formal specifications for the program and to prove that it obeys those specifications. This paper presents an assertional approach to the verification of a class of rule-based programs characterized by the absence of conflict resolution. The proof logic needed for verification is already in use by researchers in concurrent programming. The approach involves expressing the program in a language called Swarm, and its specifications as assertions over the Swarm program. Among models that employ rule-based notation, Swarm is the first to have an axiomatic proof logic. A brief review of Swarm and its proof logic is given, along with an illustration of the formal verification method used on a simple rule-based program.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-051.pdf,
51,1991,"ConstraintReasoning and Component Technologies",Learning Meta Knowledge for Database Checking,Jeffrey C. Schlimmer,"Building a large-scale system often involves creating a large knowledge store, and as these grow and are maintained by a number of individuals, errors are inevitable. Exploring databases as a specialization of knowledge stores, this paper studies the hypothesis that descriptive, learned models can be prescriptively used to find errors. To that end, it describes an implemented system called CARPER. Applying CARPER to a real-world database demonstrates the viability of the approach and establishes a baseline of performance for future research.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-052.pdf,
52,1991,"FormalMethods in Knowledge Representation",A Logic of Situated Know-How,Munindar P. Singh,"Know-how is an important concept in Artificial Intelligence. It has been argued previously that it cannot be successfully reduced to the knowledge of facts. In this paper, I present sound and complete axiomatizations for two non-reductive and intuitively natural formal definitions of the know-how of an agent situated in a complex environment. I also present some theorems giving useful properties of know-how, and discuss and resolve an interesting paradox (which is described within). This is done using a new operator in the spirit of Dynamic Logic that is introduced herein and whose semantics and proof-theory are given.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-053.pdf,
53,1991,"FormalMethods in Knowledge Representation",Provably Correct Theories of Action (Preliminary Report),"Fangzhen Lin, Yoav Shoham","Research on nonmonotonic temporal reasoning in general, and the Yale Shooting Problem in particular, has suffered from the absence of a criterion against which to evaluate solutions. Indeed, researchers in the area disagree not only on the solutions but also on the problems. We propose a formal yet intuitive criterion by which to evaluate theories of actions, define a monotonic class of theories that satisfy this criterion, and then provide their provably-correct nonmonotonic counterpart.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-054.pdf,
54,1991,"FormalMethods in Knowledge Representation",A Critique of Yoav Shoham’s Theory of Causal Reasoning,Antony Galton,"Causal reasoning is an essential part of a number of tasks that have been central to many endeavours in AI-notably planning and prediction, diagnosis and explanation. Recently it has become an object of study in its own right, drawing inspiration from the work of philosophers and logicians as well as more immediately AI-oriented concerns. In this paper I shall examine just one approach to causal reasoning, that advocated by Yoav Shoham in a recent book and article. In particular, I shall try to lay bare a number of assumptions underlying Shoham’s work, all of which I shall call into question. Key assumptions are that causality is an epistemic notion, that causal reasoning is inherently non-monotonic, and that epistemic reasoining should be handled by means of modal logic. While arguing against these assumptions, I do not offer a specific causal theory of my own, but shall conclude with some suggestions as to the general lines which I feel such a theory ought to follow.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-055.pdf,
55,1991,"FormalMethods in Knowledge Representation",A Logic and Time Nets for Probabilistic Inference,Keiji Kanazawa,"In this paper, we show a new approach for reasoning about time and probability that combines a formal declarative language with a graph representation of systems of random variables for making inferences. First, we provide a continuous-time logic for expressing knowledge about time and probability. Then, we introduce the time net, a kind of Bayesian network for supporting inference with statements in the logic. Time nets encode the probability of facts and events over time. We provide a simulation algorithm to compute probabilities for answering queries about a time net. Finally, we consider an incremental probabilistic temporal database based on the logic and time nets to support temporal reasoning and planning applications. The result is an approach that is semantically well-founded, expressive, and practical.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-056.pdf,
56,1991,"FormalMethods in Knowledge Representation",The P-Systems: A Systematic Classification of Logics of Nonmonotonicity,Wolfgang Nejdl,"In the last years many logics of nonmonotonicity have been developed using various different formalisms and axiomatizations which makes them very difficult to compare. We develop a classification scheme for these logics using only a few simple concepts and axioms based on conditional logics, properties of partial pre-orders of possible states (worlds) and centering assumptions. Our framework (the P-Systems) allows us to discuss the similarities, main differences and possible extensions of these logics in a simple and natural way.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-057.pdf,
57,1991,"FormalMethods in Knowledge Representation",Some Variations on Default Logic,Piotr Rychlik,"In the following paper, we view applying default reasoning as a construction of an argument supporting agent’s beliefs. This yields a slight reformulation of the notion of an extension for default theories. The proposed formalism enjoys a property which we call rational maximization of beliefs.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-058.pdf,
58,1991,"FormalMethods in Knowledge Representation","Default Logic, Propositional Logic, and Constraints","Rachel Ben-Eliyahu, Rina Dechter","We present a mapping from a class of default theories to sentences in propositional logic, such that each model of the latter corresponds to an extension of the former. Using this mapping we show that many properties of default theories can be determined by solving propositional satisfiability. In particular, we show how CSP techniques can be used to identify, analyze and solve tractable subsets of Reiter’s default logic.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-059.pdf,
59,1991,"FormalMethods in Knowledge Representation",Strong Introspection,Michael Gelfond,"The purpose of this paper is to expand the syntax and semantics of logic programs and deductive databases to allow for the correct representation of incomplete information in the presence of multiple extensions. The language of logic programs with classical negation, epistemic disjunction, and negation by failure is further expanded by a new modal operator K (where for the set of rules T and formula F, KF stands for ""F is known to a reasoner with a set of premises T""). Theories containing such an operator will be called strongly introspective. We will define the semantics of such theories (which expands the semantics of deductive databases from [Gelfond and Lifschitz 199Ob]) and demonstrate the applicability of strongly introspective theories to formalization of some forms of commonsense reasoning.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-060.pdf,
60,1991,"FormalMethods in Knowledge Representation",Default Reasoning From Statistics,Fahiem Bacchus,"There are two common but quite distinct interpretations of probabilities: they can be interpreted as a measure of the extent to which an agent believes an assertion, i.e., as an agent’s degree of belief, or they can be interpreted as an assertion of relative frequency, i.e., as a statistical measure. Used as statistical measures probabilities can represent various assertions about the objective statistical state of the world, while used as degrees of belief they can represent various assertions about the subjective state of an agent’s beliefs. In this paper we examine how an agent who knows certain statistical facts about the world might infer probabilistic degrees of beliefs in other assertions from these statistics. For example, an agent who knows that most birds fly (a statistical fact) may generate a degree of belief greater than 0.5 in the assertion that Tweety flies given that Tweety is a bird. This inference of degrees of belief from statistical facts is known as direct inference. We develop a formal logical mechanism for performing direct inference. Some of the inferences possible via direct inference are closely related to default inferences. We examine some features of this relationship.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-061.pdf,
61,1991,"FormalMethods in Knowledge Representation",System-Z+: A Formalism for Reasoning with Variable-Strength Defaults,"Moisés Goldszmidt, Judea Pearl","We develop a formalism for reasoning with defaults that are expressed with different levels of firmness. Necessary and sufficient conditions for consistency are established, and a unique ranking of the rules is found, called Z+, which renders models as normal as possible subject to the consistency conditions. We provide the necessary machinery for testing consistency, computing the Z+ ranking and drawing the set of plausible conclusions it entails.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-062.pdf,
62,1991,"FormalMethods in Knowledge Representation",Incorporating Nonmonotonic Reasoning in Horn Clause Theories,James P. Delgrande,"An approach for introducing default reasoning into first-order Horn clause theories is described. A default theory is expressed as a set of strict implications where the ai and b are function-free literals. A partial order of sets of formulae is obtained from these sets of (strict and default) implications. Default reasoning is defined with respect to this ordering and a set of contingent ground facts. Crucially, only strict implications appear in this structure. Consequently the complexity of default reasoning is that of classical reasoning, together with an attendant overhead for manipulating the structure. This overhead is O(n2), where n is the number of original formulae. Hence for defaults in propositional Horn clause form time complexity is O(n2m) where m is the total length of the original formulae. The approach is sound, in that default reasoning in this structure is proven to conform to that of an extant system for default reasoning.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-063.pdf,
63,1991,"FormalMethods in Knowledge Representation",Step-Logic and the Three-Wise-Men Problem,Jennifer J. Elgot-Drapkin,"The kind of resource limitation that is most evident in commonsense reasoners is the passage of time while the reasoner reasons. There is not necessarily any fixed and final set of consequences with which such a reasoning agent ends up. In formalizing commonsense reasoners, then, one must be able to take into account that time is passing as the reasoner is reasoning. The reasoner can then make use of such information in subsequent deductions. Step-logic is such a formalism. It was developed in [Elgot-Drapkin, 1988] to model the on-going process of deduction. Conclusions are drawn step-by-step. There is no ""final"" state of reasoning; the emphasis is on intermediate conclusions. In this paper we use step-logic to model the Three-wisemen Problem. Although others have formalized this problem, they have ignored the time aspect that is inherent in the problem: a correct assessment of the situation is made by recognizing that the reasoning process takes time and determining that the other wise men would have concluded such and such by now. This is an important aspect of the problem that needs to be addressed.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-064.pdf,
64,1991,"Issuesin Automated Reasoning",Logic Morphisms as a Framework for Backward Transfer of Lemmas and Strategies in Some Modal and Epistemic Logics,"Ricardo Caferra, Stephane Demri, Michel Herment","There exist methods in automated theorem proving for non-classical logics based on translation of logics from a (non-classical) source logic (abbreviated henceforth SL) into a (classical) target logic (abbreviated henceforth TL). These valuable methods do not address the important practical problem of presenting proofs in SL. We propose a framework applicable at least to S4(p), K, T, K4 for presenting proofs of theorems of these logics found in a familiar TL: Order-Sorted Predicate Logic (abbreviated henceforth OSPL). The method backward translates lemmas in a deduction (in TL) either (a) into lemmas in a corresponding deduction in SL (in the best case), or (b) into formulas semantically related to lemmas in a corresponding deduction (in the worst case). As a natural consequence we bring to the fore the fact that this framework can also be used to help in solving another important and very difficult problem: the transfer of strategies from one logic to another. One conjecture -with corresponding theorem which is a particular case of it- is stated. When (b) above holds we give sufficient (and in general satisfactory) conditions in order to obtain the lemmas in SL. Two examples are treated in full detail : the well known problem of the ""wise man puzzle"" and another one which shows how our method can be used to transfer strategies. No additional theoretical result is given in this direction, but it is clear from the example how the proposed framework can help to transfer strategies.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-065.pdf,
65,1991,"Issuesin Automated Reasoning",Mechanization of Analytic Reasoning About Sets,Alan F. McMichael,"Resolution reasoners, when applied to set theory problems, typically suffer from ""lack of focus."" MARS is a program that attempts to rectify this difficulty by exploiting the definition-like character of the set theory axioms. As in the case of its predecessor, SLIM, it employs a tableau proof procedure based on binary resolution, but MARS IS enhanced by an equality substitution rule and a device for introducing previously proved theorems as lemmas. MARS’s performance compares favorably with that of other existing automated reasoners for this domain. MARS finds proofs for many basic facts about functions, construed as sets of ordered pairs. MARS is being used to attack the homomorphism test problem, the theorem that the composition of two group homomorphisms is a group homomorphism.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-066.pdf,
66,1991,"Issuesin Automated Reasoning",Depth-First Versus Best-First Search,"Nageshwara Rao Vempaty, Vipin Kumar, Richard E. Korf","We present a comparison of three well known heuristic search algorithms: best-first search (BFS) , iterative-deepening (ID), and depth-first branch-and-bound (DFBB). We develop a model to analyze the time and space complexity of these three algorithms in terms of the heuristic branching factor and solution density. Our analysis identifies the types of problems on which each of the search algorithms performs better than the other two. These analytical results are validated through experiments on different problems. We also present a new algorithm, DFS*, which is a hybrid of iterative deepening and depth-first branch-and-bound, and show that it outperforms the other three algorithms on some problems.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-067.pdf,
67,1991,"Issuesin Automated Reasoning",Optimal Satisficing Tree Searches,"Dan Geiger, Jeffrey A. Barnett","We provide an algorithm that finds optimal search strategies for AND trees and OR trees. Our model includes three outcomes when a node is explored: (1) finding a solution, (2) not finding a solution and realizing that there are no solutions beneath the current node (pruning), and (3) not finding a solution but not pruning the nodes below. The expected cost of examining a node and the probabilities of the three outcomes are given. Based on this input, the algorithm generates an order that minimizes the expected search cost.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-068.pdf,
68,1991,"Issuesin Automated Reasoning",A New Admissible Heuristic for Minimal-Cost Proofs,"Eugene Charniak, Saadia Husain","Finding best explanations is often formalized in AI in terms of minimal-cost proofs. Finding such proofs is naturally characterized as a best-first search of the proof-tree (actually a proof dag). Unfortunately the only known search heuristic for this task is quite poor. In this paper we present a new heuristic, a proof that it is admissible (for certain successor functions), and some experimental results suggesting that it is a significant improvement over the currently used heuristic.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-069.pdf,
69,1991,"Issuesin Automated Reasoning",Is There any Need for Domain-Dependent Control Information?,"Matthew L. Ginsberg, Donald F. Geddis","In this paper, we make an observation and a claim.The observation is that there are in fact two distincttypes of metalevel information. On the one hand, onecan have metalevel information about one’s base-levelknowledge itself; on the other, one can have controlinformation about what to do with that knowledge.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-070.pdf,
70,1991,"Issuesin Automated Reasoning",Integrating Rules in Term Subsumption Knowledge Representation Servers,Brian R. Gaines,"This paper addresses the integration of services for rule-based reasoning in knowledge representation servers based on term subsumption languages. AS an alternative to previous constructions of rules as concept+concept links, a mechanism is proposed based on intensional roles implementing the axiom of comprehension in set theory. This has the benefit of providing both rules as previously defined, and set aggregation, using a simple mechanism that is of identical computational complexity to that for rules alone. The extensions proposed have been implemented as part of KRS, a knowledge representation server written as a class library in C++. The paper gives an example of their application to the ripple-down rule technique for large-scale knowledge base operation, acquisition and maintenance.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-071.pdf,
71,1991,"Issuesin Automated Reasoning",Deduction as Parsing: Tractable Classification in the KL-ONE Framework,Marc Vilain,"This paper addresses the integration of services for rule-based reasoning in knowledge representation servers based on term subsumption languages. AS an alternative to previous constructions of rules as concept+concept links, a mechanism is proposed based on intensional roles implementing the axiom of comprehension in set theory. This has the benefit of providing both rules as previously defined, and set aggregation, using a simple mechanism that is of identical computational complexity to that for rules alone. The extensions proposed have been implemented as part of KRS, a knowledge representation server written as a class library in C++. The paper gives an example of their application to the ripple-down rule technique for large-scale knowledge base operation, acquisition and maintenance.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-072.pdf,
72,1991,"Issuesin Automated Reasoning",Concept Languages as Query Languages,"Maurizio Lenzerini, Andrea Schaerf","We study concept languages (also called terminological languages) as means for both defining a knowledge base and expressing queries. In particular, we investigate on the possibility of using two different concept languages, one for asserting facts about individual objects, and the other for querying a set of such assertions. Contrary to many negative results on the complexity of terminological reasoning, our work shows that, provided that a limited language is used for the assertions, it is possible to employ a richer query language while keeping the reasoning process tractable. We also show that, on the other hand, there are constructs that make query answering inherently intractable.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-073.pdf,
73,1991,"Issuesin Automated Reasoning",Combining Opinions About the Order of Rule Execution,Jeffrey A. Barnett,"How should opinions of control knowledge sources be represented and combined? These issues are addressed for the case where control knowledge is used to form an agenda, i.e., a proposed knowledge source execution order. A formal model is developed in the Dempster/Shafer belief calculus and computational problems are discussed as well. The model is applicable to many other problems where it is desired to order a set of candidates using a knowledge-based approach.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-074.pdf,
74,1991,"Issuesin Automated Reasoning","Explanation, Irrelevance, and Statistical Independence",Solomon E. Shimony,"We evaluate current explanation schemes. These are either insufficiently general, or suffer from other serious drawbacks. We propose a domain-independent explanation system that is based on ignoring irrelevant variables in a probabilistic setting. We then prove important properties of some specific irrelevance-based schemes and discuss how to implement them.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-075.pdf,
75,1991,"Issuesin Automated Reasoning",Conditions for the Existence of Belief Functions Corresponding to Intervals of Belief,"John F. Lemmer, Henry E. Kyburg, Jr.","While every Shafer belief function corresponds to a set of interval beliefs on the atoms of the frame of discernment, an arbitrarily specified set of intervals of belief may not correspond to any belief function, even when it does correspond to bounds imposed by sets of probability functions. This paper proves necessary and sufficient conditions which must be met by a set of belief intervals over atoms if a corresponding belief function exists. The sufficiency is proved via an an O(n) algorithm which will always construct an corresponding belief function, if one exits, for a specific set of intervals.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-076.pdf,
76,1991,"Issuesin Automated Reasoning",An Efficient First-Order Horn-Clause Abduction System Based on the ATMS,"Hwee Tou Ng, Raymond J. Mooney","This paper presents an algorithm for first-order Horn-clause abduction that uses an ATMS to avoid redundant computation. This algorithm is either more efficient or more general than any other previous abduction algorithm. Since computing all minimal abductive explanations is intractable, we also present a heuristic version of the algorithm that uses beam search to compute a subset of the simplest explanations. We present empirical results on a broad range of abduction problems from text understanding, plan recognition, and device diagnosis which demonstrate that our algorithm is at least an order of magnitude faster than an alternative abduction algorithm that does not use an ATMS.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-077.pdf,
77,1991,"Issuesin Automated Reasoning",The Common Order-Theoretic Structure of Version Spaces and ATMS’s (Extended Abstract),"Carl A. Gunter, Teow-Hin Ngair, Prakash Panangaden, Devika Subramanian",This paper arose out of the observation that the version space algorithm and the ATMS label-update algorithms operate on very similar structures. The version space algorithm learns concept descriptions from examples. Central to this algorithm is the notion of all concept descriptions consistent with a given set of positive and negative examples. The assumption-based truth maintenance system for recording dependencies during reasoning maintains labels for a proposition which encode all environments in which that proposition is true.,https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-078.pdf,
78,1991,"Issuesin Automated Reasoning",ACP: Reason Maintenance and Inference Control for Constraint Propagation Over Intervals,Walter Hamscher,"ACP is a fully implemented constraint propagation system that computes numeric intervals for variables [Davis, 1987] along with an ATMS label [de Kleer, I986a] for each such interval. The system is built within a ""focused"" ATMS architecture [Forbus and de Kleer, 1988, Dressler and Farquhar, 1989] and incorporates a variety of techniques to improve efficiency.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-079.pdf,
79,1991,"Issuesin Automated Reasoning",Controlling Inequality Reasoning in a TMS-Based Analog Diagnosis System,David Jerald Goldstone,"A system, called Skordos, has been implemented for model-based diagnosis of analog circuits. One of the difficulties of model-based diagnosis of analog circuits is managing the tremendous number of predictions which may be generated by a constraint propagation system. Fortunately, not all of those predictions are valuable for the diagnostic process. A process called hibernation, which is used in Skordos to prevent generation of useless predictions, is introduced and described here. Another technique is introduced and described which further assits in controlling the inequality reasoning by exploiting hibernation. This technique involves changing the structure in which values are combined. It uses hibernation as an early filter to reduce the number of interactions resulting from Kirchhoff’s current law from exponential to quadratic in the number of interacting variables.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-080.pdf,
80,1991,Learning,Synthesizing UNIX Shell Scripts Using Derivational Analogy: An Empirical Assessment,"Sanjay Bhansali, Mehdi T. Harandi","The feasibility of derivational analogy as a mechanism for improving problem-solving behavior has been shown for a variety of problem domains by several researchers. However, most of the implemented systems have been empirically evaluated in the restricted context of an already supplied base analog, or on a few isolated examples. In this paper, we address the utility of a derivational analogy based approach when the cost of retrieving analogs from a sizable case library, and the cost of retrieving inappropriate analogs is factored in.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-081.pdf,
81,1991,Learning,SteppingStone: An Empirical and Analytical Evaluation,"David Ruby, Dennis Kibler","Decomposing a difficult problem into simpler subproblems is a classic problem solving technique. Unfortunately, the most difficult subproblems can be as difficult, if not more difficult, than the original problem. This is not an obstacle to problem solving if the difficult subproblems recur in other problems. If the difficult subproblems recur often, then its solution need only be learned once and reused. Steppingstone is a learning problem solver that decomposes a problem into simple and difficult-but-recurring subproblems. It solves the simple subproblems with an inexpensive constrained problem solver. To solve the difficult subproblems, Steppingstone uses an unconstrained problem solver. Once it solves a difficult subproblem, it uses the solution to generate a sequence of subgoals, or stepping-stones, that can be used by the constrained problem solver to solve this difficult subproblem when it occurs again. In this paper we provide analytical evidence for Steppingstone’s capabilities as well as empirical results from our work with the domain of logic synthesis.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-082.pdf,
82,1991,Learning,STATIC: A Problem-Space Compiler for PRODIGY,Oren Etzioni,"Explanation-Based Learning (EBL) can be used to significantly speed up problem solving. Is there sufficient structure in the definition of a problem space to enable a static analyzer, using EBL-style optimizations, to speed up problem solving without utilizing training examples? If so, will such an analyzer run in reasonable time? This paper demonstrates that for a wide range of problem spaces the answer to both questions is ""yes."" The STATIC program speeds up problem solving for the PRODIGY problem solver without utilizing training examples. In Minton’s problem spaces [1988], STATIC acquires control knowledge from twenty six to seventy seven times faster, and speeds up PRODIGY up to three times as much as PRODIGY/EBL. This paper presents STATIC'S algorithms, derives a condition under which STATIC is guaranteed to achieve polynomial-time problem solving, and contrasts STATIC with PRODIGY/EBL.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-083.pdf,
83,1991,Learning,Integrating Abstraction and Explanation-Based Learning in PRODIGY,"Craig A. Knoblock, Steven Minton, Oren Etzioni","This paper describes the integration of abstraction and explanation-based learning (EBL) in the context of the PRODIGY system. PRODIGY'S abstraction module creates a hierarchy of abstract problem spaces, so problem solving can proceed in a more directed fashion. The EBL module acquires search control knowledge by analyzing problem-solving traces. When the two modules are integrated, they tend to complement each other’s capabilities, resulting in performance improvements that neither system can achieve independently. We present empirical results showing the effect of combining the two modules and describe the factors that influence the overall performance of the integrated system.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-084.pdf,
84,1991,Learning,Learning with Many Irrelevant Features,"Hussein Almuallim, Thomas G. Dietterich","In many domains, an appropriate inductive bias is the MIN-FEATURES bias, which prefers consistent hy- potheses definable over as few features as possible. This paper defines and studies this bias. First, it is shown that any learning algorithm implementing the MIN-FEATURES bias requires q(1/e ln 1/d + 1/e [2P + p ln n]) training examples to guarantee PAC-learning a concept having p relevant features out of n avail- able features. This bound is only logarithmic in the number of irrelevant features. The paper also presents a quasi-polynomial time algorithm, FOCUS, which implements MIN-FEATURES. Experimental studies are presented that compare FOCUS to the ID3 and FRINGE algorithms. These experiments show that- contrary to expectations-these algorithms do not implement good approximations of MIN-FEATURES. The coverage, sample complexity, and generalization performance of FOCUS is substantially better than either ID3 or FRINGE on learning problems where the MIN-FEATURES bias is appropriate. This suggests that, in practical applications, training data should be preprocessed to remove irrelevant features before being given to ID3 or FRINGE.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-085.pdf,
85,1991,Learning,Analyses of Instance-Based Learning Algorithms,"Marc K. Albert, David W. Aha","This paper presents PAC-learning analyses for instance-based learning algorithms for both symbolic and numeric-prediction tasks. The algorithms analyzed employ a variant of the k-nearest neighbor pattern classifier. The main results of these analyses are that the IB1 instance-based learning algorithm can learn, using a polynomial number of instances, a wide range of symbolic concepts and numeric functions. In addition, we show that a bound on the degree of difficulty of predicting symbolic values may be obtained by considering the size of the boundary of the target concept, and a bound on the degree of difficulty in predicting numeric values may be obtained by considering the maximum absolute value of the slope between instances in the instance space. Moreover, the number of training instances required by IBl is polynomial in these parameters. The implications of these results for the practical application of instance-based learning algorithms are discussed.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-086.pdf,
86,1991,Learning,Regularity and Structure,Alexander Botta,"We present an approach to unsupervised concept formation, based on accumulation of partial regularities. Using an algorithmic complexity framework, we define regularity as a model that achieves a compressed coding of data, We discuss induction of models. We present induction of finite automata models for regularities of strings, and induction of models based on vector translations for sets of points. The concepts we develop are particularly appropriate for natural spaces - structures that accept a decomposition into recurrent, recognizable parts. They are usually hierarchical and suggest that a vocabulary of basic constituents can be learned before focussing on how they are assembled. We define and illustrate: Basic regularities as algorithmically independent building blocks of structures. They are identifiable as local maxima of compression as a function of model complexity. Stepwise induction consists in finding a model, using it to compress the data, then applying the same procedure on the code. It is a way to induce, in polynomial time, structures whose basic components have bound complexity. Libraries are sets of partial regularities, a theoretical basis for clustering and concept formation. Finally, we use the above concepts to present a new perspective on explanation based generalization. We prove it to be a language independent method to specialize the background knowledge.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-087.pdf,
87,1991,Learning,A Minimal Encoding Approach to Feature Discovery,Mark Derthick,"This paper discusses unsupervised learning of orthogonal concepts on relational data. Relational predicates, while formally equivalent to the features of the concept-learning literature, are not a good basis for defining concepts. Hence the current task demands a much larger search space than traditional concept learning algorithms, the sort of space explored by connectionist algorithms. However the intended application, using the discovered concepts in the Cyc knowledge base, requires that the concepts be interpretable by a human, an ability not yet realized with connectionist algorithms. Interpretability is aided by including a characterization of simplicity in the evaluation function. For Hinton’s Family Relations data, we do find cleaner, more intuitive features. Yet when the solutions are not known in advance, the difficulty of interpreting even features meeting the simplicity criteria calls into question the usefulness of any reformulation algorithm that creates radically new primitives in a knowledge-based setting. At the very least, much more sophisticated explanation tools are needed.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-088.pdf,
88,1991,Learning,Error-Correcting Output Codes: A General Method for Improving Multiclass Inductive Learning Programs,"Thomas G. Dietterich, Ghulum Bakiri","Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k > 2 values (i.e., k ""classes""). The definition is acquired by studying large collections of training examples of the form (xi, f(xi)). Existing approaches to this problem include (a) direct application of multiclass algorithms such as the decision-tree algorithms ID3 and CART, (b) application of binary concept learning algorithms to learn individual binary functions for each of the Ic classes, and (c) application of binary concept learning algorithms with distributed output codes such as those employed by Sejnowski and Rosenberg in the NETtalk system. This paper compares these three approaches to a new technique in which BCH error-correcting codes are employed as a distributed output representation. We show that these output representations improve the performance of ID3 on the NETtalk task and of backpropagation on an isolated-letter speech-recognition task. These results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-089.pdf,
89,1991,Learning,Analysis of the Internal Representations in Neural Networks for Machine Intelligence,Lai-Wan Chan,"The internal representation of the training patterns of multi-layer perceptrons was examined and we demonstrated that the connection weights between layers are effectively transforming the representation format of the information from one layer to another one in a meaningful way. The internal code, which can be in analog or binary form, is found to be dependent on a number of factors, including the choice of an appropriate representation of the training patterns, the similarities between the patterns as well as the network structure; i.e. the number of hidden layers and the number of hidden units in each layer.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-090.pdf,
90,1991,Learning,Direct Transfer of Learned Information Among Neural Networks,"Lorien Y. Pratt, Jack Mostow, Candace A. Kamm","A touted advantage of symbolic representations is the ease of transferring learned information from one intelligent agent to another. This paper investigates an analogous problem: how to use information from one neural network to help a second network learn a related task. Rather than translate such information into symbolic form (in which it may not be readily expressible), we investigate the direct transfer of information encoded as weights. Here, we focus on how transfer can be used to address the important problem of improving neural network learning speed. First we present an exploratory study of the somewhat surprising effects of pre-setting network weights on subsequent learning. Guided by hypotheses from this study, we sped up back-propagation learning for two speech recognition tasks. By transferring weights from smaller networks trained on subtasks, we achieved speedups of up to an order of magnitude compared with training starting with random weights, even taking into account the time to train the smaller networks. We include results on how transfer scales to a large phoneme recognition problem.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-091.pdf,
91,1991,Learning,Rule Learning by Searching on Adapted Nets,LiMin Fu,"If the backpropagation network can produce an inference structure with high and robust performance, then it is sensible to extract rules from it. The KT algorithm is a novel algorithm for generating rules from an adapted net efficiently. The algorithm is able to deal with both single-layer and muti-layer networks, and can learn both confirming and disconfirming rules. Empirically, the algorithm is demonstrated in the domain of wind shear detection by infrared sensors with success.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-092.pdf,
92,1991,Learning,Two Kinds of Training Information For Evaluation Function Learning,"Paul E. Utgoff, Jeffrey A. Clouse",This paper identifies two fundamentally different kinds of training information for learning search control in terms of an evaluation function. Each kind of training information suggests its own set of methods for learning an evaluation function. The paper shows that one can integrate the methods and learn simultaneously from both kinds of information.,https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-093.pdf,
93,1991,Learning,Adaptive Pattern-Oriented Chess,"Robert Levinson, Richard Snyder","Psychological evidence indicates that human chess players base their assessments of chess positions on structural/perceptual patterns learned through experience. Morph is a computer chess program that has been developed to be more consistent with the cognitive models. The learning mechanism used by Morph combines weight-updating, genetic, explanation-based and temporal-difference learning to create, delete, generalize and evaluate chess positions. An associative pattern retrieval system organizes the database for efficient processing. The main objectives of the project are to demonstrate capacity of the system to learn, to deepen our understanding of the interaction of knowledge and search, and to build bridges in this area between AI and cognitive science. To strengthen connections with the cognitive literature limitations have been place on the system, such as restrictions to l-ply search, to little domain knowledge, and to no supervised training.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-094.pdf,
94,1991,Learning,A Complexity Analysis of Cooperative Mechanisms in Reinforcement Learning,Steven D. Whitehead,"Reinforcement learning algorithms, when used to solve multi-stage decision problems, perform a kind of online (incremental) search to find an optimal decision policy. The time complexity of this search strongly depends upon the size and structure of the state space and upon a priori knowledge encoded in the learners initial parameter values. When a priori knowledge is not available, search is unbiased and can be excessive. Cooperative mechanisms help reduce search by providing the learner with shorter latency feed-back and auxiliary sources of experience. These mechanisms are based on the observation that in nature, intelligent agents exist in a cooperative social environment that helps structure and guide learning. Within this context, learning involves information transfer as much as it does discovery by trial-and-error. Two cooperative mechanisms are described: Learning with an External Critic (or LEC) and Learning By Watching (or LBW). The search time complexity of these algorithms, along with unbiased Q-learning, are analyzed for problem solving tasks on a restricted class of state spaces. The results indicate that while unbiased search can be expected to require time moderately exponential in the size of the state space, the LEC and LBW algorithms require at most time linear in the size of the state space and under appropriate conditions, are independent of the state space size altogether; requiring time proportional to the length of the optimal solution path. While these analytic results apply only to a restricted class of tasks, they shed light on the complexity of search in reinforcement learning in general and the utility of cooperative mechanisms for reducing search.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-095.pdf,
95,1991,Learning,Constructive Induction on Domain Information,"James P. Callan, Paul E. Utgoff","It is well-known that inductive learning algorithms are sensitive to the way in which examples of a concept are represented. Constructive induction reduces this sensitivity by enabling the inductive algorithm to create new terms with which to describe examples. However, new terms are usually created as functions of existing terms, so an extremely poor initial representation makes the search for new terms intractable. This work considers inductive learning within a problem-solving environment. It shows that information about the problem-solving task can be used to create terms that are suitable for learning search control knowledge. The resulting terms describe the problem-solver’s progress in achieving its goals. Experimental evidence from two domains is presented in support of the approach.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-096.pdf,
96,1991,"Planning,Perception, and Robotics",On the NP-Hardness of Blocks World,Stephen V. Chenoweth,"Blocks world (cube world) has been one of the most popular model domains in artificial intelligence search and planning. The operation and effectiveness of alternative heuristic strategies, both basic and complex, can be observed easily in this domain. We show that finding an optimal solution is NP-hard in an important variant of the domain, and popular extensions. This enlarges the range of model domains whose complexity has been explored mathematically, and it demonstrates that the complexity of search in blocks world is on the same level as for sliding block problems, the traveling salesperson problem, bin-packing problems, and the like. These results also support the practice of using blocks world as a tutorial search domain in courses on artificial intelligence, to reveal both the value and limitations of heuristic search when seeking optimal solutions.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-097.pdf,
97,1991,"Planning,Perception, and Robotics",Complexity Results for Blocks-World Planning,"Naresh Gupta, Dana S. Nau","Although blocks-world planning is well-known, its complexity has not previously been analyzed, and different planning researchers have expressed conflictingopinions about its difficulty. In this paper, we present the following results:1. Finding optimal plans in a well-known formulation of the blocks-world planning domain is NP-hard, even if the goal state is completely specified.2. Classical examples of deleted-condition interactions such as Sussman’s anomaly and creative destruction are not difficult to handle in this domain, provided that the right planning algorithm is used. Instead,the NP-hardness of the problem results from difficulties in determining which of several different actions will best help to achieve multiple goals.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-098.pdf,
98,1991,"Planning,Perception, and Robotics",Systematic Nonlinear Planning,"David McAllester, David Rosenblitt","This paper presents a simple, sound, complete, and systematic algorithm for domain independent STRIPS planning. Simplicity is achieved by starting with a ground procedure and then applying a general, and independently verifiable, lifting transformation. Previous planners have been designed directly aa lifted procedures. Our ground procedure is a ground version of Tate’s NONLIN procedure. In Tate’s procedure one is not required to determine whether a prerequisite of a step in an unfinished plan is guaranteed to hold in all linearizations. This allows Tate’s procedure to avoid the use of Chapman’s modal truth criterion. Systematicity is the property that the same plan, or partial plan, is never examined more than once. Systematicity is achieved through a simple modification of Tate’s procedure.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-099.pdf,
99,1991,"Planning,Perception, and Robotics",An Efficient Reactive Planner for Synthesizing Reactive Plans,"Patrice Godefroid, Froduald Kabanza",We present a nonlinear forward-search method suitable for planning the reactions of an agent operating in a highly unpredictable environment. We show that this method is more efficient than existing linear methods. We then introduce the notion of safety and liveness rules. This makes possible a sharper exploitation of the information retrieved when exploring the future of the agent.,https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-100.pdf,
100,1991,"Planning,Perception, and Robotics",Dealing with Uncertainties in CAD-Based Assembly Motion Planning,"S. N. Gottschlich, A. C. Kak","A common problem in robotic assembly is that of mating tightly fitting parts when the locations and the dimensions of the parts are somewhat uncertain. It is necessary to be able to reason about these uncertainties in conjunction with the geometry of the parts involved in order to develop motion plans for assembly operations. In this paper we will present a method for the treatment of three types of uncertainties usually prevalent in robotic assembly systems: uncertainties in the initial locations of parts, uncertainties in the control of the robot used to assemble these parts and uncertainties in the dimensions of these parts. The method we will present, used by a CAD-based planning system we have developed, discovers which portions of an assembly operation must be carried out using force/torque guided motions because the composite uncertainties exceed the clearance during these portions of the operation. The method further suggests the type of force/torque guided motions that need to be used for these portions. With this knowledge our planning system formulates motion plans for assembly operations. Plans for a variety of assemblies have been produced by our planning system and have been experimentally verified on both a Cincinnati Milacron T3-726 robot and a Puma 762 robot.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-101.pdf,
101,1991,"Planning,Perception, and Robotics",Augmenting a Nominal Assembly Motion Plan with a Compliant Behavior,"Gordon A. Dakin, Robin J. Popplestone","A methodology is presented whereby a nominal trajectory for an assembly operation, computed from kinematic constraints alone, is augmented with a fine-motion strategy synthesized through uncertainty and force analyses. Insertion clearances and size tolerances are introduced into the assembly part models in parallel with the manual selection of a perturbed nominal trajectory in contact space. The selection of small clearances, and in turn, small insertion angles allows us to linearize contact space about discrete points in the nominal trajectory. Contact states are represented as affine spaces in a generalized C-space of model error and pose variables. The feasibility of proposed command velocities to be executed in the presence of position, control, and model error is determined through an uncertainty analysis technique based on the forward-projection of convex polytopes in contact space. Our approach further the automates the so-called ""manual"" methods of motion planning with uncertainty.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-102.pdf,
102,1991,"Planning,Perception, and Robotics",A Fast Path Planner for a Car-Like Indoor Mobile Robot,Jean-Claude Latombe,"A car-like indoor mobile robot is a kinematically constrained robot that can be modelled as a 2D object translating and rotating in the horizontal plane among well-defined obstacles. The kinematic constraints impose that the linear velocity of the robot point along its main axis (no sidewise motion is possible) and restrict the range of admissible values for the steering angle. In this paper we describe a fast path planner for such a robot. This planner is one to two orders of magnitude faster than previously implemented planners for the same type of robot. In addition, it has an anytime flavor that allows it to return a path in a short amount of time, and to improve that path through iterative optimization according to the amount of time that is devoted to path planning, The planner is essentially a combination of preexisting ideas. Its efficiency derives from the good match between these ideas and from various technical improvements brought to them.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-103.pdf,
103,1991,"Planning,Perception, and Robotics",Path Planning for Highly Redundant Manipulators Using a Continuous Model,"Akira Hayashi, Benjamin J. Kuipers","There is a need for highly redundant manipulators to work in complex, cluttered environments. Our goal is to plan paths for such manipulators efficiently. The path planning problem has been shown to be PSPACE-complete in terms of the number of degrees of freedom (DOF) of the manipulator. We present a method which overcomes the complexity with a strong heuristic: utilizing redundancy by means of a continuous manipulator model. The continuous model allows us to change the complexity of the problem from a function of both the DOF of the manipulator (believed to be exponential) and the complexity of the environment (polynomial), to a polynomial function of the complexity of the environment only.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-104.pdf,
104,1991,"Planning,Perception, and Robotics",A Quantitative Theory for Plan Merging,"David E. Foulser, Ming Li, Qiang Yang","Merging operators in a plan can yield significant savings in the cost to execute a plan. Past research in planning has concentrated on handling harmful interactions among plans, but the understanding of positive ones has remained at a qualitative, heuristic level. This paper provides a quantitative study for plan optimization and presents both optimal and approximate algorithms for finding minimum-cost merged plans. With worst and average case complexity analysis and empirical tests, we demonstrate that efficient and well-behaved approximation algorithms are applicable for optimizing general plans with large sizes.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-105.pdf,
105,1991,"Planning,Perception, and Robotics",Explanation-Based Generalization of Partially Ordered Plans,"Subbarao Kambhampati, Smadar Kedar","Most previous work in analytic generalization of plans dealt with totally ordered plans. These methods cannot be directly applied to generalizing partially ordered plans, since they do not capture all interactions among plan operators for all total orders of such plans. In this paper we introduce a new method for generalizing partially ordered plans. This method is based on providing EBG with explanations which systematically capture the interactions among plan operators for all the total orders of a partially-ordered plan. The explanations are based on the Modal Truth Criterion [2], which states the necessary and sufficient conditions for ensuring the truth of a proposition at any point in a plan (for a class of partially ordered plans). The generalizations obtained by this method guarantee successful and interaction-free execution of any total order of the generalized plan. In addition, the systematic derivation of the generalization algorithms from the Modal Truth Criterion obviates the need for carrying out a separate formal proof of correctness of the EBG algorithms.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-106.pdf,
106,1991,"Planning,Perception, and Robotics",Search Reduction in Hierarchical Problem Solving,Craig A. Knoblock,"It has long been recognized that hierarchical problem solving can be used to reduce search. Yet, there has been little analysis of the problem-solving method and few experimental results. This paper provides the first comprehensive analytical and empirical demonstrations of the effectiveness of hierarchical problem solving. First, the paper shows analytically that hierarchical problem solving can reduce the size of the search space from exponential to linear in the solution length and identifies a sufficient set of assumptions for such reductions in search. Second, it presents empirical results both in a domain that meets all of these assumptions as well as in domains in which these assumptions do not strictly hold. Third, the paper explores the conditions under which hierarchical problem solving will be effective in practice.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-107.pdf,
107,1991,"Planning,Perception, and Robotics",Characterizing Abstraction Hierarchies for Planning,"Craig A. Knoblock, Josh D. Tenenberg, Qiang Yang","The purposes of this paper are threefold. The first is to provide a crisp formalization of ABSTRIPS-style abstraction, since the lack of such formalizations has made it difficult to ascertain the uses and value of this type of abstraction in previous research. Second, we define the refinement relationship between solutions at different levels of the abstraction hierarchy. Such definitions are crucial to developing efficient search strategies with this type of hierarchical planning. And third, we provide a restriction on the abstraction mapping that provides a criterion for generating useful abstractions.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-108.pdf,
108,1991,"Planning,Perception, and Robotics",Preferential Semantics for Goals,"Michael P. Wellman, Jon Doyle","Goals, as typically conceived in AI planning, provide an insufficient basis for choice of action, and hence are deficient as the sole expression of an agent’s objectives. Decision-theoretic utilities offer a more adequate basis, yet lack many of the computational advantages of goals. We provide a preferential semantics for goals that grounds them in decision theory and preserves the validity of some, but not all, common goal operations performed in planning. This semantic account provides a criterion for verifying the design of goal-based planning strategies, thus providing a new framework for knowledge-level analysis of planning systems.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-109.pdf,
109,1991,"Planning,Perception, and Robotics",AGENT0: A Simple Agent Language and Its Interpreter,Yoav Shoham,"In [9] we defined the concept of agent oriented programming (AOP), which can be viewed as a specialization of object oriented programming (OOP). AOP views objects as agents with mental state, and, in the spirit of speech act theory, identifies a number of message types - informing, requesting, offering, and so on. AOP is a general framework. In this paper we present a specific and simple language called AGENTO; we define its syntax, present its interpreter, and illustrate both through an example.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-110.pdf,
110,1991,"Planning,Perception, and Robotics",Automatic Generation of Object Class Descriptions Using Symbolic Learning Techniques,"R. L. Cromwell, A. C. Kak","Object recognition requires complicated domain-specific rules. For many problem domains, it is impractical for a programmer to generate these rules. A method for automatically generating the required object class descriptions is needed - this paper presents a method to accomplish this goal. In our approach, the supervisor provides a series of example scene descriptions to the system, with accompanying object class assignments. Generalization rules then produce object class descriptions. These rules manipulate non-symbolic descriptors in a symbolic framework; the resulting class descriptions are useful both for object recognition and for providing clear explanations of the decision process. We present a simple method for maintaining an optimal description set as new examples (possibly of previously unseen classes) become available, providing needed updates to the description set. Finally, the system’s performance is shown as it learns object class descriptions from realistic scenes - video images of electronic components.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-111.pdf,
111,1991,"Planning,Perception, and Robotics",An Algorithm for Real-Time Tracking of Non-Rigid Objects,"John Woodfill, Ramin Zabih","We describe an algorithm for tracking an unknown object in natural scenes. We require that the object’s approximate initial location be available, and further assume that the it can be distinguished from the background by motion or stereo. No constraint is placed on the object’s shape, other than that it not change too rapidly from frame to frame. Objects are tracked and segmented cross-temporally using a massively parallel bottom-up approach. Our algorithm has been implemented on a Connection Machine, and runs in real time (15-20 frames per second).",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-112.pdf,
112,1991,"Planning,Perception, and Robotics",A New Framework for Sensor Interpretation: Planning to Resolve Sources of Uncertainty,"Norman Carver, Victor Lesser","Sensor interpretation involves the determination of high-level explanations of sensor data. Blackboard-based interpretation systems have usually been limited to incre,mental hypothesize and test strategies for resolving uncertainty. We have developed a new interpretation framework that supports the use of more sophisticated strategies like differential diagnosis. The RESUN framework has two key components: an evidential representation that includes explicit, symbolic encodings of the sources of uncertainty (SOUs) in the evidence for hypotheses and a script-based, incremental control planner. Interpretation is viewed as an incremental process of gathering evidence to resolve particular sources of uncertainty. Control plans invoke actions that examine the symbolic SOUs associated with hypotheses and use the resulting information to post goals to resolve uncertainty. These goals direct the system to expand methods appropriate for resolving the current sources of uncertainty in the hypotheses. The planner’s refocusing mechanism makes it possible to postpone focusing decisions when there is insufficient information to make decisions and provides opportunistic control capabilities, The RESUN framework has been implemented and experimentally verified using a simulated aircraft monitoring application.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-113.pdf,
113,1991,"Planning,Perception, and Robotics",The Geometry of Visual Coordination,"Jean-Yves Hervé, Rajeev Sharma, Peter Cucka","We present a new model for the perceptual reasoning involved in hand/eye coordination, and we show how this model can be developed into a control mechanism for a robot manipulator with a visual sensor. This new approach overcomes the high computational cost, the lack of robustness, and the need for precise calibration that plague traditional approaches. At the heart of our model is the Perceptual Kinematic Map (PKM), a direct mapping from the control space of the manipulator onto a space defined by a set of measurable image parameters. By exploring its workspace, the robot learns, qualitatively, the topology of its PKM and thus acquires the dexterity for future tasks, in a striking parallel to biological systems.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-114.pdf,
114,1991,"Planning,Perception, and Robotics",Anytime Problem Solving Using Dynamic Programming,Mark Boddy,"In previous work, we have advocated explicitly scheduling computation time for planning and problem solving (deliberetion) using a framework called ezpectation-driven iterative refinement. Within this framework, we have explored the problem of allocating deliberation time when the procedures used for deliberation implement anytime algorithms: algorithms that return some answer for any allocation of time. In our search for useful techniques for constructing anytime algorithms, we have discovered that dynemic programming shows considerable promise for the construction of anytime algorithms for a wide variety of problems. In this paper, we show how dynamic programming techniques can be used to construct useful anytime procedures for two problems: multiplying sequences of matrices, and the Travelling Salesman Problem. Dynamic programming can be applied to a wide variety of optimization and control problems, many of them relevant to current AI research (e.g., scheduling, probabilistic reasoning, and controlling machinery). Being able to solve these kinds of problems using anytime procedures increases the range of problems to which expectation-driven iterative refinement can be applied.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-115.pdf,
115,1991,"Planning,Perception, and Robotics",An Analysis of Error Recovery and Sensory Integration for Dynamic Planners,Bruce Abramson,"Strategic planners for robots designed to operate in a dynamic environment must be able to decide (i) how often a sensory request should be granted, and (ii) how to recover from a detected error. This paper derives closed-form formulas for the appropriate frequency of sensor integration as a function of parameters of the equipment, the domain, and the types of errors from which the system wishes to recover.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-116.pdf,
116,1991,"Planning,Perception, and Robotics",Global Symbolic Maps from Local Navigation,"David P. Miller, Marc G. Slack","In order to navigate autonomously, most robot systems are provided with some sort of global terrain map. To make storage practical, these maps usually have a high-level symbolic representation of the terrain. The robot’s symbolic map is then used to plan a local path. This paper describes a system which uses the reverse (and perhaps more natural) process. This system processes local sensor data in such a way as to allow efficient, reactive local navigation. A byproduct of this navigation process is an abstraction of the terrain information which forms a global symbolic terrain map of the terrain through which the robot has passed. Since this map is in the same format as that used by the local navigation system, the map is easy for the system to use, augment, or correct. Compared with the data from which the maps are created, the maps are very space efficient, and can be modified, or used for navigation in real-time. Experiments with this system, both in simulation, and with a real robot operating in natural terrain, are described.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-117.pdf,
117,1991,"Planning,Perception, and Robotics",Sensible Planning: Focusing Perceptual Attention,"Lonnie Chrisman, Reid Simmons","A primary problem facing real-world robots is the question of which sensing actions should be performed at any given time. It is important that an agent be economical with its allocation of sensing when sensing is expensive or when there are many possible sensing operations available. Sensing is rational when the expected utility from the information obtained outweighs the execution cost of the sensing operation itself. This paper outlines an approach to the efficient construction of plans containing explicit sensing operations with the objective of finding nearly optimal cost effective plans with respect to both action and sensing. The scheduling of sensing operations, in addition to the usual scheduling of physical actions, potentially results in an enornous increase in the computational complexity of planning. Our approach avoids this pitfall through strict adherence to a static sensing policy. The approach, based upon the Markov Decision Process paradigm, handles a significant amount of uncertainty in the outcomes of actions.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-118.pdf,
118,1991,"Planning,Perception, and Robotics",Fuzzy Modeling Using Generalized Neural Networks Kalman Filter Algorithm,Jyh-Shing R. Jang,"We propose a new approach to build a fuzzy inference system of which the parameters can be updated to achieve a desired input-output mapping. The structure of the proposed fuzzy inference system is called generalized neural networks, and its learning procedure (rules to update parameters) is basically composed of a gradient descent algorithm and Kalman filter algorithm. Specifically, we first introduce the concept of generalized neural networks (GNN’s) and develop a gradient-descent-based supervised learning procedure to update the GNN’s parameters. Secondly, we observe that if the overall output of a GNN is a linear combination of some of its parameters, then these parameters can be identified by one-time application of Kalman filter algorithm to minimize the squared error. According to the simulation results, it is concluded that the proposed new fuzzy inference system can not only incorporate prior knowledge about the original system but also fine-tune the membership functions of the fuzzy rules as the training data set varies.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-119.pdf,
119,1991,"Planning,Perception, and Robotics",Automatic Programming of Behavior-Based Robots Using Reinforcement Learning,"Sridhar Mahadevan, Jonathan Connell","This paper describes a general approach for automatically programming a behavior-based robot. New behaviors are learned by trial and error using a performance feedback function as reinforcement. Two algorithms for behavior learning are described that combine techniques for propagating reinforcement values temporally across actions and spatially across states. A behavior-based robot called OBELIX (see Figure 1) is described that learns several component behaviors in an example task involving pushing boxes. An experimental study using the robot suggests two conclusions. One, the learning techniques are able to learn the individual behaviors, sometimes outperforming a hand-coded program. Two, using a behavior-based architecture is better than using a monolithic architecture for learning the box pushing task.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-120.pdf,
120,1991,"Planning,Perception, and Robotics",Cost-Sensitive Reinforcement Learning for Adaptive Classification and Control,Ming Tan,"Standard reinforcement learning methods assume they can identify each state distinctly before making an action decision. In reality, a robot agent only has a limited sensing capability and identifying each state by extensive sensing can be time consuming. This paper describes an approach that learns active perception strategies in reinforcement learning and considers sensing costs explicitly. The approach integrates cost-sensitive learning with reinforcement learning to learn an efficient internal state representation and a decision policy simultaneously in a finite, deterministic environment. It not only maximizes the long-term discounted reward per action but also reduces the average sensing cost per state. The initial experimental results in a simulated robot navigation domain are encouraging.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-121.pdf,
121,1991,"Planning,Perception, and Robotics",Programming Robots Using Reinforcement Learning and Teaching,Long-Ji Lin,"Programming robots is a tedious task. So, there is growing interest in building robots which can learn by themselves. Self-improving, which involves trial and error, however, is often a slow process and could be hazardous in a hostile environment. By teaching robots how tasks can be achieved, learning time can be shortened and hazard can be minimized. This paper presents a general approach to making robots which can improve their performance from experiences as well as from being taught. Based on this proposed approach and other learning speedup techniques, a simulated learning robot was developed and could learn three moderately complex behaviors, which were then integrated in a subsumption style so that the robot could navigate and recharge itself. Interestingly, a real robot could actually use what was learned in the simulator to operate in the real world quite successfully.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-122.pdf,
122,1991,"Planning,Perception, and Robotics",Stabilizing Environments to Facilitate Planning and Activity: An Engineering Argument,"Kristian J. Hammond, Timothy M. Converse","An underlying assumption of research on learning from planning and activity is that agents can exploit regularities they find in the world. For agents that interact with a world over an extended period of time, there is another possibility: the exploited regularities can be created and maintained, rather than discovered. We explore the ways in which agents can actively stabilize the world to increase the predictability and tractability of acting within in it.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-123.pdf,
123,1991,"Planning,Perception, and Robotics",Underwater Experiments Using a Reactive System for Autonomous Vehicles,R. Peter Bonasso,"This paper describes a situated reasoning architecture, originally used with ground mobile robots, which is shown to easily integrate control theoretic algorithms, navigation heuristics and human supervision for semi-autonomous robot control in underwater field environments. The control architecture produces reaction plans that exploit low-level competences as operators. The low-level competences include both obstacle avoidance heuristics and control-theoretic algorithms for generating and following a velocity/acceleration trajectory. Experiments with an undersea remotely-piloted robot in a test tank at the Deep Submergence Laboratory at Woods Hole, MA are described. The robot performed both pilot-aided and autonomous exploration tasks robustly during normal changes in the task environment. The architecture was implemented in the GAPPS/REX situated automata programming language. The guaranteed constant cycle time of the synchronous REX circuits allowed for rapid tuning of the parameters of the control-theoretic and heuristic algorithms to obtain smooth, safe motion.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-124.pdf,
124,1991,"Planning,Perception, and Robotics",Failure Recovery: A Model and Experiments,"Adele E. Howe, Paul R. Cohen","This paper presents a model of failure recovery from which we have designed and tested sets of failure recovery methods in the Phoenix system. We derive the model, document its assumptions, and then test the validity of the assumptions and predictions of the model. We present three experiments. One derives baselines for failure recovery in the Phoenix environment. The second compares the performance of two strategies for selecting failure recovery methods. The third compares the performance of an initial set of failure recovery methods with a redesigned set t is predicted to have Bower expected cost.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-125.pdf,
125,1991,"ReasoningAbout Physical Systems",Automated Phase Portrait Analysis by Integrating Qualitative and Quantitative Analysis,"Toyoaki Nishida, Kenji Mizutani, Atsushi Kubota, Shuji Doshita","It has been widely believed that qualitative analysis guides quantitative analysis, while sufficient study has not been made from technical viewpoints. In this paper, we present a case study with PSX2NL, a program which autonomously analyzes the behavior of two-dimensional nonlinear differential equations, by integrating knowledge-based methods and numerical algorithms. PSX2NL focuses on geometric properties of solution curves of ordinary differential equations in the phase space. PSX2NL is designed based on a couple of novel ideas: (a) a set of flow mappings which is an abstract description of the behavior of solution curves, and (b) a flow grammar which specifies all possible patterns of solution curves, enabling PSX2NL to derive the most plausible interpretation when complete information is not available. We describe the algorithms for deriving flow mappings.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-126.pdf,
126,1991,"ReasoningAbout Physical Systems",Qualitative Analysis of Causal Feedback,"Philippe Rose, Mark A. Kramer","An analysis of the properties of qualitative differential equations involving feedback structures is presented. The topological interpretation of this theory serves as the basis for a simulator of qualitative differential equations, QUAF. QUAF predicts the initial trend and final state of each variable, thereby elucidating the general character of the response. The approach requires that causal differential equations replace algebraic forms derived from pseudo-steady state (moving equilibrium) assumptions. QUAF is compared to the qualitative simulator QSIM on an example involving interconnected tanks, and a significant narrowing of the number of interpretations of system behavior is observed.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-127.pdf,
127,1991,"ReasoningAbout Physical Systems",The Qualitative Difference Resolution Rule,Tom Bylander,"Consolidation is inferring the behavioral description of a device by composing the behavioral descriptions of its components, e.g., deriving the qualitative differential equations (QDEs) of a device from those of its components. In previous work, Dormoy and Raiman described the qualitative resolution rule, which is a general rule for deriving QDEs of combinations of components. However, the qualitative resolution rule is intractable in general. As a step toward understanding tractable qualitative reasoning, I present a new QDE resolution rule, the qualitative difference resolution rule, that supports the tractable consolidation of components in which direction of flow is dependent on the signs of pressure differences. Pipes and containers are general types of components that match this rule. The pressure regulator example also matches this rule.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-128.pdf,
128,1991,"ReasoningAbout Physical Systems",Analytic Solution of Qualitative Differential Equations,Philip Schaefer,"Numerical simulation, phase-space analysis, and analytic techniques are three methods used to solve quantitative differential equations. Most work in Qualitative Reasoning has dealt with analogs of the first two techniques, producing capabilities applicable to a wide range of systems. Although potentially of benefit, little has been done to provide closed-form, analytic solution techniques for qualitative differential equations (QDEs). This paper presents one such technique for the solution of a class of ordinary linear and nonlinear differential equations. The technique is capable of deriving closed-form descriptions of the qualitative temporal behavior represented by such equations. A language QFL for describing qualitative temporal behaviors is presented, and procedures and an implementation QDIFF that solves equations in this form are demonstrated.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-129.pdf,
129,1991,"ReasoningAbout Physical Systems",Model-Based Reconfiguration: Toward an Integration with Diagnosis,"Judith Crow, John Rushby","We extend Reiter’s general theory of model-based diagnosis [Reiter, 1987] to a theory of reconfiguration. The generality of Reiter’s theory readily supports an extension in which the problem of reconfiguration is viewed as a close analogue of the problem of diagnosis. Using a reconfiguration predicate rcfg analogous to the abnormality predicate ab, we formulate a strategy for reconfiguration by transforming that for diagnosis. A benefit of this approach is that algorithms for diagnosis can be exploited as algorithms for reconfiguration, thereby promoting an integrated approach to fault detection, identification, and reconfiguration.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-130.pdf,
130,1991,"ReasoningAbout Physical Systems",Focusing on Probable Diagnoses,Johan de Kleer,"Model-based diagnosis is based on first-principles reasoning using the behavioral specifications of the primitive components of a device. Unless the computational architecture of the model-based reasoning engine is carefully designed, combinatorial explosion renders the approach useless for devices consisting of more than a handful of components. This paper analyzes the diverse origins of this combinatorial explosion and outlines strategies to cope with each one. The resulting computational architecture for model-based diagnosis provides orders of magnitude performance improvement on large examples, making model-based approach practical for devices consisting of on the order of 3000 components.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-131.pdf,
131,1991,"ReasoningAbout Physical Systems",Characterizing Non-Intermittent Faults,"Olivier Raiman, Johan de Kleer, Vijay Saraswat, Mark Shirley","A faulty component that behaves consistently over time is said to behave non-intermittently. For any given set of inputs, such a component will always generate the same output. Assuming that components fail non-intermittently is a common simplifying strategy used by diagnosticians, because (1) many real-world devices often fail this way, (2) this strategy removes the need to repeat experiments, and (3) this strategy allows information from independent examples of system behavior to be combined in relatively simple ways. This paper extends the formal framework for diagnosis developed in [7, 12] to allow non-intermittency assumptions. In addition we show how the definitions can be easily integrated into ATMS-based diagnosis engines. Within our formulation, components can be individually assumed to be intermittent or non-intermittent.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-132.pdf,
132,1991,"ReasoningAbout Physical Systems",Domain Structure and the Complexity of Diagnostic Problem Solving,Thomas D. Wu,"This paper provides a quantitative analysis of domain structure and its effects on the complexity of diagnostic problem solving. It introduces a hypothesis about the modular structure of domains and proposes a measured called explanatory power. The distribution of explanatory power reveals the inherent structure of domains. We conjecture that such structure might facilitate problem solving, even when the problem solver does not exploit it explicitly. To test this hypothesis, we create a domain without structure by randomizing the distribution of explanatory power. We use the structured and randomized knowledge bases to study the effect of domain structure on two diagnostic algorithms, candidate generation and symptom clustering. The results indicate that inherent domain structure, even when not encoded explicitly, can facilitate problem solving. Such facilitation occurs for both the candidate generation and symptom clustering algorithms. Moreover, domain structure appears to benefit symptom clustering more than candidate generation, suggesting that the efficiency of symptom clustering derives in part from exploiting domain structure.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-133.pdf,
133,1991,"ReasoningAbout Physical Systems",Behavioral Aggregation Within Complex Situations: A Case Study Involving Dynamic Equilibria,"Shankar A. Rajamoney, Sang Hoe Koo","The analysis of large complex situations poses difficult problems for qualitative reasoning due to the complexity of reasoning from first principles and the proliferation of ambiguities. Abstraction is a promising solution to these problems. In this paper, we study a type of abstraction, behavioral aggregation- the process of grouping a set of individual entities that collectively behave as a unit. In particular, we show how to build aggregate models of situations involving dynamic equilibria and how to reason about their behavior. Finally, we demonstrate, through several examples, the benefits of reasoning at the aggregate level: a reduction in the complexity of reasoning and a compact, easily interpretable, description of the behavior.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-134.pdf,
134,1991,"ReasoningAbout Physical Systems",Structural Aggregation in Common-Sense Reasoning,"Zheng-Yang Liu, Arthur M. Farley","Structural aggregation is an inherent aspect of our ability in reasoning about the real-world. In this paper, we present our investigation of structural aggregation to simplify domain models and suppress irrelevant details of complex physical systems. We address the role of clustering, cluster-orientation and creation of black boxes in qualitative causal reasoning. We describe algorithms that automate structural aggregation to achieve efficiency and clarity in planning and explanation of physical system behaviors.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-135.pdf,
135,1991,"ReasoningAbout Physical Systems",Geometric Reasoning for Shape Design,"George Turkiyyah, Omar Ghattas","This paper describes a methodology for the design of shapes. Starting from an initial shape, a geometric reasoning kernel is used to generate and control a sequence of numerical optimization subproblems that converges to a final design- a topology and associated geometry- that can be significantly different from the starting shape. A subproblem in the sequence is systematically formulated from a geometric abstraction of current shape, and its objective function, constraints and bounds are dynamically derived. The geometric representation of the shape is adaptive and changes throughout the problem solving process to accommodate the shape change trends that occur. Shape evolution takes place within each subproblem and between subproblems. Intra-subproblem evolution is responsible for geometric modifications while inter-subproblem evolution handles topology modifications. The combination of geometric reasoning and numerical optimization techniques provides a robust and systematic methodology for shape synthesis that can generate new design shapes without relying on heuristic or domain specific knowledge.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-136.pdf,
136,1991,"ReasoningAbout Physical Systems",The Use of Intelligently Controlled Simulation to Predict a Machine’s Long-Term Behavior,Andrew Gelsey,"I present algorithms for automated long-term behavior prediction which can recognize when a simulation has run long enough to produce a representative behavior sample, characterize the behavior, and determine whether this behavior will continue forever, or eventually terminate or otherwise change its character. I have implemented these algorithms in a working program which does long-term behavior prediction for mechanical devices.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-137.pdf,
137,1991,"ReasoningAbout Physical Systems",Incremental Configuration Space Construction for Mechanism Analysis,"Leo Joskowicz, Elisha Sacks","We present an incremental configuration space (CS) construction algorithm for mechanisms described as collections of subassemblies of rigid parts. The inputs are the initial subassembly configurations and the sub-assembly CSs partitioned into uniform motion regions in which part contacts are constant and motions are monotonic. The output is a partition of the mechanism CS into uniform motion regions. The algorithm optimizes CS construction by incrementally enumerating and testing only the regions reachable from the initial configuration. We implement the algorithm for subassemblies whose uniform motion regions are polyhedral or are of dimension two or lower. The program constructs the exact CS when possible and an approximate CS otherwise. The approximate CS usually is qualitatively correct and in good quantitative agreement with the true CS. The program covers most mechanisms composed of linkages and fixed-axes kinematic pairs, two subassembly types for which CS construction programs are available.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-138.pdf,
138,1991,"TractableInference",Negation and Proof by Contradiction in Access-Limited Logic,"J. M. Crawford, B. J. Kuipers","Access-Limited Logic (ALL) is a language for knowledge representation which formalizes the access limitations inherent in a network structured knowledge-base. Where a deductive method such as resolution would retrieve all assertions that satisfy a given pattern, an access-limited logic retrieves all assertions reachable by following an available access path. In this paper, we extend previous work to include negation, disjunction, and the ability to make assumptions and reason by contradiction. We show that the extended ALLneg remains Socratically Complete (thus guaranteeing that for any fact which is a logical consequence of the knowledge-base, there exists a series of preliminary queries and assumptions after which a query of the fact will succeed) and computationally tractable. We show further that the key factor determining the computational difficulty of finding such a series of preliminary queries and assumptions is the depth of assumption nesting. We thus demonstrate the existence of a family of increasingly powerful inference methods, parameterized by the depth of assumption nesting, ranging from incomplete and tractable to complete and intractable.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-139.pdf,
139,1991,"TractableInference",Knowledge Compilation using Horn Approximations,"Bart Selman, Henry Kautz","We present a new approach to developing fast and efficient knowledge representation systems. Previous approaches to the problem of tractable inference have used restricted languages or incomplete inference mechanisms - problems include lack of expressive power, lack of inferential power, and/or lack of a formal characterization of what can and cannot be inferred. To overcome these disadvantages, we introduce a knowledge compilation method. We allow the user to enter statements in a general, unrestricted representation language, which the system compiles into a restricted language that allows for efficient inference. Since an exact translation into a tractable form is often impossible, the system searches for the best approximation of the original information. We will describe how the approximation can be used to speed up inference without giving up correctness or completeness. We illustrate our method by studying the approximation of logical theories by Horn theories. Following the formal definition of Horn approximation, we present ""anytime"" algorithms for generating such approximations. We subsequently discuss extensions to other useful classes of approximations.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-140.pdf,
140,1991,"TractableInference",Observations on Cognitive Judgments,David McAllester,"It is obvious to anyone familiar with the rules of the game of chess that a king on an empty board can reach every square. It is true, but not obvious, that a knight can reach every square. Why is the first fact obvious but the second fact not? This paper presents an analytic theory of a class of obviousness judgments of this type. Whether or not the specifics of this analysis are correct, it seems that the study of obviousness judgments can be used to construct integrated theories of linguistics, knowledge representation, and inference.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-141.pdf,
141,1991,"TractableInference",Natural Language Based Inference Procedures Applied to Schubert’s Steamroller,"Robert Givan, David McAllester, Sameer Shalaby",We have previously argued that the syntactic structure of natural language can be exploited to construct powerful polynomial time inference procedures. This paper supports the earlier arguments by demonstrating that a natural language based polynomial time procedure can solve Schubert' s steamroller in a single step.,https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-142.pdf,
142,1991,"InvitedTalks",Approximate Reasoning Systems: A Personal Perspective,Piero P. Bonissone,"In this presentation we provide a personal perspective of the progress made in the field of approximate reasoning systems. Because of time and space limitations, we will limit the scope of our discussion to cover the most notable trends and efforts in reasoning with uncertainty and vagueness.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-143.pdf,
143,1991,"InvitedTalks",Robot Planning,Drew McDermott,"This paper discusses robots and planning, surveys several subareas of robot planning, discusses how much planning robots need, and recommends a theoretical framework that will allow us to talk about operations on plans.",https://aaai.org/Library/AAAI/1991/../../../Papers/AAAI/1991/AAAI91-144.pdf,
