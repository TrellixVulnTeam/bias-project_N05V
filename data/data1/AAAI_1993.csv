,conference_year,category,title,author,abstract,download_url,keywords
0,1993,Automated Reasoning,On Computing Minimal Models,"Rachel Ben-Eliyahu, Rina Dechter","This paper addresses the problem of computing the minimal models of a given CNF propositional theory. We present two groups of algorithms. Algorithms in the first group are efficient when the theory is almost Horn, that is, when there are few non-Horn clauses and/or when the set of all literals that appear positive in any non-Horn clause is small. Algorithms in the other group are efficient when the theory can be represented as an acyclic network of low-arity relations. Our algorithms suggest several characterizations of tractable subsets for the problem of finding minimal models.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-001.pdf,
1,1993,Automated Reasoning,On the Adequateness of the Connection Method,"Antje Beringer, Steffen Hölldobler","Roughly speaking, adequatness is the property of a theorem proving method to solve simpler problems faster than more difficult ones. Automated inferencing methods are often not adequate as they require thousands of steps to solve problems which humans solve effortlessly, spontaneously, and with remarkable efficiency. L. Shastri and V. Ajjanagadde - who call this gap the artificial intelligence paradox - suggest that their connectionist inference system is a first step toward bridging this gap. In this paper we show that their inference method is equivalent to reasoning by reductions in the well-known connection method. In particular, we extend a reduction technique called evaluation of isolated connections such that this technique - together with other reduction techniques - solves all problems which can be solved by Shastri and Aianagadde’s system under the same parallel time and space requirements. Consequently, we obtain a semantics for Shastri and Ajjanagadde’s logic. But, most importantly, if Shastri and Ajjanagadde' s logic really captures the kind of reasoning which humans can perform efficiently, then this paper shows that a massively parallel implementation of the connection method is adequate.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-002.pdf,
2,1993,Automated Reasoning,Rough Resolution: A Refinement of Resolution to Remove Large Literals,"Heng Chu, David A. Plaisted","Semantic hyper-linking [Plaisted et al., 1992, Chu and Plaisted, 1993, Chu and Plaisted, 1992] has been proposed recently to use semantics with hyper-linking [Lee and Plaisted, 1992], an instance-based theorem proving technique. Ground instances are generated until an unsatisfiable ground set is obtained; semantics is used to greatly reduce the search space. One disadvantage of semantic hyper-linking is,that large ground literals, if needed in the proofs, sometimes are hard to generate. In this paper we propose rough resolution, a refinement of resolution [Robinson, 1965], to only resolve upon maximum literals, that are potentially large in ground instances, and obtain rough resoluents. Rough resolvents can be used by semantic hyper-linking to avoid generating large ground literals since maximum literals have been deleted. As an example, we will show how rough resolution helps to prove LIM3 [Bledsoe, 1990 , which cannot be proved using semantic 9 hyper-linking only. We will also show other results in which rough resolution helps to find the proofs faster. Though incomplete, rough resolution can be used with other complete methods that prefer small clauses.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-003.pdf,
3,1993,Automated Reasoning,Experimental Results on the Crossover Point in Satisfiability Problems,"James M. Crawford, Larry D. Auton","Determining whether a propositional theory is satisfiable is a prototypical example of an NP-complete problem. Further, a large number of problems that occur in knowledge representation, learning, planning, and other areas of AI are essentially satisfiability problems. This paper reports on a series of experiments to determine the location of the crossover point - the point at which half the randomly generated propositional theories with a given number of variables and given number of clauses are satisfiable - and to assess the relationship of the crossover point to the difficulty of determining satisfiability. We have found empirically that, for Q-SAT, the number of clauses at the crossover point is a linear function of the number of variables. This result is of theoretical interest since it is not clear why such a linear relationship should exist, but it is also of practical interest since recent experiments [Mitchell et al. 92; Cheeseman et al. 91] indicate that the most computationally difficult problems tend to be found near the crossover point. We have also found that for random 3-SAT problems below the crossover point, the average time complexity of satisfiability problems seems empirically to grow linearly with problem size. At and above the crossover point the complexity seems to grow exponentially, but the rate of growth seems to be greatest near the crossover point.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-004.pdf,
4,1993,Automated Reasoning,Towards an Understanding of Hill-Climbing Procedures for SAT,"Ian P. Gent, Toby Walsh","Recently several local hill-climbing procedures for propositional satisfiability have been proposed which are able to solve large and difficult problems beyond the reach of conventional algorithms like Davis-Putnam. By the introduction of some new variants of these procedures, we provide strong experimental evidence to support our conjecture that neither greediness nor randomness is important in these procedures. One of the variants introduced seems to offer significant improvements over earlier procedures. In addition, we investigate experimentally how performance depends on their parameters. Our results suggest that runtime scales less than simply exponentially in the problem size.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-005.pdf,
5,1993,Automated Reasoning,Reasoning with Characteristic Models,"Henry A. Kautz, Michael J. Kearns, Bart Selman","Formal AI systems traditionally represent knowledge using logical formulas. We will show, however, that for certain kinds of information, a model-based representation is more compact and enables faster reasoning than the corresponding formula-based representation. The central idea behind our work is to represent a large set of models by a subset of characteristic models. More specifically, we examine model-based representations of Horn theories, and show that there are large Horn theories that can be exactly represented by an exponentially smaller set of characteristic models. In addition, we will show that deduction based on a set of characteristic models takes only linear time, thus matching the performance using Horn theories. More surprisingly, abduction can be performed in polynomial time using a set of characteristic models, whereas abduction using Horn theories is NP-complete.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-006.pdf,
6,1993,Automated Reasoning,The Breakout Method for Escaping from Local Minima,Paul Morris,"A number of algorithms have recently been proposed that use iterative improvement (a form of hill-climbing) to solve constraint satisfaction problems. These techniques have had dramatic success on certain problems. However, one factor limiting their wider application is the possibility of getting stuck at non-solution local minima. In this paper we describe an iterative improvement algorithm, called Breakout, that can escape from local minima. We present empirical evidence that this method is very effective in cases where previous approaches have difficulty. Although Breakout is not, theoretically complete, in practice it appears to almost always find solutions ,for solvable problems. We prove that an idealized (but less efficient) version of the algorithm is complete.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-007.pdf,
7,1993,Automated Reasoning,An Empirical Study of Greedy Local Search for Satisfiability Testing,"Bart Selman, Henry A. Kautz","GSAT is a randomized local search procedure for solving propositional satisfiability problems. GSAT can solve hard, randomly generated problems that are an order of magnitude larger than those that can be handled by more traditional approaches, such as the Davis-Putnam procedure. This paper presents the results of numerous experiments we have performed with GSAT, in order to improve our understanding of its capabilities and limitations. We first characterize the space traversed by GSAT. We will see that for nearly all problem classes we have encountered, the space consists of a steep descent followed by broad flat plateaus. We then compare GSAT with simulated annealing, and show how GSAT can be viewed as an efficient method for executing the low-temperature tail of an annealing schedule. Finally, we report on extensions to the basic GSAT procedure. We discuss two general, domain-independent extensions that dramatically improve GSAT’s performance on structured problems: the use of clause weights, and a way to average in near-solutions when initializing the procedure before each try.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-008.pdf,
8,1993,Case-Based Reasoning,Projective Visualization: Acting from Experience,Marc Goodman,"This paper describes Projective Visualization, which uses previous observation of a process or activity to project the results of an agent’s actions into the future. Actions which seem likely to succeed are selected and applied. Actions which seem likely to fail are rejected, and other actions can be generated and evaluated. This paper presents a description of the architecture for Projective Visualization, preliminary results on learning to act from observations of a reactive system, and a comparison of two types of Case Projection (how situations are projected into the future).",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-009.pdf,
9,1993,Case-Based Reasoning,Representing and Using Procedural Knowledge to Build Geometry Proofs,"Thomas F. McDougal, Kristian J. Hammond","What is the nature of expertise? This paper posits an answer to that question in the domain of geometry problem-solving. We present a computer program called POLYA which makes use of explicit planning knowledge to solve geometry proof problems, integrating the processes of parsing the diagram and writing the proof.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-010.pdf,
10,1993,Case-Based Reasoning,Case-Based Diagnostic Analysis in a Blackboard Architecture,"Edwina L. Rissland, Jody J. Daniels, Zachary B. Rubinstein, David B. Skalak","In this project we study the effect of a user’s high-level expository goals upon the details of how case-based reasoning (CBR) is performed, and, vice versa, the effect of feedback from CBR on them. Our thesis is that case retrieval should reflect the user’s ultimate goals in appealing to cases and that these goals can be affected by the cases actually available in a case base. To examine this thesis, we have designed and built FRANK (Flexible Report and Analysis System), which is a hybrid, blackboard system that integrates case-based, rule-based, and planning components to generate a medical diagnostic report that reflects a user’s viewpoint and specifications. FRANK’s control module relies on a set of generic hierarchies that provide taxonomies of standard report types and problem-solving strategies in a mixed-paradigm environment. Our second focus in FRANK is on its response to a failure to retrieve an adequate set of supporting cases. We describe FRANK’s planning mechanisms that dynamically re-specify the memory probe or the parameters for case retrieval when an inadequate set of cases is retrieved, and give an extended example of how the system responds to retrieval failures.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-011.pdf,
11,1993,Case-Based Reasoning,A Framework and an Analysis of Current Proposals for the Case-Based Organization and Representation of Procedural Knowledge,"Roland Zito-Wolf, Richard Alterman","Case-based reasoning refers to the class of memory-based problem solving methods which emphasize the adaptation of recalled solutions (explanations, diagnoses, plans) over the generation of solutions from first principles. CBR has become a popular methodology, resulting in a proliferation of case organization and representation proposals. The goal of this paper is to sort through some of these proposals. Using the formal models of ""procedure,, and ""case-based reasoning"" introduced in Zito-Wolf and Alterman (1992), we compare three current proposals for the organization of procedural case-bases: individual cases, microcases, and multicases. We give a worst-case analysis that shows the advantages of the multicase in terms of case storage and retrieval costs. The model predicts that multicases reduce case storage and retrieval costs as compared to the other two models. We then provide some empirical evidence from an implemented system that suggests that the trends observed in the formal model are also observable in case bases of practical size.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-012.pdf,
12,1993,Complexity in Machine Learning,Cryptographic Limitations on Learning One-Clause Logic Programs,William W. Cohen,"An active area of research in machine learning is learning logic programs from examples. This paper investigates formally the problem of learning a single Horn clause: we focus on generalizations of the language of constant-depth determinate clauses, which is used by several practical learning systems. We show first that determinate clauses of logarithmic depth are not learnable. Next we show that learning indeterminate clauses with at most k indeterminate variables is equivalent to learning DNF. Finally, we show that recursive constant-depth determinate clauses are not learnable. Our primary technical tool is the method of prediction-preserving reducibilities introduced by Pitt and Warmuth [1990]; as a consequence our results are independent of the representations used by the learning system.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-013.pdf,
13,1993,Complexity in Machine Learning,Pac-Learning a Restricted Class of Recursive Logic Programs,William W. Cohen,"A crucial problem in ""inductive logic programming"" is learning recursive logic programs from examples alone; current systems such as GOLEM and FOIL often achieve success only for carefully selected sets of examples. We describe a program called FORCE2 that uses the new technique of ""forced simulation"" to learn two-clause ""closed"" linear recursive ij-determinate programs; although this class of programs is fairly restricted, it does include most of the standard benchmark problems. Experimentally, FORCE2 requires fewer examples than FOIL, and is more accurate when learning from randomly chosen datasets. Formally, FORCE2 is also shown to be a pat-learning algorithm in a variant of Valiant’s [1984] model, in which we assume the ability to make two types of queries: one which gives an upper bound on the depth of the proof for an example, and one which determines if an example can be proved in unit depth.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-014.pdf,
14,1993,Complexity in Machine Learning,Learnability in Inductive Logic Programming: Some Basic Results and Techniques,"Michael Frazier, C. David Page, Jr.","Inductive logic programming is a rapidly growing area of research that centers on the development of inductive learning algorithms for first-order definite clause theories. An obvious framework for inductive logic programming research is the study of the pac-learnability of various restricted classes of these theories. Of particular interest are theories that include recursive definite clauses. Because little work has been done within this framework, the need for initial results and techniques is great. This paper presents results about the pac-learnability of several classes of simple definite clause theories that are allowed to include a recursive clause. In so doing, the paper uses techniques that may be useful in studying the learnability of more complex classes.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-015.pdf,
15,1993,Complexity in Machine Learning,Complexity Analysis of Real-Time Reinforcement Learning,"Sven Koenig, Reid G. Simmons","This paper analyzes the complexity of on-line reinforcement learning algorithms, namely asynchronous realtime versions of Q-learning and value-iteration, applied to the problem of reaching a goal state in deterministic domains. Previous work had concluded that, in many cases, tabula rasa reinforcement learning was exponential for such problems, or was tractable only if the learning algorithm was augmented. We show that, to the contrary, the algorithms are tractable with only a simple change in the task representation or initialization. We provide tight bounds on the worst-case complexity, and show how the complexity is even smaller if the reinforcement learning algorithms have initial knowledge of the topology of the state space or the domain has certain special properties. We also present a novel bi-directional Q-learning algorithm to find optimal paths from all states to a goal state and show that it is no more complex than the other algorithms.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-016.pdf,
16,1993,Constraint-Based Reasoning,Arc-Consistency and Arc-Consistency Again,"Christian Bessière, Marie-Odile Cordier","Constraint networks are known as a useful way to formulate problems such as design, scene labeling, temporal reasoning, and more recently natural language parsing. The problem of the existence of solutions in a constraint network is NP-complete. Hence, consistency techniques have been widely studied to simplify constraint networks before or during the search of solutions. Arc-consistency is the most used of them. Mohr and Henderson have proposed AC-4, an algorithm having an optimal worst-case time complexity. But it has two drawbacks: its space complexity and its average time complexity. In problems with many solutions, where the size of the constraints is large, these drawbacks become so important that users often replace AC-4 by AC-3, a non-optimal algorithm. In this paper, we propose a new algorithm, AC-6, which keeps the optimal worst-case time complexity of AC-4 while working out the drawback of space complexity. More, the average time complexity of AC-6 is optimal for constraint networks where nothing is known about the semantic of the constraints. At the end of the paper, experimental results show how much AC-6 outperforms AC-3 and AC-4.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-017.pdf,
17,1993,Constraint-Based Reasoning,On the Consistency of General Constraint-Satisfaction Problems,Philippe Jégou,"The problem of checking for consistency of Constraint-Satisfaction Problems (CSPs) is a fundamental problem in the field of constraint-based reasoning. Moreover, it is a hard problem since satisfiability of CSPs belongs to the class of NP-complete problems. So, in (Freuder 1982), Freuder gave theoretical results concerning consistency of binary CSPs (two variables per constraints). In this paper, we proposed an extension to these results to general CSP (n-ary constraints). On one hand, we define a partial consistency well adjusted to general CSPs called hyper-k-consistency. On the other hand, we proposed a measure of the connectivity of hypergraphs called width of hypergraphs. Using width of hypergraphs and hyper-k-consistency, we derive a theorem defining a sufficient condition for consistency of general CSPs.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-018.pdf,
18,1993,Constraint-Based Reasoning,Integrating Heuristics for Constraint Satisfaction Problems: A Case Study,Steven Minton,"This paper describes a set of experiments with a system that synthesizes constraint satisfaction programs. The system, MULTI-TAC, is a CSP ""expert"" that can specialize a library of generic algorithms and methods for a particular application. MULTI-TAC not only proposes domain-specific versions of its generic heuristics, but also searches for the best combination of these heuristics and integrates them into a complete problem-specific program. We demonstrate MULTI-TAC'S capabilities on a combinatorial problem, ""Minimum Maximal Matching"", and show that MULTI-TAC can synthesize programs for this problem that are on par with hand-coded programs. In synthesizing a program, MULTI-TAC bases its choice of heuristics on the instance distribution, and we show that this capability has a significant impact on the results.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-019.pdf,
19,1993,Constraint-Based Reasoning,Coping With Disjunctions in Temporal Constraint Satisfaction Problems,"Eddie Schwalb, Rina Dechter","Path-consistency algorithms, which are polynomial for discrete problems, are exponential when applied to problems involving quantitative temporal information. The source of complexity stems from specifying relationships between pairs of time points as disjunction of intervals. We propose a polynomial algorithm, called ULT, that approximates path-consistency in Temporal Constraint Satisfaction Problems (TCSPs). We compare ULT empirically to path-consistency and directional path-consistency algorithms. When used as a preprocessing to backtracking, ULT is shown to be 10 times more effective then either DPC or PC-2.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-020.pdf,
20,1993,Constraint-Based Reasoning,Nondeterministic Lisp as a Substrate for Constraint Logic Programming,"Jeffrey Mark Siskind, David Allen McAllester","We have implemented a comprehensive constraint-based programming language as an extension to COMMON LISP. This constraint package provides a unified framework for solving both numeric and non-numeric systems of constraints using a combination of local propagation techniques including binding propagation, Boolean constraint propagation, generalized forward checking, propagation of bounds, and unification. The backtracking facility of the nondeterministic dialect of COMMON LISP used to implement this constraint package acts as a general fallback constraint solving method mitigating the incompleteness of local propagation.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-021.pdf,
21,1993,Constraint-Based Reasoning,Slack-Based Heuristics for Constraint Satisfaction Scheduling,"Stephen F. Smith, Cheng-Chung Cheng","In this paper, we define and empirically evaluate new heuristics for solving the job shop scheduling problem with non-relaxable time windows. The hypothesis underlying our approach is that by approaching the problem as one of establishing sequencing constraints between pairs of operations requiring the same resource (as opposed to a problem of assigning start times to each operation) and by exploiting previously developed analysis techniques for limiting search through the space of possible sequencing decisions, simple, localized look-ahead techniques can yield problem solving performance comparable to currently dominating techniques that rely on more sophisticated analysis of resource contention. We define a series of attention focusing heuristics based on simple analysis of the temporal flexibility associated with different sequencing decisions, and a similarly motivated heuristic for determining how to sequence a given operation pair. Performance results are reported on a suite of benchmark problems previously investigated by two advanced approaches, and our simplified look-ahead analysis techniques are shown to provide comparable problem solving leverage at reduced computational cost.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-022.pdf,
22,1993,Constraint-Based Reasoning,A Constraint Decomposition Method for Spatio-Temporal Configuration Problems,Toshikazu Tanimoto,"This paper describes a flexible framework and an efficient algorithm for constraint-based spatio-temporal configuration problems. Binary constraints between spatio-temporal objects are first converted to constraint regions, which are then decomposed into hierarchical data structures; based on this constraint decomposition, an improved backtracking algorithm called HBT can compute a solution quite efficiently. In contrast to other approaches, the proposed method is characterized by the efficient handling of arbitrarily-shaped objects, and the flexible integration of quantitative and qualitative constraints; it allows a wide range of objects and constraints to be utilized for specifying a spatio-temporal configuration. The method is intended primarily for configuration problems in user interfaces, but can effectively be applied to similar problems in other areas as well.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-023.pdf,
23,1993,Constraint-Based Reasoning,Extending Deep Structure,"Colin P. Williams, Tad Hogg","In a previous paper we defined the ""deep structure"" of a constraint satisfaction problem to be that set system produced by collecting the nogood ground instances of each constraint and keeping only those that are not supersets of any other. We then showed how to use such deep structure to predict where, in a space of problem instances, an abrupt transition in computational cost is to be expected. This paper explains how to augment this model with enough extra details to make more accurate estimates of the location of these phase transitions. We also show that the phase transition phenomenon exists for a much wider class of search algorithms than had hitherto been thought and explain theoretically why this is the case.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-024.pdf,
24,1993,Diagnostic Reasoning,Multiple Dimensions of Generalization In Model-Based Troubleshooting,"Randall Davis, Paul Resnick","Two observations motivate our work: (a) model-based diagnosis programs are powerful but do not learn from experience, and (b) one of the long-term trends in learning research has been the increasing use of knowledge to guide and inform the process of induction. We have developed a knowledge-guided learning method, based in EBL, that allows a model-based diagnosis program to selectively accumulate and generalize its experience. Our work is novel in part because it produces several different kinds of generalizations from a single example. Where previous work in learning has for the most part intensively explored one or another specific kind of generalization, our work has focused on accumulating and using multiple different grounds for generalization, i.e., multiple domain theories. As a result our system not only learns from a single example (as in all EBL), it can learn multiple things from a single example. Simply saying there ought to be multiple grounds for generalization only opens up the possibility of exploring more than one domain theory. We provide some guidance in determining which grounds to explore by demonstrating that in the domain of physical devices, causal models are a rich source of useful domain theories. We also caution that adding more knowledge can sometimes degrade performance. Hence we need to select the grounds for generalization carefully and analyze the resulting rules to ensure that they improve performance. We illustrate one such quantitative analysis in the context of a model-based troubleshooting program, measuring and analyzing the gain resulting from the generalizations produced.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-025.pdf,
25,1993,Diagnostic Reasoning,Hybrid Case-Based Reasoning for the Diagnosis of Complex Devices,"M. P. Féret, J. I. Glasgow","A novel approach to integrating case-based reasoning with model-based diagnosis is presented. The main idea is to use the model of the device and the results of diagnostic tests to index and match cases representing past diagnostic situations with the current one. The initial diagnostic methodology is presented as well as the problems encountered while applying this methodology to two real-world devices. The incorporation of a case-based reasoning system is then motivated and described in detail. Experimental results show the effectiveness of both the indexing schema and the matching algorithm. The paper also discusses how and why these results can be generalized to a multiple fault situation, to other types of device models and to other applications in the field of artificial intelligence.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-026.pdf,
26,1993,Diagnostic Reasoning,An Epistemology for Clinically Significant Trends,"Ira J. Haimowitz, Isaac S. Kohane","We have written a computer program called TrenD, for automated trend detection during process monitoring. The program uses a representation called ttwzd templates that define disorders as typical patterns of relevant variables. These patterns consist of a partially ordered set of temporal intervals with uncertain endpoints. Attached to each temporal interval are value constraints on real-valued functions of measurable parameters. As TrenD, receives measured data of the monitored process, the program creates hypotheses of how the process has varied over time. We introduce the importance of a distinct trend representation in knowledge-based systems. Then we demonstrate how trend templates may represent trends that occur at fixed times or at unknown times, and their utility for domains that are quantitatively both poorly and well understood. Finally we present experimental results of TrenD, diagnosing pediatric growth disorders from heights, weights, bone ages, and pubertal data of twenty patients seen at Boston Children’s Hospital.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-027.pdf,
27,1993,Diagnostic Reasoning,A Framework for Model-Based Repair,"Ying Sun, Daniel S. Weld","We describe IRS, a program that combines partial-order planning with GDE-style, model-based diagnosis to achieve an integrated approach to repair. Our system makes three contributions to the field of diagnosis. First, we provide a unified treatment of both information-gathering and state-altering actions via the UWL representation language. Second, we describe a way to use part-replacement operations (in addition to probes) to gather diagnostic information. Finally, we define a cost function for decision making that accounts for both the eventual need to repair broken parts and the dependence of costs on the device state.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-028.pdf,
28,1993,Discourse Analysis,A Method for Development of Dialogue Managers for Natural Language Interfaces,Arne Jönsson,"This paper describes a method for the development of dialogue managers for natural language interfaces. A dialogue manager is presented designed on the basis of both a theoretical investigation of models for dialogue management and an analysis of empirical material. It is argued that for natural language interfaces many of the human interaction phenomena accounted for in, for instance, plan-based models of dialogue do not occur. Instead, for many applications, dialogue in natural language interfaces can be managed from information on the functional role of an utterance as conveyed in the linguistic structure. This is modelled in a dialogue grammar which controls the interaction. Focus structure is handled using dialogue objects recorded in a dialogue tree which can be accessed through a scoreboard by the various modules for interpretation, generation and background system access. A sublanguage approach is proposed. For each new application the Dialogue Manager is customized to meet the needs of the application. This requires empirical data which are collected through Wizard of Oz simulations. The corpus is used when updating the different knowledge sources involved in the natural language interface. In this paper the customization of the Dialogue Manager for database information retrieval applications is also described.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-029.pdf,
29,1993,Discourse Analysis,Mutual Beliefs of Multiple Conversants: A Computational Model of Collaboration in Air Traffic Control,"David G. Norvick, Karen Ward","This work develops a computational model for representing and reasoning about dialogue in terms of the mutuality of belief of the conversants. We simulated cooperative dialogues at the speech act level and compared the simulations with actual dialogues between pilots and air traffic controllers engaged in real tasks. In the simulations, addressees and overhearers formed beliefs and took actions appropriate to their individual roles and contexts. The result is a computational model capable of representing the evolving context of complete real-world multiparty task-oriented conversations in the air traffic control domain.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-030.pdf,
30,1993,Discourse Analysis,An Optimizing Method for Structuring Inferentially Linked Discourse,"Ingrid Zukerman, Richard McConachy","In recent times, there has been an increase in the number of Natural Language Generation systems that take into consideration a user’s inferences. The statements generated by these systems are typically connected by inferential links, which are opportunistic in nature. In this paper, we describe a discourse structuring mechanism which organizes inferentially linked statements as well as statements connected by certain prescriptive links. Our mechanism first extracts relations and constraints from the output of a discourse planner. It then uses this information to build a directed graph whose nodes are rhetorical devices, and whose links are the relations between these devices. The mechanism then applies a search procedure to optimize the traversal through the graph. This process generates an ordered set of linear discourse sequences, where the elements of each sequence are maximally connected. Our mechanism has been implemented as the discourse organization component of a system called WISHFUL which generates concept explanations.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-031.pdf,
31,1993,Distributed Problem Solving,A One-shot Dynamic Coordination Algorithm for Distributed Sensor Networks,"Keith Decker, Victor Lesser","This paper presents a simple, fast coordination algorithm for the dynamic reorganization of agents in a distributed sensor network. Dynamic reorganization is a technique for adapting to the current local problem-solving situation that can both increase expected system performance and decrease the variance in performance. We compare our dynamic organization algorithm to a static algorithm with lower overhead. 'One-shot' refers to the fact that the algorithm only uses one meta-level communication action. The other theme of this paper is our methodology for analyzing complex control and coordination issues without resorting to a handful of single-instance examples. Using a general model that we have developed of distributed sensor network environments [Decker and Lesser, 1993a], we present probabilistic performance bounds for our algorithm given any number of agents in any environment that fits our assumptions. This model also allows us to predict exactly in what situations and environments the performance benefits of dynamic reorganization outweigh the overhead.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-032.pdf,
32,1993,Distributed Problem Solving,Quantitative Modeling of Complex Computational Task Environments,"Keith Decker, Victor Lesser","Formal approaches to specifying how the mental state of an agent entails that it perform particular actions put the agent at the center of analysis. For some questions and purposes, it is more realistic and convenient for the center of analysis to be the task environment, domain, or society of which agents will be a part. This paper presents such a task environment-oriented modeling framework that can work hand-in-hand with more agent-centered approaches. Our approach features careful attention to the quantitative computational interrelationships between tasks, to what information is available (and when) to update an agent’s mental state, and to the general structure of the task environment rather than single-instance examples. A task environment model can be used for both analysis and simulation; it avoids the methodological problems of relying solely on single-instance examples, and provides concrete, meaningful characterizations with which to state general theories. This paper will give an example of a model in the context of cooperative distributed problem solving, but our framework is used for analyzing centralized and parallel control as well.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-033.pdf,
33,1993,Distributed Problem Solving,Overeager Reciprocal Rationality and Mixed Strategy Equilibria,"Edmund H. Durfee, Jaeho Lee, Piotr J. Gmytrasiewicz","A rational agent in a multiagent world must decide on its actions based on the decisions it expects others to make, but it might believe that they in turn might be basing decisions on what they believe the initial agent will decide. Such reciprocal rationality leads to a nesting of models that can potentially become intractable. To solve such problems, game theory has developed techniques for discovering rational, equilibrium solutions, and AI has developed computational, recursive methods. These different approaches can involve different solution concepts. For example, the Recursive Modeling Method (RMM) finds different solutions than game-theoretic methods when solving problems that require mixed-strategy equilibrium solutions. In this paper, we show that a crucial difference between the approaches is that RMM employs a solution concept that is overeager. This eagerness can be reduced by introducing into RMM second-order knowledge about what it knows, in the form of a flexible function for mapping relative expected utility of an option into the probability that the agent will pursue that option. This modified solution concept can allow RMM to derive the same mixed equilibrium solutions as game-theory, and thus helps us delineate the types of knowledge that lead to alternative solution concepts.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-034.pdf,
34,1993,Distributed Problem Solving,Solving the Really Hard Problems with Cooperative Search,"Tad Hogg, Colin P. Williams",We present and experimentally evaluate the hypothesis that cooperative parallel search is well suited for hard graph coloring problems near a previously identified transition between under- and overconstrained instances. We find that simple cooperative methods can often solve such problems faster than the same number of independent agents.,https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-035.pdf,
35,1993,Distributed Problem Solving,A Fast First-Cut Protocol for Agent Coordination,Andrew P. Kosoresow,"This paper presents a fast probabilistic method for coordination based on Markov processes, provided the agents’ goals and preferences are sufficiently compatible. By using Markov chains as the agents’ inference mechanism, we are able to analyze convergence properties of agent interactions and to determine bounds on the expected times of convergence. Should the agents’ goals or preferences not be compatible, they can detect this situation since coordination has not been achieved within a probabilistic time bound and the agents can then resort to a higher-level protocol. The application, used for motivating the discussion, is the scheduling of tasks, though the methodology may be applied to other domains. Using this domain, we develop a model for coordinating the agents and demonstrate its use in two examples.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-036.pdf,
36,1993,Distributed Problem Solving,Agents Contracting Tasks in Non-Collaborative Environments,Sarit Kraus,"Agents may sub-contract some of their tasks to other agent(s) even when they don’t share a common goal. An agent tries to contract some of its tasks that it can’t perform by itself, or when the task may be performed more efficiently or better by other agents. A ""selfish"" agent may convince another ""selfish"" agent to help it with its task, even if the agents are not assumed to be benevolent, by promises of rewards. We propose techniques that provide efficient ways to reach subcontracting in varied situations: the agents have full information about the environment and each other vs. subcontracting when the agents don’t know the exact state of the world. We consider situations of repeated encounters, cases of asymmetric information, situations where the agents lack information about each other, and cases where an agent subcontracts a task to a group of agents. We also consider situations where there is a competition either among contracted agents or contracting agents. In all situations we would like the contracted agent to carry out the task efficiently without the need of close supervision by the contracting agent. The contracts that are reached are simple, Pareto-optimal and stable.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-037.pdf,
37,1993,Distributed Problem Solving,IPUS: An Architecture for Integrated Signal Processing and Signal Interpretation in Complex Environments,"Victor Lesser, Izaskun Gallastegi, Frank Klassner, Hamid Nawab","This paper presents the IPUS (Integratecl Processing and Understanding of Signals) architecture to address the traditional perceptual paradigm’s shortcomings in complex environments. It has two premises: (1) the search for correct interpretations of signal processing algorithms’ (SPAS) outputs requires concurrent search for SPAS and control parameters appropriate for the environment, and (2) interaction between these search processes must be structured by a formal theory of how inappropriate SPA usage can distort SPA output. We describe IPUS’s key components (discrepancy detection, diagnosis, reprocessing, and differential diagnosis) and their instantiation in an acoustic interpretation system. This application, along with another in the radar domain, supports our claim that the IPUS paradigm is feasible and generic.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-038.pdf,
38,1993,Distributed Problem Solving,An Implementation of the Contract Net Protocol Based on Marginal Cost Calculations,Tuomas Sandholm,"This paper presents a formalization of the bidding and awarding decision process that was left undefined in the original contract net task allocation protocol. This formalization is based on marginal cost calculations based on local agent criteria. In this way, agents having very different local criteria (based on their self-interest) can interact to distribute tasks so that the network as a whole functions more effectively. In this model, both competitive and cooperative agents can interact. In addition, the contract net protocol is extended to allow for clustering of tasks, to deal with the possibility of a large number of announcement and bid messages and to effectively handle situations, in which new bidding and awarding is being done during the period when the results of previous bids are unknown. The protocol is verified by the TRACONET (TRAnsportation COoperation NET) system, where dispatch centers of different companies cooperate automatically in vehicle routing. The implementation is asynchronous and truly distributed, and it provides the agents extensive autonomy. The protocol is discussed in detail and test results with real data are presented.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-039.pdf,
39,1993,Intelligent User Interfaces,Generating Explanations of Device Behavior Using Compositional Modeling and Causal Ordering,"Patrice O. Gautier, Thomas R. Gruber","Generating explanations of device behavior is a long-standing goal of AI research in reasoning about physical systems. Much of the relevant work has concentrated on new methods for modeling and simulation, such as qualitative physics, or on sophisticated natural language generation, in which the device models are specially crafted for explanatory purposes. We show how two techniques from the modeling research-compositional modeling and causal ordering-can be effectively combined to generate natural language explanations of device behavior from engineering models. The explanations offer three advances over the data displays produced by conventional simulation software: (1) causal interpretations of the data, (2) summaries at appropriate levels of abstraction (physical mechanisms and component operating modes), and (3) query-driven, natural language summaries. Furthermore, combining the compositional modeling and causal ordering techniques allows models that are more scalable and less brittle than models designed solely for explanation. However, these techniques produce models with detail that can be distracting in explanations and would be removed in hand-crafted models (e.g., intermediate variables). We present domain-independent filtering and aggregation techniques that overcome these problems.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-040.pdf,
40,1993,Intelligent User Interfaces,Generating Natural Language Descriptions with Examples: Differences between Introductory and Advanced Texts,"Vibhu O. Mittal, Cécile L. Paris","Examples form an integral and very important part of many descriptions, especially in contexts such as tutoring and documentation generation. The ability to tailor a description for a particular situation is particularly important when different situations can result in widely varying descriptions. This paper considers the generation of descriptions with examples for two different situations: introductory texts and advanced, reference manual style texts. Previous studies have focused on any the examples or the language component of the explanation in isolation. However, there is a strong interaction between the examples and the accompanying description and it is therefore important to study how both these components are affected by changes in the situation. In this paper, we characterize examples in the context of their description along three orthogonal axes: the information content, the knowledge type of the example and the text-type in which the explanation is being generated. While variations along either of the three axes can result in different descriptions, this paper addresses variation along the text-type axis. We illustrate our discussion with a description of a list from our domain of LISP documentation, and present a trace of the system as it generates these descriptions.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-041.pdf,
41,1993,Intelligent User Interfaces,Building Models to Support Synthesis in Early Stage Product Design,"R. Bharat Rao, Stephen C-Y. Lu","Current computer-aided engineering paradigms for supporting synthesis activities in engineering design require the designer to use analysis simulators iteratively in an optimization loop. While optimization is necessary to achieve a good final design, it has a number of disadvantages during the early stages of design. In the inverse engineering methodology, machine learning techniques are used to learn a multidirectional model that provides vastly improved synthesis (and analysis) support to the designer. This methodology is demonstrated on the early design of a diesel engine combustion chamber for a truck.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-042.pdf,
42,1993,Intelligent User Interfaces,A Conversational Model of Multimodal Interaction in Information Systems,"Adelheit Stein, Ulrich Thiel","We propose a comprehensive framework for modeling and specifying multimodal interactions. To this end, we employ an extended notion of ' dialogue acts’ which can be realized by linguistic and non-linguistic means. First, a set of constraints is presented that describes the temporal structure and all patterns of exchange during a cooperative information-seeking dialogue. Second, we introduce a strategic level of description which allows the specification of the topical structure according to an information-seeking strategy. The model was used to design and implement the MERIT system, and led to a reduction in the complexity of the user interface while preserving most of the useful, but sometimes confusing, dialogue options of advanced direct manipulation interfaces.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-043.pdf,
43,1993,Large Scale Knowledge Bases,"Matching 100,000 Learned Rules",Robert B. Doorenbos,"This paper examines several systems which learn a large number of rules (productions), including one which learns 113,938 rules - the largest number ever learned by an AI system, and the largest number in any production system in existence. It is important to match these rules efficiently, in order to avoid the machine learning utility problem. Moreover, examination of such large systems reveals new phenomena and calls into question some common assumptions based on previous observations of smaller systems. We first show that the Rete and Treat match algorithms do not scale well with the number of rules in our systems, in part because the number of rules affected by a change to working memory increases with the total number of rules in these systems. We also show that the sharing of nodes in the beta part of the Rete network becomes more and more important as the number of rules increases. Finally, we describe and evaluate a new optimization for Rete which improves its scalability and allows two of our systems to learn over 100,000 rules without significant performance degradation.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-044.pdf,
44,1993,Large Scale Knowledge Bases,Massively Parallel Support for Computationally Effective Recognition Queries,"Matthew P. Evett, James A. Hendler, William A. Andersen","PARKA, a frame-based knowledge representation system implemented on the Connection Machine, provides a representation language consisting of concept descriptions (frames) and binary relations on those descriptions (slots). The system is designed explicitly to provide extremely fast property inheritance inference capabilities. PARKA performs fast ""recognition"" queries of the form ""find all frames satisfying p property constraints"" in O(d+p) time-proportional only to the depth, (i, of the knowledge base (KB), and independent of its size. For conjunctive queries of this type, PARKA’s performance is measured in tenths of a second, even for KBs with 100,000+ frames, with similar results for timings on the Cyc KB. Because PARKA' s run-time performance is independent of KB size, it promises to scale up to arbitrarily larger domains. With such run-time performance, we believe PARKA is a contender for the title of ""fastest knowledge representation system in the world"".",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-045.pdf,
45,1993,Large Scale Knowledge Bases,Case-Method: A Methodology for Building Large-Scale Case-Based Systems,"Hiroaki Kitano, Hideo Shimazu, Akihiro Shibata","Developing large-scale systems are major efforts which require careful planning and solid methodological foundations. This paper describes CASE-METHOD, the methodology for building large-scale case-based systems. CASE-METHOD defines the procedure which managers, engineers, and domain experts should follow in developing case-based systems, and provides a set of supporting tools. An empirical study shows that the use of CASE-METHOD attains significant workload reduction in system development and maintenance (more than 1/12) as well as qualitative change in corporate activities.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-046.pdf,
46,1993,Large Scale Knowledge Bases,Automated Index Generation for Constructing Large-Scale Conversational Hypermedia Systems,"Richard Osgood, Ray Bareiss","At the Institute for the Learning Sciences we have been developing large scale hypermedia systems, called ASK systems, that are designed to simulate aspects of conversations with experts. They provide access to manually indexed, multimedia databases of story units. We are particularly concerned with finding a practical solution to the problem of finding indices for thes units when the database grows too large for manual techniques. Our solution is to provide automated assistance that proposes relative links between units, eliminating the need for manual unit-to-unit comparison. In this paper we describe eight classes of links, and show a representation and inference procedure to assist in locating instances of each.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-047.pdf,
47,1993,Machine Learning,Probabilistic Prediction of Protein Secondary Structure Using Causal Networks,"Arthur L. Delcher, Simon Kasif, Harry R. Goldberg, William H. Hsu","In this, paper we present a probabilistic approach to analysis and prediction of protein structure. We argue that this approach provides a flexible and convenient mechanism to perform general scientific data analysis in molecular biology. We apply our approach to an important problem in molecular biology-predicting the secondary structure of proteins-and obtain experimental results comparable to several other methods. The causal networks that we use provide a very convenient medium for the scientist to experiment with different empirical models and obtain possibly important insights about the problem being studied.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-048.pdf,
48,1993,Machine Learning,OC1: A Randomized Induction of Oblique Decision Trees,"Sreerama Murthy, Simon Kasif, Steven Salzberg, Richard Beigel","This paper introduces OC1, a new algorithm for generating multivariate decision trees. Multivariate trees classify examples by testing linear combinations of the features at each non-leaf node of the tree. Each test is equivalent to a hyperplane at an oblique orientation to the axes. Because of the computational intractability of finding an optimal orientation for these hyperplanes, heuristic methods must be used to produce good trees. This paper explores a new method that combines deterministic and randomized procedures to search for a good tree. Experiments on several different real-world data sets demonstrate that the method consistently finds much smaller trees than comparable methods using univariate tests. In addition, the accuracy of the trees found with our method matches or exceeds the best results of other machine learning methods.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-049.pdf,
49,1993,Machine Learning,Finding Accurate Frontiers: A Knowledge-Intensive Approach to Relational Learning,"Michael Pazzani, Clifford Brunk","An approach to analytic learning is described that searches for accurate entailments of a Horn Clause domain theory. A hill-climbing search, guided by an information based evaluation function, is performed by applying a set of operators that derive frontiers from domain theories. The analytic learning system is one component of a multi-strategy relational learning system. We compare the accuracy of concepts learned with this analytic strategy to concepts learned with an analytic strategy that operationalizes the domain theory.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-050.pdf,
50,1993,Machine Learning,Learning Non-Linearly Separable Boolean Functions With Linear Threshold Unit Trees and Madaline-Style Networks,Mehran Sahami,"This paper investigates an algorithm for the construction of decisions trees comprised of linear threshold units and also presents a novel algorithm for the learning of non-linearly separable boolean functions using Madaline-style networks which are isomorphic to decision trees. The construction of such networks is discussed, and their performance in learning is compared with standard Back-Propagation on a sample problem in which many irrelevant attributes are introduced. Littlestone’s Winnow algorithm is also explored within this architecture as a means of learning in the presence of many irrelevant attributes. The learning ability of this Madaline-style architecture on non-optimal (larger than necessary) networks is also explored.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-051.pdf,
51,1993,Natural Language Generation,Generating Argumentative Judgment Determiners,Michael Elhadad,"This paper presents a procedure to generate judgment determiners, e.g., many, few. Although such determiners carry very little objective information, they are extensively used in everyday language. The paper presents a precise characterization of a class of such determiners using three semantic tests. A conceptual representation for sets is then derived from this characterization which can serve as an input to a generator capable of producing judgment determiners. In a second part, a set of syntactic features controlling the realization of complex determiner sequences is presented. The ma@ing from the conceptual input to this set of syntactic features is then presented. The presented procedure relies on a description of the speaker’s argumentative intent to control this mapping and to select appropriate judgment determiners.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-052.pdf,
52,1993,Natural Language Generation,Bidirectional Chart Generation of Natural Language Texts,"Masahiko Haruno, Makoto Nagao, Yasuharu Den, Yuji Matsumoto","This paper presents Bidirectional Chart Generation (BCG) algorithm as an uniform control mechanism for sentence generation and text planning. It is an extension of Semantic Head Driven Generation algorithm [Shieber et al., 1989] in that recomputation of partial structures and backtracking are avoided by using a chart table. These properties enable to handle a large scale grammar including text planning and to implement the algorithm in parallel programming languages. Other merits of the algorithm are to deal with multiple contexts and to keep every partial structure in the chart. It becomes easier for the generator to find a recovery strategy when user cannot understand the generated text.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-053.pdf,
53,1993,Natural Language Generation,Communicative Acts for Generating Natural Language Arguments,Mark T. Maybury,"The ability to argue to support a conclusion or to encourage some course of action is fundamental to communication. Guided by examination of naturally occurring arguments, this paper classifies the communicative structure and function of several different kinds of arguments and indicates how these can be formalized as plan-based models of communication. The paper describes the use of these communication plans in the context of a prototype which cooperatively interacts with a user to allocate scarce resources. This plan-based approach to argument helps improve the cohesion and coherence of the resulting communication.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-054.pdf,
54,1993,Natural Language Generation,Corpus Analysis for Revision-Based Generation of Complex Sentences,"Jacques Robin, Kathleen McKeown","The complex sentences of newswire reports contain floating content units that appear to be opportunistically placed where the form of the surrounding text allows. We present a corpus analysis that identified precise semantic and syntactic constraints on where and how such information is realized. The result is a set of revision tools that form the rule base for a report generation system, allowing incremental generation of complex sentences.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-055.pdf,
55,1993,Natural Language Sentence Analysis,Machine Translation of Spatial Expressions: Defining the Relation between an Interlingua and a Knowledge Representation System,"Bonnie J. Dorr, Clare R. Voss","In this paper we present one aspect of our research on machine translation (MT): defining the relation between the interlingua (IL) and a knowledge representation (KR) within an MT system. Our interest lies in the translation of natural language (NL) sentences where the ""message"" contains a spatial relation - in particular, where the sentence conveys information about the location or path of physical entities in the real, physical world. We explore several arguments for clarifying the source of constraints on the particular IL structures needed to translate these sentences. This paper develops one approach to defining these constraints and building an MT system where the IL structures designed to satisfy these constraints may be tested. In this way, we have begun to address one of the basic issues in MT research, providing independent justification for the IL itself.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-056.pdf,
56,1993,Natural Language Sentence Analysis,Having Your Cake and Eating It Too: Autonomy and Interaction in a Model of Sentence Processing,"Kurt P. Eiselt, Kavi Mahesh, Jennifer K. Holbrook","Is the human language understander a collection of modular processes operating with relative autonomy, or is it a single integrated process? This ongoing debate has polarized the language processing community, with two fundamentally different types of model posited, and with each camp concluding that the other is wrong. One camp puts forth a model with separate processors and distinct knowledge sources to explain one body of data, and the other proposes a model with a single processor and a homogeneous, monolithic knowledge source to explain the other body of data. In this paper we argue that a hybrid approach which combines a unified processor with separate knowledge sources provides an explanation of both bodies of data, and we demonstrate the feasibility of this approach with the computational model called COMPERE. We believe that this approach brings the language processing community significantly closer to offering human-like language processing systems.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-057.pdf,
57,1993,Natural Language Sentence Analysis,Efficient Heuristic Natural Language Parsing,"Christian R. Huyck, Steven L. Lytinen","Most artificial natural language processing (NLP) systems make use of some simple algorithm for parsing. These algorithms overlook the inextricable link between parsing natural language and understanding it. Humans parse language in a linear fashion. Our goal is to develop an NLP system that parses in a linear and psychologically valid fashion. When this goal is achieved, our NLP system will be efficient, and it will generate the correct interpretation in ambiguous situations. In this paper, we describe two NLP systems, whose parsing is driven by several heuristics. The first is a bottom-up system which is based on the work of (Ford, Bresnan and Kaplan 1982). The second system is a more expansive attempt, incorporating the initial heuristics and several more. This system runs on a much larger domain and incorporates several new syntactic forms. It has its weaknesses, but it shows good progress toward the goal of linearity.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-058.pdf,
58,1993,Natural Language Sentence Analysis,Towards a Reading Coach that Listens: Automated Detection of Oral Reading Errors,"Jack Mostow, Alexander G. Hauptmann, Lin Lawrence Chase, Steven Roth","What skill is more important to teach than reading? Unfortunately, millions of Americans cannot read. Although a large body of educational software exists to help teach reading, its inability to hear the student limits what it can do. This paper reports a significant step toward using automatic speech recognition to help children learn to read: an implemented system that displays a text, follows as a student reads it aloud, and automatically identifies which words he or she missed. We describe how the system works, and evaluate its performance on a corpus of second graders’ oral reading that we have recorded and transcribed.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-059.pdf,
59,1993,Nonmonotonic Logic,Minimal Belief and Negation as Failure: A Feasible Approach,"Antje Beringer, Torsten Schaub","Lifschitz introduced a logic of minimal belief and negation as failure, called MBNF, in order to provide a theory of epistemic queries to nonmonotonic databases. We present a feasible subsystem of MBNF which can be translated into a logic built on first order logic and negation as failure, called FONF. We give a semantics for FONF along with an extended connection calculus. In particular, we demonstrate that the obtained system is still more expressive than other approaches.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-060.pdf,
60,1993,Nonmonotonic Logic,A Context-based Framework for Default Logics,"Philippe Besnard, Torsten Schaub","We present a new context-based approach to default logic, called contextual default logic. The approach""extends the notion of a default rule and supplies each extension with a context. Contextual default logic allows for embedding all existing variants of default logic along with more traditional approaches like the closed world assumption. A key advantage of contextual default logic is that it provides a syntactical instrument for comparing existing default logics in a unified setting. In particular, it reveals that existing default logics mainly differ in the way they deal with an explicit or implicit underlying context.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-061.pdf,
61,1993,Nonmonotonic Logic,Propositional Logic of Context,"Sasa Buvac, Ian A. Mason","In this paper we investigate the simple logical properties of contexts. We describe both the syntax and semantics of a general propositional language of context, and give a Hilbert style proof system for this language. A propositional logic of context extends classical propositional logic in two ways. Firstly, a new modality, ist (k, f), is introduced. It is used to express that the sentence, f, holds in the context k. Secondly, each context has its own vocabulary, i.e. a set of propositional atoms which are defined or meaningful in that context. The main results of this paper are the soundness and completeness of this Hilbert style proof system. We also provide soundness and completeness results (i.e. correspondence theory) for various extensions of the general system.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-062.pdf,
62,1993,Nonmonotonic Logic,Generating Explicit Orderings for Non-monotonic Logics,"James Cussens, Anthony Hunter, Ashwin Srinivasan","For non-monotonic reasoning, explicit orderings over formulae offer an important solution to problems such as 'multiple extensions’ . However, a criticism of such a solution is that it is not clear, in general, from where the orderings should be obtained. Here we show how orderings can be derived from statistical information about the domain which the formulae cover. For this we provide an overview of prioritized logics-a general class of logics that incorporate explicit orderings over formulae. This class of logics has been shown elsewhere to capture a wide variety of proof-theoretic approaches to non-monotonic reasoning, and in particular, to highlight the role of preferences-both implicit and explicit-in such proof theory. We take one particular prioritized logic, called SF logic, and describe an experimental approach for comparing this logic with an important example of a logic that does not use explicit orderings of preference-namely Horn clause logic with negation-as-failure. Finally, we present the results of this comparison, showing how SF logic is more skeptical and more accurate than negation-as-failure.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-063.pdf,
63,1993,Nonmonotonic Logic,Reasoning Precisely with Vague Concepts,"Nita Goyal, Yoav Shoham","Many knowledge-based systems need to represent vague concepts. Although the practical approach of representing vague concepts as precise intervals over numbers is well-accepted in AI, there is no systematic method to delimit the boundaries of intervals, only ad hoc methods. We present a framework to reason precisely with vague concepts based on the observation that the vague concepts and their interval-boundaries are constrained by the underlying domain knowledge. The framework is comprised of a constraint language to represent logical constraints on vague concepts, as well as numerical constraints on the interval-boundaries; a query language to request information about the interval boundaries; and a computational mechanism to answer the queries. A key step in answering queries is preprocessing the constraints by extracting the numerical constraints from the logical constraints and combining them with the given numerical constraints.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-064.pdf,
64,1993,Nonmonotonic Logic,Restricted Monotonicity,Vladimir Lifschitz,"A knowledge representation problem can be sometimes viewed as an element of a family of problems, with parameters corresponding to possible assumptions about the domain under consideration. When additional assumptions are made, the class of domains that are being described becomes smaller, so that the class of conclusions that are true in all the domains becomes larger. As a result, a satisfactory solution to a parametric knowledge representation problem on the basis of some nonmonotonic formalism can be expected to have a certain formal property, that we call restricted monotonicity. We argue that it is important to recognize parametric knowledge representation problems and to verify restricted monotonicity fir their proposed solutions.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-065.pdf,
65,1993,Nonmonotonic Logic,Subnormal Modal Logics for Knowledge Representation,"Grigori Schwarz, Miroslaw Truszczynski","Several widely accepted modal nonmonotonic logics for reasoning about knowledge and beliefs of rational agents with introspection powers are based on strong modal logics such as KD45, S4.4, S4F and S5. In this paper we argue that weak modal logics, without even the axiom K and, therefore, below the range of normal modal logics, also give rise to useful nonmonotonic systems. We study two such logics: the logic N, containing propositional calculus and necessitation but no axiom schemata for manipulating the modality, and the logic NT - the extension of N by the schema T. For the nonmonotonic logics N and NT we develop minimal model semantics. We use it to show that the nonmonotonic logics N and NT are at least as expressive as autoepistemic logic, reflexive autoepistemic logic and default logic. In fact, each can be regarded as a common generalization of these classic nonmonotonic systems. We also show that the nonmonotonic logics N and NT have the property of being conservative with respect to adding new definitions, and prove that computationally they are equivalent to autoepistemic and default logics.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-066.pdf,
66,1993,Nonmonotonic Logic,Algebraic Semantics for Cumulative Inference Operations,Zbigniew Stachniak,In this paper we propose preferential matrix semantics for nonmonotonic inference systems and show how this algebraic framework can be used in methodological studies of cumulative inference operations.,https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-067.pdf,
67,1993,Novel Methods in Knowledge Acquisition,Question-based Acquisition of Conceptual Indices for Multimedia Design Documentation,"Catherine Baudin, Smadar Kadar, Jody Gevins Underwood, Vinod Baya","Information retrieval systems that use conceptual indexing to describe the information content perform better than syntactic indexing methods based on words from a text. However, since conceptual indices represent the semantics of a piece of information, it is difficult to extract them automatically from a document, and it is tedious to build them manually. We implemented an information retrieval system that acquires conceptual indices of text, graphics and videotaped documents. Our approach is to use an underlying model of the domain covered by the documents to constrain the user’s queries. This facilitates question-based acquisition of conceptual indices: converting user queries into indices which accurately model the content of the documents, and can be reused. We discuss Dedal, a system that facilitates the indexing and retrieval of design documents in the mechanical engineering domain. A user formulates a query to the system, and if there is no corresponding index, Dedal uses the underlying domain model and a set of retrieval heuristics to approximate the retrieval, and ask for confirmation from the user. If the user finds the retrieved information relevant, Dedal acquires a new index based on the query. We demonstrate the relevance and coverage of the acquired indices through experimentation.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-068.pdf,
68,1993,Novel Methods in Knowledge Acquisition,Learning Interface Agents,"Pattie Maes, Robyn Kozierok","Interface agents are computer programs that employ Artificial Intelligence techniques in order to provide assistance to a user dealing with a particular computer application. The paper discusses an interface agent which has been modelled closely after the metaphor of a personal assistant. The agent learns how to assist the user by (i) observing the user’s actions and imitating them, (ii) receiving user feedback when it takes wrong actions and (iii) being trained by the user on the basis of hypothetical examples. The paper discusses how this learning agent was implemented using memory-based learning and reinforcement learning techniques. It presents actual results from two prototype agents built using these techniques: one for a meeting scheduling application and one for electronic mail. It argues that the machine learning approach to building interface agents is a feasible one which has several advantages over other approaches: it provides a customized and adaptive solution which is less costly and ensures better user acceptability. The paper also argues what the advantages are of the particular learning techniques used.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-069.pdf,
69,1993,Novel Methods in Knowledge Acquisition,Learning from an Approximate Theory and Noisy Examples,"Somkiat Tangkitvanich, Masamichi Shimura","This paper presents an approach to a new learning problem, the problem of learning from an approximate theory and a set of noisy examples. This problem requires a new learning approach since it cannot be satisfactorily solved by either indictive, or analytic learning algorithms or their existing combinations. Our approach can be viewed as an extension of the minimum description length (MDL) principle, and is unique in that it is based on the encoding of the refinement required to transform the given theory into a better theory rather than on the encoding of the resultant theory as in traditional MDL. Experimental results show that, based on our approach, the theory learned from an approximate theory and a set, of noisy examples is more accurate than either the approximate theory itself or a theory learned from the examples alone. This suggests that our approach can combine useful information from both the theory and the training set even though both of them are only partially correct.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-070.pdf,
70,1993,Novel Methods in Knowledge Acquisition,Scientific Model-Building as Search in Matrix Spaces,"Raúl E. Valdés-Pérez, Herbert A. Simon, Jan M. Zytkow","Many reported discovery systems build discrete models of hidden structure, properties, or processes in the diverse fields of biology, chemistry, and physics. We show that the search spaces underlying many well-known systems are remarkably similar when re-interpreted as search in matrix spaces. A small number of matrix types are used to represent the input data and output models. Most of the constraints can be represented as matrix constraints; most notably, conservation laws and their analogues can be represented as matrix equations. Typically, one or more matrix dimensions grow as these systems consider more complex models after simpler models fail, and we introduce a notation to express this. The novel framework of matrix-space search serves to unify previous systems and suggests how at least two of them can be integrated. Our analysis constitutes an advance toward a generalized account of model-building in science.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-071.pdf,
71,1993,Plan Generation,An Average Case Analysis of Planning,Tom Bylander,"I present an average case analysis of propositional STRIPS planning. The analysis assumes that each possible precondition (likewise postcondition) is equally likely to appear within an operator. Under this assumption, I derive bounds for when it is highly likely that a planning instance can be efficiently solved, either by finding a plan or proving that no plan exists. Roughly, if planning instances have n conditions (ground atoms), g goals, and O(n g÷d) operators, then a simple, efficient algorithm can prove that no plan exists for at least 1 - d of the instances. If instances have W(n(ln g)(ln g/d)) operators, then a simple, efficient algorithm can find a plan for at least 1 - d of the instances. A similar result holds for plan modification, i.e., solving a planning instance that is close to another planning instance with a known plan. Thus it would appear that propositional STRIPS planning, a PSPACE-complete problem, is hard only for narrow parameter ranges, which complements previous average-case analyses for NP-complete problems. Future work is needed to narrow the gap between the bounds and to consider more realistic distributional assumptious and more sophisticated algorithms.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-072.pdf,
72,1993,Plan Generation,Granularity in Multi-Method Planning,"Soowon Lee, Paul S. Rosenbloom","Multi-method planning is an approach to using a set of different planning methods to simultaneously achieve planner completeness, planning time efficiency, and plan length reduction. Although it has been shown that coordinating a set of methods in a coarse-grained, problem-by-problem manner has the potential for approaching this ideal, such an approach can waste a significant amount of time in trying methods that ultimately prove inadequate. This paper investigates an approach to reducing this wasted effort by refining the granularity at which methods are switched. The experimental results show that the fine-grained approach can improve the planning time significantly compared with coarse-grained and single-method approaches.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-073.pdf,
73,1993,Plan Generation,Threat-Removal Strategies for Partial-Order Planning,"Mark A. Peot, David E. Smith","McAllester and Rosenblitts’ (1991) systematic nonlinear planner (SNLP) removes threats as they are discovered. In other planners such as SIPE (Wilkins, 1988), and NOAH (Sacerdoti, 1977), threat resolution is partially or completely delayed. In this paper, we demonstrate that planner efficiency may be vastly improved by the use of alternatives to these threat removal strategies. We discuss five threat removal strategies and prove that two of these strategies dominate the other three--resulting in a provably smaller search space. Furthermore, the systematicity of the planning algorithm is preserved for each of the threat removal strategies. Finally, we confirm our results experimentally using a large number of planning examples including examples from the literature.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-074.pdf,
74,1993,Plan Generation,Postponing Threats in Partial-Order Planning,"Mark A. Peot, David E. Smith","An important aspect of partial-order planning is the resolution of threats between actions and causal links in a plan. We present a technique for automatically deciding which threats should be resolved during planning, and which should be delayed until planning is otherwise complete. In particular we show that many potential threats can be provably delayed until the end; that is, if the planner can find a plan for the goal while ignoring these threats, there is a guarantee that the partial ordering in the resulting plan can be extended to eliminate the threats. Our technique involves: 1) construction of an operator graph that captures the interaction between operators relevant to a given goal, 2) decomposition of this graph into groups of related threats, and 3) postponement of threats with certain properties.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-075.pdf,
75,1993,Plan Learning,Permissive Planning: A Machine Learning Approach to Linking Internal and External Worlds,"Gerald DeJong, Scott Bennett","Because complex real-world domains defy perfect formalization, real-world planners must be able to cope with incorrect domain knowledge. This paper offers a theoretical framework for pemissive planning, a machine learning method for improving the real-world behavior of planners. Permissive planning aims to acquire techniques that tolerate the inevitable mismatch between the planner’s internal beliefs and the external world. Unlike the reactive approach to this mismatch, permissive planning embraces projection. The method is both problem-independent and domain-independent. Unlike classical planning, permissive planning does not exclude real-world performance from the formal definition of planning.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-076.pdf,
76,1993,Plan Learning,Relative Utility of EBG based Plan Reuse in Partial Ordering vs. Total Ordering Planning,"Subbarao Kambhampati, Jengchin Chen","This paper provides a systematic analysis of the relative utility of basing EBG based plan reuse techniques in partial ordering vs. total ordering planning frameworks. We separate the potential advantages into those related to storage compaction, and those related to the ability to exploit stored plans. We observe that the storage compactions provided by partially ordered partially instantiated plans can, to a large extent, be exploited regardless of the underlying planner. We argue that it is in the ability to exploit stored plans during planning that partial ordering planners have some distinct advantages. In particular, to be able to flexibly reuse and extend the retrieved plans, a planner needs the ability to arbitrarily and efficiently ""splice in"" new steps and sub-plans into the retrieved plan. This is where partial ordering planners, with their least-commitment strategy, and flexible plan representations, score significantly over state-based planners as well as planners that search in the space of totally ordered plans. We will clarify and support this hypothesis through an empirical study of three planners and two reuse strategies.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-077.pdf,
77,1993,Plan Learning,Learning Plan Transformations from Self-Questions: A Memory-Based Approach,"R. Oehlmann, D. Sleeman, P. Edwards","Recent work in planning has focused on the reuse of previous plans. In order to re-use a plan in a novel situation the plan has to be transformed into an applicable plan. We describe an approach to plan transformation which utilises reasoning experience as well as planning experience. Some of the additional information is generated by a series of self generated questions and answers, as well as appropriate experiments. Furthermore, we show how transformation strategies can be learned.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-078.pdf,
78,1993,Plan Learning,On the Masking Effect,"Milind Tambe, Paul S. Rosenbloom","Machine learning approaches to knowledge compilation seek to improve the performance of problem-solvers by storing solutions to previously solved problems in an efficient, generalized form. The problem-solver retrieves these learned solutions in appropriate later situations to obtain results more efficiently. However, by relying on its learned knowledge to provide a solution, the problem-solver may miss an alternative solution of higher quality - one that could have been generated using the original (non-learned) problem-solving knowledge. This phenomenon is referred to as the masking effect of learning. In this paper, we examine a sequence of possible solutions for the masking effect. Each solution refines and builds on the previous one. The final solution is based on cascaded filters. When learned knowledge is retrieved, these filters alert the system about the inappropriateness of this knowledge so that the system can then derive a better alternative solution. We analyze conditions under which this solution will perform better than the others, and present experimental data supportive of the analysis. This investigation is based on a simulated robot domain called Groundworld.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-079.pdf,
79,1993,Qualitative Reasoning,Qualitatively Describing Objects Using Spatial Prepositions,"Alicia Abella, John R. Kender","The objective in this paper is to present a framework for a system that describes objects in a qualitative fashion. A subset of spatial prepositions is chosen and an appropriate quantification is applied to each of them that capture their inherent qualitative properties. The quantifications use such object attributes as area, centers, and elongation properties. The familiar zeroth, first, and second order moments are used to characterize these attributes. This paper will detail how and why the particular quantifications were chosen. Since spatial prepositions are by their nature rather vague and dependent on context a technique for fuzzifying the definition of the spatial preposition is explained. Finally an example task is chosen to illustrate the appropriateness of the quantification techniques.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-080.pdf,
80,1993,Qualitative Reasoning,Numeric Reasoning with Relative Orders of Magnitude,Philippe Dague,"In [Dague, 1993], a formal system ROM(K) involving four relations has been defined to reason with relative orders of magnitude. In this paper, problems of introducing quantitative information and of ensuring validity of the results in R are tackled. Correspondent overlapping relations are defined in R and all rules of ROM(K) are transposed to R. Unlike other proposed systems, the obtained system ROM(R) ensures a sound calculus in R , while keeping the ability to provide commonsense explanations of the results. If needed, these results can be refined by using additional and complementary techniques: k-bound-consistency, which generalizes interval propagation; symbolic computation, which considerably improves the results by delaying numeric evaluation; symbolic algebra calculus of the roots of partial derivatives, which allows the exact extrema to be obtained; transformation of rational functions, when possible, so that each variable occurs only once, which allows interval propagation to give the exact results. ROM(R), possibly supplemented by these various techniques, constitutes a rich, powerful and flexible tool for performing mixed qualitative and numeric reasoning, essential for engineering tasks.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-081.pdf,
81,1993,Qualitative Reasoning,Efficient Reasoning in Qualitative Probabilistic Networks,"Marek J. Druzdzel, Max Henrion","Qualitative Probabilistic Networks (QPNs) are an abstraction of Bayesian belief networks replacing numerical relations by qualitative influences and synergies [Wellman, 1990b]. To reason in a QPN is to find the effect of new evidence on each node in terms of the sign of the change in belief (increase or decrease). We introduce a polynomial time algorithm for reasoning in QPNs, based on local sign propagation. It extends our previous scheme from singly connected to general multiply con- nected networks. Unlike existing graph-reduction algorithms, it preserves the network structure and determines the effect of evidence on all nodes in the network. This aids meta-level reasoning about the model and automatic generation of intuitive explanations of probabilistic reasoning.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-082.pdf,
82,1993,Qualitative Reasoning,Generating Quasi-symbolic Representation of Three-Dimensional Flow,Toyoaki Nishida,"Understanding flow in the three-dimensional phase space is challenging both to human experts and current computer science technology. To break through the barrier, we are building a program called PSX3 that can autonomously explore the flow in a three-dimensional phase space, by integrating AI and numerical techniques. In this paper, I point out that quasi-symbolic representation called flow mappings is effective as a means of capturing qualitative aspects of three-dimensional flow and present a method of generating flow mappings for a system of ordinary differential equations with three unknown functions. The method is based on a finding that geometric cues for generating a set of flow patterns can be classified into five categories. I demonstrate how knowledge about interaction of geometric cues is utilized for intelligently controlling numerical computation.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-083.pdf,
83,1993,Real-Time Planning and Simulation,Real-Time Self-Explanatory Simulation,"Franz G. Amador, Adam Finkelstein, Daniel S. Weld","We present Pika, an implemented self-explanatory simulator that is more than 5000 times faster than SimGen Mk2 [Forbus and Falkenhainer, 1992], the previous state of the art. Like SimGen, Pika automatically prepares and runs a numeric simulation of a physical device specified as a particular instantiation of a general domain theory, and it is capable of explaining its reasoning and the simulated behavior. Unlike SimGen, Pika’s modeling language allows arbitrary algebraic and differential equations with no prespecified causal direction; Pika infers the appropriate causality and solves the equations as necessary to prepare for numeric integration.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-084.pdf,
84,1993,Real-Time Planning and Simulation,A Comparison of Action-Based Hierarchies and Decision Trees for Real-Time Performance,"David Ash, Barbara Hayes-Roth","Decision trees have provided a classical mechanism for progressively narrowing down a search from a large group of possibilities to a single alternative. The structuring of a decision tree is based on a heuristic that maximizes the value of the information gained at each level in the hierarchy. Decision trees are effective when an agent needs to reach the goal of complete diagnosis as quickly as possible and cannot accept a partial solution. We present an alternative to the decision tree heuristic which is useful when partial solutions do have value and when limited resources may require an agent to accept a partial solution. Our heuristic maximizes the improvement in the value of the partial solution gained at each level in the hierarchy; we term the resulting structure an action-based hierarchy. We present the results of a set of experiments designed to compare these two heuristics for hierarchy structuring. Finally, we describe some preliminary work we have done in applying these ideas to a medical domain--surgical intensive care unit (SICU) patient monitoring.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-085.pdf,
85,1993,Real-Time Planning and Simulation,Planning With Deadlines in Stochastic Domains,"Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, Ann Nicholson","We provide a method, based on the theory of Markov decision problems, for efficient planning in stochastic domains. Goals are encoded as reward functions, expressing the desirability of each world state; the planner must find a policy (mapping from states to actions) that maximizes future rewards. Standard goals of achievement, as well as goals of maintenance and prioritized combinations of goals, can be specified in this way. An optimal policy can be found using existing methods, but these methods are at best polynomial in the number of states in the domain, where the number of states is exponential in the number of propositions (or state variables). By using information about the starting state, the reward function, and the transition probabilities of the domain, we can restrict the planner’s attention to a set of world states that are likely to be encountered in satisfying the goal. Furthermore, the planner can generate more or less complete plans depending on the time it has available. We describe experiments involving a mobile robotics application and consider the problem of scheduling different phases of the planning algorithm given time constraints.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-086.pdf,
86,1993,Real-Time Planning and Simulation,Task Interdependencies in Design-to-time Real-time Scheduling,"Alan Garvey, Marty Humphrey, Victor Lesser","Design-to-time is an approach to real-time scheduling in situations where multiple methods exist for many tasks that the system needs to solve. Often these methods will have relationships with one other, such as the execution of one method enabling the execution of another, or the use of a rough approximation by one method affecting the performance of a method that uses its result. Most previous work in the scheduling of real-time AI tasks has ignored these relationships. This paper presents an optimal design-to-time scheduler for particular kinds of relationships that occur in an actual AI application, and examines the performance of that scheduler in a simulation environment that models the tasks of that application.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-087.pdf,
87,1993,Reasoning about Physical Systems,Sensible Scenes: Visual Understanding of Complex Structures through Causal Analysis,"Matthew Brand, Lawrence Birnbaum, Paul Cooper","An important result of visual understanding is an explanation of a scene’s causal structure: How action-usually motion-is originated, constrained, and prevented, and how this determines what will happen in the immediate future. To be useful for a purposeful agent, these explanations must also capture the scene in terms of the functional properties of its objects-their purposes, uses, and affordances for manipulation. Design knowledge describes how the world is organized to suit these functions, and causal knowledge describes how these arrangements work. We have been exploring the hypothesis that vision is an explanatory process in which causal and functional reasoning plays an intimate role in mediating the activity of low-level visual processes. In particular, we have explored two of the consequences of this view for the construction of purposeful vision systems: Causal and design knowledge can be used to 1) drive focus of attention, and 2) choose between ambiguous image interpretations. Both principles are at work in SPROCKET, a system which visually explores simple machines, integrating diverse visual clues into an explanation of a machine’s design and function.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-088.pdf,
88,1993,Reasoning about Physical Systems,Intelligent Model Selection for Hillclimbing Search in Computer-Aided Design,"Thomas Ellman, John Keane, Mark Schwabacher","Models of physical systems can differ according to computational cost, accuracy and precision, among other things. Depending on the problem solving task at hand, different models will be appropriate. Several investigators have recently developed methods of automatically selecting among multiple models of physical systems. Our research is novel in that we are developing model selection techniques specifically suited to computer-aided design. Our approach is based on the idea that artifact performance models for computer-aided design should be chosen in light of the design decisions they are required to support. We have developed a technique called ""Gradient Magnitude Model Selection"" (GMMS), which embodies this principle. GMMS operates in the context of a hillclimbing search process. It selects the simplest model that meets the needs of the hillclimbing algorithm in which it operates. We are using the domain of sailing yacht design as a testbed for this research. We have implemented GMMS and used it in hillclimbing search to decide between a computationally expensive potential-flow program and an algebraic approximation to analyze the performance of sailing yachts. Experimental tests show that GMMS makes the design process faster than it would be if the most expensive model were used for all design evaluations. GMMS achieves this performance improvement with little or no sacrifice in the quality of the resulting design.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-089.pdf,
89,1993,Reasoning about Physical Systems,Ideal Physical Systems,Brian Falkenhainer,"Accuracy plays a central role in developing models of continuous physical systems, both in the context of developing a new model to fit observation or approximating an existing model to make analysis faster. The need for simple, yet sufficiently accurate, models pervades engineering analysis, design, and diagnosis tasks. This paper focuses on two issues related to this topic. First, it examines the process by which idealized models are derived. Second, it examines the problem of determining when an idealized model will be sufficiently accurate for a given task in a way that is simple and doesn’t overwhelm the benefits of having a simple model. It describes IDEAL, a system which generates idealized versions of a given model and specifies each idealized model’s credibility domain. This allows valid future use of the model without resorting to more expensive measures such as search or empirical confirmation. The technique is illustrated on an implemented example.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-090.pdf,
90,1993,Reasoning about Physical Systems,Numerical Behavior Envelopes for Qualitative Models,"Herbert Kay, Benjamin Kuipers","Semiquantitative models combine both qualitative and quantitative knowledge within a single semiquantitative qualitative differential equation (SQDE) representation. With current simulation methods, the quantitative knowledge is not exploited as fully as possible. This paper describes dynamic envelopes - a method to exploit quantitative knowledge more fully by deriving and numerically simulating an extremad system whose solution is guaranteed to bound all solutions of the SQDE. It is shown that such systems can be determined automatically given the SQDE and an initial condition. As model precision increases, the dynamic envelope bounds become more precise than those derived by other semiquantitative inference methods. We demonstrate the utility of our method by showing how it improves the dynamic monitoring and diagnosis of a vacuum pumpdown system.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-091.pdf,
91,1993,Reasoning about Physical Systems,A Qualitative Method to Construct Phase Portraits,"Wood W. Lee, Benjamin Kuipers","We have developed and implemented in the QPORTRAIT program a qualitative simulation based method to construct phase portraits for a significant class of systems of two coupled first order autonomous differential equations, even in the presence of incomplete, qualitative knowledge. Differential equation models are important for reasoning about physical systems. The field of non-linear dynamics has introduced the powerful phase portrait representation for the global analysis of nonlinear differential equations. QPORTRAIT uses qualitative simulation to generate the set of all possible qualitative behaviors of a system. Constraints on two-dimensional phase portraits from nonlinear dynamics make it possible to identify and classify trajectories and their asymptotic limits, and constrain possible combinations. By exhaustively forming all combinations of features, and filtering out inconsistent combinations, QPORTRAIT is guaranteed to generate all possible qualitative phase portraits. We have applied QPORTRAIT to obtain tractable results for a number of nontrivial dynamical systems. Guaranteed coverage of all possible behaviors of incompletely known systems complements the more detailed, but approximation-based results of recently-developed methods for intelligently-guided numeric simulation [Nishida et al; Sacks; Yip; Zhao]. Combining the strengths of both approaches would better facilitate automated understanding of dynamical systems.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-092.pdf,
92,1993,Reasoning about Physical Systems,Understanding Linkages,Howard E. Shrobe,"Mechanical linkages are used to transmit and transform motion. In this paper we investigate what it means for a program to ""understand"" a linkage. Our system extracts its understanding by analyzing the results of a numerical simulation of the mechanism, finding interesting qualitative features, looking for symbolic relationships between these features and conjecturing a causal relationship between them. Our system is capable of understanding a variety of mechanisms, producing explanations very much like those in standard texts.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-093.pdf,
93,1993,Reasoning about Physical Systems,CFRL: A Language for Specifying the Causal Functionality of Engineered Devices,"Marcos Vescovi, Yumi Iwasaki, Richard Fikes, B. Chandrasekaran","Understanding the design of an engineered device requires both knowledge of the general physical principles that determine the behavior of the device and knowledge of what the device is intended to do (i.e., its functional specification). However, the majority of work in model-based reasoning about device behavior has focused on modeling a device in terms of general physical principles or intended functionality, but not both. In order to use both functional and behavioral knowledge in understanding a device design, it is crucial that the functional knowledge is represented in such a way that it has a clear interpretation in terms of actual behavior. We propose a new formalism for representing device functions with well-defined semantics in terms of actual behavior. We call the language CFRL (Causal Functional Representation Language). CFRL allows the specification of condistions that a behavior must satisfy, such as occurrence or a temporal sequence of expected events and causal relations among the events and the behavior of device components. We have used CFRL as the basis for a functional verification program which determines whether a behavior achieves an intended function.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-094.pdf,
94,1993,Reasoning about Physical Systems,Model Simplification by Asymptotic Order of Magnitude Reasoning,Kenneth Man-kam Yip,"One of the hardest problems in reasoning about a physical system is finding an approximate model that is mathematically tractable and yet captures the essence of the problem. Approximate models in science are often constructed by informal reasoning based on consideration of limiting cases, knowledge of relative importance of terms in the model, and understanding of gross features of the solution. We show how an implemented program can combine such knowledge with a heuristic simplification procedure and an inequality reasoner to simplify difficult fluid equations.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-095.pdf,
95,1993,Representation and Reasoning,Abduction As Belief Revision: A Model of Preferred Explanations,"Craig Boutilier, Veronica Becher","We propose a natural model of abduction based on the revision of the epistemic state of an agent. We require that explanations be sufficient to induce belief in an observation in a manner that adequately accounts for factual and hypothetical observations. Our model will generate explanations that nonmonotonically predict an observation, thus generalizing most current accounts, which require some deductive relationship between explanation and observation. It also provides a natural preference ordering on explanations, defined in terms of normality or plausibility. We reconstruct the Theorist system in our framework, and show how it can be extended to accommodate our predictive explanations and semantic preferences on explanations.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-096.pdf,
96,1993,Representation and Reasoning,Revision by Conditional Beliefs,"Craig Boutilier, Moisés Goldszmidt","Both the dynamics of belief change and the process of reasoning by default can be based on the conditional belief set of an agent, represented as a set of ""if-then"" rules. In this paper we address the open problem of formalizing the dynamics of revising this conditional belief set by new if-then rules, be they interpreted as new default rules or new revision policies. We start by providing a purely semantic characterization, based on the semantics of conditional rules, which induces logical constraints on any such revision process. We then introduce logical (syntax-independent) and syntax-dependent techniques, and provide a precise characterization of the set of conditionals that hold after the revision. In addition to formalizing the dynamics of revising a default knowledge base, this work also provides some of the necessary formal tools for establishing the truth of nested conditionals, and attacking the problem of learning new defaults.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-097.pdf,
97,1993,Representation and Reasoning,Reasoning about Only Knowing with Many Agents,Joseph Y. Halpern,"We extend two notions of ""only knowing"", that of Halpern and Moses [1984], and that of Levesque [1990], to many agents. The main lesson of this paper is that these approaches do have reasonable extensions to the multi-agent case. Our results also shed light on the single-agent case. For example, it was always viewed as significant that the HM notion of only knowing was based on S5, while Levesque’s was based on K45. In fact, our results show that the HM notion is better understood in the context of K45. Indeed, in the single-agent case, the HM notion remains unchanged if we use K45 (or KD45) instead of S5. However, in the multi-agent case, there are significant differences between K45 and S5. Moreover, all the results proved by Halpern and Moses for the single-agent case extend naturally to the multi-agent case for K45, but not for S5.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-098.pdf,
98,1993,Representation and Reasoning,All They Know About,Gerhard Lakemeyer,"We address the issue of agents reasoning about other agents’ nonmonotonic reasoning ability in the framework of a multi-agent autoepistemic logic (AEL). In single-agent AEL, nonmonotonic inferences are drawn based on all the agent knows. In a multi-agent context such as Jill reasoning about Jack’s nonmonotonic inferences, this assumption must be abandoned since it cannot be assumed that Jill knows everything Jack knows. Given a specific subject matter like Tweety the bird, it is more realistic and sufficient if Jill only assumes to know all Jack knows about Tweety in order to arrive at Jack’s nonmonotonic inferences about Tweety. This paper provides a formalization of all an agent knows about a certain subject mutter based on possible-world semantics in a multi-agent AEL. Besides discussing various properties of the new notion, we use it to characterize formulas that are about a subject matter in a very strong sense. While our main focus is on subject matters that consist of atomic propositions, we also address the case where agents are the subject matter.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-099.pdf,
99,1993,Representation for Actions and Motion,Towards Knowledge-Level Analysis of Motion Planning,"Ronen I. Brafman, Jean-Claude Latombe, Yoav Shoham","Inspired by the success of the distributed computing community in applying logics of knowledge and time to reasoning about distributed protocols, we aim for a similarly powerful and high-level abstraction when reasoning about control problems involving uncertainty. Here we concentrate on robot motion planning, with uncertainty in both control and sensing. This problem has already been well studied within the robotics community. Our contributions include the following: We define a new, natural problem in this domain: obtaining a sound and complete termination condition, given initial and goal locations. We consider a specific class of (simple) motion plans in Rn from the literature, and provide necessary and sufficient conditions for the existence of sound and complete termination conditions for plans in that class. We define a high-level language, a logic of time and knowledge, to reason about motion plans in the presence of uncertainty, and use them to provide general conditions for the existence of sound and complete termination conditions for a broader class of motion plans.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-100.pdf,
100,1993,Representation for Actions and Motion,"EL: A Formal, Yet Natural, Comprehensive Knowledge Representation","Chung Hee Hwang, Lenhart K. Schubert","We present Episodic Logic (EL), a highly expressive knowledge representation well-adapted to general commonsense reasoning as well as the interpretive and inferential needs of natural language processing. One of the distinctive features of EL is its extremely permissive ontology, which admits situations (episodes, events, states of affairs, etc.), propositions, possible facts, and kinds and collections, and which allows representation of generic sentences. EL is natural language-like in appearance and supports intuitively understandable inferences. At the same time it is both formally analyzable and mechanizable as an efficient inference engine.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-101.pdf,
101,1993,Representation for Actions and Motion,The Semantics of Event Prevention,"Charles L. Ortiz, Jr.","In planning tasks an agent may often find himself in a situation demanding that he choose an action that would prevent some unwanted event from occurring. Similarly, in tasks involving the generation of descriptions or explanations of sequences of events, it is often useful to draw as many informative connections as possible between events in the sequence; often, this means explaining why certain events are not possible. In this paper, I consider the semantics of event prevention and argue that a naive semantics which equates prevention with the elimination of all future possibility of the event in question is often difficult, if not impossible, to implement. I argue for a more useful semantics which falls out of some reasonable assumptions regarding restrictions on the set of potential actions available to an agent: (1) those actions about which the agent has formed intentions, (2) those actions consistent with the agent’s attitudes (including its other intentions), and (3) the set of actions evoked by the type of situation in which the agent is embedded.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-102.pdf,
102,1993,Representation for Actions and Motion,The Frame Problem and Knowledge-Producing Actions,"Richard B. Scherl, Hector J. Levesque","This paper proposes a solution to the frame problem for knowledge-producing actions. An example of a knowledge-producing action is a sense operation performed by a robot to determine whether or not there is an object of a particular shape within its grasp. The work is an extension of Reiter’s solution to the frame problem for ordinary actions and Moore' s work on knowledge and action. The properties of our specification are that knowledge-producing actions do not affect fluents other than the knowledge fluent, and actions that are not knowledge-producing only affect the knowledge fluent as appropriate. In addition, memory emerges as a side-effect: if something is known in a certain situation, it remains known at successor situations, unless something relevant has changed. Also, it will be shown that a form of regression examined by Reiter for reducing reasoning about future situations to reasoning about the initial situation now also applies to knowledge-producing actions.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-103.pdf,
103,1993,Rule-Based Reasoning,The Paradoxical Success of Fuzzy Logic,Charles Elkan,"This paper investigates the question of which aspects of fuzzy logic are essential to its practical usefulness. We show that as a formal system, a standard version of fuzzy logic collapses mathematically to two-valued logic, while empirically, fuzzy logic is not adequate for reasoning about uncertain evidence in expert systems. Nevertheless, applications of fuzzy logic in heuristic control have been highly successful. We argue that the inconsistencies of fuzzy logic have not been harmful in practice because current fuzzy controllers are far simpler than other knowledge-based systems. In the future, the technical limitations of fuzzy logic can be expected to become important in practice, and work on fuzzy controllers will also encounter several problems of scale already known for other knowledge-based systems.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-104.pdf,
104,1993,Rule-Based Reasoning,Exploring the Structure of Rule Based Systems,"Clifford Grossner, Alun D. Preece, P. Gokul Chander, T. Radhakrishnan, Ching Y. Suen","In order to measure and analyze the performance of rule-based expert systems, it is necessary to explicate the internal structure of their rule bases. Although a number of attempts have been made in the literature to formalize the structure of a rule base using the notion of a rule base execution path, none of these are entirely adequate. This paper reports a new formal definition for the notion of a rule base execution path, which adequately supports both validation and performance analysis of rule-based expert systems. This definition for the execution paths in a rule base has been embodied in a rule base analysis tool called Path Hunter. Path Hunter is used to analyse a rule base consisting of 442 CLIPS rules. In this analysis, the problem of combinatorial explosion, which arises during path enumeration, is controlled due to the manner in which paths are defined. The analysis raises several issues which should be taken into account in the engineering of rule-based systems.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-105.pdf,
105,1993,Rule-Based Reasoning,Supporting and Optimizing Full Unification in a Forward Chaining Rule System,Howard E. Shrobe,"The Rete and Treat algorithms are considered the most efficient implementation techniques for Forward Chaining rule systems. These algorithms support a language of limited expressive power. Assertions are not allowed to contain variables, making universal quantification impossible to express except as a rule. In this paper we show how to support full unification in these algorithms. We also show that: Supporting full unification is costly; Full unification is not used frequently; A combination of compile time and run time checks can determine when full unification is not needed. We present data to show that the cost of supporting full unification can be reduced in proportion to the degree that it isn' t employed and that for many practical systems this cost is negligible.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-106.pdf,
106,1993,Rule-Based Reasoning,Comprehensibility Improvement of Tabular Knowledge Bases,"Atsushi Sugiura, Yoshiyuki Koseki, Maximilian Riesenhuber","This paper discusses the important. issue of knowledge base comprehensibility and describes a technique for comprehensibility improvement. Comprehensibility is often measured by simplicity of concept description. Even in the simplest form, however, there will be a number of different DNF (Disjunctive Normal Form) descriptions possible to represent the same concept, and each of these will have a different degree of comprehensibility. In other words, simplification does not necessarily guarantee improved comprehensibility. In this paper, the authors introduce three new comprehensibility criteria, similarity, continuity, and conformity, for use with tabular knowledge bases. In addition, they propose an algorithm to convert a decision table with poor comprehensibility to one with high comprehensibility, while preserving logical equivalency. In experiments, the algorithm generated either the same or similar tables to those generated by humans.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-107.pdf,
107,1993,Search,Time-Saving Tips for Problem Solving with Incomplete Information,"Michael R. Genesereth, Illah R. Nourbakhsh","Problem solving with incomplete information is usually very costly, since multiple alternatives must be taken into account in the planning process. In this paper, we present some pruning rules that lead to substantial cost savings. The rules are all based on the simple idea that, if goal achievement is the sole criterion for performance, a planner need not consider one ""branch"" in its search space when there is another ""branch"" characterized by equal or greater information. The idea is worked out for the cases of sequential planning, conditional planning, and interleaved planning and execution. The rules are of special value in this last case, as they provide a way for the problem solver to terminate its search without planning all the way to the goal and yet be assured that no important alternatives are overlooked.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-108.pdf,
108,1993,Search,Decomposition of Domains Based on the Micro-Structure of Finite Constraint-Satisfaction Problems,Philippe Jégou,"In this paper, we present a method for improving search efficiency in the area of Constraint-Satisfaction-Problems in finite domains. This method is based on the analysis of the ""micro-structure"" of a CSP. We call micro-structure of a CSP, the graph defined by the compatible relations between variable-value pairs: vertices are these pairs, and edges are defined by pairs of compatible vertices. Given the micro-structure of a CSP, we can realize a preprocessing to simplify the problem with a decomposition of the domains of variables. So, we propose a new approach to problem decomposition in the field of CSPs, well adjusted in cases such as classical decomposition methods are without interest (i.e. when the constraint graph is complete). The method is described in the paper and a complexity analysis is presented, given theoretical justifications of the approach. Furthermore, two polynomial classes of CSPs are induced by this approach, the recognition of them being linear in the size of the instance of CSP considered.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-109.pdf,
109,1993,Search,Innovative Design as Systematic Search,"Dorothy Neville, Daniel S. Weld","We present a new algorithm, SIE, for designing lumped parameter models from first principles. Like the IBIS system of Williams [1989, I990], SIE uses a qualitative representation of parameter interactions to guide its search and speed the test for working designs. But SIE’s interaction set representation is considerably simpler than IBIS’s space of potential and existing interactions. Furthermore, SIE is both complete and systematic - it explores the space of possible designs in an nonredundant manner.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-110.pdf,
110,1993,Search,Generating Effective Admissible Heuristics by Abstraction and Reconstitution,"Armand Prieditis, Bhaskar Janakiraman","Admissible heuristics are worth discovering because they have desirable properties in various search algorithms. Unfortunately, effective ones-ones that are accurate and efficiently computable-are difficult for humans to discover. One source of admissible heuristics is from abstractions of a problem: the length of a shortest path solution to an abstracted problem is an admissible heuristic for the original problem because the abstraction has certain details removed. However, often too many details have to be abstracted to yield an efficiently computable heuristic, resulting in inaccurate heuristics. This paper describes a method to reconstitute the abstracted details back into the solution to the abstracted problem, thereby boosting accuracy while maintaining admissibility. Our empirical results of applying this paradigm to project scheduling suggest that reconstitution can make a good admissible heuristic even better.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-111.pdf,
111,1993,Search,Iterative Weakening: Optimal and Near-Optimal Policies for the Selection of Search Bias,Foster John Provost,"Decisions made in setting up and running search programs bias the searches that they perform. Search bias refers to the definition of a search space and the definition of the program that navigates the space. This paper addresses the problem of using knowledge regarding the complexity of various syntactic search biases to form a policy for selecting bias. In particular, this paper shows that a simple policy, iterative weakening, is optimal or nearly optimal in cases where the biases can be ordered by computational complexity and certain relationships hold between the complexity of the various biases. The results are obtained by viewing bias selection as a (higher-level) search problem. Iterative weakening evaluates the states in order of increasing complexity. An offshoot of this work is the formation of a near-optimal policy for selecting both breadth and depth bounds for depth-fist search with very large (possibly unbounded) breadth and depth.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-112.pdf,
112,1993,Search,Pruning Duplicate Nodes in Depth-First Search,"Larry A. Taylor, Richard E. Korf","Best-first search algorithms require exponential memory, while depth-first algorithms require only linear memory. On graphs with cycles, however, depth-first searches do not detect duplicate nodes, and hence may generate asymptotically more nodes than best-first searches. We present a technique for reducing the asymptotic complexity of depth-first search by eliminating the generation of duplicate nodes. The automatic discovery and application of a finite state machine (FSM) that enforces pruning rules in a depth-first search, has significantly extended the power of search in several domains. We have implemented and tested the technique on a grid, the Fifteen Puzzle, the Twenty-Four Puzzle, and two versions of Rubik’s Cube. In each case, the effective branching factor of the depth-first search is reduced, reducing the asymptotic time complexity.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-113.pdf,
113,1993,Search,Conjunctive Width Heuristics for Maximal Constraint Satisfaction,"Richard J. Wallace, Eugene C. Freuder","A constraint satisfaction problem may not admit a complete solution; in this case a good partial solution may be acceptable. This paper presents new techniques for organizing search with branch and bound algorithms so that maximal partial solutions (those having the maximum possible number of satisfied constraints) can be obtained in reasonable time for moderately sized problems. The key feature is a type of variable-ordering heuristic that combines width at a node of the constraint graph (number of constraints shared with variables already chosen) with factors such as small domain size that lead to inconsistencies in values of adjacent variables. Ordering based on these heuristics leads to a rapid rise in branch and bound’s cost function together with local estimates of future cost, which greatly enhances lower bound calculations. Roth retrospective and prospective algorithms based on these heuristics are dramatically superior to earlier branch and bound algorithms developed for this domain.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-114.pdf,
114,1993,Search,Depth-First Versus Best-First Search: New Results,"Weixiong Zhang, Richard E. Korf","Best-first search (BFS) expands the fewest nodes among all admissible algorithms using the same cost function, but typically requires exponential space. Depth-first search needs space only linear in the maximumsearch depth, but expands more nodes than BFS. Using a random tree, we analytically show that the expected number of nodes expanded by depth-first branch-and-bound (DFBnB) is no more than O(d - N), where d is the goal depth and N is the expected number of nodes expanded by BFS. We also show that DFBnB is asymptotically optimal when BFS runs in exponential time. We then consider how to select a linear-space search algorithm, from among DFBnB, iterative-deepening (ID) and recursive best first search (RBFS). Our experimental results indicate that DFBnB is preferable on problems that can be represented by bounded-depth trees and require exponential computation; and RBFS should be applied to problems that cannot be represented by bounded-depth trees, or problems that can be solved in polynomial time.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-115.pdf,
115,1993,Statistically-Based Natural Language Processing,Using an Annotated Language Corpus as a Virtual Stochastic Grammar,Rens Bod,"In Data Oriented Parsing (DOP), an annotated language corpus is used as a virtual stochastic grammar. An input string is parsed by combining subtrees from the corpus. As a consequence, one parse tree can usually be generated by several derivations that involve different subtrees. This leads to a statistics where the probability of a parse is equal to the sum of the probabilities of all its derivations. In (Scha, 1990) an informal introduction to DOP is given, while (Bod, 1992) provides a formalization of the theory. In this paper we show that the maximum probability parse can be estimated in polynomial time by applying Monte Carlo techniques. The model was tested on a set of hand-parsed strings from the Air Travel Information System (ATIS) corpus. Preliminary experiments yield 96% test set parsing accuracy.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-116.pdf,
116,1993,Statistically-Based Natural Language Processing,Equations for Part-of-Speech Tagging,"Eugene Charniak, Curtis Hendrickson, Neil Jacobson, Mike Perkowitz","We derive from first principles the basic equations for a few of the basic hidden-Markov-model word taggers as well as equations for other models which may be novel (the descriptions in previous papers being too spare to be sure). We give performance results for all of the models. The results from our best model (96.45% on an unused test sample from the Brown corpus with 181 distinct tags) is on the upper edge of reported results. We also hope these results clear up some confusion in the literature about the best equations to use. However, the major purpose of this paper is to show how the equations for a variety of models may be derived and thus encourage future authors to give the equations for their model and the derivations thereof.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-117.pdf,
117,1993,Statistically-Based Natural Language Processing,Estimating Probability Distributions over Hypotheses with Variable Unification,Dekai Wu,"We analyze the difficulties in applying Bayesian belief networks to language interpretation domains, which typically involve many unification hypotheses that posit variable bindings. As an alternative, we observe that the structure of the underlying hypothesis space permits an approximate encoding of the joint distribution based on marginal rather than conditional probabilities. This suggests an implicit binding approach that circumvents the problems with explicit unification hypotheses, while still allowing hypotheses with alternative unifications to interact probabilistically. The proposed method accepts arbitrary subsets of hypotheses and marginal probability constraints, is robust, and is readily incorporated into standard unification-based and frame-based models.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-118.pdf,
118,1993,Trainable Natural Language Systems,A Case-Based Approach to Knowledge Acquisition for Domain-Specific Sentence Analysis,Claire Cardie,"This paper describes a case-based approach to knowledge acquisition for natural language systems that simultaneously learns part of speech, word sense, and concept activation knowledge for all open class words in a corpus. The parser begins with a lexicon of function words and creates a case base of context-sensitive word definitions during a human-supervised training phase. Then, given an unknown word and the context in which it occurs, the parser retrieves definitions from the case base to infer the word’s syntactic and semantic features. By encoding context as part of a definition, the meaning of a word can change dynamically in response to surrounding phrases without the need for explicit lexical disambiguation heuristics. Moreover, the approach acquires all three classes of knowledge using the same case representation and requires relatively little training and no hand-coded knowledge acquisition heuristics. We evaluate it in experiments that explore two of many practical applications of the technique and conclude that the case-basedmethod provides a promising approach to automated dictionary construction and knowledge acquisition for sentence analysis in limited domains. In addition, we present a novel case retrieval algorithm that uses decision trees to improve the performance of a k-nearest neighbor similarity metric.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-119.pdf,
119,1993,Trainable Natural Language Systems,KITSS: A Knowledge-Based Translation System for Test Scenarios,"Van E. Kelly, Mark A. Jones","Machine-assisted language translation systems for technical documents, guide humans through a process of selecting and composing variant partial translations. The constrained nature of technical sublanguages makes language processing aids cost-effective to build and use. Analogously, we have developed KITSS, a knowledge-based translation system for converting informal English scenarios of the desired behavior of complex reactive systems into formal, executable test scripts. A trainable parser and reference resolver capture domain-specific linguistic knowledge. A logic analyzer establishes coherence in the translation process in a role comparable to a ""story understander"". It checks the consistency of each step of a translated test script using a theorem prover, a planner, and logic-encoded background knowledge about the system under test. This helps correct common but serious specification errors, including underspecificity, omitted steps, and even some outright mis-statements. To evaluate how well such technology can scale, we have exercised our technology progressively on a graduated corpus of 100 behavior scenarios spanning 7 advanced calling features for a private telephone switch (PBX), successfully translating 70% into test scripts without any manual post-hoc editing. Our experience with KITSS has enabled us to identify many of the tradeoffs in accommodating informality in specification, versus demanding formality from a human agent.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-120.pdf,
120,1993,Trainable Natural Language Systems,Automatically Constructing a Dictionary for Information Extraction Tasks,Ellen Riloff,"Knowledge-based natural language processing systems have achieved good success with certain tasks but they are often criticized because they depend on a domain-specific dictionary that requires a great deal of manual knowledge engineering. This knowledge engineering bottleneck makes knowledge-based NLP systems impractical for real-world applications because they cannot be easily scaled up orported to new domains. In response to this problem, we developed a system called AutoSlog that automatically builds a domain-specific dictionary of concepts for extracting information from text. Using AutoSlog. we constructed a dictionary for the domain of terrorist event descriptions in only 5 person-hours. We then compared the AutoSlog dictionary with a hand-crafted dictionary that was built by two highly skilled graduate students and required approximately 1500 person-hours of effort. We evaluated the two dictionaries using two blind test sets of 100 texts each. Overall, the AutoSlog dictionary achieved 98% of the performance of the hand-crafted dictionary. On the first test set, the Auto-Slog dictionary obtained 96.3% of the perfomlance of the hand-crafted dictionary. On the second test set, the overall scores were virtually indistinguishable with the AutoSlog dictionary achieving 99.7% of the performance of the handcrafted dictionary.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-121.pdf,
121,1993,Trainable Natural Language Systems,Learning Semantic Grammars with Constructive Inductive Logic Programming,"John M. Zelle, Raymond J. Mooney",Automating the construction of semantic grammars is a difficult and interesting problem for machine learning. This paper shows how the semantic-grammar acquisition problem can be viewed as the learning of search-control heuristics in a logic program. Appropriate control rules are learned using a new first-order induction algorithm that automatically invents useful syntactic and semantic categories. Empirical results show that the learned parsers generalize well to novel sentences and out-perform previous approaches based on connectionist techniques.,https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-122.pdf,
122,1993,Vision Processing,Polly: A Vision-Based Artificial Agent,Ian Horswill,"In this paper I will describe Polly, a low cost vision-based robot that gives primitive tours. The system is very simple, robust and efficient, and runs on a hardware platform which could be duplicated for less than $lOK US. The system was built to explore how knowledge about the structure the environment can be used in a principled way to simplify both visual and motor processing. I will argue that very simple and efficient visual mechanisms can often be used to solve real problems in real (unmodified) environments in a principled manner. I will give an overview of the robot, discuss the properties of its environment, show how they can be used to simplify the design of the system, and discuss what lessons can drawn for the design of other systems.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-123.pdf,
123,1993,Vision Processing,Range Estimation From Focus Using a Non-frontal Imaging Camera,"Arun Krishnan, Narendra Ahuja","This paper is concerned with active sensing of range information from focus. It describes a new type of camera whose image pla.ne is uot perpendicular to the optical axis as is standard. This special imaging geometry eliminates the usual focusing need of image plane movement. Camera movement, which is anyway necessary to process large visual fields, integrates panning, focusing, and range estimation. Thus the two standard mechanical actions of focusing and panning are replaced by panning alone. Range estimation is done at the speed of panning. An implementation of the proposed camera design is described and experiments with range estimation are reported.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-124.pdf,
124,1993,Vision Processing,Learning Object Models from Appearance,"Hiroshi Murase, Shree K. Nayar","We address the problem of automatically learning object models for recognition and pose estimation. In contrast to the traditional approach, we formulate the recognition problem as one of matching visual appearance rather than shape. The appearance of an object in a two-dimensional image depends on its shape, pose in the scene, reflectance properties, and the illumination conditions. While shape and reflectance are intrinsic properties of an object and are constant, pose and illumination vary from scene to scene. We present a new compact representation of object appearance that is parametrized by pose and illumination. For each object of interest, a large set of images is obtained by automatically varying pose and illumination. This large image set is compressed to obtain a low-dimensional subspace, called the eigenspace, in which the object is represented as a hypersurface. Given an unknown input image, the recognition system projects the image onto the eigenspace. The object is recognized based on the hypersurface it lies on. The exact position of the projection on the hypersurface determines the object’s pose in the image. We have conducted experiments using several objects with complex appearance characteristics. These results suggest the proposed appearance representation to be a valuable tool for a variety of machine vision applications.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-125.pdf,
125,1993,Vision Processing,On the Qualitative Structure of Temporally Evolving Visual Motion Fields,Richard P. Wildes,"This paper presents a qualitative analysis that relates stable structures in visual motion fields to properties of corresponding three-dimensional environments. Such an analysis is fundamental in the development of methods for recovering useful information from dynamic visual data without the need for highly accurate and precise sensing. Methodologically, the techniques of singularity theory are used to describe the mapping from image space to velocity space and to relate this mapping to the three-dimensional environment. The specific results of this paper address situations where an optical sensor is undergoing pure rotational or pure translational motion through its environment. For the case of pure rotational motion it is shown that the qualitative structure of visual motion provides information about the axes and relative magnitudes of rotation. For the case of pure translational motion it is shown that the qualitative structure of visual motion provides information about the shape and orientation of viewed surfaces as well as information about the translation itself. Further, the temporal evolution of the visual motion field is described. These results suggest that valuable information regarding three-dimensional environmental structure and motion can be recovered from qualitative consideration of visual motion fields.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-126.pdf,
126,1993,Invited Talks,Tiger in a Cage: The Applications of Knowledge-based Systems (1993),Edward A. Feigenbaum,"Some pioneers of Artificial Intelligence dreamed of the super-intelligent computer, whose problem solving performance would rival or exceed human performance. Their dream has been partially realized, for narrow areas of human endeavor, in the programs called expert systems, whose behavior is often at world-class levels of competence. Their dream was partially transformed by programs that give intelligent help to humans with problems (rather than perform super-intelligently). These are called knowledge systems.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-127.pdf,
127,1993,Invited Talks,Artificial Intelligence as an Experimental Science,Herbert A. Simon,"The journal Artificial Intelligence has experienced a rather steady drift, in recent years, from articles describing and evaluating specific computer programs that exhibit intelligence to formal articles that prove theorems about intelligence. This trend raises basic questions about the nature of theory in artificial intelligence and the appropriate form for a mature science of this discipline. During the past 35 years of AI’s history, the vast bulk of our understanding of machine intelligence has derived from experimenting: constructing innumerable programs that exhibit such intelligence, and examining and analyzing their performance. Theory has been induced by identifying components and processes that are common to many of the programs, and broad generalizations about them. Some of this theory is formal, but most takes the form of laws of qualitative structure. In this respect, artificial intelligence resembles other empirical sciences like molecular biology or geophysics much more than mathematics. Computers, however ""artificial,"" are real objects the complexity of whose behavior cannot be captured fully in simple formalisms. There are no ""Three Laws of Motion"" of AI. This talk examines the forms that theory has taken (and will take) in artificial intelligence, and shows why the progress of the discipline would be stifled by a premature or excessive preoccupation with formalizations derivable from logic and mathematics.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-128.pdf,
128,1993,Video Abstracts,A Demonstration of the Circuit Fix-it Shoppe,"D. Richard Hipp, Ronnie W. Smith","The ""Circuit Fix-it Shoppe"" is a voice interactive dialog system which has been constructed in our laboratory. The mission of the system is to help people repair electronic circuits. The system contains a domain modeler, a reasoning system, a dialog controller, a user modeling system, an error-correcting natural language parser, and a natural language generator. A commercial speech recognizer and speech synthesizer are used for voice input and output. More detailed information about our dialog system can be found in [l] and [2]. This videotape records two live dialogs between the Circuit Fix-it Shoppe program and a user who has no special knowledge of computers, electronic repair, or our system. A brief description of the experimental setup and of the Circuit Fix-it Shoppe program precedes these dialogs. The Circuit Fix-it Shoppe program is capable of varying its level of initiative. It can be highly directive, in which case it controls the conversation, or it may be passive, in which case the user controls the dialog, or it may take some level of initiative between these two extremes. In the first videotape demonstration, the system is running in directive mode. In this second demonstration, the system is set to operate in declarative mode. In this mode, the user is free to take the initiative and to control the conversation. Declarative mode is appropriate for users who are much more familiar with the circuit and require only minimal help from the computer.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-129.pdf,
129,1993,Video Abstracts,Instructo-Soar: Learning from Interactive Natural Language Instructions,"Scott B. Huffman, John E. Laird","Despite its ubiquity in human learning, very little work has been done in artificial intelligence on learning from natural language instructions. In this video, we present a system, Instructo-Soar, that can both behave and learn from natural language instructions. The system is described in papers elsewhere [Huffman and Laird, 1993a; Huffman and Laird, 1993b]. The type of instruction we particularly address is situated, interactive instruction. Situated means that the student is within the task domain, attempting to perform tasks, when instruction is given. Interactive means that the student can request instruction as needed.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-130.pdf,
130,1993,Video Abstracts,Winning the AAAI Robot Competition,"David Kortenkamp, Marcus Huber, Charles Cohen, Ulrich Raschke, Clint Bidlack, Clare Bates Congdon, Frank Koss, Terry Weymouth","Last summer, AAAI sponsored a mobile robot competition in conjunction with the AAAI-92 conference in San Jose, California. Ten robots from across the country competed in the competition, with CARMEL from the University of Michigan finishing first. CARMEL is a Cybermotion K2A mobile platform with a ring of 24 sonar sensors and a single black and white CCD camera. For computing, CARMEL has three processors: one for motor control, one for sonar ring firing and one executing high-level routines such as obstacle avoidance and object recognition. All computation and power is contained entirely on-board.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-131.pdf,
131,1993,Video Abstracts,AIR-SOAR: Intelligent Multi-Level Control,"Douglas J. Pearson, Randolph M. Jones, John E. Laird","Autonomous systems must be able to deal with dynamic, unpredictable environments in real time. Our video describes a system for intelligent control of an airplane, within a realistic flight simulator (the Silicon Graphics flight simulator). The simulator allows asynchronous control of the plane’s throttle, ailerons, elevator and other control surfaces by an external system, and it provides limited asynchronous sensing of the plane’s motion. The result is a highly dynamic, real time domain in which models of the plane (and, potentially, other aircraft) are updated 20 times a second. Control of flight is complex. Unexpected events such as wind or turbulence must be responded to in a timely fashion. Further, identical control movements have different effects depending on the plane’s position and environmental conditions, making precise prediction of action effects difficult. The agent must also deal with delays in feedback from its actions, waiting for the plane to respond to changes in the control surfaces. The domain requires simultaneous execution of a range of tasks at different levels of complexity and granularity, from high level maneuvers like takeoff, landing and banked turns to low level tasks such as maintaining altitude, keeping the wings level and controlling the stick.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-132.pdf,
132,1993,Video Abstracts,Selective Perception for Robot Driving,"Douglas A. Reece, Steven A. Shafer","Robots performing complex tasks in rich environments need very good perception modules in order to understand their situation and choose the best action. Robot planning systems have typically assumed that perception was so good that it could refresh the entire world model whenever the planning system needed it, or whenever anything in the world changed. Unfortunately, this assumption is completely unrealistic in many real-world domains because perception is far too difficult. Robots in these domains cannot use the traditional planner paradigm, but instead need a new system design that integrates reasoning with perception. Our research is aimed at showing how a robot can reason about perception, how task knowledge can be used to select perceptual targets, and how this selection dramatically reduces the computational cost of perception.",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-133.pdf,
133,1993,Video Abstracts,Computer Vision Research at the University of Massachusetts,"Edward M. Riseman, Allen R. Hanson, J. Indigo Thomas",This video first summarizes current research at the University of Massachusetts on mobile vehicle navigation using landmark recognition and a partial 3D world model. We then show how landmarks and world models might be automatically acquired and updated over time.,https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-134.pdf,
134,1993,Video Abstracts,"A Fuzzy Controller for Flakey, the Robot","Alessandro Saffiotti, Nicholas Helft, Kurt Konolige, John Lowrance, Karen Myers, Daniela Musto, Enrique Ruspini, Leonard Wesley","SRI International has a long tradition in the field of qualitative analysis and control of complex systems, starting with the development of the early mobile robot Shakey. More recently, we have developed a fuzzy controller for our new platform, Flakey. Flakey’s controller can pursue strategic goals while operating under conditions of uncertainty, incompleteness, and imprecision. This controller includes capabilities for: Robust, uncertainty-tolerating goal-directed activity. Real-time reactivity to unexpected contingencies (e.g., unknown obstacles). Blending of multiple goals (e.g., reaching a position while avoiding static and moving obstacles). In our approach, detailed in [2, 5], each goal is associated with a function that maps each perceived situation to a measure of desirability of possible actions from the point of view of that goal. The notion of a ""control structure,"" is used for representing and manipulating high-level goals (and the associated desirability functions) in the fuzzy controller. Typical control structures are associated with environment features such as locations to reach, walls, or doorways. Each desirability function induces a particular ""behavior"" -one obtained by executing the actions with higher desirability. Many behaviors, induced by many simultaneous goals can be smoothly blended together by combining their desirability functions using the inferential procedures of fuzzy logic. The fuzzy controller prefers the actions that best satisfy each behavior. Blending of behaviors is the key to combining goal-oriented activity (e.g., trying to reach a given location) and reactivity (e.g., avoiding obstacles on the way).",https://aaai.org/Library/AAAI/1993/../../../Papers/AAAI/1993/AAAI93-135.pdf,
