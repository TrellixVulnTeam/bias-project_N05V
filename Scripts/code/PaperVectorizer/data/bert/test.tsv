id	text
mapping informal settlements in developing countries using machine learning and low resolution multi-spectral data	informal settlements home socially economically vulnerable people planet order deliver effective economic social aid non-government organizations ngos united nations children 's fund unicef require detailed maps locations informal settlements however data regarding informal formal settlements primarily unavailable available often incomplete due part cost complexity gathering data large scale address challenges work provide three contributions 1 brand new machine learning dataset purposely developed informal settlement detection 2 show possible detect informal settlements using freely available low-resolution lr data contrast previous studies use very-high resolution~ vhr satellite aerial imagery something cost-prohibitive ngos 3 demonstrate two effective classification schemes curated data set one cost-efficient ngos another cost-prohibitive ngos additional utility integrate schemes semi-automated pipeline converts either lr vhr satellite image binary map encodes locations informal settlements
equalized odds implies partially equalized outcomes under realistic assumptions	equalized odds -- true positive rates false positive rates equal across groups e.g racial groups -- common quantitative measure fairness equalized outcomes -- difference predicted outcomes groups less difference observed training data -- contentious incompatible perfectly accurate predictions formalize quantify relationship two important seemingly distinct notions fairness show realistic assumptions equalized odds implies partially equalized outcomes prove comparable result approximately equalized odds addition generalize well-known previous result incompatibility equalized odds another definition fairness known calibration showing partially equalized outcomes implies non-calibration results highlight risks using trends observed across groups make predictions individuals
costs and benefits of fair representation learning	machine learning algorithms increasingly used make support important decisions people 's lives led interest problem fair classification involves learning make decisions non-discriminatory respect sensitive variable race gender several methods proposed solve problem including fair representation learning cleans input data used algorithm remove information sensitive variable show using fair representation learning intermediate step fair classification incurs cost compared directly solving problem refer thecost mistrust show fair representation learning fact addresses different problem interest data user trusted access sensitive variable quantify benefits fair representation learning showing subsequent use cleaned data unfair benefits identify result restricting decisions adversarial data users costs due applying restrictions data users
ethically aligned opportunistic scheduling for productive laziness	artificial intelligence ai mediated workforce management systems e.g. crowdsourcing long-term success depends workers accomplishing tasks productively resting well dual objective summarized concept productive laziness existing scheduling approaches mostly focus efficiency overlook worker wellbeing proper rest order enable workforce management systems follow ieee ethically aligned design guidelines prioritize worker wellbeing propose distributed computational productive laziness cpl approach paper intelligently recommends personalized work-rest schedules based local data concerning worker 's capabilities situational factors incorporate opportunistic resting achieve superlinear collective productivity without need explicit coordination messages extensive experiments based real-world dataset 5,000 workers demonstrate cpl enables workers spend 70 effort complete 90 tasks average providing ethically aligned scheduling existing approaches
semantics derived automatically from language corpora contain human-like moral choices	allowing machines choose whether kill humans would devastating world peace security equip machines ability learn ethical even moral choices show applying machine learning human texts extract deontological ethical reasoning `` right '' `` wrong '' conduct create template list prompts responses include questions `` kill people `` `` murder people `` etc answer templates `` yes/no '' model 's bias score difference model 's score positive response `` yes '' negative response `` '' given choice overall model 's bias score sum bias scores question/answer templates choice ran different choices analysis using universal sentence encoder results indicate text corpora contain recoverable accurate imprints social ethical even moral choices method holds promise extracting quantifying comparing sources moral choices culture including technology
the right to confront your accuser: opening the black box of forensic dna software	results forensic dna software systems regularly introduced compelling evidence criminal trials requests defendants evaluate results generated often denied furthermore mounting evidence problems failures disclose substantial changes methodology oversight bodies substantial differences results generated different software systems society purports guarantee defendants right face accusers confront evidence role black-box forensic software systems moral decision making criminal justice paper examine case forensic statistical tool fst forensic dna system developed 2010 new york city 's office chief medical examiner ocme 5 years expert witness review requested defense teams denied even protective order system used 1300 criminal cases first expert review finally permitted 2016 many problems identified including undisclosed function capable dropping evidence could beneficial defense overall findings substantial motion release full source code fst publicly granted paper quantify impact undisclosed function samples ocme 's validation study discuss potential impact individual defendants specifically find 104 439 samples 23.7 triggered undisclosed data-dropping behavior change skewed results toward false inclusion individuals whose dna present evidence sample beyond consider changes criminal justice system could prevent problems like going unresolved future
algorithmic greenlining: an approach to increase diversity	contexts college admissions hiring image search decision-makers often aspire formulate selection criteria yield high-quality diverse results however simultaneously optimizing quality diversity challenging especially decision-maker know true quality criterion instead must rely heuristics intuition introduce algorithmic framework takes input user 's selection criterion may yield high-quality homogeneous results using application-specific notion substitutability algorithms suggest similar criteria diverse results spirit statistical demographic parity instance given image search query `` chairman '' suggests alternative queries similar gender-diverse `` chairperson '' context college admissions apply algorithm dataset students applications rediscover texas 's `` top 10 rule '' input criterion act score cutoff output class rank cutoff automatically accepting students top decile graduating class historically policy effective admitting students perform well college come diverse backgrounds complement empirical analysis learning-theoretic guarantees estimating true diversity criterion based historical data
requirements for an artificial agent with norm competence	human behavior frequently guided social moral norms human community exist without norms robots enter human societies must therefore behave norm-conforming ways well however currently solid cognitive computational model available human norms represented activated learned provide conceptual psychological analysis key properties human norms identify demands properties put artificial agent incorporates norms-demands format norm representations structured organization learning algorithms
loss-aversively fair classification	use algorithmic learning-based decision making scenarios affect human lives motivated number recent studies investigate decision making systems potential unfairness discrimination subjects based sensitive features like gender race however judging fairness newly designed decision making system studies overlooked important influence people 's perceptions fairness new algorithm changes status quo i.e. decisions existing decision making system motivated extensive literature behavioral economics behavioral psychology prospect theory propose notion fair updates refer loss-averse updates loss-averse updates constrain updates yield improved beneficial outcomes subjects compared status quo propose tractable proxy measures would allow notion incorporated training variety linear non-linear classifiers show proxy measures combined existing measures training nondiscriminatory classifiers.our evaluation using synthetic real-world datasets demonstrates proposed proxy measures effective desired tasks
epistemic therapy for bias in automated decision-making	despite recent interest critical machine learning literature `` bias '' artificial intelligence ai systems nature specific biases stemming interaction machines humans data remains ambiguous influenced gendler 's work human cognitive biases introduce concept alief-discordant belief tension intuitive moral dispositions designers explicit representations generated algorithms discussion alief-discordant belief diagnoses ethical concerns arise designing ai systems atop human biases furthermore codify relationship data algorithms engineers components cognitive discordance comprising novel epistemic framework ethics ai
mapping missing population in rural india: a deep learning approach with satellite imagery	millions people worldwide absent country 's census accurate current granular population metrics critical improving government allocation resources measuring disease control responding natural disasters studying aspect human life communities satellite imagery provide sufficient information build population map without cost time government census present two convolutional neural network cnn architectures efficiently effectively combine satellite imagery inputs multiple sources accurately predict population density region paper use satellite imagery rural villages india population labels 2011 secc census best model achieves better performance previous papers well landscan community standard global population distribution
creating fair models of atherosclerotic cardiovascular disease risk	guidelines management atherosclerotic cardiovascular disease ascvd recommend use risk stratification models identify patients likely benefit cholesterol-lowering therapies models differential performance across race gender groups inconsistent behavior across studies potentially resulting inequitable distribution beneficial therapy work leverage adversarial learning large observational cohort extracted electronic health records ehrs develop `` fair '' ascvd risk prediction model reduced variability error rates across groups empirically demonstrate approach capable aligning distribution risk predictions conditioned outcome across several groups simultaneously models built high-dimensional ehr data also discuss relevance results context empirical trade-off fairness model performance
a comparative analysis of emotion-detecting ai systems with respect to algorithm performance and dataset diversity	recent news organizations considering use facial emotion recognition applications involving youth tackling surveillance security schools however majority efforts facial emotion recognition research focused adults children particularly early years shown express emotions quite differently adults thus algorithms deployed environments impact wellbeing circumstance youth careful examination made accuracy respect appropriateness target demographic work utilize several datasets contain facial expressions children linked emotional state evaluate eight different commercial emotion classification systems compare ground truth labels provided respective datasets labels given highest confidence classification systems assess results terms matching score tpr positive predictive value failure compute rate overall results show emotion recognition systems displayed subpar performance datasets children 's expressions compared prior work adult datasets initial human ratings identify limitations associated automated recognition emotions children provide suggestions directions enhancing recognition accuracy data diversification dataset accountability algorithmic regulation
framing artificial intelligence in american newspapers	publics perceptions new scientific advances ai often informed influenced news coverage understand artificial intelligence ai framed u.s. newspapers content analysis based framing theory journalism science communication conducted study identified dominant topics frames well risks benefits ai covered five major american newspapers 2009 2018. results indicated business technology primary topics news coverage ai benefits ai discussed frequently risks risks ai generally discussed greater specificity additionally episodic issue framing societal impact framing frequently used
degenerate feedback loops in recommender systems	machine learning used extensively recommender systems deployed products decisions made systems influence user beliefs preferences turn affect feedback learning system receives thus creating feedback loop phenomenon give rise so-called `` echo chambers '' `` filter bubbles '' user societal implications paper provide novel theoretical analysis examines role user dynamics behavior recommender systems disentangling echo chamber filter bubble effect addition offer practical solutions slow system degeneracy study contributes toward understanding developing solutions commonly cited issues complex temporal scenario area still largely unexplored
global explanations of neural networks: mapping the landscape of predictions	barrier wider adoption neural networks lack interpretability local explanation methods exist one prediction global attributions still reduce neural network decisions single set features response present approach generating global attributions called gam explains landscape neural network predictions across subpopulations gam augments global explanations proportion samples attribution best explains specifies samples described attribution global explanations also tunable granularity detect fewer subpopulations demonstrate gam 's global explanations 1 yield known feature importances simulated data 2 match feature weights interpretable statistical models real data 3 intuitive practitioners user studies transparent predictions gam help ensure neural network decisions generated right reasons
global explanations of neural networks: mapping the landscape of predictions	barrier wider adoption neural networks lack interpretability local explanation methods exist one prediction global attributions still reduce neural network decisions single set features response present approach generating global attributions called gam explains landscape neural network predictions across subpopulations gam augments global explanations proportion samples attribution best explains specifies samples described attribution global explanations also tunable granularity detect fewer subpopulations demonstrate gam 's global explanations 1 yield known feature importances simulated data 2 match feature weights interpretable statistical models real data 3 intuitive practitioners user studies transparent predictions gam help ensure neural network decisions generated right reasons
trolleymod v1.0: an open-source simulation and data-collection platform for ethical decision making in autonomous vehicles	paper presents trolleymod v1.0 open-source platform based carla simulator collection ethical decision-making data autonomous vehicles platform designed facilitate experiments aiming observe record human decisions actions high-fidelity simulations ethical dilemmas occur context driving targeting experiments class trolley problems trolleymod provides seamless approach creating new experimental settings environments realistic physics-engine high-quality graphical capabilities carla unreal engine also trolleymod provides straightforward interface carla environment python enable implementation custom controllers deep reinforcement learning agents results experiments used sociological analyses well training tuning value-aligned autonomous vehicles based social values inferred observations
uncovering and mitigating algorithmic bias through learned latent structure	recent research highlighted vulnerabilities modern machine learning based systems bias especially towards segments society under-represented training data work develop novel tunable algorithm mitigating hidden potentially unknown biases within training data algorithm fuses original learning task variational autoencoder learn latent structure within dataset adaptively uses learned latent distributions re-weight importance certain data points training method generalizable across various data modalities learning tasks work use algorithm address issue racial gender bias facial detection systems evaluate algorithm pilot parliaments benchmark ppb dataset specifically designed evaluate biases computer vision systems demonstrate increased overall performance well decreased categorical bias debiasing approach
human-ai learning performance in multi-armed bandits	people frequently face challenging decision-making problems outcomes uncertain unknown artificial intelligence ai algorithms exist outperform humans learning tasks thus opportunity ai agents assist people learning tasks effectively work use multi-armed bandit controlled setting explore direction pair humans selection agents observe well human-agent team performs find team performance beat human agent performance isolation interestingly also find agent 's performance isolation necessarily correlate human-agent team 's performance drop agent performance lead disproportionately large drop team performance settings even improve team performance pairing human agent performs slightly better make perform much better pairing agent performs make perform much worse results suggest people different exploration strategies might perform better agents match strategy overall optimizing human-agent team performance requires going beyond optimizing agent performance understanding agent 's suggestions influence human decision-making
perceptions of domestic robots?? normative behavior across cultures	domestic service robots become common widespread must programmed efficiently accomplish tasks aligning actions relevant norms first step equip domestic robots normative reasoning competence understanding norms people apply behavior robots specific social contexts end conducted online survey chinese united states participants asked select preferred normative action domestic service robot take number scenarios paper makes multiple contributions extensive survey first collect data attitudes people normative behavior domestic robots b across cultures c study relative priorities among norms domain present findings discuss implications building computational models robot normative reasoning
toward the engineering of virtuous machines	various traditions 'virtue ethics umbrella studied extensively advocated ethicists clear exists version virtue ethics rigorous enough target machine ethics take include engineering ethical sensibility machine robot study ethics humans might create artificial agents begin address presenting embryonic formalization key part virtue-ethics theory namely learning virtue focus exemplars moral virtue work based part computational formal logic previously used formally model ethical theories principles therein implement models artificial agents
a formal approach to explainability	regard explanations blending input sample model 's output offer definitions capture various desired properties function generates explanations study links properties explanation-generating functions intermediate representations learned models able show example activations given layer consistent explanation subsequent layers addition study intersection union explanations way construct new explanations
rightful machines and dilemmas	tn paper set new kantian approach resolving conflicts dilemmas obligation semi-autonomous machine agents self-driving cars first argue efforts build explicitly moral machine agents focus kant refers duties right justice rather duties virtue ethics society everyone morally equal one individual group normative authority unilaterally decide moral conflicts resolved everyone public institutions everyone could consent authority define enforce adjudicate rights obligations respect one show shift ethics standard justice resolves conflict obligations known `` trolley problem '' rightful machine agents finally consider deontic logic suitable governing explicitly rightful machines might meet normative requirements justice
the seductive allure of artificial intelligence-powered neurotechnology	neuroscience explanations-even completely irrelevant-have shown exert `` seductive allure '' individuals leading judge bad explanations arguments favorably seems similarly seductive allure artificial intelligence ai technologies leading people `` overtrust '' systems even witnessed system perform poorly ai-powered neurotechnologies begun proliferate recent years particularly based electroencephalography eeg represent potentially doubly-alluring combination enormous potential benefit applying ai techniques neuroscience `` decode '' brain activity associated mental states efforts still early stages danger using unproven technologies prematurely especially important real-world contexts yet premature use begun emerge several high-stakes set-tings including law health wellness employment transportation light potential seductive allure technologies need vigilant monitoring scientific validity challenging unsubstantiated claims misuse still actively supporting continued development proper use
what are the biases in my word embedding?	paper presents algorithm enumerating biases word embeddings algorithm exposes large number offensive associations related sensitive features race gender publicly available embeddings including supposedly `` debiased '' embedding biases concerning light widespread use word embeddings associations identified geometric patterns word embeddings run parallel people 's names common lower-case tokens algorithm highly unsupervised even require sensitive features pre-specified desirable many forms discrimination racial discrimination-are linked social constructs may vary depending context rather categories fixed definitions b makes easier identify biases intersectional groups depend combinations sensitive features inputs algorithm list target tokens e.g names word embedding outputs number word embedding association tests weats capture various biases present data illustrate utility approach publicly available word embeddings lists names evaluate output using crowdsourcing also show removing names may remove potential proxy bias
semantic adversarial network with multi-scale pyramid attention for video classification	two-stream architecture shown strong performance video classification task key idea learn spatiotemporal features fusing convolutional networks spatially temporally however problems within architecture first relies optical flow model temporal information often expensive compute store second limited ability capture details local context information video data third lacks explicit semantic guidance greatly decrease classification performance paper proposed new two-stream based deep framework video classification discover spatial temporal information rgb frames moreover multi-scale pyramid attention mpa layer semantic adversarial learning sal module introduced integrated framework mpa enables network capturing global local feature generate comprehensive representation video sal make representation gradually approximate real video semantics adversarial manner experimental results two public benchmarks demonstrate proposed methods achieves state-of-the-art results standard video datasets
grn: gated relation network to enhance convolutional neural network for named entity recognition	dominant approaches named entity recognitionm ner mostly adopt complex recurrent neural networks rnn e.g. long-short-term-memory lstm however rnns limited recurrent nature terms computational efficiency contrast convolutional neural networks cnn fully exploit gpu parallelism feedforward architectures however little attention paid performing ner cnns mainly owing difficulties capturing long-term context information sequence paper propose simple effective cnn-based network ner i.e. gated relation network grn capable common cnns capturing long-term context specifically grn firstly employ cnns explore local context features word model relations words use gates fuse local context features global ones predicting labels without using recurrent layers process sentence sequential manner grn allows computations performed parallel across entire sentence experiments two benchmark ner datasets i.e. conll2003 ontonotes 5.0 show proposed grn achieve state-of-the-art performance without external knowledge also enjoys lower time costs train test
point cloud processing via recurrent set encoding	present new permutation-invariant network 3d point cloud processing network composed recurrent set encoder convolutional feature aggregator given unordered point set encoder firstly partitions ambient space parallel beams points within beam modeled sequence encoded subregional geometric features shared recurrent neural network rnn spatial layout beams regular allows beam features fed efficient 2d convolutional neural network cnn hierarchical feature aggregation network effective spatial feature learning competes favorably state-of-the-arts sotas number benchmarks meanwhile significantly efficient compared sotas
matrix completion for graph-based deep semi-supervised learning	convolutional neural networks cnns provided promising achievements image classification problems however training cnn model relies large number labeled data considering vast amount unlabeled data available web important make use data conjunction small set labeled data train deep learning model paper introduce new iterative graph-based semi-supervised learning gssl method train cnn-based classifier using large amount unlabeled data small amount labeled data method first construct similarity graph nodes represent cnn features corresponding data points labeled unlabeled edges tend connect data points class label graph missing label unsupervised nodes predicted using matrix completion method based rank minimization criterion next step use constructed graph calculate triplet regularization loss added supervised loss obtained initially labeled data update cnn network parameters
multiple saliency and channel sensitivity network for aggregated convolutional feature	paper aiming two key problems instance-level image retrieval i.e. distinctiveness image representation generalization ability model propose novel deep architecture multiple saliency channel sensitivity network mscnet specifically obtain distinctive global descriptors attention-based multiple saliency learning first presented highlight important details image simple effective channel sensitivity module based gram matrix designed boost channel discrimination suppress redundant information additionally contrast existing feature aggregation methods employing pre-trained deep networks mscnet trained two modes first one unsupervised manner instance loss another supervised manner combines classification ranking loss relies limited training data experimental results several public benchmark datasets i.e. oxford buildings paris buildings holidays indicate proposed mscnet outperforms state-of-the-art unsupervised supervised methods
rsa: byzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets	paper propose class robust stochastic subgradient methods distributed learning heterogeneous datasets presence unknown number byzantine workers byzantine workers learning process may send arbitrary incorrect messages master due data corruptions communication failures malicious attacks consequently bias learned model key proposed methods regularization term incorporated objective function robustify learning task mitigate negative effects byzantine attacks resultant subgradient-based algorithms termed byzantine-robust stochastic aggregation methods justifying acronym rsa used henceforth contrast existing algorithms rsa rely assumption data independent identically distributed i.i.d workers hence fits wider class applications theoretically show rsa converges near-optimal solution learning error dependent number byzantine workers ii convergence rate rsa byzantine attacks stochastic gradient descent method free byzantine attacks numerically experiments real dataset corroborate competitive performance rsa complexity reduction compared state-of-the-art alternatives
learning a deep convolutional network for colorization in monochrome-color dual-lens system	monochrome-color dual-lens system gray image captured monochrome camera better quality color image color camera color information get high-quality color images desired colorize gray image color image reference related works usually use hand-crafted methods search best-matching pixel reference image pixel input gray image copy color best-matching pixel result propose novel deep convolution network solve colorization problem end-to-end way based observation pixel input image usually exist multiple pixels reference image correct colors method performs weighted average colors candidate pixels reference image utilize candidate pixels correct colors weight values pixels input image reference image obtained learning weight volume using deep feature representations attention operation proposed focus useful candidate pixels 3-d regulation performed learn context information addition correct wrongly colorized pixels occlusion regions propose color residue joint learning module correct colorization result input gray image guidance evaluate method scene flow cityscapes middlebury sintel datasets experimental results show method largely outperforms state-of-the-art methods
ewgan: entropy-based wasserstein gan for imbalanced learning	paper propose novel oversampling strategy dubbed entropy-based wasserstein generative adversarial network ewgan generate data samples minority classes imbalanced learning first construct entropyweighted label vector class characterize data imbalance different classes concatenate entropyweighted label vector original feature vector data sample feed wgan model train generator generator trained concatenate entropy-weighted label vector random noise feature vectors feed generator generate data samples minority classes experimental results two benchmark datasets show samples generated proposed oversampling strategy help improve classification performance data highly imbalanced furthermore proposed strategy outperforms state-of-the-art oversampling algorithms terms classification accuracy
deep embedding features for salient object detection	benefiting rapid development convolutional neural networks cnns salient object detection methods achieved remarkable results utilizing multi-level convolutional features however saliency training datasets limited scale due high cost pixel-level labeling leads limited generalization trained model new scenarios testing besides fcn-based methods directly integrate multi-level features ignoring fact noise features harmful saliency detection paper propose novel approach transforms prior information embedding space select attentive features filter outliers salient object detection network firstly generates coarse prediction map encorder-decorder structure feature embedding network fen trained embed pixel coarse map metric space incorporates much attentive features highlight salient regions suppress response non-salient regions embedded features refined deep-to-shallow recursive feature integration network rfin improve details prediction maps moreover alleviate blurred boundaries propose guided filter refinement network gfrn jointly optimize predicted results learnable guidance maps extensive experiments five benchmark datasets demonstrate method outperforms state-of-the-art results proposed method end-to-end achieves realtime speed 38 fps
dynamic capsule attention for visual question answering	visual question answering vqa recent advances well advocated use attention mechanism precisely link question potential answer areas difficulty question increases vqa models adopt multiple attention layers capture deeper visual-linguistic correlation negative consequence explosion parameters makes model vulnerable over-fitting especially limited training examples given paper propose extremely compact alternative static multi-layer architecture towards accurate yet efficient attention modeling termed dynamic capsule attention capsatt inspired recent work capsule network capsatt treats visual features capsules obtains attention output via dynamic routing updates attention weights calculating coupling coefficients underlying output capsules meanwhile capsatt also discards redundant projection matrices make model much compact quantify capsatt three benchmark vqa datasets i.e. coco-qa vqa1.0 vqa2.0 compared traditional multi-layer attention model capsatt achieves significant improvements 4.1 5.2 2.2 three datasets respectively moreover much fewer parameters approach also yields competitive results compared latest vqa models verify generalization ability capsatt also deploy another challenging multi-modal task image captioning state-of-the-art performance achieved simple network structure
the kelly growth optimal portfolio with ensemble learning	competitive alternative markowitz mean-variance portfolio kelly growth optimal portfolio drawn sufficient attention investment science growth optimal portfolio theoretically guaranteed dominate portfolio probability 1 long run practically tends highly risky short term moreover empirical analysis performance enhancement studies practical settings surprisingly short particular handle challenging realistic condition insufficient training data barely investigated order fill voids especially grappling difficulty small samples paper propose growth optimal portfolio strategy equipped ensemble learning synergically leverage bootstrap aggregating algorithm random subspace method portfolio construction mitigate estimation error analyze behavior hyperparameter selection proposed strategy simulation corroborate effectiveness comparing out-of-sample performance 10 competing strategies four datasets experimental results lucidly confirm new strategy superiority extensive evaluation criteria
towards optimal fine grained retrieval via decorrelated centralized loss with normalize-scale layer	recent advances fine-grained image retrieval prefer learning convolutional neural network cnn specific fullyconnect layer designed loss function discriminative feature representation essentially loss establish robust metric efficiently distinguish high-dimensional features within outside fine-grained categories end existing loss functions defected two aspects feature relationship encoded inside training batch local scope leads low accuracy b error established mean square needs pairwise distance computation training set results low efficiency paper propose novel metric learning scheme termed normalize-scale layer decorrelated global centralized ranking loss achieves extremely efficient discriminative learning i.e. 5× speedup triplet loss 12 recall boost cars196 method originates classic softmax loss global structure directly optimize distance metric well inter/intra class distance tackle issue hypersphere layer global centralized ranking loss pairwise decorrelated learning particular first propose normalize-scale layer eliminate gap metric distance measuring distance retrieval dot product dimension reduction classification second relationship features encoded global centralized ranking loss targets optimizing metric distance globally accelerating learning procedure finally centers decorrelated gram-schmidt process leading extreme efficiency 20 epochs training procedure discriminability feature learning conducted quantitative evaluations two fine-grained retrieval benchmark superior performance demonstrates merits proposed approach state-of-the-arts
dopamine: double-sided masked cnn for pixel adaptive multiplicative noise despeckling	propose dopamine new neural network based multiplicative noise despeckling algorithm algorithm inspired neural aide n-aide recently proposed neural adaptive image denoiser original naide designed additive noise case show framework i.e. adaptively learning network pixel-wise affine denoisers minimizing unbiased estimate mse applied multiplicative noise case well moreover derive double-sided masked cnn architecture control variance activation values layer converge fast high denoising performance supervised training experimental results show dopamine possesses high adaptivity via fine-tuning network parameters based given noisy image achieves significantly better despeckling results compared sar-drn state-of-the-art cnn-based algorithm
learning adaptive random features	random fourier features powerful framework approximate shift invariant kernels monte carlo integration drawn considerable interest scaling kernel-based learning dimensionality reduction information retrieval literature many sampling schemes proposed improve approximation performance however interesting theoretic algorithmic challenge still remains i.e. optimize design random fourier features achieve good kernel approximation input data using low spectral sampling rate paper propose compute adaptive random fourier features optimized spectral samples wj ’ feature weights pj ’ learning scheme significantly reduces spectral sampling rate needed accurate kernel approximation also allows joint optimization supervised learning framework establish generalization bounds using rademacher complexity demonstrate advantages previous methods moreover experiments show empirical kernel approximation provides effective regularization supervised learning
human-in-the-loop feature selection	feature selection crucial step conception machine learning models often performed via datadriven approaches overlook possibility tapping human decision-making model ’ designers users present human-in-the-loop framework interacts domain experts collecting feedback regarding variables samples evaluate relevant task hand information modeled via reinforcement learning derive per-example feature selection method tries minimize model ’ loss function focusing pertinent variables human perspective report results proof-of-concept image classification dataset real-world risk classification task model successfully incorporated feedback experts improve accuracy
graph cnns with motif and variable temporal block for skeleton-based action recognition	hierarchical structure different semantic roles joints human skeleton convey important information action recognition conventional graph convolution methods modeling skeleton structure consider physically connected neighbors joint joints type thus failing capture highorder information work propose novel model motif-based graph convolution encode hierarchical spatial structure variable temporal dense block exploit local temporal information different ranges human skeleton sequences moreover employ non-local block capture global dependencies temporal domain attention mechanism model achieves improvements stateof-the-art methods two large-scale datasets
efficient counterfactual learning from bandit feedback	statistically efficient way off-policy optimization batch data bandit feedback log data generated contextual bandit algorithms consider offline estimators expected reward counterfactual policy estimators shown lowest variance wide class estimators achieving variance reduction relative standard estimators apply estimators improve advertisement design major advertisement company consistent theoretical result estimators allow us improve existing bandit algorithm statistical confidence compared state-of-theart benchmark
spatial and temporal mutual promotion for video-based person re-identification	video-based person re-identification crucial task matching video sequences person across multiple camera views generally features directly extracted single frame suffer occlusion blur illumination posture changes leads false activation missing activation regions corrupts appearance motion representation explore abundant spatial-temporal information video sequences key solve problem end propose refining recurrent unit rru recovers missing parts suppresses noisy parts current frame ’ features referring historical frames rru quality frame ’ appearance representation improved use spatial-temporal clues integration module stim mine spatial-temporal information upgraded features meanwhile multilevel training objective used enhance capability rru stim cooperation modules spatial temporal features mutually promote final spatial-temporal feature representation discriminative robust extensive experiments conducted three challenging datasets i.e. ilids-vid prid-2011 mars experimental results demonstrate approach outperforms existing state-of-the-art methods video-based person re-identification ilids-vid mars achieves favorable results prid-2011
camo: a collaborative ranking method for content based recommendation	real-world recommendation tasks feedback data usually sparse therefore recommender ’ performance often determined much information extract textual contents however current methods make full use semantic information encode textual contents either “ bag-of-words ” technique recurrent neural network rnn former neglects order words latter ignores fact textual contents contain multiple topics besides exists dilemma designing recommender one hand shall use sophisticated model exploit every drop information item contents hand shall adopt simple model prevent over-fitting facing sparse feedbacks fill gaps propose recommender named camo 1. camo employs multi-layer content encoder simultaneously capturing semantic information multitopic word order moreover camo makes use adversarial training prevent complex encoder overfitting extensive empirical studies show camo outperforms state-of-the-art methods predicting users ’ preferences
ea reader: enhance attentive reader for cloze-style question answering via multi-space context fusion	query-document semantic interactions essential success many cloze-style question answering models recently researchers proposed several attention-based methods predict answer focusing appropriate subparts context document paper design novel module produce query-aware context vector named multi-space based context fusion mscf following considerations 1 interactions applied across multiple latent semantic spaces 2 attention measured bit level token level moreover extend mscf multi-hop architecture unified model called enhanced attentive reader ea reader iterative inference process reader equipped novel memory update rule maintains understanding documents read update write operations conduct extensive experiments four real-world datasets results demonstrate ea reader outperforms state-of-the-art models
multi-dimensional classification via knn feature augmentation	multi-dimensional classification mdc deals problem one instance associated multiple class variables specifies class membership w.r.t one specific class space existing approaches learn mdc examples focusing modeling dependencies among class variables potential usefulness manipulating feature space ’ investigated paper first attempt towards feature manipulation mdc proposed enriches original feature space knnaugmented features specifically simple counting statistics class membership neighboring mdc examples used generate augmented feature vector way discriminative information class space encoded feature space help train multi-dimensional classification model validate effectiveness proposed feature augmentation techniques extensive experiments eleven benchmark data sets well four state-of-the-art mdc approaches conducted experimental results clearly show compared original feature space classification performance existing mdc approaches significantly improved incorporating knn-augmented features
optimization of hierarchical regression model with application to optimizing multi-response regression k-ary trees	fast convenient well-known way toward regression induce prune binary tree however little attempt toward improving performance induced regression tree paper presents meta-algorithm capable minimizing regression loss function thus improving accuracy given hierarchical model k-ary regression trees proposed method minimizes loss function node one one split nodes leads solving instance-based cost-sensitive classification problem node ’ data points leaf nodes method leads simple regression problem case binary univariate multivariate regression trees computational complexity training linear samples hence method scalable large trees datasets also briefly explore possibilities applying proposed method classification tasks show algorithm significantly better test error compared state-ofthe- art tree algorithms end accuracy memory usage query time method compared recently introduced forest models depict time proposed method able achieve better similar accuracy tangibly faster query time smaller number nonzero weights
neural collective graphical models for estimating spatio-temporal population flow from aggregated data	propose probabilistic model estimating population flow defined populations transition areas time given aggregated spatio-temporal population data since information individual trajectories aggregated data straightforward estimate population flow proposed method utilize collective graphical model learn individual transition models aggregated data analytically marginalizing individual locations learning spatio-temporal collective graphical model aggregated data ill-posed problem since number parameters estimated exceeds number observations proposed method reduces effective number parameters modeling transition probabilities neural network takes locations origin destination areas time day inputs modeling automatically learn nonlinear spatio-temporal relationships flexibly among transitions locations times four real-world population data sets japan china demonstrate proposed method estimate transition population accurately existing methods
jointly extracting multiple triplets with multilayer translation constraints	triplets extraction essential pivotal step automatic knowledge base construction captures structural information unstructured text corpus conventional extraction models use pipeline named entity recognition relation classification extract entities relations respectively ignore connection two tasks recently several neural network-based models proposed tackle problem achieved state-of-the-art performance however unable extract multiple triplets single sentence yet commonly seen real-life scenarios close gap propose paper joint neural extraction model multitriplets namely tme capable adaptively discovering multiple triplets simultaneously sentence via ranking translation mechanism experiment tme exhibits superior performance achieves improvement 37.6 f1 score state-of-the-art competitors
mfbo-ssm: multi-fidelity bayesian optimization for fast inference in state-space models	nonlinear state-space models ubiquitous modeling real-world dynamical systems sequential monte carlo smc techniques also known particle methods well-known class parameter estimation methods general class state-space models existing smc-based techniques rely excessive sampling parameter space makes computation intractable large systems tall data sets bayesian optimization techniques used fast inference state-space models intractable likelihoods techniques aim find maximum likelihood function sequential sampling parameter space single smc approximator various smc approximators different fidelities computational costs often available sample-based likelihood approximation paper propose multi-fidelity bayesian optimization algorithm inference general nonlinear state-space models mfbo-ssm enables simultaneous sequential selection parameters approximators accuracy speed algorithm demonstrated numerical experiments using synthetic gene expression data gene regulatory network model real data vix stock price index
regularizing fully convolutional networks for time series classification by decorrelating filters	deep neural networks prone overfitting especially small training data regimes often networks overparameterized resulting learned weights tend strong correlations however convolutional networks general fully convolution neural networks fcns particular shown relatively parameter efficient recently successfully applied time series classification tasks paper investigate application different regularizers correlation learned convolutional filters fcns using batch normalization bn regularizer time series classification tsc tasks results demonstrate despite orthogonal initialization filters average correlation across filters especially filters higher layers tends increase training proceeds indicating redundancy filters mitigate redundancy propose strong regularizer using simple yet effective filter decorrelation proposed method yields significant gains classification accuracy 44 diverse time series datasets ucr tsc benchmark repository
