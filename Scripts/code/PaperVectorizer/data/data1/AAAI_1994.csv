,conference_year,category,title,author,abstract,download_url,keywords
0,1994,Art,"Criticism, Culture, and the Automatic Generation of Artworks","Lee Spector, Adam Alpern","Researchers wishing to create computational systems that themselves generate artworks face two interacting challenges. The first is that the standards by which artistic output is judged are notoriously difficult to quantify. The larger AI community is currently involved in a rich internal dialogue on methodological issues, standards, and rigor, and hence murkiness with regard to the assessment of output must be faced squarely. The second challenge is that any artwork exists within an extraordinarily rich cultural and historical context, and it is rare that an artist who is ignorant of this context will produce acceptable works. In this paper we assert that these considerations argue for case-based AI/Art systems that take critical criteria as parameters. We describe an example system that produces new bebop jazz melodies from a case-base of melodies, using genetic programming techniques and a fitness function based on user-provided critical criteria. We discuss the role that such techniques may play in future work on AI and the arts.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-001.pdf,
1,1994,Believable Agents,Research Problems in the Use of a Shallow Artificial Intelligence Model of Personality and Emotion,Clark Elliott,"This paper presents an overview of some open research problems in the representation of emotion on computers. The issues discussed arise in the context of a broad, albeit shallow, emotion reasoning platform based originally on the ideas of Ortony, Clore, and Collins(Ortony, Clore, and Collins 1988). In addressing these problems we hope to (1) correct and expand our content theory of emotion, and pseudo personality, which underlies all aspects of the research; (2) answer feasibility questions regarding a usable representation of the emotion domain in the computer, and (3) build agents capable of emotional interaction with users. A brief description of a semantics-based AI program, the Aflectiue Reasoner, and its recent multi-media extensions is given. Issues pertaining to affective user modeling, an expert system on emotion eliciting situations, the building of a sympathetic computer, models of relationship, personality in games, and the motivation behind the study of emotion on computers are discussed. References to the current literature and recent workshops are made.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-002.pdf,
2,1994,Believable Agents,"ChatterBots, TinyMuds, and the Turing Test: Entering the Loebner Prize Competition",Michael L. Mauldin,"The Turing Test was proposed by Alan Turing in 1950; he called it the Imitation Game. In 1991 Hugh Loebner started the Loebner prize competition, offering a 100,000 prize to the author of the first computer program to pass an unrestricted Turing test. Annual competitions are held each year with smaller prizes for the best program on a restricted Turing test. This paper describes the development of one such Turing System, including the technical design of the program and its performance on the first three Loebner Prize competitions. We also discuss the program’s four year development effort, which has depended heavily on constant interaction with people on the Internet via Tinymuds (multiuser network communication servers). Finally, we discuss the design of the Loebner competition itself, and address its usefulness in furthering the development of Artificial Intelligence.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-003.pdf,
3,1994,Believable Agents,Social Interaction: Multimodal Conversation with Social Agents,"Katashi Nagao, Akikazu Takeuchi","We present a new approach to human-computer interaction, called social interaction. Its main characteristics are summarized by the following three points. First, interactions are realized as multimodal (verbal and nonverbal) conversation using spoken language, facial expressions, and so on. Second, the conversants are a group of humans and social agents that are autonomous and social. Autonomy is an important property that allows agents to decide how to act in an ever-changing environment. Socialness is also an important property that allows agents to behave both cooperatively and collaboratively. Generally, conversation is a joint work and ill-structured. Its participants are required to be social as well as autonomous. Third, conversants often encounter communication mismatches (misunderstanding others’ intentions and beliefs) and fail to achieve their joint goals. The social agents, therefore, are always concerned with detecting communication mismatches. We realize a social agent that hears human-to-human conversation and informs what is causing the misunderstanding. It can also interact with humans by voice with facial displays and head (and eye) movement.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-004.pdf,
4,1994,Case-Based Reasoning,Experience-Aided Diagnosis for Complex Devices,"Michel P. Féret, Janice I. Glasgow","This paper presents a novel approach to diagnosis which addresses the two problems - computational complexity of abduction and device models - that have prevented model-based diagnostic techniques from being widely used. The Experience-Aided Diagnosis (EAD) model is defined that combines deduction to rule out hypotheses, abduction to generate hypotheses and induction to recall past experiences and account for potential errors in the device models. A detailed analysis of the relationship between case-based reasoning and induction is also provided. The EAD model yields a practical method for solving hard diagnostic problems and provides a theoretical basis for overcoming the problem of partially incorrect device models.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-005.pdf,
5,1994,Case-Based Reasoning,Heuristic Harvesting of Information for Case-Based Argument,"Edwina L. Rissland, David B. Skalak, M. Timur Friedman","The BankXX system models the process of perusing and gathering information for argument as a heuristic best-first search for relevant cases, theories, and other domain-specific information. As BankXX searches its heterogeneous and highly interconnected network of domain knowledge, information is incrementally analyzed and amalgamated into a dozen desirable ingredients for argument (called argument pieces), such as citations to cases, applications of legal theories, and references to prototypical factual scenarios. At the conclusion of the search, BankXX outputs the set of argument pieces filled with harvested material relevant to the input problem situation. This research explores the appropriateness of the search paradigm as a framework for harvesting and mining information needed to make legal arguments. We discuss how we tackled the problem of evaluation of BankXX from both the case-based reasoning (CBR) and task-performance perspectives. In particular, we discuss how various system parameters- start node, evaluation function, resource limit-affected BankXX from the CBR perspective and how well BankXX performs its assigned task of gathering information useful for legal argumentation by running BankXX on real legal cases and comparing its output with the published court opinions for those cases.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-006.pdf,
6,1994,Case-Based Reasoning,Case-Based Acquisition of User Preferences for Solution Improvement in Ill-Structured Domains,"Katia Sycara, Kazuo Miyashita","We have developed an approach to acquire complicated user optimization criteria and use them to guide iterative solution improvement. The effectiveness of the approach was tested on job shop scheduling problems. The ill-structuredness of the domain and the desired optimization objectives in real-life problems, such as factory scheduling, makes the problems difficult to formalize and costly to solve. Current optimization technology requires explicit global optimization criteria in order to control its search for the optimal solution. But often, a user’s optimization preferences are state-dependent and cannot be expressed in terms of a single global optimization criterion. In our approach, the optimization preferences are represented implicitly and extensionally in a case base. Experimental results in job shop scheduling problems support the hypotheses that our approach (1) is capable of capturing diverse user optimization preferences and re-using them to guide solution quality improvement, (2) is robust in the sense that it improves solution quality independent of the method of initial solution generation, and (3) produces high quality solutions, which are comparable with solutions generated by traditional iterative optimization techniques, such as simulated annealing, at much lower computational cost.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-007.pdf,
7,1994,Case-Based Reasoning,Towards More Creative Case-Based Design Systems,"Linda M. Wills, Janet L. Kolodner","Case-based reasoning (CBR) has a great deal to offer in supporting creative design, particularly processes that rely heavily on previous design experience, such as framing the problem and evaluating design alternatives. However, most existing CBR systems are not living up to their potential. They tend to adapt and reuse old solutions in routine ways, producing robust but uninspired results. Little research effort has been directed towards the kinds of situation assessment, evaluation, and assimilation processes that facilitate the exploration of ideas and the elaboration and redefinition of problems that are crucial to creative design. Also, their typically rigid control structures do not facilitate the kinds of strategic control and opportunism inherent in creative reasoning. In this paper, we describe the types of behavior we would like case-based design systems to support, based on a study of designers working on a mechanical engineering problem. We show how the standard CBR framework should be extended and we describe an architecture we are developing to experiment with these ideas.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-008.pdf,
8,1994,Case-Based Reasoning,Retrieving Semantically Distant Analogies with Knowledge-Directed Spreading Activation,"Michael Wolverton, Barbara Hayes-Roth","Techniques that traditionally have been useful for retrieving same-domain analogies from small single-use knowledge bases, such as spreading activation and indexing on selected features, are inadequate for retrieving cross-domain analogies from large multi-use knowledge bases. In this paper, we describe Knowledge-Directed Spreading Activation (KDSA), a new method for retrieving analogies in a large semantic network. KDSA uses task-specific knowledge to guide a spreading activation search to a case or concept in memory that meets a desired similarity condition. Specifically, KDSA exploits evaluations of near-analogies encountered during the search to direct the search toward progressively more promising analogies. We describe a specific instantiation of this method for the task of innovative design, and we summarize the theoretical and experimental results used to validate KDSA.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-009.pdf,
9,1994,Cognitive Modeling,A Reading Agent,"Tamitha Carpenter, Richard Alterman","Recent work in agency has explored the interactive nature of goal-driven behavior (""activity""). An interactive agent is responsive to events and is affected by and effects the context in which it exists. Our contention is that interaction is also an important characteristic of a reader. By treating reading as an activity, the reading agent can interact with the text, achieving goals and planning what to read, when to read, and how to read. This paper will discuss how reading within a context of activity provides goals and enables planning, thus creating the reading agent. The system described, SPRITe, reads natural language, primarily instructions, in order to facilitate other activities.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-010.pdf,
10,1994,Cognitive Modeling,The Capacity of Convergence-Zone Episodic Memory,"Mark Moll, Risto Miikkulainen, Jonathan Abbey","Human episodic memory provides a seemingly unlimited storage for everyday experiences, and a retrieval system that allows us to access the experiences with partial activation of their components. This paper presents a neural network model of episodic memory inspired by Damasio’s idea of Convergence Zones. The model consists of a layer of perceptual feature maps and a binding layer. A perceptual feature pattern is coarse coded in the binding layer, and stored on the weights between layers. A partial activation of the stored features activates the binding pattern which in turn reactivates the entire stored pattern. A worst-case analysis shows that with realistic-size layers, the memory capacity of the model is several times larger than the number of units in the model, and could account for the large capacity of human episodic memory.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-011.pdf,
11,1994,Cognitive Modeling,A Model of Creative Understanding,"Kenneth Moorman, Ashwin Ram","Although creativity has largely been studied in problem solving contexts, creativity consists of both a generative component and a comprehension component. In particular, creativity is an essential part of reading and understanding of natural language stories. We have formalized the understanding process and have developed an algorithm capable of producing creative understanding behavior. We have also created a novel knowledge organization scheme to assist the process. Our model of creativity is implemented as a portion of the ISAAC (Integrated Story Analysis And Creativity) reading system, a system which models the creative reading of science fiction stories.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-012.pdf,
12,1994,Cognitive Modeling,Ordering Relations in Human and Machine Planning,"Lee Spector, Mary Jo Rattermann, Kristen Prentice","Analytical results from AI planning research provide the motivation for this experimental study of ordering relationships in human planning. We examine timings of humans performing specific tasks from the AI planning literature and present evidence that normal human planners, like ""state of the art"" AI planning systems, use partial-order plan representations. We also describe ongoing experiments that are designed to shed light on the plan representations used by children and by adults with planning deficits due to brain damage. Several points of interest for collaboration between AI scientists and neuropsychologists are noted, as are impacts that we feel this research may have on future work in AI planning.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-013.pdf,
13,1994,Cognitive Modeling,Experimentally Evaluating Communicative Strategies: The Effect of the Task,Marilyn A. Walker,"Effective problem solving among multiple agents requires a better understanding of the role of communication in collaboration. In this paper we show that there are communicative strategies that greatly improve the performance of resource-bounded agents, but that these strategies are highly sensitive to the task requirements, situation parameters and agents’ resource limitations. We base our argument on two sources of evidence: (1) an analysis of a corpus of 55 problem solving dialogues, and (2) experimental simulations of collaborative problem solving dialogues in an experimental world, Design-World, where we parameterize task requirements, agents’ resources and communicative strategies.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-014.pdf,
14,1994,Music and Audition,Automated Accompaniment of Musical Ensembles,"Lorin Grubb, Roger B. Dannenberg","This paper describes a computer accompaniment system capable of providing musical accompaniment for an ensemble of performers. The system tracks the performance of each musician in the ensemble to determine current score location and tempo of the ensemble. ""Missing parts"" in the composition (i.e., the accompaniment) are synthesized and synchronized to the ensemble. The paper presents an overview of the component problems of automated musical accompaniment and discusses solutions and their implementation. The system has been tested with solo performers as well as ensembles having as many as three performers.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-015.pdf,
15,1994,Music and Audition,Auditory Stream Segregation in Auditory Scene Analysis with a Multi-Agent System,"Tomohiro Nakatani, Hiroshi G. Okuno, Takeshi Kawabata","We propose a novel approach to auditory stream segregation which extracts individual sounds (auditory stream) from a mixture of sounds in auditory scene analysis. The HBSS (Harmonic-Based Stream Segregation) system is designed and developed by employing a multi-agent system. HBSS uses only harmonics as a clue to segregation and extracts auditory streams incrementally. When the tracer-generator agent detects a new sound, it spawns a tracer agent, which extracts an auditory stream by tracing its harmonic structure. The tracer sends a feedforward signal so that the generator and other tracers should not work on the same stream that is being traced. The quality of segregation may be poor due to redundant and ghost tracers. HBSS copes with this problem by introducing monitor agents, which detect and eliminate redundant and ghost tracers. HBSS can segregate two streams from a mixture of man’s and woman’s speech. It is easy to resynthesize speech or sounds from the corresponding streams. Additionally, HBSS can be easily extended by adding agents of a new capability. HBSS can be considered as the first step to computational auditory scene analysis.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-016.pdf,
16,1994,Music and Audition,Simulating Creativity in Jazz Performance,"Geber Ramalho, Jean-Gabriel Ganascia","This paper considers the problem of simulating creativity in the domain of Jazz improvisation and accompaniment. Unlike most current approaches, we try to model the musicians’ behavior by taking into account their experience and how they use it with respect to the evolving contexts of live performance. To represent this experience we introduce the notion of Musical Memory, which explores the principles of Case-Based Reasoning (Slade 1991). To produce live music using this Musical Memory we propose a problem solving method based on the notion of PACTS (Potential ACTions) that are activated according to the context and then combined in order to produce notes. We show that our model supports two of the main features of creativity: non-determinism and absence of well-defined goals (Johnson-Laird 1992).",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-017.pdf,
17,1994,Music and Audition,The Synergy of Music Theory and AI: Learning Multi-Level Expressive Interpretation,Gerhard Widmer,"The paper presents interdisciplinary research in the intersection of AI (machine learning) and Art (music). We describe an implemented system that learns expressive interpretation of music pieces from performances by human musicians. The problem, shown to be very difficult in the introduction, is solved by combining insights from music theory with a new machine learning algorithm. Theoretically founded knowledge about music perception is used to transform the original learning problem to a more abstract level where relevant regularities become apparent. Experiments with performances of Chopin waltzes are presented; the results indicate musical understanding and the ability to learn a complex task from very little training data. As the system’s domain knowledge is based on two established theories of tonal music, the results also have interesting implications for music theory.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-018.pdf,
18,1994,Theater and Video,Knowledge Representation for Video,Marc Davis,"Current computing systems are just beginning to enable the computational manipulation of temporal media like video and audio. Because of the opacity of these media they must be represented in order to be manipulable according to their contents. Knowledge representation techniques have been implicitly designed for representing the physical world and its textual representations. Temporal media pose unique problems and opportunities for knowledge representation which challenge many of its assumptions about the structure and function of what is represented. The semantics and syntax of temporal media require representational designs which employ fundamentally different conceptions of space, time, identity, and action. In particular, the effect of the syntax of video sequences on the semantics of video shots demands a representational design which can clearly articulate the differences between the context-dependent and context-independent semantics of video data. This paper outlines the theoretical foundations for designing representations of video, discusses Media Streams, an implemented system for video representation and retrieval, and critiques related efforts in this area.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-019.pdf,
19,1994,Theater and Video,Semi-Autonomous Animated Actors,Steve Strassmann,"This paper describes an interdisciplinary experiment in controlling semi-autonomous animated human forms with natural language input. These computer-generated characters resemble traditional stage actors, in that they are more autonomous than traditional hand-guided animated characters, and less autonomous than fully improvisational agents. We introduce the desktop theater metaphor, reserving for users the creative role of a theatrical writer or director.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-020.pdf,
20,1994,Automated Reasoning,Rule Based Updates on Simple Knowledge Bases,Chitta Baral,"In this paper we consider updates that are specified as rules and consider simple knowledge bases consisting of ground atoms. We present a translation of the rule based update specifications to extended logic programs using situation calculus notation so as to compute the updated knowledge base. We show that the updated knowledge base that we compute satisfies the update specifications and yet is minimally different from the original database. We then expand our approach to incomplete knowledge bases. We relate our approach to the standard revision and update operators, the formalization of actions and its effects using situation calculus and the formalization of database evolution using situation calculus.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-021.pdf,
21,1994,Automated Reasoning,Recovering Software Specifications with Inductive Logic Programming,William W. Cohen,"We consider using machine learning techniques to help understand a large software system. In particular, we describe how learning techniques can be used to reconstruct abstract Datalog specifications of a certain type of database software from examples of its operation. In a case study involving a large (more than one million lines of C) real-world software system, we demonstrate that off-the-shelf inductive logic programming methods can be successfully used for specification recovery; specifically, Grende12 can extract specifications for about one-third of the modules in a test suite with high rates of precision and recall. We then describe two extensions to Grende12 which improve performance on this task: one which allows it to output a set of candidate hypotheses, and another which allows it to output specifications containing determinations. In combination, these extensions enable specifications to be extracted for nearly two-thirds of the benchmark modules with perfect recall, and precision of better than 60%.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-022.pdf,
22,1994,Automated Reasoning,Can We Enforce Full Compositionality in Uncertainty Calculi?,"Didier DuBois, Henri Prade","At AAAI'93, Elkan has claimed to have a result trivializing fuzzy logic. This trivialization is based on too strong a view of equivalence in fuzzy logic and relates to a fully compositional treatment of uncertainty. Such a treatment is shown to be impossible in this paper. We emphasize the distinction between i) degrees of partial truth which are allowed to be truth functional and which pertain to gradual (or fuzzy) propositions, and ii) degrees of uncertainty which cannot be compositional with respect to all the connectives when attached to classical propositions. This distinction is exemplified by the difference between fuzzy logic and possibilistic logic. We also investigate an almost compositional uncertainty calculus, but it is shown to lack expressiveness.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-023.pdf,
23,1994,Automated Reasoning,An Empirical Evaluation of Knowledge Compilation by Theory Approximation,"Henry Kautz, Bart Selman","Computational efficiency is a central concern in the design of knowledge representation systems. Compiling a knowledge-base into a more tractable form has been suggested as a way around the inherent intractability of many representation formalisms. Because not all theories can be put into an equivalent tractable form, Selrnan and Kautz (1991) have suggested compiling a theory into upper and lower bounds (one logically weaker, the other logical stronger) that approximate the original information. A central question in this approach is how well the bounds capture the original knowledge. This question is inherently empirical. We present a detailed empirical evaluation of the compilation of two kinds of theories: computationally challenging randomly generated theories, and propositional encodings of planning problems. Our results show that one can answer a very high percentage of queries even using unit clause bounds, which are much easier to compute than more general tractable approximations. Furthermore, we demonstrate that many of the queries that can be answered by the bounds are expensive to answer using only the original theory: in other words, knowledge compilation does not just ""skim off"" easy queries. In fact, we show substantial total computational savings in using the bounds together with the original theory to answer all queries (with no errors) from a large benchmark set, over using the original theory alone. This study suggests that knowledge compilation may indeed be a practical approach for dealing with intractability in knowledge representation systems.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-024.pdf,
24,1994,Automated Reasoning,ModGen: Theorem Proving by Model Generation,"Sun Kim, Hantao Zhang","ModGen (Model Generation) is a complete theorem prover for first order logic with finite Herbrand domains. ModGen takes first order formulas as input, and generates models of the input formulas. ModGen consists of two major modules: a module for transforming the input formulas into propositional clauses, and a module to find models of the propositional clauses. The first module can be used by other researchers so that the SAT problems can be easily represented, stored and communicated. An important issue in the design of ModGen is to ensure that transformed propositional clauses are satisfiable iff the original formulas are. The second module can be easily replaced by any advanced SAT problem solver. ModGen is easy to use and very efficient. Many problems which are hard for general resolution theorem provers are found easy for ModGen.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-025.pdf,
25,1994,Automated Reasoning,Small is Beautiful: A Brute-Force Approach to Learning First-Order Formulas,"Steven Minton, Ian Underwood","We describe a method for learning formulas in first-order logic using a brute-force, smallest-first search. The method is exceedingly simple. It generates all irreducible well-formed formulas up to a fixed size and tests them against a set of examples. Although the method has some obvious limitations due to its computational complexity, it performs surprisingly well on some tasks. This paper describes experiments with two applications of the method in the MULTI-TAC sys- tem, a program synthesizer for constraint satisfaction problems. In the first application, axioms are learned, and in the second application, search control rules are learned. We describe these experiments, and consider why searching the space of small formulas makes sense in our applications.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-026.pdf,
26,1994,Automated Reasoning,Avoiding Tests for Subsumption,"Anavai Ramesh, Neil V. Murray","Useful equivalence-preserving operations based on antilinks are described. These operations eliminate a potentially large number of subsumed paths in a negation normal form formula. Those anti-links that directly indicate the presence of subsumed paths are characterized. These operations are useful for prime implicant/implicate algorithms because most of the computational effort in computing the prime implicants and prime implicates of a propositional formula is spent on subsumption checks. The problem of removing all subsumed paths in an NNF formula is shown to be NP-hard, even though such formulas may be small relative to the size of their path sets. The general problem of determining whether a pair of subsumed paths is associated with an arbitrary anti-link is shown to be NP-complete. Further reductions of subsumption checks are shown to be available when strictly put-e full blocks are present. The effectiveness of operations based on anti-links and strictly pure full blocks is examined with respect to some benchmark examples from the literature.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-027.pdf,
27,1994,Automated Reasoning,On Kernel Rules and Prime Implicants,Ron Rymon,"We draw a simple correspondence between kernel rules and prime implicants. Kernel (minimal) rules play an important role in many induction techniques. Prime implicants were previously used to formally model many other problem domains, including Boolean circuit minimization and such classical AI problems as diagnosis, truth maintenance and circumscription. This correspondence allows computing kernel rules using any of a number of prime implicant generation algorithms. It also leads us to an algorithm in which learning is boosted by an auxiliary domain theory, e.g., a set of rules provided by an expert, or a functional description of a device or system; we discuss this algorithm in the context of SE-tree-based generation of prime implicants.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-028.pdf,
28,1994,Automated Reasoning,Using Hundreds of Workstations to Solve First-Order Logic Problems,"Alberto Maria Segre, David B. Sturgill","This paper describes a distributed, adaptive, first-order logic engine with exceptional performance characteristics. The system combines serial search reduction techniques such as bounded-overhead subgoal caching and intelligent backtracking with a novel parallelization strategy particularly well-suited to coarse-grained parallel execution on a network of workstations. We present empirical results that demonstrate our system’s performance using 100 workstations on over 1400 first-order logic problems drawn from the ""Thousands of Problems for Theorem Provers"" collection.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-029.pdf,
29,1994,Automated Reasoning,Termination Analysis of OPS5 Expert Systems,"Hsiu-yen Tsai, Albert Mo Kim Cheng","Bounded response time is an important requirement when rule-based expert systems are used in real-time applications. In the case the rule-based system cannot terminate in bounded time, we should detect the ""culprit"" conditions causing the non-termination to assist programmers in debugging. This paper describes a novel tool which analyzes OPS5 programs to achieve this goal. The first step is to verify that an OPS5 program can terminate in bounded time. A graphical representation of an OPS5 program is defined and evaluated. Once the termination of the OPS5 program is not expected, the ""culprit"" conditions are detected. These conditions are then used to correct the problem by adding extra rules to the original program.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-030.pdf,
30,1994,Description Logic,Refining the Structure of Terminological Systems: Terminology = Schema + Views,"M. Buchheit, W. Nutt, F. M. Donini, A. Schaerf","Traditionally, the core of a Terminological Knowledge Representation System (TKRS) consists of a so-called TBox, where concepts are introduced, and an ABox, where facts about individuals are stated in terms of these concepts. This design has a drawback because in most applications the TBox has to meet two functions at a time: on the one hand, similar to a database schema, framelike structures with typing information are introduced through primitive concepts and primitive roles; on the other hand, views on the objects in the knowledge base are provided through defined concepts. We propose to account for this conceptual separation by partitioning the TBox into two components for primitive and defined concepts, which we call the schema and the view part. We envision the two parts to differ with respect to the language for concepts, the statements allowed, and the semantics. We argue that by this separation we achieve more conceptual clarity about the role of primitive and defined concepts and the semantics of terminological cycles. Moreover, three case studies show the computational benefits to be gained from the refined architecture.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-031.pdf,
31,1994,Description Logic,Boosting the Correspondence between Description Logics and Propositional Dynamic Logics,"Giuseppe De Giacomo, Maurizio Lenzerini","One of the main themes in the area of Terminological Reasoning has been to identify description logics (DLs) that are both very expressive and decidable. A recent paper by Schild showed that this issue can be profitably addressed by relying on a correspondence between DLs and propositional dynamic logics (PDL). However Schild left open three important problems, related to the translation into PDLs of functional restrictions on roles (both direct and inverse), number restrictions, and assertions on individuals. The work reported in this paper presents a solution to these problems. The results have a twofold significance. From the standpoint of DLs, we derive decidability and complexity results for some of the most expressive logics appeared in the literature, and from the standpoint of PDLs, we derive a general methodology for the representation of several forms of program determinism and for the specification of partial computations.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-032.pdf,
32,1994,Description Logic,A Description Classifier for the Predicate Calculus,Robert M. MacGregor,"A description classifier organizes concepts and relations into a taxonomy based on the results of subsumption computations applied to pairs of relation definitions. Until now, description classifiers have only been designed to operate over definitions phrased in highly restricted subsets of the predicate calculus. This paper describes a classifier able to reason with definitions phrased in the full first order predicate calculus, extended with sets, cardinality, equality, scalar inequalities, and predicate variables. The performance of the new classifier is comparable to that of existing description classifiers. Our classifier introduces two new techniques, dual representations and auto-Socratic elaboration, that may be expected to improve the performance of existing description classifiers.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-033.pdf,
33,1994,Causal Reasoning,Forming Beliefs about a Changing World,"Fahiem Bacchus, Adam J. Grove, Joseph Y. Halpern, Daphne Koller","The situation calculus is a popular technique for reasoning about action and change. However, its restriction to a first-order syntax and pure deductive reasoning makes it unsuitable in many contexts. In particular, we often face uncertainty, due either to lack of knowledge or to some probabilistic aspects of the world. While attempts have been made to address aspects of this problem, most notably using nonmonotonic reasoning formalisms, the general problem of uncertainty in reasoning about action has not been fully dealt with in a logical framework. In this paper we present a theory of action that extends the situation calculus to deal with uncertainty. Our framework is based on applying the random-worlds approach of [BGHK94] to a situation calculus ontology, enriched to allow the expression of probabilistic action effects. Our approach is able to solve many of the problems imposed by incomplete and probabilistic knowledge within a unified framework. In particular, we obtain a default Markov property for chains of actions, a derivation of conditional independence from irrelevance, and a simple solution to the frame problem.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-034.pdf,
34,1994,Causal Reasoning,Probabilistic Evaluation of Counterfactual Queries,"Alexander Balke, Judea Pearl","Evaluation of counterfactual queries (e.g., ""If A were true, would C have been true?"") is important to fault diagnosis, planning, and determination of liability. We present a formalism that uses probabilistic causal networks to evaluate one’s belief that the counterfactual consequent, C, would have been true if the antecedent, A, were true. The antecedent of the query is interpreted as an external action that forces the proposition A to be true, which is consistent with Lewis’ Miraculous Analysis. This formalism offers a concrete embodiment of the ""closest world"" approach which (I) properly reflects common understanding of causal influences, (2) deals with the uncertainties inherent in the world, and (3) is amenable to machine representation.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-035.pdf,
35,1994,Causal Reasoning,Symbolic Causal Networks,"Adnan Darwiche, Judea Pearl","For a logical database to faithfully represent our beliefs about the world, one should not only insist on its logical consistency but also on its causal consistency. Intuitively, a database is causally inconsistent if it supports belief changes that contradict with our perceptions of causal influences - for example, coming to conclude that it must have rained only because the sprinkler was observed to be on. In this paper, we (1) suggest the notion of a causal structure to represent our perceptions of causal influences; (2) provide a formal definition of when a database is causally consistent with a given causal structure; (3) introduce symbolic causal networks as a tool for constructing databases that are guaranteed to be causally consistent; and (4) d iscuss various applications of causal consistency and symbolic causal networks, including nonmonotonic reasoning, Dempster-Shafer reasoning, truth maintenance, and reasoning about actions.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-036.pdf,
36,1994,Causal Reasoning,Causal Default Reasoning: Principles and Algorithms,Hector Geffner,"The minimal model semantics is a natural interpretation of defaults yet it often yields a behavior that is too weak. This weakness has been traced to the inabiity of minimal models to reflect certain implicit preferences among defaults, in particular, preferences for defaults grounded on more ’specific' information and preferences arising in causal domains. Recently, ’specificity' preferences have been explained in terms of conditionals. Here we aim to explain causal preferences. We draw mainly from ideas known in Bayesian Networks to formulate and formalize two principles that explain the basic preferences that arise in causal default reasoning. We then define a semantics based on those principles and show how variations of the algorithms used for inheritance reasoning and temporal projection can be used to compute in the resulting formalism.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-037.pdf,
37,1994,Causal Reasoning,Testing Physical Systems,Peter Struss,"We present a formal theory of model-based testing, an algorithm for test generation based on it, and outline how testing is implemented by a diagnostic engine. The key to making the complex task of test generation feasible for systems with continuous domains is the use of model abstraction. Tests can be generated using manageable finite models and then mapped back to a detailed level. We state conditions for the correctness of this approach and discuss the preconditions and scope of applicability of the theory.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-038.pdf,
38,1994,Uncertainty Management,Abstraction in Bayesian Belief Networks and Automatic Discovery from Past Inference Sessions,Wai Lam,"An abstraction scheme is developed to simplify Bayesian belief network structures for future inference sessions. The concepts of abstract networks and abstract junction trees are proposed. Based on the inference time efficiency, good abstractions are characterized. Furthermore, an approach for automatic discovery of good abstractions from the past inference sessions is presented. The learned abstract network is guaranteed to have a better average inference time efficiency if the characteristic of the future sessions remains moreorless the same. A preliminary experiment is conducted to demonstrate the feasibility of this abstraction scheme.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-039.pdf,
39,1994,Uncertainty Management,Noise and Uncertainty Management in Intelligent Data Modeling,"Xiaohui Liu, Gongxian Cheng, John Xingwang Wu","The management of uncertain and noisy data plays an important role in many problem solving tasks. One traditional approach is to quantify the magnitude of noise or uncertainty in the data and to take this information into account when using this type of data for different purposes. In this paper we propose an alternative way of handling uncertain and noisy data. In particular, noise in the data is positively identified and deleted so that quality data can be obtained. Using the assumption that interesting properties in data are more stable than the noise, we propose a general strategy which involves machine learning from data and domain knowledge. This strategy has been shown to provide a satisfactory way of locating and rejecting noise in large quantities of visual field test data, crucial for the diagnosis of a variety of blinding diseases.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-040.pdf,
40,1994,Uncertainty Management,Markov Chain Monte-Carlo Algorithms for the Calculation of Dempster-Shafer Belief,"Serafin Moral, Nic Wilson",A simple Monte-Carlo algorithm can be used to calculate Dempster-Shafer belief very efficiently unless the confiict between the evidences is very high. This paper introduces and explores Markov Chain Monte-Carlo algorithms for calculating Dempster-Shafer belief that can also work well when the conflict is high.,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-041.pdf,
41,1994,Uncertainty Management,Focusing on the Most Important Explanations: Decision-Theoretic Horn Abduction,Paul O'Rorke,"This paper describes a new method, called Decision-Theoretic Horn Abduction (DTHA), for generating and focusing on the most important explanations. A procedure is given that can be used iteratively to generate a sequence of explanations from the most to the least important. The new method considers both the likelihood and utility of partial explanations and is applica ble to a wide range of tasks. This paper shows how it applies to an important engineering design task, namely Failure Modes and Effects Analysis (FMEA). A concrete example illustrates the advantages of the general approach in the context of FMEA.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-042.pdf,
42,1994,Uncertainty Management,The Emergence of Ordered Belief from Initial Ignorance,Paul Snow,"Some simple assumptions about prior ignorance, and the idea that a sticiently arresting contrast in the likelihoods of evidence will elicit belief that one proposition is at least as belief-worthy as another, lead to a partial ordering of propositions without the use of any hind of prior probability. The partial ordering is mt a posterior probability distribution, but does share some intuitively pleasing properties of a probability, such as complementarity. Deciding the order (if any) between two disjunctions depends only on the highest likelihood disjunct in each, and so query handling in partitioned domains is efficient. In the event that an ordinary probability distribution is required for coherent decision making one can be quickly calculated from the partial order.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-043.pdf,
43,1994,Advances in Backtracking,The Hazards of Fancy Backtracking,Andrew B. Baker,"There has been some recent interest in intelligent backtracking procedures that can return to the source of a difficulty without erasing the intermediate work. In this paper, we show that for some problems it can be counterproductive to do this, and in fact that such ""inteIIigence"" can cause an exponential increase in the size of the ultimate search space. We discuss the reason for this phenomenon, and we present one way to deal with it.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-044.pdf,
44,1994,Advances in Backtracking,Dead-End Driven Learning,"Daniel Frost, Rina Dechter","The paper evaluates the effectiveness of learning for speeding up the solution of constraint satisfaction problems. It extends previous work (Dechter 1990) by introducing a new and powerful variant of learning and by presenting an extensive empirical study on much larger and more difficult problem instances. Our results show that learning can speed up backjumping when using either a fixed or dynamic variable ordering. However, the improvement with a dynamic variable ordering is not as great, and for some classes of problems learning is helpful only when a limit is placed on the size of new constraints learned.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-045.pdf,
45,1994,Advances in Backtracking,In Search of the Best Constraint Satisfaction Search,"Daniel Frost, Rina Dechter","We present the results of an empirical study of several constraint satisfaction search algorithms and heuristics. Using a random problem generator that allows us to create instances with given characteristics, we show how the relative performance of various search methods varies with the number of variables, the tightness of the constraints, and the sparseness of the constraint graph. A version of backjumping using a dynamic variable ordering heuristic is shown to be extremely effective on a wide range of problems. We conducted our experiments with problem instances drawn from the 50% satisfiable range.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-046.pdf,
46,1994,Advances in Backtracking,Solution Reuse in Dynamic Constraint Satisfaction Problems,"Gerard Verfaillie, Thomas Schiex","Many AI problems can be modeled as constraint satisfaction problems (CSP), but many of them are actually dynamic: the set of constraints to consider evolves because of the environment, the user or other agents in the framework of a distributed system. In this context, computing a new solution from scratch after each problem change is possible, but has two important drawbacks: inefficiency and instability of the successive solutions. In this paper, we propose a method for reusing any previous solution and producing a new one by local changes on the previous one. First we give the key idea and the corresponding algorithm. Then we establish its properties: termination, correctness and completeness. We show how it can be used to produce a solution, either from an empty assignment, or from any previous assignment and how it can be improved using filtering or learning methods, such as forward-checking or nogood-recording. Experimental results related to efficiency and stability are given, with comparisons with well known algorithms such as backtrack, heuristic repair or dynamic backtracking.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-302.pdf,
47,1994,Advances in Backtracking,Weak-Commitment Search for Solving Constraint Satisfaction Problems,Makoto Yokoo,"The min-conflict heuristic (Minton et al. 1992) has been introduced into backtracking algorithms and iterative improvement algorithms as a powerful heuristic for solving constraint satisfaction problems. Backtracking algorithms become inefficient when a bad partial solution is constructed, since an exhaustive search is required for revising the bad decision. On the other hand, iterative improvement algorithms do not construct a consistent partial solution and can revise a bad decision without exhaustive search. However, most of the powerful heuristics obtained through the long history of constraint satisfaction studies (e.g., forward checking (Haralick and Elliot 1980)) presuppose the existence of a consistent partial solution. Therefore, these heuristics can not be applied to iterative improvement algorithms. Furthermore, these algorithms are not theoretically complete. In this paper, a new algorithm called weak-commitment search which utilizes the min-conflict heuristic is developed. This algorithm removes the drawbacks of backtracking algorithms and iterative improvement algorithms, i.e., the algorithm can revise bad decisions without exhaustive search, the completeness of the algorithm is guaranteed, and various heuristics can be introduced since a consistent partial solution is constructed. The experimental results on various example problems show that this algorithm is 3 to 10 times more efficient than other algorithms.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-047.pdf,
48,1994,Constraint Satisfaction Techniques,Planning from First Principles for Geometric Constraint Satisfaction,"Sanjay Bhansali, Glenn A. Kramer","An important problem in geometric reasoning is to find the configuration of a collection of geometric bodies so as to satisfy a set of given constraints. Recently, it has been suggested that this problem can be solved efficiently by symbolically reasoning about geometry using a degrees of freedom analysis. The approach employs a set of specialized routines called plan fragments that specify how to change the configuration of a set of bodies to satisfy a new constraint while preserving existing constraints. In this paper we show how these plan fragments can be automatically synthesized using first principles about geometric bodies, actions, and topology.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-048.pdf,
49,1994,Constraint Satisfaction Techniques,GENET: A Connectionist Architecture for Solving Constraint Satisfaction Problems by Iterative Improvement,"Andrew Davenport, Edward Tsang, Chang J. Wang, Kangmin Zhu","New approaches to solving constraint satisfaction problems using iterative improvement techniques have been found to be successful on certain, very large problems such as the million queens. However, on highly constrained problems it is possible for these methods to get caught in local minima. In this paper we present GENET, a connectionist architecture for solving binary and general constraint satisfaction problems by iterative improvement. GENET incorporates a learning strategy to escape from local minima. Although GENET has been designed to be implemented on VLSI hardware, we present empirical evidence to show that even when simulated on a single processor GENET can outperfomr existing iterative improvement techniques on hard instances of certain constraint satisfaction problems.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-049.pdf,
50,1994,Constraint Satisfaction Techniques,Expected Gains from Parallelizing Constraint Solving for Hard Problems,"Tad Hogg, Colin P. Williams","A number of recent studies have examined how the difficulty of various NP-hard problems varies with simple parameters describing their structure. In particular, they have identified parameter values that distinguish regions with many hard problem instances from relatively easier ones. In this paper we continue this work by examining independent parallel search. SpecificalIy, we evaluate the speedup as function of connectivity and search difficulty for the particular case of graph coloring with a standard heuristic search method. This requires examining the full search cost distribution rather than just the more commonly reported mean and variance. We also show similar behavior for a single-agent search strategy in which the search is restarted whenever it fails to complete within a specified cost bound.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-050.pdf,
51,1994,Constraint Satisfaction Techniques,Noise Strategies for Improving Local Search,"Bart Selman, Henry A. Kautz, Bram Cohen","It has recently been shown that local search is surprisingly good at finding satisfying assignments for certain computationally hard classes of CNF formulas. The performance of basic local search methods can be further enhanced by introducing mechanisme for escaping from local minima in the search space. We will compare three such mechanisms: simulated annealing, random noise, and a strategy called ""mixed random walk"". We show that mixed random walk is the superior strategy. We also present results demonstrating the effectiveness of local search with walk for solving circuit ByntheBiB and circuit diagnosis problems. Finally, we demonstrate that mixed random walk improves upon the best known methods for solving MAX-SAT problems.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-051.pdf,
52,1994,Constraint Satisfaction Techniques,Improving Repair-Based Constraint Satisfaction Methods by Value Propagation,"Nobuhiro Yugami, Yuiko Ohta, Hirotaka Hara","A constraint satisfaction problem (CSP) is a problem to find an assignment that satisfies given constraints. An interesting approach to CSP is a repair-based method that first generates an initial assignment, then repairs it by minimizing the number of conflicts. Min-conflicts hill climbing (MCHC) and GSAT are typical examples of this approach. A serious problem with this approach is that it is sometimes trapped by local minima. This makes it difficult to use repair-based methods for solving problems with many local minima. We propose a new procedure, EFLOP, for escaping from local minima. EFLOP changes the values of mutually dependent variables by propagating changes through satisfied constraints. We can greatly improve the performance of repair-based methods by combining them with EFLOP. We tested EFLOP with graph colorability problems, randomly generated binary CSPs and propositional satisfiability problems. EFLOP improved the performance of MCHC and GSAT for all experiments and was more efficient for large and difficult problems.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-052.pdf,
53,1994,Tractable Constraint-Satisfaction Problems,An Approach to Multiply Segmented Constraint Satisfaction Problems,"Randall A. Helzerman, Mary P. Harper","This paper describes an extension to the constraint satisfaction problem (CSP) approach called MUSE CSP (Multiply SEgmented Constraint Satisfaction Problem). This extension is especially useful for those problems which segment into multiple sets of partially shared variables. Such problems arise naturally in signal processing applications including computer vision, speech processing, and handwriting recognition. For these applications, it is often difficult to segment the data in only one way given the low-level information utilized by the segmentation algorithms. MUSE CSP can be used to efficiently represent several similar instances of the constraint satisfaction problem simultaneously. If multiple instances of a CSP have some common variables which have the same domains and compatible constraints, then they can be combined into a single instance of a MUSE CSP, reducing the work required to enforce node and arc consistency.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-053.pdf,
54,1994,Tractable Constraint-Satisfaction Problems,Reasoning about Temporal Relations: A Maximal Tractable Subclass of Allen’s Interval Algebra,"Bernhard Nebel, Hans-Jürgen Bürckert","We introduce a new subclass of Allen’s interval algebra we call ""ORD-Horn subclass,"" which is a strict superset of the ""pointisable subclass."" We prove that reasoning in the ORD-Horn subclass is a polynomial-time problem and show that the path-consistency method is sufficient for deciding satisfiability. Further, using an extensive machine-generated case analysis, we show that the ORD-Horn subclass is a maximal tractable subclass of the full algebra. In fact, it is the unique greatest tractable subclass amongst the subclasses that contain all basic relations.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-054.pdf,
55,1994,Tractable Constraint-Satisfaction Problems,A Filtering Algorithm for Constraints of Difference in CSPs,Jean-Charles Régin,"Many real-life Constraint Satisfaction Problems (CSPs) involve some constraints similar to the alldifferent constraints. These constraints are called constraints of difference. They are defined on a subset of variables by a set of tuples for which the values occuring in the same tuple are all different. In this paper, a new filtering algorithm for these constraints is presented. It achieves the generalized arc-consistency condition for these non-binary constraints. It is based on matching theory and its complexity is low. In fact, for a constraint defined on a subset of p variables having domains of cardinality at most d, its space complexity is OCpd) and its time complexity is O(p2d2). This filtering algorithm has been successfully used in the system RESYN (Vismara et al. 1992), to solve the subgraph isomorphism problem.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-055.pdf,
56,1994,Tractable Constraint-Satisfaction Problems,On the Inherent Level of Local Consistency in Constraint Networks,Peter van Beek,"We present a new property called constraint looseness and show how it can be used to estimate the level of local consistency of a binary constraint network. Specifically, we present a relationship between the looseness of the constraints, the size of the domains, and the inherent level of local consistency of a constraint network. The results we present are useful in two ways. First, a common method for finding solutions to a constraint network is to first preprocess the network by enforcing local consistency conditions, and then perform a backtracking search. Here, our results can be used in deciding which low-order local consistency techniques will not change a given constraint network and thus are not useful for preprocessing the network. Second, much previous work has identified conditions for when a certain level of local consistency is sufficient to guarantee a network is backtrack-free. Here, our results can be used in deciding which local consistency conditions, if any, still need to be enforced to achieve the specified level of local consistency. As well, we use the looseness property to develop an algorithm that can sometimes find an ordering of the variables such that a network is backtrack-free.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-056.pdf,
57,1994,Collaboration,Divide and Conquer in Multi-Agent Planning,"Eithan Ephrati, Jeffrey S. Rosenschein","In this paper, we suggest an approach to multiagent planning that contains heuristic elements. Our method makes use of subgoals, and derived sub-plans, to construct a global plan. Agents solve their individual sub-plans, which are then merged into a global plan. The suggested approach may reduce overall planning time and derives a plan that approximates the optimal global plan that would have been derived by a central planner, given those original subgoals. We consider two different scenarios. The first involves a group of agents with a common goal. The second considers how agents can interleave planning and execution when planning towards a common, though dynamic, goal.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-057.pdf,
58,1994,Collaboration,Progressive Negotiation for Resolving Conflicts among Distributed Heterogeneous Cooperating Agents,"Taha Khedro, Michael R. Genesereth","Progressive negotiation is a strategy for resolving conflicts among distributed heterogeneous cooperating agents. This strategy aims at minimizing backtracking to previous solutions and provably ensures consistency of agents’ distributed solutions and convergence on a globally-satisfiable solution. The progressive negotiation strategy is enforced by a task-independent agent called Facilitator, which coordinates and controls the interaction of cooperating agents. The interaction of cooperating agents includes the communication of messages, the identification of conflicts, and the negotiation of conflicts as a way to resolve them. In this paper, we formally present our conceptualization of cooperating agents and their interaction via the facilitator. We next discuss the conflict types identified by agents and then present the progressive negotiation strategy for resolving conflicts. We then present two theorems that discuss the consistency and convergence of distributed solutions ensured by the strategy. Finally, we conclude with a summary of this paper and remarks about the strategy.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-058.pdf,
59,1994,Collaboration,A Collaborative Parametric Design Agent,"Daniel Kuokka, Brian Livezey","ParMan combines the use of agent communication protocols, constraint logic programming, and a graphical presentation interface to yield an intelligent parametric design tool supporting collaborative engineering. This provides one of the first complete, end-to-end applications of distributed knowledge-level communication among engineering tools, as envisioned by PACT (Cutkosky et al. 1993). In addition, it represents a significant extension of parametric design to a distributed setting. This paper describes the underlying technologies of ParMan, based on CLP(R), the Knowledge Query and Manipulation Language, and knowledge-based facilitation agents.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-059.pdf,
60,1994,Collaboration,Exploiting Meta-Level Information in a Distributed Scheduling System,"Daniel E. Neiman, David W. Hildum, Victor R. Lesser, Tuomas Sandholm","In this paper, we study the problem of achieving efficient interaction in a distributed scheduling system whose scheduling agents may borrow resources from one another. Specifically, we expand on Sycara’s use of resource texture measures in a distributed scheduling system with a central resource monitor for each resource type and apply it to the decentralized case. We show how analysis of the abstracted resource requirements of remote agents can guide an agent’s choice of local scheduling activities not only in determining local constraint tightness, but also in identifying activities that reduce global uncertainty. We also exploit meta-level information to allow the scheduling agents to make reasoned decisions about when to attempt to solve impasses locally through backtracking and constraint relaxation and when to request resources from remote agents. Finally, we describe the current state of negotiation in our system and discuss plans for integrating a more sophisticated cost model into the negotiation protocol. This work is presented in the context of the Distributed Airport Resource Management System, a multi-agent system for solving airport ground service scheduling problems.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-060.pdf,
61,1994,Collaboration,A Computational Market Model for Distributed Configuration Design,Michael P. Wellman,"This paper presents a precise market model for a well-defined class of distributed configuration design problems. Given a design problem, the model defines a computational economy to allocate basic resources to agents participating in the design. The result of running these ""design economies"" constitutes the market solution to the original problem. After defining the configuration design framework, I describe the mapping to computational economies and our results to date. For some simple examples, the system can produce good designs relatively quickly. However, analysis shows that the design economies are not guaranteed to find optimal designs, and we identify and discuss some of the major pitfalls. Despite known shortcomings and limited explorations thus far, the market model offers a useful conceptual viewpoint for analyzing distributed design problems.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-061.pdf,
62,1994,Coordination,Emergent Coordination through the Use of Cooperative State-Changing Rules,"Claudia V. Goldman, Jeffrey S. Rosenschein","Researchers in Distributed Artificial Intelligence have suggested that it would be worthwhile to isolate ""aspects of cooperative behavior,"" general rules that cause agents to act in ways conducive to cooperation. One kind of cooperative behavior is when agents independently alter the environment to make it easier for everyone to function effectively. Cooperative behavior of this kind might be to put away a hammer that one finds lying on the floor, knowing that another agent will be able to find it more easily later on. We examine the effect a specific ""cooperation rule"" has on agents in the multi-agent Tileworld domain. Agents are encouraged to increase tiles’ degrees of freedom, even when the tile is not involved in an agent’s own primary plan. The amount of extra work an agent is willing to do is captured in the agent' s cooperation level. Results from simulations are presented. We present a way of characterizing domains as multi-agent deterministic finite automata, and characterizing cooperative rules as transformations of these automata. We also discuss general characteristics of cooperative state-changing rules. It is shown that a relatively simple, easily calculated rule can sometimes improve global system performance in the Tileworld. Coordination emerges from agents who use this rule of cooperation, without any explicit coordination or negotiation.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-062.pdf,
63,1994,Coordination,Forming Coalitions in the Face of Uncertain Rewards,Steven Ketchpel,"When agents are in an environment where they can interact with each other, groups of agents may agree to work together for the benefit of all the members of the group. Finding these coalitions of agents and determining how the joint reward should be divided among them is a difficult problem. This problem is aggravated when the agents have different estimates of the value that the coalition will obtain. A ""two agent auction"" mechanism is suggested to complement an existing coalition formation algorithm for solving this problem.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-063.pdf,
64,1994,Coordination,The Impact of Locality and Authority on Emergent Conventions: Initial Observations,James E. Kittock,"In the design of systems of multiple agents, we must deal with the potential for conflict that is inherent in the interactions among agents; to ensure efficient operation, these interactions must be coordinated. We extend, in two related ways, an existing framework that allows behavioral conventions to emerge in agent societies. We first consider localizing agents, thus limiting their interactions. We then consider giving some agents authority over others by implementing asymmetric interactions. Our primary interest is to explore how locality and authority affect the emergence of conventions. Through computer simulations of agent societies of various configurations, we begin to develop an intuition about what features of a society promote or inhibit the spontaneous generation of coordinating conventions.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-064.pdf,
65,1994,Coordination,Learning to Coordinate without Sharing Information,"Sandip Sen, Mahendra Sekaran, John Hale","Researchers in the field of Distributed Artificial Intelligence (DAI) h ave been developing efficient mechanisms to coordinate the activities of multiple autonomous agents. The need for coordination arises because agents have to share resources and expertise required to achieve their goals. Previous work in the area includes using sophisticated information exchange protocols, investigating heuristics for negotiation, and developing formal models of possibilities of conflict and cooperation among agent interests. In order to handle the changing requirements of continuous and dynamic environments, we propose learning as a means to provide additional possibilities for effective coordination. We use reinforcement learning techniques on a block pushing problem to show that agents can learn complimentary policies to follow a desired path without any knowledge about each other. We theoretically analyze and experimentally verify the effects of learning rate on system convergence, and demonstrate benefits of using learned coordination knowledge on similar problems. Reinforcement learning based coordination can be achieved in both cooperative and non-cooperative domains, and in domains with noisy communication channels and other stochastic characteristics that present a formidable challenge to using other coordination schemes.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-065.pdf,
66,1994,Coordination,"Coalition, Cryptography, and Stability: Mechanisms for Coalition Formation in Task Oriented Domains","Gilad Zlotkin, Jeffrey S. Rosenschein","Negotiation among multiple agents remains an important topic of research in Distributed Artificial Intelligence (DAI). Most previous work on this subject, however, has focused on bilateral negotiation, deals that are reached between two agents. There has also been research on n-agent agreement which has considered ""consensus mechanisms"" (such as voting), that allow the full group to coordinate itself. These group decision-making techniques, however, assume that the entire group will (or has to) coordinate its actions. Sub-groups cannot make sub-agreements that exclude other members of the group. In some domains, however, it may be possible for beneficial agreements to be reached among sub-groups of agents, who might be individually motivated to work together to the exclusion of others outside the group. This paper considers this more general case of n-agent coalition formation. We present a simple coalition formation mechanism that uses cryptographic techniques for subadditive Task Oriented Domains. The mechanism is efficient, symmetric, and individual rational. When the domain is also concave, the mechanism also satisfies coalition rationality.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-066.pdf,
67,1994,Software Agents,An Experiment in the Design of Software Agents,"Henry Kautz, Bart Selman, Michael Coen, Steven Ketchpel, Chris Ramming","We describe a bottom-up approach to the design of software agents. We built and tested an agent system that addresses the real-world problem of handling the activities involved in scheduling a visitor to our laboratory. The system employs both task-specific and user-centered agents, and communicates with users using both email and a graphical interface. This experiment has helped us to identify crucial requirements in the successful deployment of software agents, including issues of reliability, security, and ease of use. The architecture we developed to meet these requirements is flexible and extensible, and is guiding our current research on principles of agent design.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-067.pdf,
68,1994,Software Agents,Collaborative Interface Agents,"Yezdi Lashkari, Max Metral, Pattie Maes","Interface agents are semi-intelligent systems which assist users with daily computer-based tasks. Recently, various researchers have proposed a learning approach towards building such agents and some working prototypes have been demonstrated. Such agents learn by 'watching over the shoulder' of the user and detecting patterns and regularities in the user' s behavior. Despite the successes booked, a major problem with the learning approach is that the agent has to learn from scratch and thus takes some time becoming useful. Secondly, the agent’s competence is necessarily limited to actions it has seen the user perform. Collaboration between agents assisting different users can alleviate both of these problems. We present a framework for multi-agent collaboration and discuss results of a working prototype, based on learning agents for electronic mail.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-068.pdf,
69,1994,Enabling Technologies,Combining Left and Right Unlinking for Matching a Large Number of Learned Rules,Robert B. Doorenbos,"In systems which learn a large number of rules (productions), it is important to match the rules efficiently, in order to avoid the machine learning utility problem. So we need match algorithms that scale well with the number of productions in the system. (Doorenbos 1993) introduced right unlinking as a way to improve the scalability of the Rete match algorithm. This paper introduces a symmetric optimization, left unlinking, and demonstrates that it makes Rete scale well on an even larger class of systems. Unfortunately, when left and right unlinking are combined in the same system, they can interfere with each other. We give a particular way to combine them which we prove minimizes this interference, and analyze the worst-case remaining interference. Finally, we present empirical results showing that the interference is very small in practice, and that the combination of left and right unlinking allows five of our seven testbed systems to learn over 100,000 rules without incurring a significant increase in match cost.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-069.pdf,
70,1994,Enabling Technologies,Discovering Procedural Executions of Rule-Based Programs,"David Gadbois, Daniel Miranker","Executing production system programs involves directly or indirectly executing an interpretive match/select/act cycle. An optimal compilation of a production system program would generate code that requires no appeal to an interpreter to mediate control. However, while a great deal of implicit control information is available in rule-based programs, it is generally impossible to avoid deferring some decisions to run-time. We introduce an approach that may resolve this problem. We propose an execution model that permits the execution of the usual cycle when necessary and otherwise executes procedural functions. The system is best characterized as a rule rewrite system where rules are replaced with chunks, such that the chunks may have procedural components. Correctness for replacement of rules is derived by a constrained abstract evaluation of the entire program via a general-purpose theorem prover at compile time. The analysis gives a global dependency analysis of the interaction of each of the rules. For a group of popular benchmark programs, we show that there is ample opportunity to automatically substitute interpretive pattern matching with procedural elements and a concomitant improvement in performance.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-070.pdf,
71,1994,Enabling Technologies,Mechanisms for Efficiency in Blackboard Systems,"Michael Hewett, Rattikorn Hewett","The RETE algorithm had a great impact on the development of efficient production systems by providing a fast pattern matching mechanism for activation. No similar mechanism has been available to speed up activation and scheduling in blackboard systems. In this paper we describe efflcient, general-purpose efficiency mechanisms that are better suited to blackboard systems than RETE-like networks. We describe a knowledge source compiler that produces match networks and demons for efficient activation and rating while compiling the entire system for increased execution speed. Experiments using the enhancements in a general-purpose blackboard shell illustrate a substantial improvement in run time, including an 80-92% decrease in activation time. The mechanisms we describe are general enough to be used in most existing blackboard systems.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-071.pdf,
72,1994,Enabling Technologies,Model-Based Automated Generation of User Interfaces,"Angel R. Puerta, Henrik Eriksson, John H. Gennari, Mark A. Musen","User interface design and development for knowledge-based systems and most other types of applications is a resource-consuming activity. Thus, many attempts have been made to automate, to certain degrees, the construction of user interfaces. Current tools for automated design of user interfaces are able to generate the static layout of an interface from the application’s data model using an intelligent program that applies design rules. These tools, however, are not capable of generating the dynamic behavior of the interface, which must be specified programmatically, and which constitutes most of the effort of interface construction. Mecano is a model-based user-interface development environment that uses a domain model to generate both the static layout and the dynamic behavior of an interface. A knowledge-based system applies sets of dialog design and layout rules to produce interfaces from the domain model. Mecano has been used successfully to completely generate the layout and the dynamic behavior of relatively large and complex, domain-specific, form- and graph-based interfaces for applications in medicine and several other domains.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-072.pdf,
73,1994,Enabling Technologies,The Relationship between Architectures and Example-Retrieval Times,"Eiichiro Sumita, Naoya Nisiyama, Hitoshi Iida","This paper proposes a method to find the most suitable architecture for a given response time requirement for Example-Retrieval (ER), which searches for the best match from a bulk collection of lingusitic examples. In the Example-Based Approach(EBA), which attains substantially higher accuracy than traditional approaches, ER is extensively used to carry out natural language processing tasks, e.g., parsing and translation. ER, however, is so computationally demanding that it often takes up most of the total sentence processing time. This paper compares several accelerations of ER on different architectures, i.e., serial, MIMD and SIMD. Experimental results reveal the relationship between architectures and response times, which will allows us to find the most suitable architecture for a given response time requirement.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-073.pdf,
74,1994,Instructional Environments,An Instructional Environment for Practicing Argumentation Skills,"Vincent Aleven, Kevin D. Ashley","CAT0 is an instructions environment for practicing basic skills of legal research: to use cases in arguments about a problem situation and to test a theory about a legal domain. Using the CAT0 tools, law students analyze a legal problem, frame queries of CATO'S database of legal cases, and judge how relevant the retrieved cases are to their developing argument or theory. CAT0 aids hming by making explicit an abstract model of the process of argument. It allows students to focus on the high-level argumentation issues, by assisting the student in various ways. By providing an abstract representation of the text of cases, it helps students to reason about the texts and helps guide their critical analysis of the texts. CAT0 makes available opportunities for practice that are hard to set up with traditional instructional methods. CAT0 differs from other instructional environments in the following respects: Few instructional environments focus on argumentation skills. Although there are other instructional environments in which students work with an abstract representation of the task domain, abstracting from text is unusual. CATO demonstrates a contribution that case-based reasoning techniques can make to instructional environments.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-074.pdf,
75,1994,Instructional Environments,Tailoring Retrieval to Support Case-Based Teaching,"Robin Burke, Alex Kass","This paper describes how a computer program can support learning by retrieving and presenting relevant stories drawn from a video case base. Although this is an information retrieval problem, it is not a problem that fits comfortably within the classical IR model (Salton and McGill, 1983) because in the classical model the computer system is too passive. The standard model of IR assumes that the user will take the initiative to formulate retrieval requests, but a teaching system must be able to initiate retrieval and formulate retrieval requests automatically. We describe a system, called SPIEL, that performs this type of retrieval, and discuss theoretical challenges addressed in implementing such a system. These challenges include the development of a representation language for indexing the system’s video library, and the development of set of retrieval strategies and recognition knowledge that allow the system to locate educationally relevant stories.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-075.pdf,
76,1994,Instructional Environments,Situated Plan Attribution for Intelligent Tutoring,"Randall W. Hill, Jr., W. Lewis Johnson","Plan recognition techniques frequently make rigid assumptions about the student’s plans, and invest substantial effort to infer unobservable properties of the student. The pedagogical benefits of plan recognition analysis are not always obvious. We claim that these difficulties can be overcome if greater attention is paid to the situational context of the student’s activity and the pedagogical tasks which plan recognition is intended to support. This paper describes an approach to plan recognition called situated plan attribution that takes these factors into account. It devotes varying amounts of effort to the interpretation process, focusing the greatest effort on interpreting impasse points, i.e., points where the student encounters some difficulty completing the task. This approach has been implemented and evaluated in the context of the REACT tutor, a trainer for Operators of deep space communications stations.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-076.pdf,
77,1994,Instructional Environments,Learning from Highly Flexible Tutorial Instruction,"Scott B. Huffman, John E. Laird","Situated, interactive tutorial intructions give flexibility in teaching tasks, by allowing communication of a variety of types of knowledge in a variety of situations. To exploit this flexibility, however, an instructable agent must be able to learn different types of knowledge from different instructional interactions. This paper presents an approach to learning from flexible tutorial instruction, called situated explanation, that takes advantage of constraints in different instructional contexts to guide the learning process. This makes it applicable to a wide range of instructional interactions. The theory is implemented in an agent called Instructo-Soar, that learns new tasks and other domain knowledge from natural language instructions. Instructo-Soar meets three key requirements of flexible instructability: it can (A) take any command at each instruction point, (B) handle instructions that apply to either the current situation or a hypothetical one (e.g., conditionals), and (C) 1 earn each type of knowledge it uses (derived from its underlying computational model) from instructions.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-077.pdf,
78,1994,Instructional Environments,Case-Based Retrieval Interface Adapted to Customer-Initiated Dialogues in Help Desk Operations,"Hideo Shimazu, Akihiro Shibata, Katsumi Nihei","Help desk systems have become increasingly important in the efforts of corporations to maintain customer satisfaction, and Case-Based Reasoning (CBR) p rovides promising techniques for use in the improvement of such systems. This paper describes multiple interface modes by which customer service operators can respond rapidly to customer-initiated inquiries, retrieving/storing case data from/into a case-base. The proposed interface addresses a major situation assessment problem, the difficulty of attempting to match what may be completely different descriptions of an item to be retrieved, i.e. descriptions resulting from vastly differing points of view. The proposed interface and the similarity assessment algorithm are implemented in the CARET case-based retrieval tool operating on commercial Relational Database Management Systems (RDBMS).",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-078.pdf,
79,1994,"Knowledge Acquisition, Capture, and Integration",Knowledge Refinement in a Reflective Architecture,Yolanda Gil,"A knowledge acquisition tool should provide a user with maximum guidance in extending and debugging a knowledge base, by preventing inconsistencies and knowledge gaps that may arise inadvertently. Most current acquisition tools are not very flexible in that they are built for a predetermined inference structure or problem-solving mechanism, and the guidance they provide is specific to that inference structure and hard-coded by their designer. This paper focuses on EXPECT, a reflective architecture that supports knowledge acquisition based on an explicit analysis of the structure of a knowledge-based system, rather than on a fixed set of acquisition guidelines. EXPECT'S problem solver is tightly integrated with LOOM, a state-of-the-art knowledge representation system. Domain facts and goals are represented declaratively, and the problem solver keeps records of their functionality within the task domain. When the user corrects the system’s knowledge, EXPECT tracks any possible implications of this change in the overall system and cooperates with the user to correct any potential problems that may arise. The key to the flexibility of this knowledge acquisition tool is that it adapts its guidance as the knowledge bases evolve in response to changes introduced by the user.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-079.pdf,
80,1994,"Knowledge Acquisition, Capture, and Integration",A User Interface for Knowledge Acquisition from Video,Henry Lieberman,"In conventional knowledge acquisition, a domain expert interacts with a knowledge engineer, who interviews the expert, and codes knowledge about the domain objects and procedures in a rule-based language, or other textual representation language. This indirect methodology can be tedious and error-prone, since the domain expert’s verbal descriptions can be inaccurate or incomplete, and the knowledge engineer may not correctly interpret the expert’s intent. We describe a user interface that allows a domain expert who is not a programmer to construct representations of objects and procedures directly from a video of a human performing an example procedure. The domain expert need not be fluent in the underlying representation language, since all interaction is through direct manipulation. Starting from digitized video, the user selects significant frames that illustrate before- and after- states of important operations. Then the user graphically annotates the contents of each selected frame, selecting portions of the image to represent each part, labeling the parts, and indicating part/whole relationships. Finally, programming by demonstration techniques describe the actions that represent the transition between frames. The result is object descriptions for each object in the domain, generalized procedural descriptions, and visual and natural language documentation of the procedure. We illustrate the system in the domain of documentation of operational and maintenance procedures for electrical devices.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-080.pdf,
81,1994,"Knowledge Acquisition, Capture, and Integration",Building Non-Brittle Knowledge-Acquisition Tools,"Jay T. Runkel, William P. Birmingham","Existing model-based knowledge-acquisition tools can acquire large knowledge bases and update these knowledge bases as knowledge changes. These tools, however, are brittle. They can only be used to acquire knowledge for a particular problem solver performing a specific task and they are not easily adapted to new problem solvers. Brittleness limits the effectiveness of these tools because the dynamic nature of knowledge systems make modifications both necessary and frequent. This paper presents a model of knowledge systems that reduces brittleness by separating acquisition techniques for search-control knowledge from other types of knowledge, by driving knowledge acquisition from properties of a knowledge-level description of the task instead of the problem solver, and by using ontologies to reuse knowledge bases.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-081.pdf,
82,1994,"Knowledge Acquisition, Capture, and Integration","The Acquisition, Analysis and Evaluation of Imprecise Requirements for Knowledge-Based Systems","John Yen, Xiaoqing Liu, Swee Hor Teh","In this paper, a theoretical foundation has been laid and a practical method has been developed for specifying, analyzing and evaluating the complex relationships between imprecise requirements in knowledge-based systems. Imprecise requirements are represented by the canonical form in test-score semantics. The relationships between requirements are classified to be conflicting and cooperative based on the qualitative and quantitative analysis of relationships between requirements. This kind of analysis makes it possible to formulate a feasible overall requirement from conflicting individual requirements. It also facilitates to find better trade-off strategies for conflicting requirements by using fuzzy multicriteria optimization technique. A requirement engineering process has also been developed to incorporate imprecise requirements into the requirement analysis for knowledge-based systems.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-082.pdf,
83,1994,Knowledge Bases,Extracting Viewpoints from Knowledge Bases,"Liane Acker, Bruce Porter","Viewpoints are coherent collections of facts that describe a concept from a particular perspective. They are essential for a wide variety of tasks, such as explanation generation and qualitative modeling. We have identified many types of viewpoints and developed a program, the View Retriever, for extracting them from knowledge bases, either singly or in combinations. The View Retriever provides a general solution to the central problem in extracting viewpoints: determining which facts are relevant to requested viewpoints. Our evaluation indicates that viewpoints extracted by the View Retriever are comparable in coherence to those people construct.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-083.pdf,
84,1994,Knowledge Bases,Using Induction to Refine Information Retrieval Strategies,"Catherine Baudin, Barney Pell, Smadar Kedar","Conceptual information retrieval systems USC structured document indices, domain knowledge and a set of heuristic retrieval strategies to match user queries with a set of indices describing the document’s content. Such retrieval strategies increase the set of relevant documents retrieved (increase recall), but at the expense of returning additional irrelevant documents (decrease precision). Usually in conceptual information retrieval systems this tradeoff is managed by hand and with difficulty. This paper discusses ways of managing this tradeoff by the application of standard induction algorithms to refine the retrieval strategies in an engineering design domain. We gathered examples of query/retrieval pairs during the system’s operation using feedback from a user on the retrieved information. We then fed these examples to the induction a,lgorithm and generated decision trees that refine the existing set of rcLrieva1 strategies. We found that (1) induction improved the precision on a set of queries generated by another user, without a significant loss in recall, and (2) in an interactive mode, the decision trees pointed out flaws in the retrieval and indexing knowledge and suggested ways to refine the retrieval strategies.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-084.pdf,
85,1994,Knowledge Bases,Formalizing Ontological Commitment,"Nicola Guarino, Massimiliano Carrara, Pierdaniele Giaretta","Formalizing the ontological commitment of a logical language means offering a way to specify the intended meaning of its vocabulary by constraining the set of its models, giving explicit information about the intended nature of the modelling primitives and their a priori relationships. We present here a formal definition of ontological commitment which aims to capture the very basic ontological assumptions about the intended domain, related to issues such as identity and internal structure. To tackle such issues, a modal framework endowed with mereo-topological primitives has been adopted. The paper is mostly based on a revisitation of philosophical (and linguistic) literature in the perspective of knowledge representation.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-085.pdf,
86,1994,Control Learning,Exploiting the Ordering of Observed Problem-Solving Steps for Knowledge Base Refinement: An Apprenticeship Approach,"Steven K. Donoho, David C. Wilkins","Apprenticeship is a powerful method of learning among humans whereby a student refines his knowledge simply by observing and analyzing the problem-solving steps taken by an expert. This paper focuses on knowledge base (KB) refinement for classification problems and examines how the ordering of the problem-solving steps taken by an observed expert can be used to yield leverage in KB refinement. Questions examined include: What added information can be extracted from attribute ordering? How can this added information be utilized to identify and repair KB shortcomings? What assumptions must be made about the observed expert, and how important of a role do these assumptions play? The principles explored have been implemented in the SKIPPER apprentice, and empirical results are given for the audiology domain.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-086.pdf,
87,1994,Control Learning,Improving Learning Performance through Rational Resource Allocation,"Jonathan Gratch, Gerald DeJong, Steve Chien",This article shows how rational analysis can be used to minimize learning cost for a general class of statistical learning problems. We discuss the factors that influence learning cost and show that the problem of efficient learning can be cast as a resource optimization problem. Solutions found in this way can be significantly more efficient than the best solutions that do not account for these factors. We introduce a heuristic learning algorithm that approximately solves this optimization problem and document its performance improvements on synthetic and real-world problems.,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-087.pdf,
88,1994,Control Learning,Learning Explanation-Based Search Control Rules for Partial Order Planning,"Suresh Katukam, Subbarao Kambhampati","This paper presents SNLP+EBL, the first implementation of explanation based learning techniques for a partial order planner. We describe the basic learning framework of SNLP+EBL, including regression, explanation propagation and rule generation. We then concentrate on SNLP+EBL'S ability to learn from failures and present a novel approach that uses stronger domain and planner specific consistency checks to detect, explain and learn from the failures of plans at depth limits. We will end with an empirical evaluation of the efficacy of this approach in improving planning performance.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-088.pdf,
89,1994,Control Learning,Creating Abstractions Using Relevance Reasoning,Alon Y. Levy,"Reasoning with multiple levels of abstraction is a powerful method of controlling problem solving in complex domains. We consider the problem of simplifying a knowledge base by creating an abstraction that is tailored for a given set of queries. Our approach is based on associating formally an abstraction with some irrelevant detail that is removed from the knowledge base. We show how creating an abstraction and determining its utility amounts to automatically deciding which aspects of a representation are irrelevant to a query. As a result, we derive a general algorithm schema for automatically generating abstractions for a query. As an instance of the schema, we describe a novel algorithm for automatically abstracting a KB by projecting out relation arguments.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-089.pdf,
90,1994,Control Learning,Flexible Strategy Learning: Analogical Replay of Problem Solving Episodes,Manuela M. Veloso,"This paper describes the integration of analogical reasoning into general problem solving as a method of learning at the strategy level to solve problems more effectively. Learning occurs by the generation and replay of annotated derivational traces of problem solving episodes. The problem solver is extended with the ability to examine its decision cycle and accumulate knowledge from the chains of successes and failures encountered during its search experience. Instead of investing substantial effort deriving general rules of behavior to apply to individual decisions, the analogical reasoner compiles complete problem solving cases that are used to guide future similar situations. Learned knowledge is flexibly applied to new problem solving situations even if only a partial match exists among problems. We relate this work with other alternative strategy learning methods, and also with plan reuse. We demonstrate the effectiveness of the analogical replay strategy by providing empirical results on the performance of a fully implemented system, PRODIGY/ANALOGY, accumulating and reusing a large case library in a complex problem solving domain.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-090.pdf,
91,1994,Decision-Tree Learning,Branching on Attribute Values in Decision Tree Generation,Usama M. Fayyad,"The problem of deciding which subset of values of a categorical-valued attribute to branch on during decision tree generation is addressed. Algorithms such as ID3 and C4 do not address the issue and simply branch on each value of the selected attribute. The GID3* algorithm is presented and evaluated. The GID3* algorithm is a generalized version of Quinlan’s ID3 and C4, and is a non-parametric version of the GID3 algorithm presented in an earlier paper. It branches on a subset of individual values of an attribute, while grouping the rest under a single DEFAULT branch. It is empirically demonstrated that GID3* outperforms ID3 (C4) and GID3 for any parameter setting of the latter. The empirical tests include both controlled synthetic (randomized) domains as well as real-world data sets. The improvement in tree quality as measured by number of leaves and estimated error rate is significant.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-091.pdf,
92,1994,Decision-Tree Learning,Induction of Multivariate Regression Trees for Design Optimization,"B. Forouraghi, L. W. Schmerr, G. M. Prabhu","In this paper we introduce a methodology within which multiobjective design optimization is approached from an entirely new perspective. Specifically, we demonstrate that multiple-objective optimization through induction of multivariate regression trees is a powerful alternative to the conventional vector optimization techniques. Furthermore, in an attempt to investigate the effect of various types of splitting rules on the overall performance of the optimizing system, we present a tree partitioning algorithm which utilizes a number of techniques derived from diverse fields of statistics and fuzzy logic. These include: three multivariate statistical approaches based on dispersion matrices, two newly-formulated fuzzy splitting rules based on Pearson’s parametric and Kendall’s nonparametric measures of association, Bellman and Zadeh’s fuzzy decision-maximizing approach within an inductive framework, and finally, the multidimensional extension of a widely-used fuzzy entropy measure. In terms of potential application areas, we highlight the advantages of our methodology by presenting the problem of multiobjective design optimization of a beam structure.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-092.pdf,
93,1994,Decision-Tree Learning,Bottom-Up Induction of Oblivious Read-Once Decision Graphs: Strengths and Limitations,Ron Kohavi,"We report improvements to HOODG, a supervised learning algorithm that induces concepts from labelled instances using oblivious, read-once decision graphs as the underlying hypothesis representation structure. While it is shown that the greedy approach to variable ordering is locally optimal, we also show an inherent limitation of all bottom-up induction algorithms, including HOODG, that construct such decision graphs bottom-up by minimizing the width of levels in the resulting graph. We report our empirical experiments that demonstrate the algorithm’s generalization power.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-093.pdf,
94,1994,Decision-Tree Learning,Learning Decision Lists Using Homogeneous Rules,"Richard Segal, Oren Etzioni","A decision list is an ordered list of conjunctive rules (Rivest 1987). Inductive algorithms such as AQ and CN2 learn decision lists incrementally, one rule at a time. Such algorithms face the rule overlap problem - the classification accuracy of the decision list depends on the overlap between the learned rules. Thus, even though the rules are learned in isolation, they can only be evaluated in concert. Existing algorithms solve this problem by adopting a greedy, iterative structure. Once a rule is learned, the training examples that match the rule are removed from the training set. We propose a novel solution to the problem: composing decision lists from homogeneous rules, rules whose classification accuracy does not change with their position in the decision list. We prove that the problem of finding a maximally accurate decision list can be reduced to the problem of finding maximally accurate homogeneous rules. We report on the performance of our algorithm on data sets from the UC1 repository and on the MONK’s problems.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-094.pdf,
95,1994,Decision-Tree Learning,Decision Tree Pruning: Biased or Optimal?,"Sholom M. Weiss, Nitin Indurkhya","We evaluate the performance of weakest-link pruning of decision trees using cross-validation. This technique maps tree pruning into a problem of tree selection: Find the best (i.e. the right-sized) tree, from a set of trees ranging in size from the unpruned tree to a null tree. For samples with at least 200 cases, extensive empirical evidence supports the following conclusions relative to tree selection: (a) 10-fold cross-validation is nearly unbiased; (b) not pruning a covering tree is highly biased; (c) 10-fold cross-validation is consistent with optimal tree selection for large sample sizes and (d) the accuracy of tree selection by 10-fold cross-validation is largely dependent on sample size, irrespective of the population distribution.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-095.pdf,
96,1994,Discovery,An Implemented Model of Punning Riddles,"Kim Binsted, Graeme Ritchie","In this paper, we discuss a model of simple question-answer punning, implemented in a program, JAPE-1, which generates riddles from humour-independent lexical entries. The model uses two main types of structure: schemata, which determine the relationships between key words in a joke, and templates, which produce the surface form of the joke. JAPE-1 succeeds in generating pieces of text that are recognizably jokes, but some of them are not very good jokes. We mention some potential improvements and extensions, including post-production heuristics for ordering the jokes according to quality.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-096.pdf,
97,1994,Discovery,Bootstrapping Training-Data Representations for Inductive Learning: A Case Study in Molecular Biology,"Haym Hirsh, Nathalie Japkowicz","This paper describes a ""bootstrapping"" approach to the engineering of appropriate training-data representations for inductive learning. The central idea is to begin with an initial set of human-created features and then generate additional features that have syntactic forms that are similar to the human-engineered features. More specifically, we describe a two-stage process for the engineering of good representations for learning: first, generating by hand (usually in consultation with domain experts) an initial set of features that seem to help learning, and second, ""bootstrapping"" off of these features by developing and applying operators that generate new features that look syntactically like the expert-based features. Our experiments in the domain of DNA sequence identification show that an initial successful human-engineered representation for data can be expanded in this fashion to yield dramatically improved results for learning.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-097.pdf,
98,1994,Discovery,A Discovery System for Trigonometric Functions,"Tsuyoshi Murata, Masami Mizutani, Masamichi Shimura","This paper describes a discovery system for trigonometric fuctions (DST), which has abilities to acquire new knowledge in the form of theorems and formulas in a plane geometry domain. The system is composed of two subsystems: a plane geometry analysis system and a mathematical formula transformation system. The former changes the length and angles of a figure and extracts geometric relations, and the latter transforms the relations to acquire useful formulas. With little basic knowledge such as the definition of the congruence of triangles and the definition of fundamental trigonometric functions, our system has rediscovered many trigonometric formulas and geometric theorems, including the Pythagorean theorem.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-098.pdf,
99,1994,Induction,Compositional Instance-Based Learning,"Patrick Broos, Karl Branting","This paper proposes a new algorithm for acquisition of preference predicates by a learning apprentice, termed Compositional Instance-Based Learning (CIBL), that permits multiple instances of a preference predicate to be composed, directly exploiting the transitivity of preference predicates. In an empirical evaluation, CIBL was consistently more accurate than a I-NN instance-based learning strategy unable to compose instances. The relative performance of CIBL and decision tree induction was found to depend upon (1) the complexity of the preference predicate being acquired and (2) the dimensionality of the feature space.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-099.pdf,
100,1994,Induction,Learning to Recognize Promoter Sequences in E. coli by Modeling Uncertainty in the Training Data,Steven W. Norton,"Automatic recognition of promoter sequences is an important open problem in molecular biology. Unfortunately, the usual machine learning version of this problem is critically flawed. In particular, the dataset available from the Irvine repository was drawn from a compilation of promoter sequences that were preprocessed to conform to the biologists’ related notion of the consensus sequence, a first-order approximation with a number of shortcomings that are well-known in molecular biology. Although concept descriptions learned from the Irvine data may represent the consensus sequence, they do not represent promoters. More generally, imperfections in preprocessed data and statistical variations in the locations of biologically meaningful features within the raw data invalidate standard attribute-based approaches. I suggest a dataset, a concept-description language, and a model of uncertainty in the promoter data that are all biologically justified, then address the learning problem with incremental probabilistic evidence combination. This knowledge-based approach yields a more accurate and more credible solution than other more conventional machine learning systems.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-100.pdf,
101,1994,Induction,Inductive Learning For Abductive Diagnosis,"Cynthia A. Thompson, Raymond J. Mooney","A new inductive learning system, LAB (Learning for ABduction), is presented which acquires abductive rules from a set of training examples. The goal is to find a small knowledge base which, when used abductively, diagnoses the training examples correctly and generalizes well to unseen examples. This contrasts with past systems that inductively learn rules that are used deductively. Each training example is associated with potentially multiple categories (disorders), instead of one as with typical learning systems. LAB uses a simple hill-climbing algorithm to efficiently build a rule base for a set-covering abductive system. LAB has been experimentally evaluated and compared to other learning systems and an expert knowledge base in the domain of diagnosing brain damage due to stroke.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-101.pdf,
102,1994,Induction,Learning Fault-Tolerant Speech Parsing with SCREEN,"Stefan Wermter, Volker Weber","This paper describes a new approach and a system SCREEN' for fault-tolerant speech parsing. Speech parsing describes the syntactic and semantic analysis of spontaneous spoken language. The general approach is based on incremental immediate flat analysis, learning of syntactic and semantic speech parsing, parallel integration of current hypotheses, and the consideration of various forms of speech related errors. The goal for this approach is to explore the parallel interactions between various knowledge sources for learning incremental fault-tolerant speech parsing. This approach is examined in a system SCREEN using various hybrid connectionist techniques. Hybrid connection techniques are examined because of their promising properties of inherent fault tolerance, learning, gradedness and parallel constraint integration. The input for SCREEN is hypotheses about recognized words of a spoken utterance potentially analyzed by a speech system, the output is hypotheses about the flat syntactic and semantic analysis of the utterance. In this paper we focus on the general approach, the overall architecture, and examples for learning flat syntactic speech parsing. Different from most other speech language architectures SCREEN emphasizes an interactive rather than an autonomous position, learning rather than encoding, flat analysis rather than in-depth analysis, and fault-tolerant processing of phonetic, syntactic and semantic knowledge.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-102.pdf,
103,1994,PAC Learning,Pac-Learning Nondeterminate Clauses,William W. Cohen,"Several practical inductive logic programming systems efficiently learn ""determinate"" clauses of constant depth. Recently it has been shown that while nonrecursive constant-depth determinate clauses are pat-learnable, most of the obvious syntactic generalizations of this language are not pat-learnable. In this paper we introduce a new restriction on logic programs called ""locality"", and present two formal results. First, the language of nonrecursive clauses of constant locality is pac-learnable. Second, the language of nonrecursive clauses of constant locality is strictly more expressive than the language of nonrecursive determinate clauses of constant depth. Hence, constant-locality clauses are a pat-learnable generalization of constant-depth determinate clauses.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-103.pdf,
104,1994,PAC Learning,Learning to Reason,"Roni Khardon, Dan Roth","We introduce a new framework for the study of reasoning. The Learning (in order) to Reason approach developed here combines the interfaces to the world used by known learning models with the reasoning task and a performance criterion suitable for it. We show how previous results from learning theory and reasoning fit into this framework and illustrate the usefulness of the Learning to Reason approach by exhibiting new results that are not possible in the traditional setting. First, we give a Learning to Reason algorithm for a class of propositional languages for which there are no efficient reasoning algorithms, when represented as a traditional (formula-based) knowledge base. Second, we exhibit a Learning to Reason algorithm for a class of propositional languages that is not known to be learnable in the traditional sense.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-104.pdf,
105,1994,Reinforcement Learning,Catching a Baseball: A Reinforcement Learning Perspective Using a Neural Network,"Rajarshi Das, Sreerupa Das","Moments after a baseball batter has hit a fly ball, an outfielder has to decide whether to run forward or backward to catch the ball. Judging a fly ball is a difficult task, especially when the fielder is in the plane of the ball' s trajectory. There exists several alternative hypotheses in the literature which identify different perceptual features available to the fielder that may provide useful cues as to the location of the ball’s landing point. A recent study in experimental psychology suggests that to intercept the ball, the fielder has to run such that the double derivative of tanf with respect to time is close to zero, where f is the elevation angle of the ball from the fielder’s perspective (McLeod and Dlenes 1993). We investigate whether d2 (tanf)/dt2 information is a useful cue to learn this task in the Adaptive Heuristic Critic (AHC) reinforcement learning framework. Our results provide supporting evidence that d2(tanf)/dt2 information furnishes strong initial cue in determining the landing point of the ball and plays a key role in the learning process. However our simulations show that during later stages of the ball’s flight, yet another perceptual feature, the perpendicular velocity of the ball (vp) with respect to the fielder, provides stronger cues as to the location of the landing point. The trained network generalized to novel circumstances and also exhibited some of the behaviors recorded by experimental psychologists on human data. We believe that much can be gained by using reinforcement learning approaches to learn common physical tasks, and similarly motivated work could stimulate useful interdisciplinary research on the subject.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-105.pdf,
106,1994,Reinforcement Learning,Incorporating Advice into Agents that Learn from Reinforcements,"Richard Maclin, Jude W. Shavlik","Learning from reinforcements is a promising approach for creating intelligent agents. However, reinforcement learning usually requires a large number of training episodes. We present an approach that addresses this shortcoming by allowing a connectionist Q-learner to accept advice given, at any time and in a natural manner, by an external observer. In our approach, the advice-giver watches the learner and occasionally makes suggestions, expressed as instructions in a simple programming language. Based on techniques from knowledge-based neural networks, these programs are inserted directly into the agent’s utility function. Subsequent reinforcement learning further integrates and refines the advice. We present empirical evidence that shows our approach leads to statistically-significant gains in expected reward. Importantly, the advice improves the expected reward regardless of the stage of training at which it is given.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-106.pdf,
107,1994,Reinforcement Learning,Reinforcement Learning Algorithms for Average-Payoff Markovian Decision Processes,Satinder P. Singh,"Reinforcement learning (RL) has become a central paradigm for solving learning-control problems in robotics and artificial intelligence. R L researchers have focussed almost exclusively on problems where the controller has to maximize the discounted sum of payoffs. However, as emphasized by Schwartz (1993), in many problems, e.g., those for which the optimal behavior is a limit cycle, it is more natural and computationally advantageous to formulate tasks so that the controller’s objective is to maximize the average payoff received per time step. In this paper I derive new average-payoff RL algorithms as stochastic approximation methods for solving the system of equations associated with the policy evaluation and optimal control questions in average-payoff RL tasks. These algorithms are analogous to the popular td and Q-learning algorithms already developed for the discounted-payoff case. One of the algorithms clerived here is a significant variation of Schwartz’s R-learning algorithm. Preliminary empirical results are presented to validate these new algorithms.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-107.pdf,
108,1994,Meta AI,Using Knowledge Acquisition and Representation Tools to Support Scientific Communities,"Brian R. Gaines, Mildred L. G. Shaw","Widespread access to the Internet has led to the formation of geographically dispersed scientific communities collaborating through the network. The tools supporting such collaboration currently are based primarily on electronic mail through mailing list servers, and access to archives of research reports through ftp, gopher and world wide web. However, electronic communication can support the knowledge processes of scientific communities more directly through overtly represented knowledge structures. This paper describes some experiments in the use of knowledge acquisition (ISA) and representation (KR) tools to define and analyze major policy and technical issues in an international research community responsible for one of the test cases in the Intelligent Manufacturing Systems (IMS) research program. It is concluded that distributed knowledge support systems in routine use by world-class scientific communities collaborating through the Internet will provide a major impetus to artificial intelligence research.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-108.pdf,
109,1994,Meta AI,Talking About AI: Socially Defined Linguistic Subcontexts in AI,"Amy M. Steier, Richard K. Belew","This paper describes experiments documenting significant variations in word usage patterns within social subgroups of AI researchers. As some phrases have very different collocational patterns than their constituent words, we look beyond occurrences of individual words, to consider word phrases. The mutual information statistic is used to measure the information content of phrases beyond that of their constituent words. Previous research has shown that some phrases are much more informative as word pairs outside topicadly defined subsets of a document corpus than within it. In this paper we show that individual universities provide an analogous, socicslly defined context in which locally-used phrases are ""exported"" into general AI vocabulary.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-109.pdf,
110,1994,Corpus-Based Natural Language Processing,Some Advances in Transformation-Based Part of Speech Tagging,Eric Brill,"Most recent research in trainable part of speech taggers has explored stochastic tagging. While these taggers obtain high accuracy, linguistic information is captured indirectly, typically in tens of thousands of lexical and contextual probabilities. In (Brill 1992), a trainable rule-based tagger was described that obtained performance comparable to that of stochastic taggers, but captured relevant linguistic information in a small number of simple non-stochastic rules. In this paper, we describe a number of extensions to this rule-based tagger. First, we describe a method for expressing lexical relations in tagging that stochastic taggers are currently unable to express. Next, we show a rule-based approach to tagging unknown words. Finally, we show how the tagger can be extended into a k-best tagger, where multiple tags can be assigned to words in some cases of uncertainty.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-110.pdf,
111,1994,Corpus-Based Natural Language Processing,Context-Sensitive Statistics for Improved Grammatical Language Models,"Eugene Charniak, Glenn Carroll","We develop a language model using probabilistic context-free grammars (PCFGs) that is ""pseudo context-sensitive"" in that the probability that a non-terminal N expands using a rule T depends on N’s parent. We give the equations for estimating the necessary probabilities using a variant of the inside-outside algorithm. We give experimental results showing that, beginning with a high-performance PCFG, one can develop a pseudo PCSG that yields significant performance gains. Analysis shows that the benefits from the context-sensitive statistics are localized, suggesting that we can use them to extend the original PCFG. Experimental results confirm that this is both feasible and the resulting grammar retains the performance gains. This implies that our scheme may be useful as a novel method for PCFG induction.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-111.pdf,
112,1994,Corpus-Based Natural Language Processing,Toward the Essential Nature of Statistical Knowledge in Sense Resolution,Jill Fain Lehman,"The statistical basis for sense resolution decisions is arrived at by the application of a process to a corpus of instances. In general, once the process has been applied to the corpus, the system contains both some residual representation of the instances and some explicit augmentation of that representation with information that was implicit in the corpus. For example, part of the residual representation of He feels happy on Fridays might be the (word sense) pair (happy feel-as-emotion), and part of the augmentation might be the probability of happy co-occurring with the sense of feel as an emotion. We show that for the simple residual representation of (word sense) pairs, the existence of such a representation in and of itself captures much of the regularity inherent in the data. We also demonstrate that augmenting the residual representation with the actual number of times each pair occurs in the training corpus provides most of the remainder of the power of probabalistic approaches. Finally, we show how viewing this residual representation as a form of episodic memory can enable symbolic, knowledge-rich systems to take advantage of this source of regularity in performing sense resolution.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-112.pdf,
113,1994,Corpus-Based Natural Language Processing,A Probabilistic Algorithm for Segmenting Non-Kanji Japanese Strings,"Virginia Teller, Eleanor Olds Batchelder","We present an algorithm for segmenting unrestricted Japanese text that is able to detect up to 98% of the words in a corpus. The segmentation technique, which is simple and extremely fast, does not depend on a lexicon or any formal notion of what a word is in Japanese, and the training procedure does not require annotated text of any kind. Relying almost exclusively on character type information and a table of hiragana bigram frequencies, the algorithm makes a decision as to whether to create word boundaries or not. This method divides strings of Japanese characters into units that are computationally tractable and that can be justified on lexical and syntactic grounds as well.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-113.pdf,
114,1994,Corpus-Based Natural Language Processing,Inducing Deterministic Prolog Parsers from Treebanks: A Machine Learning Approach,"John M. Zelle, Raymond J. Mooney",This paper presents a method for constructing deterministic Prolog parsers from corpora of parsed sentences. Our approach uses recent machine learning methods for inducing Prolog rules from examples (inductive logic programming). We discuss several advantages of this method compared to recent statistical methods and present results on learning complete parsers from portions of the ATIS corpus.,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-114.pdf,
115,1994,Lexical Acquisition,The Ups and Downs of Lexical Acquisition,"Peter M. Hastings, Steven L. Lytinen","We have implemented an incremental lexical acquisition mechanism that learns the meanings of previously unknown words from the context in which they appear, as a part of the process of parsing and semantically interpreting sentences. Implement at ion of this algorithm brought to light a fundamental difference between learning verbs and learning nouns. Specifically, because verbs typically play the predicate role in English sentences, whereas nouns typically function as arguments, we found that different mechanisms were required to learn verbs and nouns. Because of this difference in usage, our learning algorithm formulates the most specific hypotheses possible, consistent with the data, for verb meanings, but the most general hypotheses possible for nouns. Subsequent examples may falsify a current hypothesis, causing verb meanings to be generalized and noun meanings to be made more specific. This paper describes the two approaches used to learn verbs and nouns in the system, and reports on the system’s performance in substantial empirical testing.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-115.pdf,
116,1994,Lexical Acquisition,Lexical Acquisition in the Presence of Noise and Homonymy,Jeffrey Mark Siskind,"This paper conjectures a computational account of how children might learn the meanings of words in their native language. First, a simplified version of the lexical acquisition task faced by children is modeled by a precisely specified formal problem. Then, an implemented algorithm for solving this formal problem is presented. Key advances of this algorithm over previously proposed algorithms are its ability to learn homonymous word senses in the presence of noisy input and its ability to scale up to problems of the size faced by real children.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-116.pdf,
117,1994,Natural Language Applications,Kalos -- A System for Natural Language Generation with Revision,"Ben E. Cline, J. Terry Nutter","Using revision to produce extended natural language text through a series of drafts provides three significant advantages over a traditional natural language generation system. First, it reduces complexity through task decomposition. Second, it promotes text polishing techniques that benefit from the ability to examine generated text in the context of the underlying knowledge from which it was generated. Third, it provides a mechanism for the interaction of conceptual and stylistic decisions. Kalos is a natural language generation system that produces advanced draft quality text for a microprocessor users’ guide from a knowledge base describing the microprocessor. It uses revision iteratively to polish its initial generation. The system performs both conceptual and stylistic revisions. Example output of the system, showing both types of revision, is presented and discussed.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-117.pdf,
118,1994,Natural Language Applications,Building a Large-Scale Knowledge Base for Machine Translation,"Kevin Knight, Steve K. Luk","Knowledge-based machine translation (KBMT) systems have achieved excellent results in constrained domains, but have not yet scaled up to newspaper text. The reason is that knowledge resources (lexicons, grammar rules, world models) must be painstakingly handcrafted from scratch. One of the hypotheses being tested in the PANGLOSS machine translation project is whether or not these resources can be semi-automatically acquired on a very large scale. This paper focuses on the construction of a large ontology (or knowledge base, or world model) for supporting KBMT. It contains representations for some 70,000 commonly encountered objects, processes, qualities, and relations. The ontology was constructed by merging various online dictionaries, semantic networks, and bilingual resources, through semi-automatic methods. Some of these methods (e.g., conceptual matching of semantic taxonomies) are broadly applicable to problems of importing/exporting knowledge from one KB to another. Other methods (e.g., bilingual matching) allow a knowledge engineer to build up an index to a KB in a second language, such as Spanish or Japanese.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-118.pdf,
119,1994,Natural Language Applications,Automated Postediting of Documents,"Kevin Knight, Ishwar Chander","Large amounts of low- to medium-quality English texts are now being produced by machine translation (MT) systems, optical character readers (OCR), and non-native speakers of English. Most of this text must be postedited by hand before it sees the light of day. Improving text quality is tedious work, but its automation has not received much research attention. Anyone who has postedited a technical report or thesis written by a non-native speaker of English knows the potential of an automated postediting system. For the case of MT-generated text, we argue for the construction of postediting modules that are portable across MT systems, as an alternative to hardcoding improvements inside any one system. As an example, we have built a complete self-contained postediting module for the task of article selection (a, an, the) for English noun phrases. This is a notoriously difficult problem for Japanese-English MT. Our system contains over 200,000 rules derived automatically from online text resources. We report on learning algorithms, accuracy, and comparisons with human performance.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-119.pdf,
120,1994,Natural Language Applications,A Prototype Reading Coach that Listens,"Jack Mostow, Steven F. Roth, Alexander G. Hauptmann, Matthew Kane","We report progress on a new approach to combatting illiteracy -- getting computers to listen to children read aloud. We describe a fully automated prototype coach for oral reading. It displays a story on the screen, listens as a child reads it, and decides whether and how to intervene. We report on pilot experiments with low-reading second graders to test whether these interventions are technically feasible to automate and pedagogically effective to perform. By adapting a continuous speech recognizer, we detected 49% of the misread words, with a false alarm rate under 4%. By incorporating the interventions in a simulated coach, we enabled the children to read and comprehend material at a reading level 0.6 years higher than what they could read on their own. We show how the prototype uses the recognizer to trigger these interventions automatically.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-120.pdf,
121,1994,Natural Language Applications,Visual Semantics: Extracting Visual Information from Text Accompanying Pictures,"Rohini K. Srihari, Debra T. Burhans","This research explores the interaction of textual and photographic information in document understanding. The problem of performing general-purpose vision without a priori knowledge is difficult at best. The use of collateral information in scene understanding has been explored in computer vision systems that use scene context in the task of object identification. The work described here extends this notion by defining visual semantics, a theory of systematically extracting picture-specific information from text accompanying a photograph. Specifically, this paper discusses the multi-stage processing of textual captions with the following objectives: (i) predicting which objects (implicitly or explicitly mentioned in the caption) are present in the picture and (ii) generating constraints useful in locating/identifying these objects. The implementation and use of a lexicon specifically designed for the integration of linguistic and visual information is discussed. Finally, the research described here has been successfully incorporated into PICTION, a caption-based face identification system.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-121.pdf,
122,1994,Natural Language Discourse,A Plan-Based Model for Response Generation in Collaborative Task-Oriented Dialogues,"Jennifer Chu-Carroll, Sandra Carberry","This paper presents a plan-based architecture for response generation in collaborative consultation dialogues, with emphasis on cases in which the system (consultant) and user (executing agent) disagree. Our work contributes to an overall system for collaborative problem-solving by providing a plan-based framework that captures the Propose-Evaluate-Modify cycle of collaboration, and by allowing the system to initiate subdialogues to negotiate proposed additions to the shared plan and to provide support for its claims. In addition, our system handles in a unified manner the negotiation of proposed domain actions, proposed problem-solving actions, and beliefs proposed by discourse actions. Furthermore, it captures cooperative responses within the collaborative framework and accounts for why questions are sometimes never answered.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-122.pdf,
123,1994,Natural Language Discourse,Classifying Cue Phrases in Text and Speech Using Machine Learning,Diane J. Litman,"Cue phrases may be used in a dkozsrse sense to explicitly signal discourse structure, but also in a sentential sense to convey semantic rather than structural information. This paper explores the use of machine learning for classifying cue phrases as discourse or sentential. Two machine learning programs (CGRENDEL and C4.5) are used to induce classification rules from sets of pre-classified cue phrases and their features. Machine learning is shown to be an effective technique for not only rautomating the generation of classification rules, but also for improving upon previous results.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-123.pdf,
124,1994,Natural Language Discourse,An Artificial Discourse Language for Collaborative Negotiation,Candace L. Sidner,"Collaborations to accomplish common goals necessitate negotiation to share and reach agreement on the beliefs that agents hold as part of the collaboration. Negotiation in communication can be simulated by a series of exchanges in which agents propose, reject, counterpropose or seek supporting information for beliefs they wish to be held mutually. In an artificial language of negotiation, messages display the state of the agents’ beliefs. Dialogues consisting of such messages clarify the means by which agents come to agree or fail to agree on mutual beliefs and individual intentions.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-124.pdf,
125,1994,Natural Language Discourse,Emergent Linguistic Rules from Inducing Decision Trees: Disambiguating Discourse Clue Words,"Eric V. Siegel, Kathleen R. McKeown",We apply decision tree induction to the problem of discourse clue word sense disambiguation. The automatic partitioning of the training set which is intrinsic to decision tree induction gives rise to linguistically viable rules.,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-125.pdf,
126,1994,Natural Language Discourse,Corpus-Driven Knowledge Acquisition for Discourse Analysis,"Stephen Soderland, Wendy Lehnert","The availability of large on-line text corpora provides a natural and promising bridge between the worlds of natural language processing (NLP) and machine learning (ML). In recent years, the NLP community has been aggressively investigating statistical techniques to drive part-of-speech taggers, but application-specific text corpora can be used to drive knowledge acquisition at much higher levels as well. In this paper we will show how ML techniques can be used to support knowledge acquisition for information extraction systems. It is often very difficult to specify an explicit domain model for many information extraction applications, and it is always labor intensive to implement hand-coded heuristics for each new domain. We have discovered that it is nevertheless possible to use ML algorithms in order to capture knowledge that is only implicitly present in a representative text corpus. Our work addresses issues traditionally associated with discourse analysis and intersentential inference generation, and demonstrates the utility of ML algorithms at this higher level of language analysis. The benefits of our work address the portability and scalability of information extraction (IE) technologies. When hand-coded heuristics are used to manage discourse analysis in an information extraction system, months of programming effort are easily needed to port a successful IE system to a new domain. We will show how ML algorithms can reduce this development time to a few days of automated corpus analysis without any resulting degradation of overall system performance.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-126.pdf,
127,1994,Syntax,Principled Multilingual Grammars for Large Corpora,"Sharon Flank, Paul Krause, Carol Van Ess-Dykema","In-depth text understanding for large-scale applications requires a broad-coverage, robust grammar. We describe a multilingual implementation of such a grammar, and its advantages over both principle-based parsing and ad-hoc grammar design. We show how X-bar theory and language-independent semantic constraints facilitate grammar development. Our implementation includes innovative handling of (1) syntactic gaps, (2) logical structure alternations, and (3) conjunctions. Each of these innovations enhances performance in both large-scale and multilingual natural language processing applications. Phrase structure grammars are hardly new. The novelty in this paper comes from the use of practical guidelines and real numbers based on our experience with three languages and tens of thousands of texts. The issue of grammar design is worth revisiting because of the increasing bifurcation between semantic phrase grammars on thv one hand, and principle-based parsing in toy domains on the other. Semantic grammars are brittle and must be rewritten for each new domain and language; principle-based parsing is not yet mature enough for our applications. We offer an extensible, multilingual application of the traditional approach that extends theoretical linguistic insights to industrial strength data.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-127.pdf,
128,1994,Syntax,L* Parsing: A General Framework for Syntactic Analysis of Natural Language,"Eric K. Jones, Linton M. Miller","We describe a new algorithm for table-driven parsing with context-free grammars designed to support efficient syntactic analysis of natural language. The algorithm provides a general framework in which a variety of parser control strategies can be freely specified: bottom-up strategies, top-down strategies, and strategies that strike a balance between the two. The framework permits better sharing of parse forest substructure than other table-driven approaches, and facilitates the early termination of semantically ill-formed partial parses. The algorithm should thus find ready application to large-scale natural language processing.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-128.pdf,
129,1994,Neural Networks,"Unclear Distinctions Lead to Unnecessary Shortcomings: Examining the Rule Versus Fact, Role versus Filler, and Type Versus Predicate Distinctions from a Connectionist Representation and Reasoning Perspective",Venkat Ajjanagadde,"This paper deals with three distinctions pertaining to knowledge representation, namely, the rules vs facts distinction, roles vs fillers distinction, and predicates vs types distinction. Though these distinctions may indeed have some intuitive appeal, the exact natures of these distinctions are not entirely clear. This paper discusses some of the problems that arise when one accords these distinctions a prominent status in a connectionist system by choosing the representational structures so as to reflect these distinctions. The example we will look at in this paper is the connectionist reasoning system developed by Ajjanagadde and Shastri. Their system performs an interesting class of inferences using activation synchrony to represent dynamic bindings. The rule/fact, role/filler, type/predicate distinctions figure predominantly in the way knowledge is encoded in their system. We will discuss some significant shortcomings this leads to. Then, we will propose a much more uniform scheme for representing knowledge. The resulting system enjoys some significant advantages over Ajjanagadde and Shastri’s system, while retaining the idea of using synchrony to represent bindings.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-129.pdf,
130,1994,Neural Networks,Associative Memory in an Immune-Based System,"C.J. Gibert, T.W. Routen","The immune system offers to be a rich source of metaphors to guide the exploration of the notion of an adaptive system. We might define a class of systems which are inspired by, but diverge from, descriptions of the immune system, and refer to them as immune-bused systems. The research reported here is motivated by a desire to explore the possibilities of such systems. Specifically, we attempt to construct an associative memory using immune system modelling as a starting point.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-130.pdf,
131,1994,Neural Networks,Parsing Embedded Clauses with Distributed Neural Networks,"Risto Miikkulainen, Dennis Bijwaard","A distributed neural network model called SPEC for processing sentences with recursive relative clauses is described. The model is based on separating the tasks of segmenting the input word sequence into clauses, forming the case-role representations, and keeping track of the recursive embeddings into different modules. The system needs to be trained only with the basic sentence constructs, and it generalizes not only to new instances of familiar relative clause structures, but to novel structures as well. SPEC exhibits plausible memory degradation as the depth of the center embeddings increases, its memory is primed by earlier constituents, and its performance is aided by semantic constraints between the constituents. The ability to process structure is largely due to a central executive network that monitors and controls the execution of the entire system. This way, in contrast to earlier subsymbolic systems, parsing is modeled as a controlled high-level process rather than one based on automatic reflex responses.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-131.pdf,
132,1994,Neural Networks,Spurious Symptom Reduction in Fault Monitoring Using a Neural Network and Knowledge Base Hybrid System,"Roger M. Records, Jai J. Choi","An approach to reduce number of spurious symptoms in aircraft engine fault monitoring is investigated. Two strategies were utilized. A set of rules designed to filter spurious symptoms was created. Then a neural network was designed to generate expectation value for each of the sensors monitored. The neural net was trained for a specific engine during normal operation. After capturing patterns for normal engine behavior in the neural net, an expectation value for the sensor is predicted. The success of this approach relies on generating better expectation values which in turn produce smaller variation from actual operating behavior and hence generate fewer spurious symptoms. Resulting hybrid system of neural networks and rule-based model demonstrates a drastic reduction of overall spurious symptoms.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-132.pdf,
133,1994,Neural Networks,Learning to Learn: Automatic Adaptation of Learning Bias,Steve G. Romaniuk,"Traditionally, large areas of research in machine learning have concentrated on pattern recognition and its application to many diversified problems both within the realm of AI as well as outside of it. Over several decades of intensified research, an array of learning methodologies have been proposed, accompanied by attempts to evaluate these methods, with respect to one another on small sets of real world problems. Unfortunately, little emphasis was placed on the problem of learning bias - common to all learning algorithms - and a major culprit in preventing the construction of a zsniuerscsl pattern recognizer. State of the art learning algorithms exploit some inherent bias when performing pattern recognition on yet unseen patterns. Automatically adapting this learning bias - dependent on the type of pattern classification problems seen over time - is largely lacking. In this paper, weaknesses of the traditional one-shot learning environments are pointed out and the move towards a learning method displaying the ability to learn about lecarning is undertaken. Trans-dimensional learning is introduced as a means to automatically adjust learning bias and empirical evidence is provided showing that in some instances learning the whole can be simpler than learning a part of it.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-133.pdf,
134,1994,Neural Networks,Neural Programming Language,Hava T. Siegelmann,"Analog recurrent neural networks have attracted much attention lately as powerful tools of automatic learning. We formally define a high level language, called NEural Langage, which is rich enough to express any computer algorithm or rule-based system. We show how to compile a NEL program to a network which computes exactly as the original program and requires the same computation time. We suggest this language along with its compiler as the ultimate bridge from symbolic to analog computation, and propose its outcome as an initial network for learning.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-134.pdf,
135,1994,Neural Networks,Multi-Recurrent Networks for Traffic Forecasting,Claudia Ulbricht,"Recurrent neural networks solving the task of short-term traffic forecasting are presented in this report. They turned out to be very weII suited to this task, they even outperformed the best results obtained with conventional statistical methods. The outcome of a comparative study shows that multiple combinations of feedback can greatly enhance the network performance. Best results were obtained with the newly developed Multi-recurrent Network combining output, hidden, and input layer memories having self-recurrent feedback loops of different strengths. The outcome of this research wiIl be used for installing an actual tool at a highway check point. The investigated methods provide short-term memories of different length which are not only needed for the given application, but which are of importance for numerous other real world tasks.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-135.pdf,
136,1994,Neural Networks,Knowledge Matrix -- An Explanation and Knowledge Refinement Facility for a Rule Induced Neural Network,"Daniel S. Yeung, Hank-shun Fong","One of the major shortcomings of neural network as a problem solving tool lies in its opaque nature of knowledge representation and manipulation. For instance, the way that a learning algorithm modifies the connection weights of a network cannot be easily understood in the context of the application domain knowledge. Thus, the applications of neural networks is limited in areas where user’s understanding of the situation is critical. This paper introduces a facility called knowledge matrix for a rule induced Neocognitron network. It represents the correlation between the knowledge stored internally in the network and the symbolic knowledge used in the application domain. Another facility called response matrix is developed to represent the network’s response to an input. These two facilities are then employed cooperatively to generate symbolic interpretations of the network’s response. Based on the interpretations, queries can be made against the networks responses and explanations can be provided by the system. Two detailed examples are discussed. It can be shown that the network knowledge can be refined evolutionarily without degrading its comprehensibility. An algorithm has also been formulated to adapt the system with respect to one type of recognition error.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-136.pdf,
137,1994,Neural Networks,Epsilon-Transformation: Exploiting Phase Transitions to Solve Combinatorial Optimization Problems Initial Results,"Weixiong Zhang, Joseph C. Pemberton","It has been shown that there exists a transition in the average-case complexity of searching a random tree, from exponential to polynomial in the search depth. We develop a state-space transformation method, called e-transformation, that makes use of this complexity transition to find a suboptimal solution. The expected number of random tree nodes expanded by branch-and-bound (BnB) using e-transformation is cubic in the search depth, and the relative error of the solution cost compared to the optimal solution cost is bounded by a small constant. We also present an iterative version of e-transformation that can be used to find both optimal and suboptimal solutions. Depth-first BnB (DFBnB) using iterative e-transformation significantly improves upon truncated DFBnB on random trees with large branching factors and deep goal nodes, finding better solutions sooner on average. On the asymmetric traveling salesman problem, DFBnB using e-transformation outperforms a well-known local search method, and DFBnB using iterative e-transformation is superior to truncated DFBnB.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-137.pdf,
138,1994,Belief Revision,A Preference-Based Approach to Default Reasoning: Preliminary Report,James P. Delgrande,"An approach to nonmonotonic inference, based on preference orderings between possible worlds or states of affairs, is presented. We begin with an extant weak theory of default conditionals; using this theory, orderings on worlds are derived. The idea is that if a conditional such as ""birds fly"" is true then, all other things being equal, worlds in which birds fly are preferred over those where they don’t. In this case, a red bird would fly by virtue of red-bird-worlds being among the least exceptional worlds in which birds fly. In this approach, irrelevant properties are correctly handled, as is specificity, reasoning within exceptional circumstances, and inheritance reasoning. A sound proof-theoretic characterisation is also given. Lastly, the approach is shown to subsume that of conditional entailment.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-138.pdf,
139,1994,Belief Revision,On the Relation between the Coherence and Foundations Theories of Belief Revision,Alvaro del Val,"Two recent, papers, (Ggrdenfors 1990; Doyle 1992), try to assess the relative merits of the two main approaches to belief revision, the foundations and coherence theories, but leave open the question of the mathematical connections between them. We answer this question by showing that the foundations and coherence theories of belief revision are mathematically equivalent. The result also has consequences for nonmonotonic reasoning, as it, entails that Poole’s system of default, reasoning and Shoham’s preferential logic are expressively equivalent, in that they can represent the same set of non monotonic consequence relations.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-139.pdf,
140,1994,Belief Revision,Conditional Logics of Belief Change,"Nir Friedman, Joseph Y. Halpern","The study of belief change has been an active area in philosophy and AI. In recent years two special cases of belief change, belief revision and belief update, have been studied in detail. Belief revision and update are clearly not the only possible notions of belief change. In this paper we investigate properties of a range of possible belief change operations. We start with an abstract notion of a belief change system and provide a logical language that describes belief change in such systems. We then consider several reasonable properties one can impose on such systems and characterize them axiomatically. We show that both belief revision and update fit into our classification. As a consequence, we get both a semantic and an axiomatic (proof-theoretic) characterization of belief revision and update (as well as some belief change operations that generalize them), in one natural framework.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-140.pdf,
141,1994,Belief Revision,Incremental Recompilation of Knowledge,"Goran Gogic, Christos H. Papadimitriou, Martha Sideri","Approximating a general formula from above and below by Horn formulas (its Horn envelope and Horn core, respectively) was proposed in [SIC] as a form of ""' knowledge compilation,"" supporting rapid approximate reasoning; on the negative side, this scheme is static in that it supports no updates, and has certain complexity drawbacks pointed out in [KPS]. On the other hand, the many frameworks and schemes proposed in the literature for theory update and revision are plagued by serious complexity-theoretic impediments, even in the Horn case, as was pointed out in [EG2] and the present paper. More fundamentally, these schemes are not inductive, in that they lose in a single update any positive properties of the represented sets of formulas (small size, Horn, etc.). In this paper we propose a new scheme, incremental recompilation, combining Horn approximation and model-based updates; this scheme is inductive and very efficient, free of the problems facing its constituents. A set of formulas is represented by an upper and lower Horn approximation. To update, we replace the upper Horn formula by the Horn envelope of its minimum-change update, and similarly the lower one by the Horn core of its update; the key fact is that Horn envelopes and cores are easy to compute when the underlying formula is the result of a minimum-change update of a Horn formula by a clause. We conjecture that eficient algorithms are possible for more complex updates.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-141.pdf,
142,1994,Belief Revision,Qualitative Decision Theory,"Sek-Wah Tan, Judea Pearl","We describe a framework for specifying conditional desires and evaluating preference queries ""would you prefer s1 over s2 given f"" under uncertainty. We refine the semantics presented in (Tan and Pearl 1994) to allow conditional desires to be overridden by more specific desires in the database. Within this framework, we also enable consideration of surprising worlds having extreme desirability values and the determination of degrees of preference.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-142.pdf,
143,1994,Nonmonotonic Reasoning,Soundness and Completeness of a Logic Programming Approach to Default Logic,"Grigoris Antoniou, Elmar Langetepe","We present a method of representing some classes of default theories as normal logic programs. The main point is that the standard semantics (i.e. SLDNF-resolution) computes answer substitutions that correspond exactly to the extensions of the represented default theory. We explain the steps of constructing a logic program LogProg(P,D) from a given default theory (P,D), and present the proof ideas of the soundness and completeness results for the approach.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-143.pdf,
144,1994,Nonmonotonic Reasoning,Reasoning about Priorities in Default Logic,Gerhard Brewka,"In this paper we argue that for realistic applications involving default reasoning it is necessary to reason about the priorities of defaults. Existing approaches require the knowledge engineer to explicitly state all relevant priorities which are then handled in an extralogical manner, or they are restricted to priorities based on specificity, neglecting other relevant criteria. We present an approach where priority information can be represented within the logical language. Our approach is based on PDL, a prioritized extension of Reiter’s Default Logic recently proposed by the same author. In PDL the generation of extensions is controlled by an ordering of the defaults. This property is used here in the following way: we first build Reiter extensions of a given default theory. These extensions contain explicit information about the priorities of defaults. We then eliminate every extension E that cannot be reconstructed as a PDL extension based on a default ordering that is compatible with the priority information in E. An example from legal reasoning illustrates the power of our approach.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-144.pdf,
145,1994,Nonmonotonic Reasoning,Is Intractability of Non-Monotonic Reasoning a Real Drawback?,"Marco Cadoli, Francesco M. Donini, Marco Schaerf","Several studies about complexity of NMR showed that inferring in non-monotonic knowledge bases is significantly harder than reasoning in monotonic ones. This contrasts with the general idea that NMR can be used to make knowledge representation and reasoning simpler, not harder. In this paper we show that, to some extent, NMR has fulfilled its goal. In particular we prove that circumscription allows for more compact and natural representation of knowledge. Results about intractability of circumscription can therefore be interpreted as the price one has to pay for having such an extra-compact representation. On the other hand, sometimes NMR really makes reasoning simpler; we give prototypical scenarios where closed-worId reasoning accounts for a faster and unsound approximation of classical reasoning.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-145.pdf,
146,1994,Nonmonotonic Reasoning,A Knowledge Representation Framework Based on Autoepistemic Logic of Minimal Beliefs,Teodor C. Przymusinski,"In recent years, various formalizations of non-monotonic reasoning and different semantics for normal and disjunctive logic programs have been proposed, including autoepisttic logic, circumscription, CWA, GCWA, ECWA, epistemic specfications, stable, well-founded, stationary and static semantics of normal and disjunctive logic programs. In this paper we introduce a simple non-monotonic knowledge representation framework which isomorphically contains all of the above mentioned non-monotonic formalisms and semantics as special cases and yet is significantly more expressive than each one of these formalisms considered individually. The new formalism, called the AutoEpidemic Logic of minimal Beliefs, AELB, is obtained by augmenting Moore’s autoepistemic logic, AEL, with an additional minimal belief operator, B, which allows us to explicitly talk about minimally entailed formulae. The existence of such a uniform framework not only results in a new powerful non-monotonic formalism but also allows us to compare and better understand mutual relationships existing between different non-monotonic formalisms and semantics and enables us to provide simpler and more natural definitions of some of them. It also naturally leads to new, even more expressive and flexible formalizations and semantics.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-146.pdf,
147,1994,Perception,A New Approach to Tracking 3D Objects in 2D Image Sequences,"Michael Chan, Dimitri Metaxas, Sven Dickinson","We present a new technique for tracking 3D objects from 2D image sequences through the integration of qualitative and quantitative techniques. The deformable models are initialized based on a previously developed part-based qualitative shape segmentation system. Using a physics-based quantitative approach, objects are subsequently tracked without feature correspondence based on generalized forces computed from the stereo images. The automatic prediction of possible edge occlusion and disocclusion is performed using an extended Kalman filter. To cope with possible occlusion caused by a previously undetected object, we monitor the magnitude and direction of the computed image forces exerted on the models. Abrupt changes to these forces trigger scene re-segmentation and model re-initialization through the qualitative shape segmentation system. Tracking is subsequently continued using only local image forces. We demonstrate our technique in experiments involving image sequences from complex motions of 3D objects.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-147.pdf,
148,1994,Perception,Automatic Symbolic Traffic Scene Analysis Using Belief Networks,"T. Huang, D. Koller, J. Malik, G. Ogasawara, B. Rao, S. Russell, J. Weber","Automatic symbolic traffic scene analysis is essential to many areas of IVHS (Intelligent Vehicle Highway Systems). Traffic scene information can be used to optimize traffic flow during busy periods, identify stalled vehicles and accidents, and aid the decision-making of an autonomous vehicle controller. Improvements in technologies for machine vision-based surveillance and high-level symbolic reasoning have enabled us to develop a system for detailed, reliable traffic scene analysis. The machine vision component of our system employs a contour tracker and an affine motion model based on Kalman filters to extract vehicle trajectories over a sequence of traffic scene images. The symbolic reasoning component uses a dynamic belief network to make inferences about traffic events such as vehicle lane changes and stalls. In this paper, we discuss the key tasks of the vision and reasoning components as well as their integration into a working prototype.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-148.pdf,
149,1994,Perception,Sensible Decisions: Toward a Theory of Decision-Theoretic Information Invariants,Keiji Kanazawa,"We propose a decision-theoretic notion of invariance in bounded rational decision making. We show how optimal decision making in sensory robotics can be approximately preserved under transformations of the decision rule. In particular, we present a decision theoretic analysis of the use of visual routines in action arbitration in real-time robot soccer. In this domain, stochastic dominance, and therefore decisions, can be sensed approximately from the environment, and we exploit this in our decision making.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-149.pdf,
150,1994,Perception,Topological Mapping for Mobile Robots Using a Combination of Sonar and Vision Sensing,"David Kortenkamp, Terry Weymouth","Topological maps represent the world as a network of nodes and arcs: the nodes are distinctive places in the environment and the arcs represent paths between places. A significant issue in building topological maps is defining distinctive places. Most previous work in topological mapping has concentrated on using sonar sensors to define distinctive places. However, sonar sensors are limited in range and angular resolution, which can make it difficult to distinguish between different distinctive places. Our approach combines a sonar-based definition of distinctive places with visual information. We use the robot' s sonar sensors to determine where to capture images and use cues extracted from those images to help perform place recognition. Information from these two sensing modalities is combined using a simple Bayesian network. Results described in this paper show that our robot is able to perform place recognition without having to move through a sequence of places, as is the case with most currently implemented systems.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-150.pdf,
151,1994,Perception,Applying VC-Dimension Analysis to 3D Object Recognition from Perspective Projections,"Michael Lindenbaum, Shai Ben-David","We analyze the amount of information needed to carry out model-based recognition tasks, in the context of a probabilistic data collection model, and independently of the recognition method employed. We consider the very rich class of semi-algebraic 3D objects, and derive an upper bound on the number of data features that (provably) suffice for localizing the object with some pre-specified precision. Our bound is based on analysing the combinatorial complexity of the hypotheses class that one has to choose from, and quantifying it using a VC-dimension parameter. Once this parameter is found, the bounds are obtained by drawing relations between recognition and learning, and using well-known results from computational learning theory. It turns out that this bounds grow logarithmically in the algebraic complexity of the objects.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-151.pdf,
152,1994,Causal-Link Planning,Derivation Replay for Partial-Order Planning,"Laurie H. Ihrig, Subbarao Kambhampati","Derivation replay was first proposed by Carbonell as a method of transferring guidance from a previous problem-solving episode to a new one. Subsequent implementations have used state-space planning as the underlying methodology. This paper is motivated by the acknowledged superiority of partial-order (PO) planners in plan generation, and is an attempt to bring derivation replay into the realm of partial-order planning. Here we develop DerSNLP, a framework for doing replay in SNLP, a partial-order plan-space planner, and analyze its relative effectiveness. We will argue that the decoupling of planning (derivation) order and the execution order of plan steps, provided by partial-order planners, enables DerSNLP to exploit the guidance of previous cases in a more efficient and straightforward fashion. We validate our hypothesis through empirical comparisons between DerSNLP and two replay systems based on state-space planners.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-152.pdf,
153,1994,Causal-Link Planning,Tractable Planning with State Variables by Exploiting Structural Restrictions,"Peter Jonsson, Peter Jonsson","So far, tractable planning problems reported in the literature have been defined by syntactical restrictions. To better exploit the inherent structure in problems, however, it is probably necessary to study also structural restrictions on the state-transition graph. Such restrictions are typically computationally hard to test, though, since this graph is of exponential size. Hence, we take an intermediate approach, using a state-variable model for planning and restricting the state-transition graph implicitly by restricting the transition graph for each state variable in isolation. We identify three such restrictions which are tractable to test and we present a planning algorithm which is correct and runs in polynomial time under these restrictions.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-153.pdf,
154,1994,Causal-Link Planning,Least-Cost Flaw Repair: A Plan Refinement Strategy for Partial-Order Planning,"David Joslin, Martha E. Pollack","We describe the least-cost flaw repair (LCFR) strategy for performing flaw selection during partial-order causal link (POCL) planning. LCFR can be seen as a generalization of Peot and Smith’s ""Delay Unforced Threats"" (DUnf) strategy (Peot and Smith 1993); where DUnf treats threats differently from open conditions, LCFR has a uniform mechanism for handling all flaws. We provide experimental results that demonstrate that the power of DUnf does not come from delaying threat repairs per se, but rather from the fact that this delay has the effect of imposing a partial preference for least-cost flaw selection. Our experiments also show that extending this to a complete preference for least-cost selection reduces search-space size even further. We consider the computational overhead of employing LCFR, and discuss techniques for reducing this overhead. In particular, we describe QLCFR, a strategy that reduces computational overhead by approximating repair costs.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-154.pdf,
155,1994,Causal-Link Planning,Temporal Planning with Continuous Change,"J. Scott Penberthy, Daniel S. Weld","We present ZENO, a least commitment planner that handles actions occurring over extended intervals of time. Deadline goals, metric preconditions, metric effects, and continuous change are supported. Simultaneous actions are alIowed when their effects do not interfere. Unlike most planners that deal with complex languages, the ZENO planning algorithm is sound and complete. The running code is a complete implementation of the formal algorithm, capable of solving simple problems (i.e., those involving less than a dozen steps).",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-155.pdf,
156,1994,Planning: Agents,Using Abstractions for Decision-Theoretic Planning with Time Constraints,"Craig Boutilier, Richard Dearden","Recently Markov decision processes and optimal control policies have been applied to the problem of decision-theoretic planning. However, the classical methods for generating optimal policies are highly intractable, requiring explicit enumeration of large state spaces. We explore a method for generating abstractions that allow approximately optimal policies to be constructed; computational gains are achieved through reduction of the state space. Abstractions are generated by identifying propositions that are ""relevant"" either through their direct impact on utility, or their influence on actions. This information is gleaned from the representation of utilities and actions. We prove bounds on the loss in value due to abstraction and describe some preliminary experimental results.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-156.pdf,
157,1994,Planning: Agents,Acting Optimally in Partially Observable Stochastic Domains,"Anthony R. Cassandra, Leslie Pack Kaelbling, Michael L. Littman","In this paper, we describe the partially observable Markov decision process (POMDP) approach to finding optimal or near-optimal control strategies for partially observable stochastic environments, given a complete model of the environment. The POMDP approach was originally developed in the operations research community and provides a formal basis for planning problems that have been of interest to the AI community. We found the existing algorithms for computing optimal control strategies to be highly computationally inefficient and have developed a new algorithm that is empirically more efficient. We sketch this algorithm and present preliminary results on several small problems that illustrate important properties of the POMDP approach.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-157.pdf,
158,1994,Planning: Agents,Cost-Effective Sensing during Plan Execution,Eric A. Hansen,"Between sensing the world after every action (as in a reactive plan) and not sensing at all (as in an open-loop plan), lies a continuum of strategies for sensing during plan execution. If sensing incurs a cost (in time or resources), the most cost-effective strategy is likely to fall somewhere between these two extremes. Yet most work on plan execution assumes one or the other. In this paper, an efficient, anytime planner is described that controls the rate of sensing during plan execution. The sensing interval is determined by the state during plan execution, as well as by the cost of sensing, so that an agent can sense more often when necessary. The planner is based on a generalization of stochastic dynamic programming.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-158.pdf,
159,1994,Planning: Agents,Using Abstraction and Nondeterminism to Plan Reaction Loops,David J. Musliner,"By looping over a set of behaviors, reactive systems use repetition and feedback to deal with errors and environmental uncertainty. Their robust, fault-tolerant performance makes reactive systems desirable for executing plans. However, most planning systems cannot reason about the loops that characterize reactive systems. In this paper, we show how the structured application of abstraction and nondeterminism can map complex planning problems requiring loop plans into a simpler representation amenable to standard planning technologies. In the process, we illustrate key recipes for automatically building predictable reactive systems that are guaranteed to achieve their goals.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-159.pdf,
160,1994,Planning: Agents,The First Law of Robotics (A Call to Arms),"Daniel Weld, Oren Etzioni","Even before the advent of Artificial Intelligence, science fiction writer Isaac Asimov recognized that an agent must place the protection of humans from harm at a higher priority than obeying human orders. Inspired by Asimov, we pose the following fundamental questions: (1) How should one formalize the rich, but informal, notion of ""harm""? (2) How can an agent avoid performing harmful actions, and do so in a computationally tractable manner? (3) How should an agent resolve conflict between its goals and the need to avoid harm? (4) When should an agent prevent a human from harming herself? While we address some of these questions in technical detail, the primary goal of this paper is to focus attention on Asimov’s concern: society will reject autonomous agents unless we have some credible means of making them safe!",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-160.pdf,
161,1994,Planning: Representation,Omnipotence without Omniscience: Efficient Sensor Management for Planning,"Keith Golden, Oren Etzioni, Daniel Weld","Classical planners have traditionally made the closed world assumption - facts absent from the planner' s world model are false. Incomplete-information planners make the open world assumption - the truth value of a fact absent from the planner’s model is unknown, and must be sensed. The open world assumption leads to two difficulties: (1) H ow can the planner determine the scope of a universally quantified goal? (2) When is a sensory action redundant, yielding information already known to the planner? This paper describes the fully-implemented XII planner, which solves both problems by representing and reasoning about local closed world information (LCW). We report on experiments utilizing our UNIX softbot (software robot) which demonstrate that LCW can substantially improve the softbot’s performance by eliminating redundant information gathering.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-161.pdf,
162,1994,Planning: Representation,On the Nature of Modal Truth Criteria in Planning,"Subbarao Kambhampati, Dana S. Nau","Chapman’s paper, ""Planning for Conjunctive Goals,"" has been widely acknowledged for its contribution toward understanding the nature of nonlinear (partial-order) planning, and it has been one of the bases of later work by others---but it is not free of problems. This paper addresses some problems involving modal truth and the Modal Truth Criterion (MTC). Our results are as follows: Even though modal duality is a fundamental axiom of classical modal logics, it does not hold for modal truth in Chapman’s plans; i.e., ""necessarily p"" is not equivalent to ""not possibly lp."" Although the MTC for necessary truth is correct, the MTC for possible truth is incorrect: it provides necessary but insufficient conditions for ensuring possible truth. Furthermore, even though necessary truth can be determined in polynomial time, possible truth is NP-hard. If we rewrite the MTC to talk about modal conditional truth (i.e., modal truth conditional on executability) rather than modal truth, then both the MTC for necessary conditional truth and the MTC for possible conditional truth are correct; and both can be computed in polynomial time.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-162.pdf,
163,1994,Planning: Representation,Causal Pathways of Rational Action,"Charles L. Ortiz, Jr.","A proper characterization of a rational agent’s actions involves much more than simply recounting the changes in the world affected by the agent. It should also include an explanatory account connecting the upshots of an agent’s actions with the reasons behind those actions, where those upshots might represent actual changes (either intentional or unintentional) or merely counterfactual possibilities. The conventional view of action makes it difficult to distinguish, inter alia, cases of attempts, accidents, coercions, or failures - such distinctions useful to agents engaged in recognizing or assigning responsibility for actions. Such a view also makes the characterization of actions that do not involve physical change, such as maintenance events, difficult, as well as the proper representation of negative actions; the latter commonly appearing in explanations and as objects of an agent' s intentions. In this paper, I present a formal analysis of these sorts of actions in terms of the causal pathways joining an agent’s intentions with his actions.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-163.pdf,
164,1994,Planning: Representation,Temporal Reasoning with Constraints on Fluents and Events,"Eddie Schwalb, Kalev Kask, Rina Dechter","We propose a propositional language for temporal reasoning that is computationally effective yet expressive enough to describe information about fluents, events and temporal constraints. Although the complete inference algorithm is exponential, we characterize a tractable core with limited expressibility and inferential power. Our results render a variety of constraint propagation techniques applicable for reasoning with constraints on Auents.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-164.pdf,
165,1994,Planning Under Uncertainty,An Algorithm for Probabilistic Least-Commitment Planning,"Nicholas Kushmerick, Steve Hanks, Daniel Weld","We define the probabilistic planning problem in terms of a probability distribution over initial world states, a boolean combination of goal propositions, a probability threshold, and actions whose effects depend on the execution-time state of the world and on random chance. Adopting a probabilistic model complicates the definition of plan success: instead of demanding a plan that proovably achieves the goal, we seek plans whose probability of success exceeds the threshold. This paper describes a probabilistic semantics for planning under uncertainty, and presents a fully implemented algorithm that generates plans that succeed with probability no less than a user-supplied probability threshold. The algorithm is sound (if it terminates then the generated plan is sufficiently likely to achieve the goal) and complete (the algorithm will generate a solution if one exists).",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-165.pdf,
166,1994,Planning Under Uncertainty,Control Strategies for a Stochastic Planner,"Jonathan Tash, Stuart Russell","We present new algorithms for local planning over Markov decision processes. The base-level algorithm possesses several interesting features for control of computation, based on selecting computations according to their expected benefit to decision quality. The algorithms are shown to expand the agent’s knowledge where the world warrants it, with appropriate responsiveness to time pressure and randomness. We then develop an introspective algorithm, using an internal representation of what computational work has already been done. This strategy extends the agent’s knowledge base where warranted by the agent’s world model and the agent’s knowledge of the work already put into various parts of this model. It also enables the agent to act so as to take advantage of the computational savings inherent in staying in known parts of the state space. The control flexibility provided by this strategy, by incorporating natural problem-solving methods, directs computational effort towards where it’s needed better than previous approaches, providing grcatcr hopes for scalability to large domains.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-166.pdf,
167,1994,Scheduling,Generating Feasible Schedules under Complex Metric Constraints,"Cheng-Chung Cheng, Stephen F. Smith","In this paper, we consider the problem of finding feasible solutions to scheduling problems that are complicated by separation constraints on the execution of different operations. Following recent work in constraint-posting scheduling, we formulate this problem as one of establishing ordering relations between pairs of operations requiring synchronization of resource usage. This establishes contact with the recently proposed General Temporal Constraint Network (GTCN) model. Exploiting properties of the general GTCN solution procedure, we are able to directly generalize a high performance solution procedure previously developed for a much more restricted class of scheduling problems. Specifically, shortest path information in the underlying temporal constraint network is first used to establish dominance conditions for early pruning of infeasible solutions. Heuristics are then defined for variable/value ordering in the meta-CSP space of possible resolutions of the disjunctive constraints on resource usage. Th ese heuristics are based on use of shortest path information as an estimation of decision flexibility. Experimental evaluation of the resulting heuristic procedure is carried out on a set of randomly generated problems drawn from a manufacturing scenario. Results indicate extremely effective problem solving performance in relation to both the original scheduling procedure that was adapted and the general GTCN solution procedure.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-167.pdf,
168,1994,Scheduling,Experimental Results on the Application of Satisfiability Algorithms to Scheduling Problems,"James M. Crawford, Andrew B. Baker","Considerable progress has been made in recent years in understanding and solving propositional satisfiabilit y problems. Much of this work has been based on experiments on randomly generated 3SAT problems . One generally accepted shortcoming of this work is that it is not clear how the results and algorithms developed will carry over to ""real"" constraint-satisfaction problems. This paper reports on a series of experiments applying satisfiability algorithms to scheduling problems. We have found that scheduling problems bear fairly little resemblance to the previously studied hard randomly generated 3SAT problems. In particular, scheduling problems tend to be quite large but under-constrained, Further, forward checking (e.g., unit propagation) seems to be much more important on these problems than on hard random 3SAT problems. We have also found that the domain-specific heuristics developed to solve scheduling problems make surprisingly little difference in the time required to solve the problems. We suggest that the best algorithms for this problem class will probably be hill-climbing algorithms that incorporate some sort of forward checking.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-168.pdf,
169,1994,Scheduling,Just-In-Case Scheduling,"Mark Drummond, John Bresina, Keith Swanson","This paper presents an algorithm, called Just-In-Case Schedulkg, for building robust schedules that tend not to break. The algorithm implements the common sense idea of being prepared for likely errors, just in case they should occur. The Just-In-Case algorithm analyzes a given nominal schedule, determines the most likely break, and reinvokes a scheduler to generate a contingent schedule to cover that break. After a number of iterations, the Just-In-Case algorithm produces a ""multiply contingent"" schedule that is more robust than the original nominal schedule. The algorithm has been developed for a real telescope scheduling domain in order to proactively manage schedule breaks that are due to an inherent uncertainty in observation durations. The paper presents empirical results showing that the algorithm performs extremely well on a representative problem from this domain.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-169.pdf,
170,1994,Scheduling,On the Utility of Bottleneck Reasoning for Scheduling,Nicola Muscettola,"The design of better schedulers requires a deeper understanding of each component technique and of their interactions. Although widely accepted in practice, bottleneck reasoning for scheduling has not yet been sufficiently validated, either formally or empirically. This paper reports an empirical analysis of the heuristic information used by bottleneck-centered, opportunistic scheduling systems to solve constraint satisfaction scheduling problems. Different configurations of a single scheduling framework are applied to a benchmark set of scheduling problems and compared with respect to number of problems solved and processing time. We show superior performances for schedulers that use bottleneck information. We also show that focusing at the bottleneck might not only provide an effective ""most constrained first"" heuristic but also, unexpectedly, increase the utility of other heuristic information.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-170.pdf,
171,1994,Scheduling,A Constraint-Based Approach to High-School Timetabling Problems: A Case Study,"Masazumi Yoshikawa, Kazuya Kaneko, Yuriko Nomura, Masanobu Watanabe","This paper describes a case study on a general-purpose Constraint Relaxation Problem solver, COASTOOL. Using COASTOOL,~ problem can be solved merely by declaring ""what is the problem,"" without programming ""how to solve it."" The problem is solved by a novel method that generates a high-quality initial assignment using arc-consistency, and refines it using hill-climbing. This approach has been evaluated successfully by experiments with practical high-school timetabling problems in Japan. Consequently, COASTOOL is shown to be efficient at applications in high-school timetabling problems.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-171.pdf,
172,1994,Task Network Planning,Task-Decomposition via Plan Parsing,"Anthony Barrett, Daniel S. Weld","Task-decomposition planners make use of schemata that define tasks in terms of partially ordered sets of tasks and primitive actions. Most existing task-decomposition planners synthesize plans via a top-down approach, called taslc reduction, which uses schemata to replace tasks with networks of tasks and actions until only actions remain. In this paper we present a bottom-up plan pursing approach to task-decomposition. Instead of reducing tasks into actions, we use an incremental parsing algorithm to recognize which partial primitive plans match the schemata. In essence, our approach exploits the observation that schemata are a convenient means for reducing search. We compile the schemata into a declarative search control language (like that used in machine learning research), which rejects plan refinements that, cannot be parsed. We demonstrate that neither parsing nor reduction dominates the other on efficiency grounds and provide preliminary empirical results comparing the two. We note that our parsing approach allows convenient, comparison (and combination) of different search control technologies, generates minimal plans, and handles expressive languages (e.g., universal quantification and conditional effects) with ease.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-172.pdf,
173,1994,Task Network Planning,HTN Planning: Complexity and Expressivity,"Kutluhan Erol, James Hendler, Dana S. Nau","Most practical work on AI planning systems during the last fifteen years has been based on hierarchical task network (HTN) d ecomposition, but until now, there has been very little analytical work on the properties of HTN planners. This paper describes how the complexity of HTN planning varies with various conditions on the task networks.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-173.pdf,
174,1994,Task Network Planning,The Use of Condition Types to Restrict Search in an AI Planner,"Austin Tate, Brian Drabble, Jeff Dalton","Condition satisfaction in planning has received a great deal of experimental and formal attention. A ""Truth Criterion"" lies at the heart of many planners and is critical to their capabilities and performance. However, there has been little study of ways in which the search space of a planner incorporating such a Truth Criterion can be guided. The aim of this document is to give a description of the use of condition ""type"" information to inform the search of an AI planner and to guide the production of answers by a planner’s truth criterion algorithm. The authors aim to promote discussion on the merits or otherwise of using such domain-dependent condition type restrictions as a means to communicate valuable information from the domain writer to a general purpose domain-independent planner.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-174.pdf,
175,1994,Model-Based Reasoning,Prediction Sharing Across Time and Contexts,"Oskar Dressler, Hartmut Freitag","Sometimes inferences made at some specik time are valid at other times, too. In model-based diagnosis and monitoring as well as qualitative simulation inferences are often re-done although they have been performed previously. We propose a new methodfor sharing predictions done at different times, thus mutually cutting down prediction costs incurring at different times. Furthermore, we generalize the technique from ’sharing predictions across time' to ’sharing predictions across time and logical contexts'. Assumption-based truth maintenance is a form of sharing predictions across logical contexts. Because of the close connections to the ATMS we were able to use it as a means for implementation. We report empirical results on monitoring different con.Cgurations of ballast water tanks as used on offshore platforms and ships.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-175.pdf,
176,1994,Model-Based Reasoning,An Operational Semantics for Knowledge Bases,"Ronald Fagin, Joseph Y. Halpern, Yoram Moses, Moshe Y. Vardi","The standard approach in AI to knowledge representation is to represent an agent’s knowledge symbolically as a collection of formulas, which we can view as a knowledge base. An agent is then said to know a fact if it is provable from the formulas in his knowledge base. Halpern and Vardi advocated a model-theoretic approach to knowledge representation. In this approach, the key step is representing the agent’s knowledge using an appropriate semantic model. Here, we model knowledge bases operationally as multi-agent systems. Our results show that this approach offers significant advantages.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-176.pdf,
177,1994,Model-Based Reasoning,Reasoning with Models,"Roni Khardon, Dan Roth","We develop a model-based approach to reasoning, in which the knowledge base is represented as a set of models (satisfying assignments) rather then a logical formula, and the set of queries is restricted. We show that for every propositional knowledge base (KB) there exists a set of characteristic models with the property that a query is true in KB if and only if it is satisfied by the models in this set. We fully characterize a set of theories for which the model-based representation is compact and provides efficient reasoning. These include some cases where the formula-based representation does not support efficient reasoning. In addition, we consider the model-based approach to abductive reasoning and show that for any propositional KB, reasoning with its model-based representation yields an abductive explanation in time that is polynomial in its size.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-177.pdf,
178,1994,Model-Based Reasoning,Representing Multiple Theories,P. Pandurang Nayak,"Most Artificial Intelligence programs lack generality because they reason with a single domain theory that is tailored for a specific task and embodies a host of implicit assumptions. Contexts have been proposed as an effective solution to this problem by providing a mechanism for explicitly stating the assumptions underlying a domain theory. In addition, contexts can be used to focus reasoning, allow the representation of mutually incoherent domain theories, lift, axioms from one context into another, and transcend a context. In this paper we develop a simple propositional logic of context suitable for representing and reasoning with multiple domain theories. We introduce contexts as modal operators, and allow different, contexts to have different vocabularies. We analyze the computational properties of the logic, providing the central computational justification for the use of contexts. We show how the logic effectively handles the common uses of contexts. We also discuss the extensions needed to handle first-order logic.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-178.pdf,
179,1994,Model-Based Reasoning,How Things Appear to Work: Predicting Behaviors from Device Diagrams,"N. Hari Narayanan, Masaki Suwa, Hiroshi Motoda","This paper introduces a problem solving task involving common sense reasoning that humans are adept at, but one which has not received much attention within the area of cognitive modeling until recently. This is the task of predicting the operation of simple mechanical devices, in terms of behaviors of their components, from labeled schematic diagrams showing the spatial configuration of components and a given initial condition. We describe this task, present a cognitive process model developed from task and protocol analyses, and illustrate it using the example of a pressure gauge. Then the architecture of a corresponding computer model and a control algorithm embodying the cognitive strategy are proposed.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-179.pdf,
180,1994,Qualitative Reasoning: Modeling,A Qualitative Physics Compiler,Adam Farquhar,Predicting the behavior of physical systems is essential to both common sense and engineering tasks. It is made especially challenging by the lack of complete precise knowledge of the phenomena in the domain and the system being modelled. We present an implemented approach to automatically building and simulating qualitative models of physical systems. Imprecise knowledge of phenomenais expressed by qualitative representations of monotonic functions and variable values. Incomplete knowledge about the system is either inferred or alternative complete descriptions that will affect behavior are explored. The architecture and algorithms used support both effective implementation and formal analysis. The expressiveness of the modelling language and strength of the resulting predictions are demonstrated by substantial applications to complex systems.,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-180.pdf,
181,1994,Qualitative Reasoning: Modeling,Using Qualitative Physics to Build Articulate Software for Thermodynamics Education,"Kenneth D. Forbus, Peter B. Whalley","One of the original motivations for research in qualitative physics was the development of intelligent tutoring systems and learning environments for physical domains and complex systems. This paper demonstrates how a synergistic combination of qualitative physics and other AI techniques can be used to create an intelligent learning environment for students learning to analyze and design thermodynamic cycles. Pedagogically this problem is important because thermodynamic cycles express the key properties of systems which interconvert work and heat, such as power plants, propulsion systems, refrigerators, and heat pumps, and the study of thermodynamic cycles occupies a major portion of an engineering student' s training in thermodynamics. This paper describes CyclePad, a fully implemented learning environment which captures a substantial fraction of a thermodynamics textbook' s knowledge and is designed to scaffold students who are learning the principles of such cycles. We analyze the combination of ideas that made CyclePad possible, comment on some lessons learned about the utility of various techniques, and describe our plans for classroom experimentation.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-181.pdf,
182,1994,Qualitative Reasoning: Modeling,Automated Model Selection for Simulation,"Yumi Iwasaki, Alon Y. Levy","Constructing an appropriate model is crucial in reasoning successfully about the behavior of a physical situation to answer a query. In compositional modeling, a system is provided with a library of composible pieces of knowledge about the physical world called model fragments. Its task is to select appropriate model fragments to describe the situation, either for static analysis of a single state, or for the more complicated case simulation of dynamic behavior over a sequence of states. In previous work we showed how the model construction problem in general can advantageously be formulated as a problem of reasoning about relevance. This paper presents an actual algorithm, based on relevance reasoning, for selecting model fragments efficiently for the case of simulation. We show that the algorithm produces an adequate model for a given query and moreover, it is the simplest one given the constraints in the query.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-182.pdf,
183,1994,Qualitative Reasoning: Modeling,Automated Modeling for Answering Prediction Questions: Selecting the Time Scale and System Boundary,"Jeff Rickel, Bruce Porter","The ability to answer prediction questions is crucial to reasoning about physical systems. A prediction question poses a hypothetical scenario and asks for the resulting behavior of variables of interest. Prediction questions can be answered by simulating a model of the scenario. An appropriate system boundary, which separates aspects of the scenario that must be modeled from those that can be ignored, is critical to achieving a simple yet adequate model. This paper presents an efficient algorithm for system boundary selection, it shows the important role played by the model’s time scale, and it provides a separate algorithm for selecting this time scale. Both algorithms have been implemented in a compositional modeling program called TRIPEL and evaluated in the plant physiology domain.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-183.pdf,
184,1994,Qualitative Reasoning: Modeling,Decompositional Modeling through Caricatural Reasoning,"Brian C. Williams, Olivier Raiman","Many physical phenomena are sufficiently complex that the corresponding equations afford little insight, or no analytical method provides an exact solution. Decompositional modeling (DM) captures a modeler' s tacit skill at solving non-linear algebraic systems. DM divides statespace into a patchwork of simpler subregimes, called caricatures, each of which preserves only the dominant characteristics of that regime. It then solves the simpler nonlinear system and identifies its domndn of validity. The varying patchwork reflects how variations in the parameters change the dominant characteristics. The patchwork is built by extracting equational features consisting of the relative strength of terms, and then exagerating and merging these features in different combinations, resulting in the different caricatural regimes. DM operates by providing strategic guidance to a pair of symbolic manipulation systems for qualitative sign and order of magnitude algebra. The approach is sufficient to replicate a broad set of examples from acid-base chemistry.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-184.pdf,
185,1994,Qualitative Reasoning: Simulation,Comparative Simulation,"Michael Neitzke, Bernd Neumann","In this paper, a new theory of qualitative comparative descriptions for dynamic system behavior is presented. System deviations and behavior deviations are viewed relative to the normal case. In contrast to existing approaches, a deviation is not only characterized as ""less than normal"" or ""greater than normal"" (LGTN), but deviations can also be compared with each other in order to avoid ambiguities and provide more precise predictions. A fundamental problem in comparative behavior prediction is that LGTN deviations can cause non-LGTN effects like a change of the direction of a parameter or a change in the order of events. Such so-called changes in the behavioral topology (10,ll) cannot be handled by existing approaches in a satisfying way, but are covered by our theory. Our theory is incorporated into the relative simulator RSIM+. RSIM+ can be viewed as an extension of the QSIM simulator (7). It provides a refined system description with qualitative predictions which have not been achieved in other work. In particular, it is guaranteed that all behaviors following from an LGTN deviation are predicted.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-185.pdf,
186,1994,Qualitative Reasoning: Simulation,Qualitative Reasoning for Automated Exploration for Chaos,Toyoaki Nishida,"Chaos is ubiquitous in our everyday life and even a simple system may manifest chaotic behaviors. Chaos has been a challenge to the methodology of qualitative reasoning as well as classic science and engineering, due to unpredictability and complexity of behavior. In this paper, I claim that associating continuous domain with symbolic representation, a basic principle of qualitative reasoning, is vital for automating analysis of chaos, as long as it is properly formalized. As an empirical support to this claim, I present a computer program called PSX3 that can semi-automatically explore for chaotic behavior of a given system of piecewise linear ordinary differential equations with three unknown functions. The power of PSX3 originates from an ability of reasoning about smooth surfaces that implicitly exist in the phase space. PSX3 is im lemented using Common Lisp and Mathematica TM.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-186.pdf,
187,1994,Qualitative Reasoning: Simulation,Activity Analysis: The Qualitative Analysis of Stationary Points for Optimal Reasoning,"Brian C. Williams, Jonathan Cagan","We present a. theory of a modeler’s problem decomposition skills in the context of optical reasoning - the use of qualitative modeling to strategically guide numerical explorations of objective space. Our technique, called activity analysis, applies to the pervasive family of linear and non-linear, constrained optimization problems, and easily integrates with any existing numerical approach. Activity analysis draws from the power of two seemingly divergent perspectives - the global conflict-based approaches of combinatorial satisficing search, and the local gradient-based approaches of continuous optimization - combined with the underlying insights of engineering monotonicity analysis. The result is an approach that strategically cuts away subspaces that it can quickly rule out as suboptimal, and then guides the numerical methods to the remaining subspaces.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-187.pdf,
188,1994,Qualitative Reasoning: Simulation,Intelligent Automated Grid Generation for Numerical Simulations,"Ke-Thia Yao, Andrew Gelsey","Numerical simulation of partial differential equations (PDEs) plays a crucial role in predicting the behavior of physical systems and in modern engineering design. However, in order to produce reliable results with a PDE simulator, a human expert must typically expend considerable time and effort in setting up the simulation. Most of this effort is spent in generating the grid, the discretization of the spatial domain which the PDE simulator requires as input. To properly design a grid, the gridder must not only consider the characteristics of the spatial domain, but also the physics of the situation and the peculiarities of the numerical simulator. This paper describes an intelligent gridder that is capable of analyzing the topology of the spatial domain and predicting approximate physical behaviors based on the geometry of the spatial domain to automatically generate grids for computational fluid dynamics simulators. Typically gridding programs are given a pcsrtitioning of the spatial domain to assist the gridder. Our gridder is capable of performing this partitioning. This enables the gridder to automatically grid spatial domains of arbitrary configurations.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-188.pdf,
189,1994,Formal Models of Reactive Control,Structured Circuit Semantics for Reactive Plan Execution Systems,"Jaeho Lee, Edmund H. Durfee","A variety of reactive plan execution systems have been developed in recent years, each attempting to solve the problem of taking reasonable courses of action fast enough in a dynamically changing world. Comparing these competing approaches, and collecting the best features of each, has been problematic because of the diverse representations and (sometimes implicit) control structures that they have employed. To rectify this problem, we have extended the circuit semantics notion of teleo-reactive programs into richer, yet compact semantics, called structured circuit semantics (SCS), that can be used to explicitly represent the control behavior of various reactive execution systems. By transforming existing systems into SCS, we can identify underlying control assumptions and begin to identify more rigorously the strengths and limitations of these systems. Moreover, SCS provides a basis for constructing new reactive execution systems, with more understandable semantics, that can be tailored to particular domain needs.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-189.pdf,
190,1994,Formal Models of Reactive Control,Estimating Reaction Plan Size,Marcel Schoppers,"The Shannon/Ginsberg circuit size estimate, by assuming independence of Boolean inputs, is not usable as a plan size estimate. By re-estimating circuit size as a function of the number of combinations w of Boolean inputs, I show that a reaction plan over w world states should grow as O(w/log w), on average. Finally I obtain the general domain-independent result that for a domain containing w world states, the expected size of a reaction plan with variables is O.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-190.pdf,
191,1994,Learning Robotic Agents,Results on Controlling Action with Projective Visualization,Marc Goodman,"A projective visualizer learns to simulate events in the external world through observation of the world. These simulations are used to evaluate potential actions on the basis of their probable outcomes. Results are given that indicate, 1). the error rate for projective visualization is sub-linear as the system projects farther into the future, 2). the error rate is inversely proportional to the number of cases, 3). a simple domain model can be used to reduce the effect of compounding error, and 4). projection can be used to increase the performance of an agent, even when this projection is imperfect.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-191.pdf,
192,1994,Learning Robotic Agents,Learning to Select Useful Landmarks,"Russell Greiner, Ramana Isukapalli","To navigate effectively, an autonomous agent must be able to quickly and accurately determine its current location. Given an initial estimate of its position (perhaps based on dead-reckoning) and an image taken of a known environment, our agent first attempts to locate a set of landmarks (real-world objects at known locations), then uses their angular separation to obtain an improved estimate of its current position. Unfortunately, some landmarks may not be visible, or worse, may be confused with other landmarks, resulting in both time wasted in searching for invisible landmarks, and in further errors in the agent’s estimate of its position. To address these problems, we propose a method that uses previous experiences to learn a selection function that, given the set of landmarks that might be visible, returns the subset which can reliably be found correctly, and so provide an accurate registration of the agent’s position. We use statistical techniques to prove that the learned selection function is, with high probability, effectively at a local optimal in the space of such functions. This report also presents empirical evidence, using real-world data, that demonstrate the effectiveness of our approach.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-192.pdf,
193,1994,Learning Robotic Agents,Agents that Learn to Explain Themselves,W. Lewis Johnson,"Intelligent artificial agents need to be able to explain and justify their actions. They must therefore understand the rationales for their own actions. This paper describes a technique for acquiring this understanding, implemented in a multimedia explanation system. The system determines the motivation for a decision by recalling the situation in which the decision was made, and replaying the decision under variants of the original situation. Through experimentation the agent is able to discover what factors led to the decisions, and what alternatives might have been chosen had the situation been slightly different. The agent learns to recognize similar situations where the same decision would be made for the same reasons. This approach is implemented in an artificial fighter pilot that can explain the motivations for its actions, situation assessments, and beliefs.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-193.pdf,
194,1994,Learning Robotic Agents,Learning to Explore and Build Maps,"David Pierce, Benjamin Kuipers","Using the methods demonstrated in this paper, a robot with an unknown sensorimotor system can learn sets of features and behaviors adequate to explore a continuous environment and abstract it to a finite-state automaton. The structure of this automaton can then be learned from experience, and constitutes a cognitive map of the environment. A generate-and-test method is used to define a hierarchy of features defined on the raw sense vector culminating in a set of continuously differentiable local state variu bles. Control laws based on these local state variables are defined for robustly following paths that implement repeatable state transitions. These state transitions are the basis for a finite-state automaton, a discrete abstraction of the robot’s continuous world. A variety of existing methods can learn the structure of the automaton defined by the resulting states and transitions. A simple example of the performance of our implemented system is presented.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-194.pdf,
195,1994,Learning Robotic Agents,High Dimension Action Spaces in Robot Skill Learning,Jeff G. Schneider,"Table lookup with interpolation is used for many learning and adaptation tasks. Redundant mappings capture the important concept of ""motor skill,"" which is important in real, behaving systems. Few robot skill implementations have dealt with redundant mappings, in which the space to be searched to create the table has much higher dimensionality than the table. A practical method for inverting redundant mappings is important in physical systems with limited time for trials. We present the ""Guided table Fill In"" algorithm, which uses data already stored in the table to guide search through the space of potential table entries. The algorithm is illustrated and tested on a robot skill learning task both in simulation and on a robot with a flexible link. Our experiments show that the ability to search high dimensional action spaces efficiently allows skill learners to find new behaviors that are qualitatively different from what they were presented or what the system designer may have expected. Thus the use of this technique can allow researchers to seek higher dimensional action spaces for their systems rather than constraining their search space at the risk of excluding the best actions.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-195.pdf,
196,1994,"Robot Control, Locomotion and Manipulation",Robot Behavior Conflicts: Can Intelligence Be Modularized?,"Amol Dattatraya Mali, Amitabha Mukerjee","In this paper, we examine the modularity assumption of behaviour-based models: that complex functionalities can be achieved by decomposition into simpler behaviours. In particular we look at the issue of conflicts among robot behaviour modules. The chief contribution of this work is a formal characterization of temporal cycles in behaviour systems and the development of an algorithm for detecting and avoiding such conflicts. We develop the mechanisms of stimulus specialization and response generalization for eliminating conflicts. The probable conflicts can be detected and eliminated before implementation. However the process of cycle elimination weakens the behaviour structure. We show how (a) removing conflicts results in less flexible and less useful behaviour modules and (b) the probability of conflict is greater for more powerful behaviour systems. We conclude that purely reactive systems are limited by cyclic behaviours in the complexity of tasks they can perform.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-196.pdf,
197,1994,"Robot Control, Locomotion and Manipulation",Merging Path Planners and Controllers through Local Context,Sundar Narasimhan,"This paper presents an implemented approach to robotic tasks involving intermittent contact and changing dynamics in uncertain environments. The approach is to use global planning to find paths in a tesselated representation of the environment, and a set of local controllers to take into account possibly time varying dynamics. The important difference from conventional pathplanning in robotic tasks is how this approach uses local sensory information, and the important difference from reactive or behavior-based approaches is that the local controllers are learnt from simulation models or actual trials and are not programmed in a-priori.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-197.pdf,
198,1994,"Robot Control, Locomotion and Manipulation",Teleassistance: Contextual Guidance for Autonomous Manipulation,"Polly K. Pook, Dana H. Ballard","We present teleassistance, a two-tiered control structure for robotic manipulation that combines the advantages of autonomy and teleoperation. At the top level, a teleoperator provides global, deictic references via a natural sign language. Each sign indicates the next action to perform and a relative and hand-centered coordinate frame in which to perform it. For example, the teleoperator may point to an object for reaching, or preshape the hand for grasping. At the lower level autonomous servo routines run within the reference frames provided. Teleassistance offers two benefits. First, the servo routines can po- sition the robot in relative coordinates and interprefeedback within a constrained context. This significantly simplifies the computational load of the autonomous routines and requires only a sparse model of the task. Second, the operator’s actions are symbolic, conveying intent without requiring the person to literally control the robot. This helps to alleviate many of the problems inherent to teleoperation, including poor mappings between operator and robot physiology, reliance on a broad communication bandwidth, and the potential for robot damage when solely under remote control. To demonstrate the concept, a Utah/MIT hand mounted on a Puma 760 arm opens a door.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-198.pdf,
199,1994,"Robot Control, Locomotion and Manipulation",Automatically Tuning Control Systems for Simulated Legged Robots,Robert Ringrose,"Rather than create a control system from scratch each time we build a new robot creature, we would like to generate control systems automatically. I have implemented an algorithm which, given a control system that works well for one creature, automatically tunes it to work for a new, similar creature. Using this approach, the control system for a horse might be adjusted for use with elephants, giraffes, and dogs. The adjustment is accomplished by gradually altering the original creature to make it like the new one and repeatedly tuning the control system as these changes are made. Because the creature’s alteration is gradual, the control system can be tuned using a local search such as gradient descent. In simulation tests, the tuning algorithm has successfully tuned the control system of a planar quadruped simulation to accommodate a reduction in leg length by a factor of two, an increase in body mass by a factor of three, and changes in the commanded speed while trotting.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-199.pdf,
200,1994,"Robot Control, Locomotion and Manipulation",Reactive Deliberation: An Architecture for Real-Time Intelligent Control in Dynamic Environments,Michael K. Sahota,Reactive deliberation is a novel robot architecture that has been designed to overcome some of the problems posed by dynamic robot environments. It is argued that the problem of action selection in nontrivial domains cannot be intelligently resolved without attention to detailed planning. Experimental evidence is provided that the goals and actions of a robot must be evaluated at a rate commensurate with changes in the environment. The goal-oriented behaviours of reactive deliberation are a useful abstraction that allow sharing of scarce computational resources and effective goal-arbitration through inter-behaviour bidding. The effectiveness of reactive deliberation has been demonstrated through a tournament of one-on-one soccer games between real-world robots. Soccer is a dynamic environment; the locations of the ball and the robots are constantly changing. The results suggest that the architectural elements in reactive deliberation are sufficient for real-time intelligent control in dynamic environments.,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-200.pdf,
201,1994,Genetic Algorithms and Simulated Annealing,Exploiting Problem Structure in Genetic Algorithms,"Scott H. Clearwater, Tad Hogg","Recent empirical and theoretical studies have shown that simple parameters characterizing the structure of many constraint satisfaction problems also predict the cost to solve them, on average. We apply these observations to improve the performance of genetic algorithms. In particular, we use a simple cost measure to evaluate the likely solution difficulty of the different unsolved subproblems appearing in the population. This is used to determine which individuals contribute to subsequent generations and improves upon the traditional direct use of the underlying cost function. As a specific test case, we used the GENESIS genetic algorithm to search for the optimum of a class of random Walsh polynomials. We also discuss extensions to other types of machine learning and problem solving systems.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-201.pdf,
202,1994,Genetic Algorithms and Simulated Annealing,Increasing the Efficiency of Simulated Annealing Search by Learning to Recognize (Un)Promising Runs,"Yoichiro Nakakuku, Norman Sadeh","Simulated Annealing (SA) procedures can potentially yield near-optimal solutions to many difficult combinatorial optimization problems, though often at the expense of intensive computational efforts. The single most significant source of inefficiency in SA search is its inherent stochasticity, typically requiring that the procedure be rerun a large number of times before a near-optimal solution is found. This paper describes a mechanism that attempts to learn the structure of the search space over multiple SA runs on a given problem. Specifically, probability distributions are dynamically updated over multiple runs to estimate at different checkpoints how promising a SA run appears to be. Based on this mechanism, two types of criteria are developed that aim at increasing search efficiency: (1) a cutoff criterion used to determine when to abandon unpromising runs and (2) restart criteria used to determine whether to start a fresh SA run or restart search in the middle of an earlier run. Experimental results obtained on a class of complex job shop scheduling problems show (1) that SA can produce high quality solutions for this class of problems, if run a large number of times, and (2) that our learning mechanism can significantly reduce the computation time required to find high quality solutions to these problems. The results further indicate that, the closer one wants to be to the optimum, the larger the speedups.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-202.pdf,
203,1994,Genetic Algorithms and Simulated Annealing,Improving Search through Diversity,"Peter Shell, Juan Antonio Hernandez Rubio, Gonzalo Quiroga Barro","Adding diversity to symbolic search techniques has not been explored in artificial intelligence. Adding a diversity criterion provides us with a powerful new mechanism for finding global maxima in complex search spaces and helps to alleviate the problem of premature convergence to local maxima. A theoretical analysis is presented of issues in diversity searching which previously haven’t been addressed, and a domain-independent diversity-search algorithm for practical breadth-first searching is developed. Empirical results of an implementation in the CRESUS expert system for intelligent cash-management confirm that diversity can significantly improve the solution quality of symbolic searchers.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-203.pdf,
204,1994,Genetic Algorithms and Simulated Annealing,Genetic Programming and AI Planning Systems,Lee Spector,"Genetic programming (GP) is an automatic programming technique that has recently been applied to a wide range of problems including blocks-world planning. This paper describes a series of illustrative experiments in which GP techniques are applied to traditional blocks-world planning problems. We discuss genetic planning in the context of traditional AI planning systems, and comment on the costs and benefits to be expected from further work.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-204.pdf,
205,1994,Genetic Algorithms and Simulated Annealing,Hierarchical Chunking in Classifier Systems,Gerhard Weiss,"Two standard schemes for learning in classifier systems have been proposed in the literature: the bucket brigade algorithm (BBA) and the profit sharing plan (PSP). The BBA is a local learning scheme which requires less memory and lower peak computation than the PSP, whereas the PSP is a global learning scheme which typically achieves a clearly better performance than the BBA. This ""requirement versus achievement"" difference, known as the locality/globality dilemma, is addressed in this paper. A new algorithm called hierarchical chunking algorithm (HCA) is presented which aims at synthesizing the local and the global learning schemes. This algorithm offers a solution to the locality/globality dilemma for the important class of reactive classifier systems. The contents is as follows. Section 1 describes the locality/globality dilemma and motivates the necessity of its solution. Section 2 briefly introduces basic aspects of (reactive) classifier systems that are relevant to this paper. Section 3 presents the HCA. Section 4 gives an experimental comparison of the HCA, the BBA and the PSP. Section 5 concludes the paper with a discussion and an outlook on future work.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-205.pdf,
206,1994,Search,Exploiting Algebraic Structure in Parallel State Space Search,"Jonathan Bright, Simon Kasif, Lewis Stiller","In this paper we present an approach for performing very large state-space search on parallel machines. While the majority of searching methods in Artificial Intelligence rely on heuristics, the parallel algorithm we propose exploits the algebraic structure of problems to reduce both the time and space complexity required to solve these problems on massively parallel machines. Our algorithm runs in O(N^1/4/p) time using O(N^1/4) space with P processors where N is the size of the state space and P is the number of processors. The technique we present is applicable to several classes of exhaustive searches. Applications include the knapsack problem and the shortest word problem in permutation groups which is a natural generalization of several common planning benchmarks such as Rubik’s Cube and the n-puzzle.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-206.pdf,
207,1994,Search,The Trailblazer Search: A New Method for Searching and Capturing Moving Targets,"Fumihiko Chimura, Mario Tokoro","This paper proposes a new search algorithm for targets that move. Ishida and Korf presented an algorithm, called the moving target search, that captures a target while deciding each search step in constant time (Ishida and Korf 1991). However, this algorithm requires many search steps to solve problems, if it uses a heuristic function that initially returns inaccurate values. The trailblazer search stores path information of the region it has searched and exploits this information when making decisions. The algorithm maintains a map of the searched region, and chases the target once it falls on a path found on the map. We empirically show that the algorithm’s map function can significantly reduce the number of search steps, compared with the moving target search. We also discuss the efficiency of the trailblazer search, taking the maintenance cost of the map into consideration.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-207.pdf,
208,1994,Search,ITS: An Efficient Limited-Memory Heuristic Tree Search Algorithm,"Subrata Ghosh, Dana S. Nau, Ambuj Mahanti","This paper describes a new admissible tree search algorithm called Iterative Threshold Search (ITS). ITS can be viewed as a much-simplified version of MA* [1], and a generalized version of MREC [12]. We also present the following results: 1. Every node generated by ITS is also generated by IDA*, even if ITS is given no more memory than IDA*. In addition, there are trees on which ITS generates O(N) nodes in comparison to O(N log N) nodes generated by IDA*, where N is the number of nodes eligible for generation by A*. 2. Experimental tests show that if the node-generation time is high (as in most practical problems), ITS can provide significant savings in both number of node generations and running time. Our experimental results also suggest that in the average case both IDA* and ITS are asymptotically optimal on the traveling salesman problem.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-208.pdf,
209,1994,Search,Memory-Bounded Bidirectional Search,"Hermann Kaindl, Aliasghar Khorsand","Previous approaches to bidirectional search require exponential space, and they are either less efficient than unidirectional search for finding optimal solutions, or they cannot even find such solutions for difficult problems. Based on a memory-bounded unidirectional algorithm for trees (SMA*), we developed a graph search extension, and we used it to construct a very efficient memory-bounded bidirectional algorithm. This bidirectional algorithm can be run for difficult problems with bounded memory. In addition, it is much more efficient than the corresponding unidirectional search algorithm also for finding optimal solutions to difficult problems. In summary, bidirectional search appears to be the best approach to solving difficult problems, and this indicates the extreme usefulness of a paradigm that was neglected for long.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-209.pdf,
210,1994,Two-Player Games,Best-First Minimax Search: Othello Results,"Richard E. Korf, David Maxwell Chickering","We present a very simple selective search algorithm for two-player games. It always expands next the frontier node that determines the minimax value of the root. The algorithm requires no information other than a static evaluation function, and its time overhead per node is similar to that of alpha-beta minimax. We also present an implementation of the algorithm that reduces its space complexity from exponential to linear in the search depth, at the cost of increased time complexity. In the game of Othello, using the evaluation function from BiIl (Lee and Mahajan 1990), best-first minimax outplays alpha-beta at moderate depths. A hybrid best-first extension algorithm, which combines alpha-beta and best-first minimax, performs significantly better than either pure algorithm even at greater depths. Similar results were also obtained for a class of random game trees.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-210.pdf,
211,1994,Two-Player Games,Evolving Neural Networks to Focus Minimax Search,"David E. Moriarty, Risto Miikkulainen","Neural networks were evolved through genetic algorithms to focus minimax search in the game of Othello. At each level of the search tree, the focus networks decide which moves are promising enough to be explored further. The networks effectively hide problem states from minimax based on the knowledge they have evolved about the limitations of minimax and the evaluation function. Focus networks were encoded in marker-based chromosomes and were evolved against a full-width minimax opponent that used the same evaluation function. The networks were able to guide the search away from poor information, resulting in stronger play while examining fewer states. When evolved with a highly sophisticated evaluation function of the Bill program, the system was able to match Bill' s performance while only searching a subset of the moves.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-211.pdf,
212,1994,Two-Player Games,A Strategic Metagame Player for General Chesslike Games,Barney Pell,"This paper introduces METAGAMER, the first program designed within the paradigm of MetaCame Playing (Metagame) (Pell 1992a). This program plays Metagame in the class of symmetric chess-like games (Pell 1992b), which includes chess, Chinese-chess, checkers, draughts, and Shogi. METAGAMER takes as input the rules of a specific game and analyses those rules to construct for that game an efficient representation and an evaluation function, for use by a generic search engine. The strategic analysis performed by METAGAMER relates a set of general knowledge sources to the details of the particular game. Among other properties, this analysis determines the relative value of the different pieces in a given game. Although METAGAMER does not learn from experience, the values resulting from its analysis are qualitatively similar to values used by experts on known games, and are sufficient to produce competitive performance the first time METAGAMER actually plays each new game. Besides being the first Metagame-playing program, this is the first program to have derived useful piece values directly from analysis of the rules of different games. This paper describes the knowledge implemented in MJSTAGAMER, illustrates the piece values METAGAMER derives for chess and checkers, and discusses experiments with METAGAMJZR on both existing and newly generated games.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-212.pdf,
213,1994,Two-Player Games,An Analysis of Forward Pruning,"Stephen J. J. Smith, Dana S. Nau","Several early game-playing computer programs used forward pruning (i.e., the practice of deliberately ignoring nodes that are believed unlikely to affect a game tree’s minimax value), but this technique did not seem to result in good decision-making. The poor performance of forward pruning presents a major puzzle for AI research on game playing, because some version of forward pruning seems to be ""what people do,"" and the best chess-playing programs still do not play as well as the best humans. As a step toward deeper understanding of forward pruning, we have set up models of forward pruning on two different kinds of game trees, and used these models to investigate how forward pruning affects the probability of choosing the correct move. In our studies, forward pruning did better than minimaxing when there was a high correlation among the minimax values of sibling nodes in a game tree. This result suggests that forward pruning may possibly be a useful decision-making technique in certain kinds of games. In particular, we believe that bridge may be such a game.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-213.pdf,
214,1994,Spatial Reasoning,Basic Meanings of Spatial Relations: Computation and Evaluation in 3D Space,Klaus-Peter Gapp,Spatial relations play an important role in the research area of connecting visual and verbal space. In the last decade several approaches to semantics and computation of spatial relations in 2D space have been developed. Presented here is a new approach to the computation and evaluation of basic spatial relations’ meanings in 3D space. We propose the use of various kinds of approximations when defining the basic semantics. The vagueness of the applicability of a spatial relation is accounted for by a flexible evaluation component which enables a cognitively plausible continuous gradation. For validating the evolved methods we have integrated them into a workbench. This workbench allows us to investigate the structure of a spatial relation’s applicability region through various visualization methods.,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-214.pdf,
215,1994,Spatial Reasoning,Spatial Reasoning in Indeterminate Worlds,Janice Glasgow,"A possible worlds semantics for model-based spatial reasoning is presented. In this semantics, worlds are characterized by the alternative states that result from indeterminacy or partial knowledge. A world is represented as a set of symbolic arrays, where symbols in the array map to entities in the world and the relative locations of symbols correspond to the relative locations of entities. Deduction is carried out using a model-theoretic approach in which array representations are ""inspected"" using primitive array functions. Nonmonotonic reasoning using array representations is also discussed.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-215.pdf,
216,1994,Spatial Reasoning,Automatic Depiction of Spatial Descriptions,"Patrick Olivier, Toshiyuki Maeda, Jun-ichi Tsujii","A novel combination of ideas from cognitive linguistics and spatial occupancy models in robotics has led to the WIP (Words Into Pictures) system. WIP automatically generates depictions of natural language descriptions of indoor scenes. A qualitative layer in the conceptual representation of objects underlies a mechanism by which alternative depictions arise for qualitatively distinct interpretations, as often occurs as a result of deictic/intrinsic reference frame ambiguity. At the same time, a quantitative layer, in conjunction with a potential field model of the semantics of projective prepositions, is used in the process of capturing the inherently fuzzy character of the meaning of natural language spatial predications.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-216.pdf,
217,1994,Spatial Reasoning,A Model for Integrated Qualitative Spatial and Dynamic Reasoning about Physical Systems,Raman Rajagopalan,"Qualitative spatial reasoning has many applications in such diverse areas as natural language understanding, cognitive mapping, and reasoning about the physical world. We address problems whose solutions require integrated spatial and dynamic reasoning. In this paper, we present our spatial representation, based on the extremal points of objects, and show that this representation is useful for modeling the spatial extent, relative positions, and orientation of objects, and in reasoning about changes in spatial relations and orientation due to the translational and rotational motion of objects. Our theory has been implemented to support a magnetic fields problem solving application using the QPC and QSIM systems for qualitative modeling. The issues encountered in integrating spatial and dynamic reasoning in the context of these systems are also discussed.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-217.pdf,
218,1994,Spatial Reasoning,A Theory for Qualitative Spatial Reasoning Based on Order Relations,Ralf Röhrig,What is needed for an analysis of the existing approaches to qualitative spatial reasoning and for a deeper understanding of the domain of space is a unifying theory that explains all of the concepts used for the representation of the different aspects of space by some primitive but well understood relations. In order to provide such primitive relations it will be shown that the concepts used in the existing approaches can be explained by simple order relations between points on some low-dimensional structures. One of the properties of an order relation is transitivity. It will be shown that this property alone is sufficient to explain all the inferences described in the various approaches to qualitative spatial reasoning.,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-218.pdf,
219,1994,Student Abstracts,Classification of Noun Phrases into Concepts or Individuals,Saliha Azzam,"We tackle here the problem of discrimination between instances of the language representation and concepts. This procedure is necessary according to the aim of the application that uses the conceptual structures. We propose linguistic rules for doing this discrimination inside natural language texts, and indicate how these rules are combined to build an accurate procedure.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-219.pdf,
220,1994,Student Abstracts,Regression Based Causal Induction with Latent Variable Models,Lisa A. Ballesteros,"Scientists largely explain observations by inferring causal relationships among measured variables. Many algorithms with various theoretical foundations have been developed for causal induction e.g., (Spirtes, Glymour, and Scheines 1993; Pearl and Verma 1991), but it is widely believed that regression is ill-suited to the task of causal induction.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-220.pdf,
221,1994,Student Abstracts,Probabilistic Knowledge of External Events in Planning,Jim Blythe,"My research tries to improve the robustness of plans by using limited knowledge about external events. These are events that are not directly caused by the planning agent. I use a discrete-time model and assume that the probability of occurrence for a particular type of event in a given situation is known, but the specific occurrence of such an event cannot be predicted with certainty. For example, when a bicycle is left outside a building, there is some probability p that it will be stolen at each time point. The probability that the bicycle is still outside the building after n time units is then (1 - p)^n, neglecting the effects of other possible events.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-221.pdf,
222,1994,Student Abstracts,DANIEL: Integrating Case-Based and Rule-Based Reasoning in Law,Stefanie Brüninghaus,"This paper introduces DANlEL, an architecture for the integration of case-based reasoning and rule-based reasoning for legal interpretation. Rather than interleaving the reasoners and assuming their complementarity, like in previous approaches, they are applied concurrently. Conflicting interpretations are handled explicitly, based on domain knowledge and on the notion of redundancy. The principal problems of legal interpretation are the lack of deep models for legal reasoning, the existence of inherently ill-defined predicates and the frequent use of open-textured concepts, as pointed out in (Rissland and Skalak 1991). A hybrid approach to representing the legal sources and the use of meta-knowledge seems to be appropriate to solve these problems. The scope of DANIEL is not limited to this particular domain, since the noted difficulties do not occur exclusively, but prototypically in the law.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-222.pdf,
223,1994,Student Abstracts,Decision-Theoretic Plan Failure Debugging and Repair,Lisa J. Burnell,"A number of strategies exist for the recovery from execution-time plan failures. One manner in which these strategies differ is the degree of dependence on the reliability and availability of the planner’s knowledge. The best strategy, however, may be dependent on a number of considerations, including the type of plan failure, the criticality of the failure, the availability of resources, and the reliability and availability of the knowledge involved in a given plan failure instance. We are examining a decision-theoretic approach to diagnose plan fakes and to dynamically select fkom multiple failure recovery strategies when an execution-time plan failure occurs.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-223.pdf,
224,1994,Student Abstracts,Decidability of Contextual Reasoning,Vanja Buvac,"Contexts were first suggested in McCarthy’s Turing Award Paper, (McCarthy 1987), as a possible solution to the problem of generality in AI. McCarthy’s concern with the existing AI systems has been that they can reason only about some particular, predetermined task. When faced with slightly different circumstances they need to be completely rewritten. In other words, AI systems lack generality. Cyc (Guha and Lenat 1990), a large common-sense knowledge-base currently being developed at MCC, is one example of where contexts have already been put to use in attempt to solve the problem of generality. Because of the complexity of the problem of generality, it has been speculated that any reasoning system which would be able to solve this problem would itself be computationally unacceptable. The purpose of this paper is to show that propositional contextual reasoning is decidable.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-224.pdf,
225,1994,Student Abstracts,Simplifying Bayesian Belief Nets while Preserving MPE or MPGE Ordering,YaLing Chang,"The abstraction of probability inference is a process of searching for a representation in which only the desirable properties of the solutions are preserved. Simplification is one of such abstraction which reduces the size of large databases and speeds transmission and processing of probabilistic information[Sy and Sher, 94].",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-225.pdf,
226,1994,Student Abstracts,Abstract of the Forest Management Advisory Systems,"Yousong Chang, Donald Nute","Expert system technology is a powerful tool for enhancing the decision making capabilities of nonexperts with reasonable knowledge of a domain to expert level in that domain. U.S.D.A. Forest Service has been working on forest management expert systems for several years. However, building different expert systems for each kind of forest is a demanding task.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-226.pdf,
227,1994,Student Abstracts,SodaBot: A Software Agent Environment and Construction System,Michael H. Coen,"Much of the work done in the area of software agents can be placed into one of two categories: (1) highly theoretical treatment of agents’ intentions and capabilities; and (2) applied construction of specific agents. However, determining for what (and if) software agents are actually useful requires building many of them, and the agent construction process poses difficult technical challenges.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-227.pdf,
228,1994,Student Abstracts,Empirical Knowledge Representation Generation Using N-Gram Clustering,Robin Collier,"The work discussed below enables the automatic generation of structures similar to the key templates which are predefined in information extraction/retrieval conferences - this would be a significant development. The motivation is similar to that of AutoSlog (Riloff 1993) which generates a domain-specific dictionary of concepts, although the approach is quite different.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-228.pdf,
229,1994,Student Abstracts,Case-Based Introspection,Michael T. Cox,"To effectively reason about one’s own knowledge, goals, and reasoning requires an ability to explicitly introspect. A computational model of introspection is a second-order theory that contains a formal language for representing first-order processes and that processes instances of this representation. The reasoning algorithm used to perform such processing is similar to the algorithm used to reason about events and processes represented in the original domain: case-based reasoning.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-229.pdf,
230,1994,Student Abstracts,Time Units and Calendars,"Diana Cukierman, James Delgrande","We are investigating a formal representation of time units and calendars, as restricted temporal entities for reasoning about activities. We examine characteristics of time units, and provide a categorization of the hierarchical relations among them. Hence we define an abstract hierarchical unit structure (a calendar structure) that expresses specific relations and properties among the units that compose it. Calendar structures subsume systems that can be based on discrete units together with a repetitive containment relation.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-230.pdf,
231,1994,Student Abstracts,Local Search in the Coordination of Intelligent Agents,"Daniel E. Damouth, Edmund H. Durfee","In a world inhabited by numerous agents pursuing distinct goals, conflicts are inevitable. To succeed in the environment, an agent must explicitly reason about the behaviors of other agents as well as itself, and be prepared to find new behaviors that are more coordinated. Because traditional AI has had great success viewing problem solving as a search in a problem space, we have chosen to represent the process of coordination as a distributed search (Durfee et al. 1994). In searching through a joint behavior space for coherent coordination patterns, an agent must observe three kinds of constraints: its abilities, its goals, and the activities of other agents in the environment.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-231.pdf,
232,1994,Student Abstracts,GKR: A Generic Model of Knowledge Representation,"Angélica de Antonio, Jesús Cardeñosa, Loïc Martínez Normand","We show in this paper a proposal for a new generic model of KR called GKR (Generic Knowledge Representation). This model has been developed as a result of the analysis of the models described in the paper. The study of the successes and shortcomings of these models helped us to define GKR with several properties that improve its representation ability.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-232.pdf,
233,1994,Student Abstracts,Experiments Towards Robotic Learning by Imitation,John Demiris,"Learning by imitation is a form of learning, which despite the fact it has been widely studied by ethologists, has not been fully understood yet. In fact, there is a considerable disagreement even on the terminology used despite attempts to clarify it (Davis,1973; Galef, 1988;). We believe that by building robots which instantiate the mechanisms hypothesised to underlie these types of behaviour, those mechanisms will be illuminated with explanatory adequacy. Our investigation into imitative learning begins with the construction of an appropriate experimental testbed and the design of a suitable architecture which would enable one robot (the learner) to imitate another one (the teacher) which is performing a task, and learn to perform the task while imitating the teacher. Initially, the idea was to have one robot to perform a sequence of moves (""dance""), while the other robot would learn dancing by imitating the first one. Such an experiment however, does not satisfy our need to be able to evaluate whether the robot has actually learned anything. Instead, we choose to have as a testbed a maze, where the robot learner would imitate the actions that the robot teacher is performing during its maze negotiation strategy. After the learning phase, we could ask the second robot to navigate itself through a new (different) maze, which would provide us with a more evident demonstration that learning has taken place.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-233.pdf,
234,1994,Student Abstracts,Goal-Clobbering Avoidance in Non-Linear Planners,Rujith de Silva,"A central issue in non-linear planning is the ordering of operators so as to avoid undesirable interactions between their effects. The Modal Truth Criterion (Chapman 1987) states the conditions under which these interactions will occur. Non-linear planners use the Criterion, directly or indirectly, to promote or demote operators, or to co-designate variables, so as to avoid interactions. - This abstract describes a method, called Goal Clobbering Avoidance (GCA), to avoid some interactions in a partially-ordered plan by promoting or demoting a sequence of operators, rather than individual operators. Effectively, it simultaneously applies the Modal Truth Criterion to all operators in the sequence, using pm-compiled information about the domain.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-234.pdf,
235,1994,Student Abstracts,Dynamically Adjusting Categories to Accommodate Changing Contexts,"Mark Devaney, Ashwin Ram","Concept formation is the process by which generalizations are formed through observation of instances from the environment. These instances are described along a number of attributes, which are selected according to their relevance to the problem or task for which the concepts will be used. The context of a concept learning problem consists of the goals and tasks of the learner, as well as its background knowledge and domain theories and the external environment in which it operates. Context is essential to inductive concept learning for it determines which attributes to use for a given problem out of the infinitely many available, providing a bias for the learner (Mitchell, 1980). Furthermore, context is not a static entity, but is constantly changing, especially in the types of learning tasks faced by humans (e.g. Seifert 1989, Barsalou 1991). As concept formation systems are employed in tasks more typical of natural domains and ""real-world"" problems, the ability to respond to changing contexts becomes increasingly important.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-235.pdf,
236,1994,Student Abstracts,Substructure Discovery Using Minimum Description Length Principle and Background Knowledge,Surnjani Djoko,"Discovering conceptually interesting and repetitive substructures in a structural data improves the ability to interpret and compress the data. The substructures are evaluated by their ability to describe and compress the original data set using the domain’s background knowledge and the minimum description length (MDL) of the data. Once discovered, the substructure concept is used to simplify the data by replacing instances of the substructure with a pointer to the newly discovered concept. The discovered substructure concepts allow abstraction over detailed structure in the original data. Iteration of the substructure discovery and replacement process constructs a hierarchical description of the structural data in terms of the discovered substructures. This hierarchy provides varying levels of interpretation that can be accessed based on the goals of the data analysis.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-236.pdf,
237,1994,Student Abstracts,Exploiting the Ordering of Observed Problem-Solving Steps for Knowledge Base Refinement: An Apprenticeship Approach,"Steven K. Donoho, David C. Wilkins","Apprenticeship is a powerful method of learning among humans in which a student refines his knowledge by observing and analyzing the problem-solving steps of an expert. In this paper we focus on knowledge base (KB) refinement for classification problems and examine how the ordering of the intermediate steps of an observed expert can be used to yield leverage in KB refinement. In the classical classification problem, the problem-solver is given an example consisting of a set of attributes and their corresponding values, and it must put the example in one of a pre-enumerated set of classes.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-237.pdf,
238,1994,Student Abstracts,The KM / KnEd System: An Integrated Approach to Building Large-Scale Multifunctional Knowledge Bases,Erik Eilerts,"In 1987, Dr. Bruce Porter began work at the University of Texas at Austin on the Botany Knowledge Base Project. The goal of the project is to develop a largescale multi-functional knowledge base in the area of Botany. This Botany Knowledge Base (BKB) is used to support research projects in question answering, automated modeling, and intelligent tutoring. Due to the size and complexity of the BKB, a decision was made in 1990 to begin construction of a new knowledge representation language and interface to support the knowledge base. The knowledge representation language was named KM, for Knowledge Manager, and the interface was named KnEd, for Knowledge Editor. The KM/KnEd system is similar to Doug Lenat’s CYC project and Doug Skuce’s CODE4 system.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-238.pdf,
239,1994,Student Abstracts,Situated Agents Can Have Plans,Mark Fasciano,"Much of our everyday activity is not made up of solving isolated problems with single clear-cut goals, but rather dedicated to the ongoing maintenance of many goals or policies such as eating when hungry and maintaining a comfortable personal space. Furthermore, in many complex, dynamic worlds, an agent must maintain many goals at the same time and be able to act quickly and flexibly, because some decisions are time critical and the world is not perfectly predictable.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-239.pdf,
240,1994,Student Abstracts,Introspective Reasoning in a Case-Based Planner,"Susan Fox, David Leake",Many current AI systems assume that the reasoning mechanisms used to manipulate their knowledge may be fixed ahead of time by the designer. This assumption may break down in complex domains. The focus of this research is developing a model of introspective reasoning and learning to enable a system to improve its own reasoning as well as its domain knowledge. Our model is based on the proposal of (Birnbaum et al. 1991) to use a model of the ideal behavior of a case-based system to judge system performance and to refine its reasoning mechanisms; it also draws on the research of (Ram and Cox 1994) on introspective failure-driven learning.,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-240.pdf,
241,1994,Student Abstracts,A Statistical Method for Handling Unknown Words,Alexander Franz,"Robust Natural Language Processing systems must be able to handle words that are not in their lexicon. We created a classifier that was trained on tagged text to find the most likely parts of speech for unknown words. The classifier uses a contingency table to count the observed features, and a loglinear model to smooth the cell counts. After smoothing, the contingency table is used to obtain the conditional probability distribution for classification.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-241.pdf,
242,1994,Student Abstracts,Low Computation Vision-Based Navigation for a Martian Rover,Andrew S. Gavin,"In the design and construction of mobile robots vision has always been one of the most potentially useful sensory systems. In practice however, it has also become the most difficult to successfully implement. At the MIT Mobile Robotics (Mobot) Lab we have designed a small, light, cheap, and low power Mobot Vision System that can be used to guide a mobile robot in a constrained environment. The target environment is the surface of Mars, although we believe the system should be applicable to other conditions as well. It is our belief that the constraints of the Martian environment will allow the implementation of a system that provides vision based guidance to a small mobile rover.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-242.pdf,
243,1994,Student Abstracts,Learning about Software Errors Via Systematic Experimentation,"Terrance Goan, Oren Etzioni","Classical planners assume that their internal model is both correct and complete. The dynamic nature of realworld domains (e.g., multi-user software environments) makes these assumptions untenable. Several new planners (e.g.,XII [2]) have been designed to work with incomplete information, and strides have been made in planning with potentially incorrect information. But, efficient operation in the presence of incorrect information is highly dependent on a planner’s ability to detect errors. Failing to recognize errors can result in unexpected and potentially destructive effects, as well as further corruption of the world model.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-243.pdf,
244,1994,Student Abstracts,Reasoning about What to Plan,Richard Goodwin,"Agents plan in order to improve their performance. However, planning takes time and consumes resources that may in fact degrade an agents performance. Ideally, an agent should only plan when the expected improvement outweighs the expected cost and no resources should be expended on making this decision. To do this, an agent would have to be omniscient. The problem of how to approximate this ideal, without consuming too many resources in the process, is the meta-level control problem for a resource bounded rational agent.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-244.pdf,
245,1994,Student Abstracts,The Crystallographer’s Assistant,"Vanathi Gopalakrishnan, Daniel Hennessy, Bruce Buchanan, Devika Subramanian","The only routinely used technique available today for obtaining the 3-D structure of a protein or DNA molecule is by X-ray diffracting a crystal of the macromolecule. The rate limiting step in structure determination is the process of growing a crystal of the macromolecule. This process is not very well understood, and can take a few weeks to several years. Crystallographers, therefore, are in great need of tools to aid them in the process of designing and performing experiments. There is a great deal of experiential data in this domain, in the form of scientific notebooks with graphical and textual representations of previous experiments.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-245.pdf,
246,1994,Student Abstracts,Time-Critical Scheduling in Stochastic Domains,"Lloyd Greenwald, Thomas Dean","In this work we look at extending the work of (Dean et al. 1993) to handle more complicated scheduling problems in which the sources of complexity stem not only from large state spaces but from large action spaces as well. In these problems it is no longer tractable to compute optimal policies for restricted state spaces via policy iteration. We, instead, borrow from operations research in applying bottleneck-centered scheduling heuristics (Adams et al. 1988). Additionally, our techniques draw from the work of (Drummond and Bresina 1990).",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-246.pdf,
247,1994,Student Abstracts,Planning for Compotent-Based Configurations,Gail Haddock,"The Scenario-based Engineering Process (SEP) is a novel approach to developing complex systems (Haddock 8z Harbison 1994). SEP builds new application systems through a selection process that groups primitive components into application specific components. The selection of primitive components and the construction of interfaces among components in an application system is currently a tedious manual undertaking. The automation of this process will require a configuration system that can support the complex interactions of the components, the dynamic requirements of users, and the capabilities of providing multiple viewpoints and managing extensive domains.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-247.pdf,
248,1994,Student Abstracts,The Epistemology of Physical System Modeling,"Kyungsook Han, Andrew Gelsey","Modeling and simulation have been typically pursued in isolation. When a model of a complex system is reported in the literature, there is a considerable emphasis on the end result, the model. On the other hand, many works on simulation assume the existence of models, and focus on developing representations and reasoning about the models in the representations. However, not every physical system has its models ready to use for problem-solving tasks and constructing adequate models is not trivial. Choosing a simulation method.is also dependent on the kinds of models available, the creation of which in turn depends on the knowledge available and its representation. A model of a physical system is normally created by the person studying the system with considerable time and effort spent. But a hand-crafted model is often error-prone and difficult to modify to solve a similar problem about other physical systems.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-248.pdf,
249,1994,Student Abstracts,Testing a KBS Using a Conceptual Model,Corinne Haouche,"We propose a KBS testing procedure that uses a KADS conceptual model (CM). The set of Vandd Inference Paths is derived from the inference structure, and, a ""high level"" trace, representing the Czsrrernt Inference Path, is built using the links established between the CM and the KBS. The comparison of this trace to the VIP can lead to modify either the code or the CM.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-249.pdf,
250,1994,Student Abstracts,A Dynamic Organization in Distributed Constraint Satisfaction,"Katsutoshi Hirayama, Seiji Yamada, Jun'ichi Toyoda","We present a novel dynamic organization to solve DCSP(Distributed Constraint Satisfaction Problem). DCSP provides a formal framework for studying cooperative distributed problem solving[Yokoo 921. To solve DCSP, we have developed a simple algorithm using iterative improvement. This technique has had great success on certain CSP(Constraint Satisfaction Problems)[Minton 9O][Selman 92]. In our algorithm each agent performs iterative improvement and also plural agents can do in parallel. However, one drawback of this technique is the possibility of getting caught in local minima(which are defined specifically in our algorithm). LMO is a technique for escaping from local minima. It is summarized as follows: When an agent(A1) gets caught in a local minimum, (step I) Al sends its CSP(variables, domains and constraints) to an agent(A2). Al selects A2 such that it shares violated constraints at that time. Ties are broken randomly. (step 2) AZ puts its CSP and Al’s CSP together and searches for all possible assignments with simple backtracking. After that, A2 performs iterative improvement. Besides escaping from local minima, LMO prevents agents from getting caught in the same local minima as before. Therefore our algorithm for DCSP is complete.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-250.pdf,
251,1994,Student Abstracts,Tractable Anytime Temporal Constraint Propagation,Louis J. Hoebel,A major concern when reasoning about time in artificial intelligence problems is computational tractability. We present a method for applying temporal reasoners to large scale dynamic problems. We present a partitioning of the temporal database and means of constraint propagation that presents an efficient approach for producing tractable systems. Our goal is not to enhance underlying reasoners but to develop mechanisms by which reasoning about time can be practically applied to certain problems. Tractable computation is the basic consideration.,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-251.pdf,
252,1994,Student Abstracts,Processing Pragmatics for Computer-Assisted Language Instruction,Keiko Horiguchi,"Computer-assisted language instruction systems that only perform syntactic processing of input sentences are not able to offer advice on pragmatic aspects of language use, and they cannot handle the variability generally afforded by natural languages to express a given propositional content.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-252.pdf,
253,1994,Student Abstracts,Generating Rhythms with Genetic Algorithms,Damon Horowitz,"My system uses an interactive genetic algorithm to learn a user’s criteria for the task of generating musical rhythms. Interactive genetic algorithms (Smith 91) are well suited to solving this problem because they allow for a user to simply execute fitness functions (that is, to choose which rhythms or features of rhythms he likes), without necessarily understanding the details or parameters of these functions. As the system learns (develops an increasingly accurate model of the function which represents the user’s criteria), the quality of the rhythms it produces improves to suit the user’s taste. This approach is largely motivated by Richard Dawkins, who succinctly summarizes the attraction of IGAs for artistic endeavors in stating: ""Effective searching procedures become, when the search space is sufficiently large, indistinguishable from true creativity"" (Dawkins 86).",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-253.pdf,
254,1994,Student Abstracts,The Automated Mapping of Plans for Plan Recognition,"Marcus J. Huber, Edmund H. Durfee, Michael P. Wellman","To coordinate with other agents in its an environment, an agent needs models of what the other agents are trying to do. When communication is impossible or expensive, this information must be acquired indirectly via plan recognition. Typical approaches to plan recognition start with specification of the possible plans the other agents may be following and develop special techniques for discriminating among the possibilities. These structures are not the direct nor derived output of a planning system. Prior work has not yet addressed the problem of how the plan recognition structures are (or could be) derived from executable plans as generated by planning systems. Furthermore, concerns about building models of agents’ actions in all possible worlds lead to a desire for dynamically constructing belief network models for situation-specific plan recognition activities. As a step in this direction, we have developed and implemented methods that take plans, as generated by a planning system, and creates a belief network model in support of the plan recognition task.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-254.pdf,
255,1994,Student Abstracts,Preliminary Studies in Agent Design in Simulated Environments,Scott B. Hunter,"It is known that, in general, the point along the purely-reactive/classical-planning axis of the controller spectrum that is most appropriate for a particular environment/task (E/T) p air will be determined by characteristics of the environment, the agent’s perceptual and effectual capabilities, and the task. Instead of proposing another hybrid architecture, we want to determine criteria for determining which architectural compromise is best suited for a given E/T. Our goal is to understand relationships between E/T pairs and the agent architecture, so that we can predict the performance of the architecture under parametric variations of the environment and/or the architecture. This is a first step toward constructing methods for automatic synthesis of agents as in (Ros89).",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-255.pdf,
256,1994,Student Abstracts,Dempster-Shafer and Bayesian Network for CAD-Based Feature Extraction: A Comparative Investigation and Analysis,"Qiang Ji, Michael M. Marefat, Paul J. A. Lever","Information pertaining to real world problems often contains noises and uncertainties. This has been a major challenge faced by the contemporary AI researchers. Of various paradigms developed for handing uncertainties, the Dempster-Shafer theory (DS) and the Bayesian Belief Networks (BBN) have received considerable attention in the AI community recently. They have been successfully applied to problems in medical diagnosis, decision-making, image understanding, machine vision, etc.. Despite their obvious success, blindly using them without understanding their limitations may result in computational difficulty and unsatisfying inference results. The aim of this paper is to analyze and compare the performance of the two paradigms in extracting manufacturing features from the solid model descriptions of objects. Such a comparison will serve to identify their strengths, weakness, and appropriate application domains.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-256.pdf,
257,1994,Student Abstracts,Finding Multivariate Splits in Decision Trees Using Function Optimization,George H. John,"We present a new method for top-down induction of decision trees (TDIDT) with multivariate binary splits at the nodes. The primary contribution of this work is a new splitting criterion called soft entropy, which is continuous and differentiable with respect to the parameters of the splitting function. Using simple gradient descent to find multivariate splits and a novel pruning technique, our TDIDT-SEH (Soft Entropy Hyperplanes) algorithm is able to learn very small trees with better accuracy than competing learning algorithms on most datasets examined.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-257.pdf,
258,1994,Student Abstracts,When the Best Move Isn’t Optimal: Q-learning with Exploration,George H. John,"The most popular delayed reinforcement learning technique, Q-learning (Watkins 1989)) estimates the future reward expected from executing each action in every state. If these estimates are correct, then an agent can use them to select the action with maximal expected future reward in each state, and thus perform optimally. Watkins has proved that Q-learning produces an optimal policy (the function mapping states to actions) and that these estimates converge to the correct values given the optimal policy.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-258.pdf,
259,1994,Student Abstracts,HIPAIR: Interactive Mechanism Analysis and Design Using Configuration Spaces,"Leo Joskowicz, Elisha Sacks","We present an interactive problem solving environment for reasoning about shape and motion in mechanism design. Reasoning about shape and motion plays a central role in mechanism design because mechanisms perform functions by transforming motions via part interactions. The input motion, the part shapes, and the part contacts determine the output motion. Designers must reason about the interplay between shape and motion at every step of the design cycle.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-259.pdf,
260,1994,Student Abstracts,Learning Sorting Networks By Grammars,"Thomas E. Kammeyer, Richard K. Belew","We use a genetic algorithm(GA) to search for CMPX-nets which are SNets or MNets. The GA repeatedly samples the space of potential solutions in a series of generations, each using the relative fitness of the previous generation’s samples to apportion more samples in promising regions. Mutation and especially cross-over operators are applied to generate similar but novel new sample points; this process is iterated until some stopping criterion is achieved. Hillis has had encouraging success using a GA to evolve sorting networks( Hillis 1991).",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-260.pdf,
261,1994,Student Abstracts,The Formation of Coalitions among Self-Interested Agents,Steven Ketchpel,"Researchers in the multi-agent systems community of DA1 assume that agents will have to interact with others agents that were designed by different designers for different goals. These diverse agents could benefit each other by collaborating, but they will do so only if the resulting deal is beneficial from each agent’s point of view. One useful definition of beneficial is that of economic rationality, maximizing the agent’s expected payoff in terms of a utility function. An open problem in this area is to design a protocol that allows a large pool of agents to determine which of the subsets among them can profit by working together. A solution to a coalition problem is a partition of the agents into subsets (coalitions), such that each agent in every coalition receives the most utility it can expect.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-261.pdf,
262,1994,Student Abstracts,Learning from Ambiguous Examples,Stephen V. Kowalski,"Current inductive learning systems are not well suited to learning from ambiguous examples. We say that an example is ambiguous if it has multiple interpretations, only one of which may be valid. Some domains in which ambiguous learning problems can be found are natural language processing (NLP) and computer vision. An example of an ambiguous training instance with two interpretations is shown below, where @ is the Exclusive-OR function and each interpretation is a conjunction of attribute values.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-262.pdf,
263,1994,Student Abstracts,Exploiting the Environment: Urban Navigation as a Case Study,Nicholas Kushmerick,The Situated Action approach to AI emphasizes the role of the environment in the generation and control of behavior; see (Norman 1993) for an introduction. Work to date has focused mainly on activity within spatially and temporally localized environments such as kitchens and video games (Agre and Chapman 1987; Agre and Horswill 1992). How useful is this perspective when larger-scale activities are considered? I attempt to answer this question by considering some issues related to navigation in urban environments. identify several constraints on the structure of street grids that make navigation much easier than arbitrary graph search. The ultimate goal is a theory of the relationship between features of an urban environment and the computational complexity of navigation. This work extends the sort of analysis advocated by (Agre and Horswill 1992; Horswill 1993).,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-263.pdf,
264,1994,Student Abstracts,Quantitative Evaluation of the Exploration Strategies of a Mobile Robot,"David Lee, Michael Recce","How should a mobile robot explore its environment in order to build a high-quality world model as efficiently as possible? We address this question through experimentation with a sonar-equipped mobile robot. The robot is taken to be a delivery robot, such as could be used in an office, hospital or home. Its objective is to execute efficient collision-free paths between user-specified locations. A grid-based free-space map is generated for this purpose. This map is derived from a feature-based map, built using techniques similar to those of (Leonard and Durrant-Whyte 1992).",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-264.pdf,
265,1994,Student Abstracts,Everyday Reasoning Meets Geometry Theorem-Proving,Thomas F. McDougal,"In an earlier paper [McDougal and Hammond 1993], we reported on POLYA, a computer program which proves high school geometry theorems. POLYA is a memory-based problem-solver in the case-based planning tradition [Hammond 1989, Kolodner 1993]. It uses features of the problem (mostly from the diagram) to index into a library of plans. Some of those plans are solutions to entire problems; others apply a single inference. The plans are indexed in memory by the features which predict the relevance of the plan without necessarily guaranteeing it.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-265.pdf,
266,1994,Student Abstracts,Determination of Machine Condition Using Neural Networks,"John MacIntyre, Peter Smith, John Tait","Condition monitoring is a developing discipline in machinery mainace. Data such as vibration levels, temperatures, oil analysis values etc, are acquired from plant, and analyzed to determine the condition of the plant at the time of measurement. Software packages are currently available to allow graphical display of the data, with varying levels of diagnostic tools available to assist engineers in performing data analysis. This abstract outlines the development of a condition monitoring system at Blyth Power Station, owned by National Power, the major electricity generating company in the United Kingdom. The abstract goes on to describe research into the development of a data analysis system employing neural networks trained to recognise machinery defects.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-266.pdf,
267,1994,Student Abstracts,Building a Parser that Can Afford to Interact with Semantics,Kavi Mahesh,"Natural language understanding programs get bogged down by the multiplicity of possible syntactic structures while processing real world texts that human understanders do not have much difficulty with. In this work, I analyze the relationships between parsing strategies, the degree of local ambiguity encountered by them, and semantic feedback to syntax, and propose a parsing algorithm called Head-Signaled Left Corner Parsing (HSLC) that minimizes local ambiguities while supporting interactive syntactic and semantic analysis. Such a parser has been implemented in a sentence understanding program called COMPERE .",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-267.pdf,
268,1994,Student Abstracts,Using Errors to Create Piecewise Learnable Partitions,Oded Maron,"After a learning system has been trained, the usual procedure is to average the testing errors in order to obtain an estimate of how well the system has learned. However, that is tossing away a lot of potentially useful information. We present an algorithm which exploits the distribution of errors in order to find where the algorithm performs badly and partition the space into parts which can be learned easily. We will show a simple example which gives the intuition of the algorithm, and then a more complex one which brings forth some of the details of the algorithm. Let us suppose that we are trying to learn the absolute value function. Almost all learning algorithms perform well along the arms of the function, but do badly around the cusp. If we notice th 'hill' of errors around x = 0, then we can partition the space which we are trying to learn into two parts which fall on either side of the hill. Those two partitions have the property of not only being linear, but of being learnable. Each partition can be trained separately, and when tested separately gives a better answer since irrelevant and misleading training points from other partitions have not been included.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-268.pdf,
269,1994,Student Abstracts,Development of an Intelligent Forensic System for Hair Analysis and Comparison,"C. Medina, L. Pratt, C. Ganesh","reject Description: This project explores the automation of hair analysis. We are working with the Colorado Bureau of Investigation to develop a system to aid in the hair comparison process. Our system will take as input a microscopic image of hair and produce classification decisions about features like visibility of the cuticle, presence or absence of a medulla, and cortical texture and color.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-269.pdf,
270,1994,Student Abstracts,Model-Based Sensor Diagnosis: When Monitoring Should be Monitored,Joël Milgram,"A complex industrial plant, such as a nuclear power plant, is monitored thanks to a number of sensors. The instrumentation may be itself a complex system liable to failures. We propose a model-based sensor diagnosis system which relies on the topological description of the plant and on a set of component models. This model implicitly conceals relations involving only sensor data. Such relations must always be verified if components behave normally; thus, the detection task consists of verifying these relations. So, this work is a first step in extending the scope of model-based diagnosis, since we question here the information stemming from the plant and normally considered as safe. As further studies, we wish to monitor this detection system itself; i.e., whenever the instrumentation is supposed to behave correctly, non-verified constraints point out to errors in the plant model.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-270.pdf,
271,1994,Student Abstracts,Theoretical and Experimental Studies of Temporal Constraint Satisfaction Problem,Debasis Mitra,"Reasoning with time is embedded in many application domains than we are often aware of. For example, understanding a parallel program involves how each unit of the program is temporally related to the other unit through dependency. There is a growing awareness about the importance of understanding time in any dynamic or evolving situation. Within last twenty years different dimensions of reasoning have been identified, such as, qualitative reasoning versus quantitative reasoning, point-based representation versus interval-based representation, propositional expression versus first order expression. My work concentrates on interval-based qualitative propositional reasoning. The representation scheme and a polynomial approximate algorithm were proposed by James Allen. The problem of detecting global consistency has been subsequently proved to be NP-complete. Practical reasoning systems have been developed based on Allen’s S-consistency algorithm. This algorithm checks for consistency over constraints between each subset of 3 temporal entities of the full set of temporal assertions in the system (rather than checking for complete consistency between all constraints, which is called global-consistency). There has been very little systematic study on either the S-consistency problem, or the global-consistency problem.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-271.pdf,
272,1994,Student Abstracts,A Theory of Reading,"Kenneth Moorman, Ashwin Ram","Reading has been studied for decades, yet no theories exist which completely explain it. In particular, a type of knowledge intensive reading, creative reading, has been practically ignored. Creative reading is the reading of texts which contain novel concepts. Nearly all reading will be creative to some degree; thus, any theory which overlooks this will be incomplete. By combining results from psychology, artificial intelligence, and education, we have produced a functional theory of the complete reading process, aimed at explaining creative reading.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-272.pdf,
273,1994,Student Abstracts,A Hybrid Parallel IDA Search,Shubha S. Nerur,Heuristic search is a fundamental problem-solving method in artificial intelligence. The main limitation of search is its computational complexity which can be overcome by parallel implementation of the algorithms. Distributed tree search and Parallel window search are two of the approaches to parallelizing search algorithms.,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-273.pdf,
274,1994,Student Abstracts,Time-Situated Reasoning within Tight Deadlines and Realistic Space and Computation Bounds,Madhura Nirkhe,"We develop an effective representational and inferential framework for fully deadline-coupled, time-situated problem solving. Our effort is to model an agent in a tight and rigid deadline situation, in need of successfully formulating and executing a deadline-feasible plan of action as the world around the agent continues to change. We highlight the severe time-pressure under which the agent must operate with a paradigmatic problem scenario : Nell and Dudley and the railroad tracks. Nell is tied to the railroad tracks as a train approaches. Dudley, our agent, must formulate a plan to save her and carry it out before the oncoming train reaches her. He must deliberate (plan) in order to decide this, yet as he does so, the train draws nearer to Nell.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-274.pdf,
275,1994,Student Abstracts,Integrating Induction and Instruction: Connectionist Advice Taking,"David C. Noelle, Garrison W. Cottrell","Humans improve their performance by means of a variety of learning strategies, including both gradual statistical induction from experience and rapid incorporation of advice. In many learning environments, these strategies may interact in complementary ways. The focus of this work is on cognitively plausible models of multistrategy learning involving the integration of inductive generalization and learning ""by being told."" Such models might be developed by starting with an architecture for which advice taking is relatively easy, such as one based upon a sentential knowledge representation, and subsequently adding some form of inductive learning mechanism. Alternatively, such models might be grounded in a statistical learning framework appropriately extended to operationalize instruction. This latter approach is taken here. Specifically, connectionist back-propagation networks are made to instantaneously modify their behavior in response to quasi-linguistic advice.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-275.pdf,
276,1994,Student Abstracts,A Comparison of Reinforcement Learning Methods for Automatic Guided Vehicle Scheduling,DoKyeong Ok,Automatic Guided Vehicles or AGVs are increasingly being used in manufacturing plants for transportation tasks. Optimal scheduling of AGVs is a difficult problem. A learning AGV is very attractive in a manufacturing plant since it is hard to manually optimize the scheduling algorithm to each new situation.,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-276.pdf,
277,1994,Student Abstracts,Making the Most of What You've Got: Using Models and Data to Improve Learning Rate and Prediction Accuracy,Julio Ortega,"Prediction and classification in areas such as engineering, medicine, and applied expert systems often relies on two sources of knowledge: actual data and a model of the domain. Recent efforts in machine learning have developed techniques that take advantage of both sources, but the methods are often tied to particular types of models and induction techniques. We propose two general techniques that allow induction methods, C4.5 in our case, to take advantage of an available model.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-277.pdf,
278,1994,Student Abstracts,Learning Quality-Enhancing Control Knowledge,M. Alicia Pérez,"Generating production-quality plans is an essential element in transforming planners from research tools into real-world applications. However most research on planning so far has concentrated on methods for constructing sound and complete planners that find a satisficing solution, and on how to find such solution in an efficient way. Similarly most of the work to date on automated control-knowledge acquisition has been aimed at improving the eficiency of planning; this work has been termed ""speed-up learning"". Our work focuses on how control knowledge may guide a planner towards better plans, and how such control knowledge can be learned. ""Better"" may be defined in a domain-dependent way and vary over time. Perez and Carbonell (1993) contains a detailed taxonomy of plan quality metrics. We have concentrated on metrics related to plan execution cost, expressed as an evaluation function additive on the cost of the individual operators. These functions are linear and do not capture the existence of tradeoffs between different quality factors.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-278.pdf,
279,1994,Student Abstracts,Database Learning for Software Agents,"Mike Perkowitz, Oren Etzioni","With the amount of information available rapidly outstripping the ability of individuals to use it, we wish to explore how a software agent can learn a description of an information resource (such as a database on the internet) in order turn it into a well-understood tool at the agent’s disposal. An agent who could do this would have access to all the information it could find without having to cache the internet.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-279.pdf,
280,1994,Student Abstracts,Diagnosing Multiple Interacting Defects with Combination Descriptions,Nancy E. Reed,"Cases with multiple defects can be difficult to diagnose because the defects can interact, meaning that the observable cues are not a sum of the cues for the component defects. Diagnostic methods that use cue-to-defect relationships fail when interactions between defects change the observable cues. The primary alternative, model-based methods, are limited to domains with accurate and complete models, along with initialization data. Using these traditional methods, when defects interact and models aren’t available, each possible defect combination must be included in the knowledge base. This results in an explosion of possible alternatives, greatly increased knowledge acquisition effort, slower processing, and increased maintenance effort.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-280.pdf,
281,1994,Student Abstracts,Building Emotional Characters for Interactive Drama,W. Scott Reilly,"The Oz project is developing tools to create interactive, dramatic stories (Bates 1992). An important aspect of this work is to develop technology for creating the characters for these stories. We feel it’s critical that these characters appear emotional; just as characters in novels and movies that are unemotional are called ""flat"" and aren' t very believable, we expect the same applies to computer controlled drama as well.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-281.pdf,
282,1994,Student Abstracts,On the Computation of Point of View,Warren Sack,"Previous work in AI story understanding has largely been used to build tools which can summarize stories and categorize them according to the events they describe (e.g., the technologies developed for the Message Understanding Conferences). These sorts of technologies are built around the assumptions that (1) events reported as facts in news stories should be ""understood"" as facts; (2) the style of a story, i.e., the way in which a story is told, is not of interest; and, (3) the source of a story should not influence its analysis. These assumptions are obviously unrealistic. Everyone knows that one should not believe everything in the news. But, by making these simplifying assumptions most existing story understanding systems function as gullible ""readers."" l The focus of my current research is to build a less gullible story understander by encoding in it a means to recognize point of view. The techniques that I am developing will be useful, not only for information retrieval tasks which demand a search for credible stories, but also in future entertainment technologies which will be capable of fiiding and then assembling together into a unified presentation a set of texts or video clips to tell a story fiom an ensemble of points of view.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-282.pdf,
283,1994,Student Abstracts,Multi-Agent Learning in Non-Cooperative Domains,"Mahendra Sekaran, Sandip Sen","Previous work in coordination on multi-agent systems are specific either to cooperative or non-coperative problem domains. Previous work in learning in multi-agent systems have considered agents operating to solve a cooperative task with explicit information sharing and negotiations. In a companion paper we describe a general purpose system which describes a cooperative domain in which two agents work together on a joint task without explicit sharing of knowledge or information. The focus of this poster is to extend this approach to a non-cooperative domain, where the agents have conflicting goals. The strength of this work lies in the fact that there is no explicit knowledge exchange between the agents and no dependencies on agent relationships.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-283.pdf,
284,1994,Student Abstracts,Coalition Formation Methods in Multi-Agent Environments,Onn Shechory,"Autonomous agents are designed to reach goals that were pre-defined by their operators. An important way to execute tasks and to maximize utility is to share resources and to cooperate on task execution by creating coalitions of agents. If the agents are individually rational, such coalitions will take place if, and only if, each member of a coalition gains more if it joins the coalition than it could gain previously. There are several ways of creating such coalitions and dividing the joint payoff among the members. Variation in these methods is due to different environments, different settings in a specific environment, and different approaches to a specific environment with specific settings.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-284.pdf,
285,1994,Student Abstracts,Integrating Specialized Procedures in Proof Systems,Vishal Sikka,"We present the outline of a simple but powerful scheme for describing procedures that can be used by an automatic theorem prover. Our approach is to describe specialized procedures to a theorem prover by adding procedure description axioms to its set of facts, instead of building in these procedures by using attachments. Our work can be viewed as an extension of the hybrid reasoning techniques based on attachments (Myers91). In this abstract we briefly describe our approach and state some of its advantages. In (Sikka94), we present this work in detail and formally show how the full expressibility of attachment-like approaches can be achieved in a simple logic-theoretic way. The work described here is part of the author' s Ph.D. thesis research in collaboration with Prof. Michael Genesereth.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-285.pdf,
286,1994,Student Abstracts,Towards Situated Explanation,"Raja Sooriamurthi, David Leake","In AI research on explanation, the mechanisms used to construct explanations have traditionally been neutral to the environment in which the explanations are sought. Our view is that the explanation process cannot be isolated from the situation in which it occurs. Without considering the intended use for an explanation, the explanation construction process cannot be properly focussed; without considering the situation the process cannot act effectively to gather corroborating information. The emphasis of this research is to view explanation as a means to an end and in this work the end is the successful functioning of the system requesting explanation. We develop a model of explanation as a situated, utility-based, hierarchical, goal-driven process.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-286.pdf,
287,1994,Student Abstracts,Reflective Reasoning and Learning,Eleni Stroulia,"The capability of learning is a prerequisite for autonomy. Autonomous intelligent agents, who solve problems in a realistic environment need to learn in order to extend the classes of problems they can solve, to improve their performance on these problems, and to improve the quality of the solutions they produce. One way in which an intelligent agent may effectively use its experiences to learn, is by reflection upon its own problem-solving process. To do that, the agent needs to have an explicit meta-model of its own reasoning and knowledge. This work takes a functional stance towards reflective learning. This stance gives rise to a specific computational model which is based on three major hy- potheses: (i) agents can be viewed as abstract devices, (ii) their reasoning can be understood in terms of structure-behavior-function (SBF) models, and (iii) learning can be viewed as a self-redesign task in which the agent uses its understanding of its own reasoning to improve its subsequent performance.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-287.pdf,
288,1994,Student Abstracts,Case-Based Reasoning for Weather Prediction,C. Vasudevan,"Computer-based forecasting of weather was first experimented in 1950 at Princeton University. Since then, there have been newer and more accurate methods to predict the incoming climate. One common practice of weather prediction is by using the general circulation models which are based on the laws of physics. These models are highly complex and computational intensive limiting their use for only short range predictions and that too needing supercomputers. The accuracy of forecasting deteriorates rapidly for periods longer than 48 hours and it often becomes minimal beyond 10 days due to imperfections in the models. The analog technique of weather forecasting is another approach which searches for periods in the past when the current conditions were similar and use the past spatial patterns as analogs. Long term trends and recurring events guide the decisions. This is more relevant for long range predictions as well as in single station predictions. The araudog method is relatively simple compared to the complex processes of development, validation, use, and maintenance of numerical models.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-288.pdf,
289,1994,Student Abstracts,Agent Modeling Methods Using Limited Rationality,"José M. Vidal, Edmund H. Durfee","To decide what to do in a multiagent world, an agent should model what others might simultaneously be deciding to do, but that in turn requires modeling what those others might think that others are deciding to do, and so on. The Recursive Modeling Method (RMM) [I] provides representations and algorithms for developing these nested models of beliefs and using them to make rational choices of action. However, because these nested models can involve many branches and recurse deeply, making decisions in time-constrained multiagent worlds requires methods for inexpensive approximation and for metareasoning to balance decision quality with decisionmaking cost.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-289.pdf,
290,1994,Student Abstracts,Learning by Observation and Practice: A Framework for Automatic Acquisition of Planning Operators,Xuemei Wang,The knowledge engineering bottleneck is a central problem in the field of Artificial Intelligence. This work addresses this problem in the context of planning systems. It automatically learns planning operators by observing expert agents and by subsequent knowledge refinement in a learning-by-doing paradigm. Our learning method is implemented on top of the PRODIGY architecture(Carbonell et al. 1992).,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-290.pdf,
291,1994,Student Abstracts,A Modular Visual Tracking System,Mike Wessler,"I am currently building an active visual tracking system for a real world robot. The hardware is being built at MIT under the supervision of Professors Rod Brooks and Lynn Andrea Stein, and is humanoid in form. The software is also humanoid: I am basing its organization on models of early vision in the human brain. Most of the software is still in the design phase; what I describe here is the part of the system that is already up and running.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-291.pdf,
292,1994,Student Abstracts,Utility-Directed Planning,"Mike Williamson, Steve Hanks","Classical AI planning has adopted a very narrow notion of plan quality, namely that a plan is good just in case it achieves a specified goal. Goals provide a valuable point of computational leverage: despite the fact that planning is intractable in the worst case, goal-satisfying planning algorithms can effectively solve classes of problems by using the goal to focus the search for a solution (using backward-chaining techniques), and by exploiting domain-specific heuristic knowledge to control search.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-292.pdf,
293,1994,Student Abstracts,Fuzzy Irrigation Decision Support System,"Hong Xiang, Brahm P. Verma, Gerrit Hoogenboom",Water is a limiting factor in agriculture and when improperly managed reduces the yield potential of crops. The objective of this study is to develop a Fuzzy Irrigation Decision Support System (FIDSS) to optimize water management for soybean production.,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-293.pdf,
294,1994,Student Abstracts,Synthetic Robot Language Development,Holly A. Yanco,"Cooperating robots can benefit from communication. Our robots create their own adaptable synthetic robot languages (ASRLs). We have shown that robots can develop ""basic"", context dependent, and compositional ASRLs using reinforcement learning techniques. (See (Yanco 1994) for a complete description of this work.)",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-294.pdf,
295,1994,Student Abstracts,Computer Simulation of Statistics and Educational Measurement StatSim: An Intelligent Tutoring System for Statistics,"Liu Zhang, Donald Potter",The purpose of this research is to develop an adaptive tutoring system which uses AI techniques to explore how to diagnose student’s misconceptions in problem solving and generate relevant instructions from the context. The system is called StatSim.,https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-295.pdf,
296,1994,Video Program,Guardian: A Prototype Intelligent Agent for Intensive-Care Monitoring,"Barbara Hayes-Roth, Serdar Uckun, Jan Eric Larsson, David Gaba, Juliana Barr, Jane Chien","A surgical intensive care unit (ICU) is a challenging monitoring environment. The multitude of monitored variables, the high frequency of alarms, and the severity of likely complications and emergencies can overload the cognitive skills of even experienced clinicians. ICU monitoring is also complicated by changes in clinical context. Over the course of a few days, a patient may evolve from a high-vigilance immediate post-operative state to a convalescent state that involves entirely different sets of monitoring principles, problems, and treatments.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-296.pdf,
297,1994,Video Program,Dynamic Generation of Complex Behavior,Randolph M. Jones,"Simulation can be an effective training method if the simulation environment is as realistic as possible. An ' important part of the training for Navy pilots involves flying against computer-controlled agents in simulated tactical scenarios. In order for such a situation to be realistic, the computer-controlled agents must be indistinguishable from human-piloted agents within the simulated environment. The primary goal of the Soar IFOR project (Jones et al. 1993; Rosenbloom et al. 1994) is to provide such believable agents for flight training simulations.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-297.pdf,
298,1994,Video Program,HIPAIR: Interactive Mechanism Analysis and Design Using Configuration Spaces,"Leo Joskowicz, Elisha Sacks","We present an interactive problem solving environment for reasoning about shape and motion in mechanism design. Reasoning about shape and motion plays a central role in mechanism design because mechanisms perform functions by transforming motions via part interactions. The input motion, the part shapes, and the part contacts determine the output motion. Designers must reason about the interplay between shape and motion at every step of the design cycle.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-298.pdf,
299,1994,Video Program,ALIVE: Artificial Life Interactive Video Environment,"Pattie Maes, Trevor Darrell, Bruce Blumberg, Sandy Pentland","In this video we demonstrate a novel system which allows wireless full-body interaction between a 'human participant and a graphical world inhabited by autonomous agents. The system is called ""ALIVE"", an acronym for Artificial Life Interactive Video Environment. The goal of ALIVE is to present a virtual environment in which a user can interact, in natural and believable ways, with autonomous semi-intelligent agents whose behavior is equally natural and believable.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-299.pdf,
300,1994,Video Program,A Reading Coach that Listens: (Edited Video Transcript),"Jack Mostow, Alex Hauptmann, Stevn F. Roth, Matt Kane, Adam Swift, Lin Chase, Bob Weide","At Carnegie Mellon University, Project LISTEN' is t'aking a novel approach to the problem of illiteracy. We have developed a prototype automated reading coach that listens to a child read aloud, and helps when needed. The coach provides a combination of reading and listening, in which the child reads wherever possible, and the coach helps wherever necessary -- a bit like training wheels on a bicycle.",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-300.pdf,
301,1994,Video Program,Machine Rhythm,David Rosenthal,"The video discusses Machine Rhythm, a program which emulates human rhythm perception. Given a musical performance represented as a MIDI stream, the program determines the program’s rhythm - that is, it decides the meter of the performance, the rhythmic value of each note, and the location of barlines. The basic orientation of the video is to demonstrate applications of the program; more theoretical aspects are treated in (Rosenthal 1992).",https://aaai.org/Library/AAAI/1994/../../../Papers/AAAI/1994/AAAI94-301.pdf,
