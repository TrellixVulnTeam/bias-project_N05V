,conference_year,category,title,author,abstract,download_url,keywords
0,1998,AAAI-98 Outstanding Papers,Learning Evaluation Functions for Global Optimization and Boolean Satisfiability,"Justin A. Boyan, Andrew W. Moore","This paper describes Stage , a learning approach to automatically improving search performance on optimization problems. Stage learns an evaluation function which predicts the outcome of a local search algorithm, such as hillclimbing or Walksat , as a function of state features along its search trajectories. The learned evaluation function is used to bias future search trajectories toward better optima. We present positive results on six large-scale optimization domains.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-001.pdf,
1,1998,AAAI-98 Outstanding Papers,The Interactive Museum Tour-Guide Robot,"Wolfram Burgard, Armin B. Cremers, Dieter Fox, Dirk Hähnel, Gerhard Lakemeyer, Dirk Schulz, Walter Steiner, Sebastian Thrun","This paper describes the software architecture of an autonomous tour-guide/tutor robot. This robot was recently deployed in the ""Deutsches Museum Bonn,"" were it guided hundreds of visitors through the museum during a six-day deployment period. The robot’s control software integrates low-level probabilistic reasoning with high-level problem solving embedded in first order logic. A collection of software innovations, described in this paper, enabled the robot to navigate at high speeds through dense crowds, while reliably avoiding collisions with obstacles--some of which could not even be perceived. Also described in this paper is a user interface tailored towards non-expert users, which was essential for the robot’s success in the museum. Based on these experiences, this paper argues that time is ripe for the development of AI-based commercial service robots that assist people in everyday life.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-002.pdf,
2,1998,AAAI-98 Outstanding Papers,Acceleration Methods for Numeric CSPs,"Yahia Lebbah, Olivier Lhomme","This paper introduces a new way of accelerating the convergence of numeric CSP filtering algorithms, through the use of extrapolation methods. Extrapolation methods are used in numerical analysis to accelerate the convergence of real number sequences. We will show how to use them for solving numeric CSPs, leading to drastic improvement in efficiency.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-003.pdf,
3,1998,Agent Interaction,Minimal Social Laws,"David Fitoussi, Moshe Tennenholtz","Research on social laws in computational environments has proved the usefulness of the law-based approach for the coordination of multi-agent systems. Though researchers have noted that the imposition of a specification could be attained by a variety of different laws, there has been no attempt to identify a criterion for selection among alternative useful social laws. We propose such a criterion which is based on the notion of minimality. A useful social law puts constraints on the agents’ actions in such away that as a result of these constraints, they are able to achieve their goals. A minimal social law is a useful social law that minimizes the amount of constraints the agents shall obey. Minimal social laws give an agent maximal exibility inchoosing a new behavior as a function of various local changes either in his capabilities or in his objectives, without interfering with the other agents. We show that this concept can be usefully applied to a problem in robotics and present a computational study of minimal social laws.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-004.pdf,
4,1998,Agent Interaction,Optimal Auctions Revisited,"Dov Monderer, Moshe Tennenholtz","The theory of mechanism design in economics/game theory deals with a center who wishes to maximize an objective function which depends on a vector of information variables. The value of each variable is known only to a selfish agent, which is not controlled by the center. In order to obtain its objective the center constructs a game, in which every agent participates and reveals its information, because these actions maximize its utility. However, several crucial new issues arise when one tries to transform existing economic mechanisms into protocols to be used in computational environments. In this paper we deal with two such issues: 1. The communication structure, and 2. the representation (syntax) of the agents’ information. The existing literature on mechanism design implicitly assumes that these two features are not relevant. In particular, it assumes a communication structure in which every agent is directly connected to the center. We present new protocols that can be implemented in a large variety of communication structures, and discuss the sensitivity of these protocols to the way in which information is presented.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-005.pdf,
5,1998,Formal Models of Agents’ Commitments,Leveled Commitment Contracts with Myopic and Strategic Agents,"Martin R. Andersson, Tuomas W. Sandholm","In automated negotiation systems consisting of self-interested agents, contracts have traditionally been binding, i.e., impossible to breach. Such contracts do not allow the agents to efficiently deal with future events. This deficiency can be tackled by using a leveled commitment contracting protocol which allows the agents to decommit from contracts by paying a monetary penalty to the contracting partner. The efficiency of such protocols depends heavily on how the penalties are decided. In this paper, different leveled commitment protocols and their parameterizations are empirically compared to each other and to several full commitment protocols. In the different experiments, the agents are of different types: self-interested or cooperative, and they can perform different levels of lookahead. Surprisingly, self-interested myopic agents reach a higher social welfare quicker than cooperative myopic agents when decommitment penalties are low. The social welfare in settings with agents that performed lookahead did not vary as much with the decommitment penalty as the social welfare in settings that consisted of myopic agents. For a short range of values of the decommitment penalty, myopic agents performed almost as well as agents that performed lookahead. In all of the settings studied, the best way to set the decommitment penalties was to choose low penalties, but ones that were greater than zero. This indicates that leveled commitment contracting protocols outperform both full commitment protocols and commitment free protocols.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-006.pdf,
6,1998,Formal Models of Agents’ Commitments,Anytime Coalition Structure Generation with Worst Case Guarantees,"Tuomas Sandholm, Kate Larson, Martin Andersson, Onn Shehory, Fernando Tohmé","Coalition formation is a key topic in multiagent systems. One would prefer a coalition structure that maximizes the sum of the values of the coalitions, but often the number of coalition structures is too large to allow exhaustive search for the optimal one. But then, can the coalition structure found via a partial search be guaranteed to be within a bound from optimum? We show that none of the previous coalition structure generation algorithms can establish any bound because they search fewer nodes than a threshold that we show necessary for establishing a bound. We present an algorithm that establishes a tight bound within this minimal amount of search, and show that any other algorithm would have to search strictly more. The fraction of nodes needed to be searched approaches zero as the number of agents grows. If additional time remains, our anytime algorithm searches further, and establishes a progressively lower tight bound. Surprisingly, just searching one more node drops the bound in half. As desired, our algorithm lowers the bound rapidly early on, and exhibits diminishing returns to computation. It also drastically outperforms its obvious contenders. Finally, we show how to distribute the desired search across self-interested manipulative agents.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-007.pdf,
7,1998,Motivation and Emotion,A Motivational System for Regulating Human-Robot Interaction,Cynthia Breazeal (Ferrell),"This paper presents a motivational system for an autonomous robot which is designed to regulate human-robot interaction. The mode of social interaction is that of a caretaker-infant dyad where a human acts as the caretaker for the robot. An infant’s emotions and drives play a very important role in generating meaningful interactions with the caretaker, and regulating these interactions to maintain an environment suitable for the learning process (Bullowa 1979). Similarly, the learning task for the robot is to apply various communication skills acquired during social exchanges to manipulate the caretaker such that its drives are satisfied. Toward this goal, the motivational system implements drives , emotions , and facial expressions. Although the details of the learning itself are beyond the scope of this paper, this work represents an important step toward realizing robots that can engage in meaningful bi-directional social interactions with humans.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-008.pdf,
8,1998,Motivation and Emotion,Emotion Model for Life-Like Agent and Its Evaluation,"Hirohide Ushida, Yuji Hirayama, Hiroshi Nakajima","This paper proposes an emotion model for life-like agents with emotions and motivations. This model consists of reactive and deliberative mechanisms. The former generates low-level instantaneous responses to external stimuli that come from the real world and virtual worlds. The latter mechanism especially focuses on emotions. A basic idea of the model comes from a psychological theory, called the cognitive appraisal theory. In the model, cognitive and emotional processes interact with each other based on the theory. A multi-module architecture is employed in order to carry out the interactions. The model also has a learning mechanism to diversify behavioral patterns. These features are effective in giving users the illusion of life. We applied the proposed model to characters in a virtual world and show the results obtained from three experiments with users.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-009.pdf,
9,1998,Motivation and Emotion,When Robots Weep: Emotional Memories and Decision-Making,Juan D. Velásquez,"We describe an agent architecture that integrates emotions, drives, and behaviors, and that focuses on modeling some of the aspects of emotions as fundamental components within the process of decision-making. We show how the mecha-nisms of primary emotions can be used as building blocks for the acquisition of emotional memories that serve as biasing mechanisms during the process of making decisions and selecting actions. The architecture has been implemented into an object-oriented framework that has been successfully used to develop and control several synthetic agents and which is currently being used as the control system for an emotional pet robot.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-010.pdf,
10,1998,Parallel AI / Agents and Representation,Natural Language Multiprocessing: A Case Study,"Enrico Pontelli, Gopal Gupta, Janyce Wiebe, David Farwell","This paper presents two case studies of parallelization of large Natural Language Processing (NLP) applications using a parallel logic programming system (called ""ACE"") that automatically exploits implicit parallelism. The first system considered is Artwork, a system for semantic disambiguation, speech act resolution, and temporal reference resolution. The second system is ULTRA, a multilingual translation system. Both applications were originally developed in Prolog without any consideration for parallel processing. The results obtained confirm that NLP is a ripe area for exploitation of parallelism. Most previous work on parallelism in NLP focused primarily on parallelizing the parsing phase of language processing. The case studies presented here show that parallelism is also present in the semantic and discourse processing phases, which are often the most computationally intensive part of the application.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-011.pdf,
11,1998,Parallel AI / Agents and Representation,Metacognition in Software Agents Using Classifier Systems,"Zhaohua Zhang, Stan Franklin, Dipankar Dasgupta","Software agents ""living"" and acting in a real world software environment, such as an operating system, a network, or a database system, can carry out many tasks for humans. Metacognition is very important for humans. It guides people to select, evaluate, revise, and abandon cognitive tasks, goals, and strategies. Thus, metacognition plays an important role in human-like software agents. Metacognition includes metacognitive knowledge, metacognitive monitoring, and metacognitive regulation. Conscious Mattie (CMattie), ""living"" in a Unix machine, automatically reads and understands email concerning seminars (in natural language), and composes and distributes weekly seminar schedule announcements. CMattie implements Baar’s global workspace theory of consciousness and some other cognitive theories concerning metacognition, episodic memory, emotions, and learning. Thus, the CMattie project has its cognitive science side (cognitive modeling) as well as its computer science side (intelligent software). This paper describes a case study of the design and implementation of modeling metacognition in software agents like CMattie by using a classifier system.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-012.pdf,
12,1998,Social Agents,Agents that Work in Harmony by Knowing and Fulfilling their Obligations,Mihai Barbuceanu,"Societies constrain the behavior of agents by imposing multiple, often contradictory, obligations and interdictions amongst them. To work in harmony, agents must find ways to satisfy these constraints, or to break less important ones when necessary. In this paper, we present a solution to this problem based on a representation of obligations and interdictions in an organizational framework, together with an inference method that also decides which obligations to break in contradictory situations. These are integrated in an operational, practically useful agent development language that covers the spectrum from defining organizations, roles, agents, obligations, goals, conversations to inferring and executing coordinated agent behaviors in multi-agent applications. One strength of the approach is the way it supports negotiation by exchanging deontic constraints amongst agents. We illustrate this and the entire system with a negotiated solution to the feature interaction problem in the telecommunications industry.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-013.pdf,
13,1998,Social Agents,What Is Wrong With Us? Improving Robustness through Social Diagnosis,"Gal A. Kaminka, Milind Tambe","Robust behavior in complex, dynamic environments mandates that intelligent agents autonomously monitor their own run-time behavior, detect and diagnose failures, and attempt recovery. This challenge is intensified in multi-agent settings, where the coordinated and competitive behaviors of other agents affect an agent’s own performance. Previous approaches to this problem have often focused on single agent domains and have failed to address or exploit key facets of multi-agent domains, such as handling team failures. We present SAM, a complementary approach to monitoring and diagnosis for multi-agent domains that is particularly well-suited for collaborative settings. SAM includes the following key novel concepts: First, SAM’s failure detection technique, inspired by social psychology, utilizes other agents as information sources and detects failures both in an agent and in its teammates. Second, SAM performs social diagnosis, reasoning about the failures in its team using an explicit model of teamwork (previously, teamwork models have been employed only in prescribing agent behaviors in teamwork). Third, SAM employs model sharing to alleviate the inherent inefficiencies associated with representing multiple agent models. We have implemented SAM in a complex, realistic multi-agent domain, and provide detailed empirical results assessing its benefits.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-014.pdf,
14,1998,AI and Education,Procedural Help in Andes: Generating Hints Using a Bayesian Network Student Model,"Abigail S. Gertner, Cristina Conati, Kurt VanLehn","One of the most important problems for an intelligent tutoring system is deciding how to respond when a student asks for help. Responding cooperatively requires an understanding of both what solution path the student is pursuing, and the student’s current level of domain knowledge. Andes, an intelligent tutoring system for Newtonian physics, refers to a probabilistic student model to make decisions about responding to help requests. Andes’ student model uses a Bayesian network that computes a probabilistic assessment of three kinds of information: (1) the student’s general knowledge about physics, (2) the student’s specific knowledge about the current problem, and (3) the abstract plans that the student may be pursuing to solve the problem. Using this model, Andes provides feedback and hints tailored to the student’s knowledge and goals.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-015.pdf,
15,1998,AI and Education,Generating Coordinated Natural Language and 3D Animations for Complex Spatial Explanations,"Stuart G. Towns, Charles B. Callaway, James C. Lester","Dynamically providing students with clear explanations of complex spatial concepts is critical for a broad range of knowledge-based educational and training systems. This calls for a realtime solution that can dynamically create 3D animated explanations that artfully integrate well-chosen speech with rich visualizations. Unfortunately, planning the integrated creation of 3D animation and spatial linguistic utterances in realtime requires coordinating the visual presentation of 3D objects and generating appropriate spatial phrases that accurately reflect the relative position, orientation, and direction of the objects presented. We present a visuo-linguistic framework for generating multimedia spatial explanations combining 3D animation and speech that complement one another. Because 3D animation planners require spatial knowledge in a geometric form and natural language generators require spatial knowledge in a linguistic form, a realtime multimedia planner interposed between the visual and linguistic components can serve as a mediator. This framework has been implemented in CineSpeak , a multimedia explanation generator consisting of a visuo-linguistic mediator, a 3D animation planner, and a realtime natural language generator with a speech synthesizer. CineSpeak has been used in conjunction with a prototype 3D learning environment in the domain of physics to generate realtime multimedia explanations of three dimensional electromagnetic fields, forces, and electrical current.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-016.pdf,
16,1998,Belief Revision and Inconsistency,Reasoning under Inconsistency Based on Implicitly-Specified Partial Qualitative Probability Relations: A Unified Framework,"S. Benferhat, D. Dubois, J. Lang, H. Prade, A. Saffiotti, P. Smets","Coherence-based approaches to inconsistency handling proceed by selecting preferred consistent subbases of the belief base according to a predefined method which takes advantage of explicitly stated priorities. We propose here a general framework where the preference relation between subsets of the belief base is induced by a system of constraints directly expressed by the user. Postulates taking their source in the qualitative modelling of uncertainty, either probabilistic or possibilistic, are used for completing the implicit specification of the preference relations. This enables us to define various types of preference relations, including as particular cases several well-known systems such as Brewka’s preferred sub-theories or the lexicographical system. Since the number of preferred consistent subbases may be prohibitive, we propose to compile the inconsistent belief base into a new one from which it is easier to select one preferred consistent subbase.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-017.pdf,
17,1998,Belief Revision and Inconsistency,Belief Revision with Unreliable Observations,"Craig Boutilier, Nir Friedman, Joseph Y. Halpern","Research in belief revision has been dominated by work that lies firmly within the classic AGM paradigm, characterized by a well-known set of postulates governing the behavior of ""rational"" revision functions. A postulate that is rarely criticized is the success postulate: the result of revising by an observed proposition * results in belief in *. This postulate, however, is often undesirable in settings where an agent’s observations may be imprecise or noisy. We propose a semantics that captures a new ontology for studying revision functions, which can handle noisy observations in a natural way while retaining the classical AGM model as a special case. We present a characterization theorem for our semantics, and describe a number of natural special cases that allowease of specification and reasoning with revision functions. In particular, by making the Markov assumption, we can easily specify and reason about revision.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-018.pdf,
18,1998,Design and Diagnosis,Toward Design as Collaboration,Susan L. Epstein,"In design, multiple disparate goals must be addressed simultaneously. It is the thesis of this work that problems in two-dimensional layout design can be solved by collaboration among single-goal, intelligent agents, each responsible for a class of objects and responsive to explicit metrics. In this model, each agent produces conflict-free designs for its own class of objects, and then, when objects conflict with each other in the combined design, the agents that own those objects address the conflicts. A limitedly rational implementation demonstrates its efficacy for park layout design in the two-dimensional plane.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-019.pdf,
19,1998,Design and Diagnosis,An Architecture for Exploring Large Design Spaces,"John R. Josephson, B. Chandrasekaran, Mark Carroll, Naresh Iyer, Bryon Wasacz, Giorgio Rizzoni, Qingyuam Li, David A. Erb","We describe an architecture for exploring very large design spaces, for example, spaces that arise when design candidates are generated by combining components systematically from component libraries. A very large number of candidates are methodically considered and evaluated. This architecture is especially appropriate during the stage of conceptual design when high-level design decisions are under consideration, multiple evaluation criteria apply, and a designer seeks assurance that good design possibilities have not been overlooked. We present a filtering technique based on a dominance criterion that can be used to select, from millions of design candidates, a relatively small number of promising candidates for further analysis. The dominance criterion is lossless in that it insures that each candidate not selected is inferior to at least one of the selected candidates. We also describe an interactive interface in which the selected designs are presented to the designer for analysis of tradeoffs and further exploration. In our current implementation, the computational load is distributed among a large number of workstations in a client-server computing environment. We describe the results of experiments using the architecture to explore designs for hybrid electric vehicles. In a recent experiment more than two million distinct designs were evaluated.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-020.pdf,
20,1998,Design and Diagnosis,Constructing the Correct Diagnosis When Symptoms Disappear,Nancy E. Reed,"When multiple defects (also called diseases or faults) are present, there is a possibility of interactions between the defects. When defects interact, the cues (data obtainable) for a combination of defects is not a simple sum of the cues observable for the component defects. Expected cues may be missing, altered, or new cues may appear. Each of these alterations of cues makes diagnosis more difficult, as the correct defect combination may not even be considered (triggered) by a diagnostic system. We present an algorithm for heuristic solution construction that integrates multiple types of information about the case. Solutions are evaluated based on how many of the abnormal cues are accounted for, with a method that combines cues that may be altered due to interactions between defects. The method can account for cues that combine with one another in three basic ways, set union, additively and ordered dominance (some values mask other values) or with a combination of those basic ways. For the solution space of one task, diagnosing congenital heart defects, we considered seven major defects and found the solution space (exhaustive) was reduced by approximately 50% because some of the defects could not physically occur together. Experimental results on cases from hospital files demonstrate the effectiveness of the heuristic solution construction algorithm to generate the correct solution early which reduced the number of solutions explored (compared to an exhaustive search) even further on most cases. With the computational power of current workstations, even cases requiring exploration of this entire solution space required less than 4 minutes of CPU time per case.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-021.pdf,
21,1998,Graphical Probabilistic Models,Structured Representation of Complex Stochastic Systems,"Nir Friedman, Daphne Koller, Avi Pfeffer","This paper considers the problem of representing complex systems that evolve stochastically over time. Dynamic Bayesian networks provide a compact representation for stochastic processes. Unfortunately, they are often unwieldy since they cannot explicitly model the complex organizational structure of many real life systems: the fact that processes are typically composed of several interacting subprocesses, each of which can, in turn, be further decomposed. We propose a hierarchically structured representation language which extends both dynamic Bayesian networks and the object-oriented Bayesian network framework of [9], and showthat our language allows us to describe such systems in a natural and modular way. Our language supports a natural representation for certain system characteristics that are hard to capture using more traditional frameworks. For example, it allows us to represent systems where some processes evolve at a different rate than others, or systems where the processes interact only intermittently. We provide a simple inference mechanism for our represen-tation via translation to Bayesian networks, and suggest ways in which the inference algorithm can exploit the additional structure encoded in our representation.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-022.pdf,
22,1998,Graphical Probabilistic Models,Solving Very Large Weakly Coupled Markov Decision Processes,"Nicolas Meuleau, Milos Hauskrecht, Kee-Eung Kim, Leonid Peshkin, Leslie Pack Kaelbling, Thomas Dean, Craig Boutilier","We present a technique for computing approximately optimal solutions to stochastic resource allocation problems modeled as Markov decision processes (MDPs). We exploit two key properties to avoid explicitly enumerating the very large state and action spaces associated with these problems. First, the problems are composed of multiple tasks whose utilities are independent. Second, the actions taken with respect to (or resources allocated to) a task do not influence the status of any other task. We can therefore view each task as an MDP. However, these MDPs are weakly coupled by resource constraints: actions selected for one MDP restrict the actions available to others. We describe heuristic techniques for dealing with several classes of constraints that use the solutions for individual MDPs to construct an approximate global solution. We demonstrate this technique on problems involving thousandsof tasks, approximating the solution to problems that are far beyond the reach of standard methods.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-023.pdf,
23,1998,Graphical Probabilistic Models,Speech Recognition with Dynamic Bayesian Networks,"Geoffrey Zweig, Stuart Russell","Dynamic Bayesian networks (DBNs) are a useful tool for representing complex stochastic processes. Recent developments in inference and learning in DBNs allow their use in real-world applications. In this paper, we apply DBNs to the problem of speech recognition. The factored state representation enabled by DBNs allows us to explicitly represent long-term articulatory and acoustic context in addition to the phonetic-state information maintained by hidden Markov models (HMMs). Furthermore, it enables us to model the short-term correlations among multiple observation streams within single time-frames. Given a DBN structure capable of representing these long- and short-term correlations, we applied the EM algorithm to learn models with up to 500,000 parameters. The use of structured DBN models decreased the error rate by 12 to 29% on a large-vocabulary isolated-word recognition task, compared to a discrete HMM; it also improved significantly on other published results for the same task. This is the first successful application of DBNs to a large-scale speech recognition problem. Investigation of the learned models indicates that the hidden state variables are strongly correlated with acoustic properties of the speech signal.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-024.pdf,
24,1998,Model Construction and Analysis,Multimodal Reasoning for Automatic Model Construction,"Reinhard Stolle, Elizabeth Bradley","This paper describes a program called Pret that automates system identification, the process of finding a dynamical model of a black-box system. Pret performs both structural identification and parameter estimation by integrating several reasoning modes: qualitative reasoning, qualitative simulation, numerical simulation, geometric reasoning, constraint reasoning, resolution, reasoning with abstraction levels, declarative meta-level control, and a simple form of truth maintenance. Unlike other modeling programs that map structural or functional descriptions to model fragments, Pret combines hypotheses about the mathematics involved into candidate models that are intelligently tested against observations about the target system. We give two examples of system identification tasks that this automated modeling tool has successfully performed. The first, a simple linear system, was chosen because it facilitates a brief and clear presentation of Pret’s features and reasoning techniques. In the second example, a difficult real-world modeling task, we show how Pret models a radio-controlled car used in the University of British Columbia’s soccer-playing robot project.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-025.pdf,
25,1998,Model Construction and Analysis,Discovering Admissible Simultaneous Equations of Large Scale Systems,"Takashi Washio, Hiroshi Motoda","SSF is a system to discover the structure of simultaneous equations governing an objective process through experiments. SSF combined with another system SDS to discover a quantitative formula of a complete equation derives the quantitative model consisting of simultaneous equations reflecting the first principles underlying in the objective process. The power of SSF comes from the use of the complete subset structure in a set of simultaneouls equations which can be experimentally identified. The theoretical foundations of the structure identification and the algorithm of SSF are described, and its efficiency and practicality are demonstrated and discussed with large scale working examples. This work is to promote the research of scientific discovery to a novel and promising direction, since the conventional equation discovery systems could not handle such a simultaneous equation process.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-026.pdf,
26,1998,Model Construction and Analysis,"Decompositional, Model-Based Learning and its Analogy to Diagnosis","Brian C. Williams, William Millar","A new generation of sensor rich, massively distributed autonomous system is being developed, such as smart buildings and reconfigurable factories. To achieve high performance these systems will need to accurately model themselves and their environment from sensor information. Accomplishing this on a grand scale requires automating the art of large-scale modeling. To this end we have developed decompositional, model-based learning (DML). DML takes a parameterized model and sensed variables as input, decomposes it, and synthesizes a coordinated sequence of ""simplest"" estimation tasks. The method exploits a rich analogy between parameter estimation and consistency-based diagnosis. Moriarty, an implementation of DML, has been applied to thermal modeling of a smart building, demonstrating a significant improvement in learning rate.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-027.pdf,
27,1998,Modeling the Web,What Can Knowledge Representation Do for Semi-Structured Data?,"Diego Calvanese, Giuseppe De Giacomo, Maurizio Lenzerini","The problem of modeling semi-structured data is important in many application areas such as multimedia data management, biological databases, digital libraries, and data integration. Graph schemas (Buneman et al. 1997) have been proposed recently as a simple and elegant formalism for representing semistructured data. In this model, schemas are represented as graphs whose edges are labeled with unary formulae of a theory, and the notions of conformance of a database to a schema and of subsumption between two schemas are defined in terms of a simulation relation. Several authors have stressed the need of extending graph schemas with various types of constraints, such as edge existence and constraints on the number of outgoing edges. In this paper we analyze the appropriateness of various knowledge representation formalisms for representing and reasoning about graph schemas extended with constraints. We argue that neither First Order Logic, nor Logic Programming nor Frame-based languages are satisfactory for this purpose, and present a solution based on very expressive Description Logics. We provide techniques and complexity analysis for the problem of deciding schema subsumption and conformance in various interesting cases, that differ by the expressive power in the specification of constraints.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-028.pdf,
28,1998,Modeling the Web,Modeling Web Sources for Information Integration,"Craig A. Knoblock, Steven Minton, Jose Luis Ambite, Naveen Ashish, Pragnesh Jay Modi, Ion Muslea, Andrew G. Philpot, Sheila Tejada","The Web is based on a browsing paradigm that makes it difficult to retrieve and integrate data from multiple sites. Today, the only the only way to do this is to build specialized applications, which are time-consuming to develop and difficult to maintain. We are addressing this problem by creating the technology and tools for rapidly constructing information agents that extract, query, and integrate data from web sources. Our approach is based on a simple, uniform representation that makes it efficient to integrate multiple sources. Instead of building specialized algorithms for handling web sources, we have developed methods for mapping web sources into this uniform representation. This approach builds on work from knowledge representation, machine learning and automated planning. The resulting system, called Ariadne, makes it fast and cheap to build new information agents that access existing web sources. Ariadne also makes it easy to maintain these agents and incorporate new sources as they become available.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-029.pdf,
29,1998,Qualitative Modeling,An Ontology for Transitions in Physical Dynamic Systems,"Pieter J. Mosterman, Feng Zhao, Gautam Biswas","Physical systems often exhibit complex nonlinear behaviors in continuous time at multiple temporal and spatial scales. Abstractions simplify behavioral analysis and help focus on dominant system behaviors by defining sets of equivalent behavior types called modes . System behavior evolves in continuous modes with discrete transitions between modes. Subtle interactions between the continuous behaviors and discrete transitions need to be captured by well-defined hybrid modeling and analysis semantics. This paper presents a taxonomy of transition modes, and develops a formal semantics for transition conditions that lead to efficient and physically consistent simulation algorithms for physical systems.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-030.pdf,
30,1998,Qualitative Modeling,A New Architecture for Automated Modelling,Neil Smith,"Existing automated modelling systems either rely on large, complex libraries or require complete access to the modelled system’s behaviour, neither of which is desirable, To address these problems, a simpler architecture for modelling knowledge is described, based on the separation between ideal models of components and corrections that can be applied to these idea1 models. The use of this architecture to develop accurate model boundaries is described, based on consideration of interactions within such ideal models. A novel algorithm for refining models is also proposed. This algorithm considers behavioural differences between models and applies the corrections that cause the greatest differences in behaviour. Finally, some models generated by this method are shown to be parsimonious.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-031.pdf,
31,1998,Qualitative Reasoning Techniques,Qualitative Analysis of Distributed Physical Systems with Applications to Control Synthesis,"Christopher Bailey-Kellogg, Feng Zhao","Many important physical phenomena, such as temperature distribution, air flow, and acoustic waves, are described as continuous, distributed parameter fields. Analyzing and controlling these physical processes and systems are common tasks in many scientific and engineering domains. However, the challenges are multifold: distributed fields are conceptually harder to reason about than lumped parameter models; computational methods are prohibitively expensive for complex spatial domains; the underlying physics imposes severe constraints on observability and controllability. This paper develops an ontological abstraction and a structure-based design mechanism, in a framework collectively known as spatial aggregation (SA), for reasoning about and synthesizing distributed control schemes for physical fields. The ontological abstraction models a physical field as a hierarchy of networks of spatial objects. SA applies a small number of generic operators to a field to compute concise structural descriptions such as iso-contours, gradient trajectories, and influence graphs. The design mechanism uses these representations to find feasible control configurations. We illustrate the mechanism using a thermal control problem from industrial heat treatment and demonstrate that the active exploitation of structural knowledge in physical fields yields a significant computational advantage.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-032.pdf,
32,1998,Qualitative Reasoning Techniques,Qualitative Simulation as a Temporally-Extended Constraint Satisfaction Problem,"Daniel J. Clancy, Benjamin J. Kuipers","Traditionally, constraint satisfaction problems (CSPs) are characterized using a finite set of constraints expressed within a common, shared constraint language. When reasoning across time, however, it is possible to express both temporal and state{based constraints represented within multiple constraint languages. Qualitative simulation provides an instance of this class of CSP in which, traditionally, all solutions to the CSP are computed. In this paper, we formally describe this class of temporally{extended CSPs and situate qualitative simulation within this description. This is followed by a description of the DecSIM algorithm which is used to incrementally generate all possible solutions to a temporally{extended CSP. DecSIM combines problem decomposition, a tree-clustering algorithm and ideas similar to directed arc-consistency to exploit structure and causality within a qualitative model resulting in an exponential speed-up in simulation time when compared to existing techniques.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-033.pdf,
33,1998,Temporal Reasoning,Backtracking Algorithms for Disjunctions of Temporal Constraints,"Kostas Stergiou, Manolis Koubarakis","We extend the framework of simple temporal problems studied originally by Dechter, Meiri and Pearl to consider constraints of the form x1 - y1 < or = r1 V...V xn - yn < or = rn; where x1...xn, y1...yn are variables ranging over the real numbers; r1...rn are real constants; and n < or = 1. We have implemented four progressively more efficient algorithms for the consistency checking problem for this class of temporal constraints. We have partially ordered those algorithms according to the number of visited search nodes and the number of performed consistency checks. Finally, we have carried out a series of experiments on the location of the hard region. The results show that the hard problems occur at a critical value of the ratio of disjunctions to variables. This value is between 6 and 7.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-034.pdf,
34,1998,Temporal Reasoning,Fast Transformation of Temporal Plans for Efficient Execution,"Ioannis Tsamardinos, Nicola Muscettola, Paul Morris","Temporal plans permit significant exibility in specifying the occurrence time of events. Plan execution can make good use of that exibility. However, the advantage of execution exibility is counterbalanced by the cost during execution of propagating the time of occurrence of events throughout the exible plan. To minimize execution latency, this propagation needs to be very efficient. Previous work showed that every temporal plan can be reformulated as a dispatchable plan, i.e., one for which propagation to immediate neighbors is sufficient. A simple algorithm was given that finds a dispatchable plan with a minimum number of edges in cubic time and quadratic space. In this paper, we focus on the efficiency of the reformulation process, and improve on that result. A new algorithm is presented that uses linear space and has time complexity equivalent to Johnson’s algorithm for all-pairs shortest-path problems. Experimental evidence confirms the practical effectiveness of the new algorithm. For example, on a large commercial application, the performance is improved by at least two orders of magnitude. We further show that the dispatchable plan, already minimal in the total number of edges, can also be made minimal in the maximum number of edges incoming or outgoing at any node.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-035.pdf,
35,1998,Theorem Proving,An Algorithm to Evaluate Quantified Boolean Formulae,"Marco Cadoli, Andrea Giovanardi, Marco Schaerf","The high computational complexity of advanced reasoning tasks such as belief revision and planning calls for efficient and reliable algorithms for reasoning problems harder than NP. In this paper we propose Evaluate , an algorithm for evaluating Quantified Boolean Formulae, a language that extends propositional logic in a way such that many advanced forms of propositional reasoning, e.g., reasoning about knowledge, can be easily formulated as evaluation of a QBF. Algorithms for evaluation of QBFs are suitable for the experimental analysis on a wide range of complexity classes, a property not easily found in other formalisms. Evaluate is based on a generalization of the Davis-Putnam procedure for SAT, and is guaranteed to work in polynomial space. Before presenting Evaluate , we discuss all the abstract properties of QBFs that we singled out to make the algorithm more efficient. We also briefly mention the main results of the experimental analysis, which is reported elsewhere.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-036.pdf,
36,1998,Theorem Proving,Two Forms of Dependence in Propositional Logic: Controllability and Definability,"Jérôme Lang, Pierre Marquis","We investigate two forms of dependence between variables and/or formulas within a propositional knowledge base: controllability (a set of variables C controls a formula G if there is a way to fix the truth value of the variables in C in order to achieve G to have a prescribed truth value) and definability (C defines a variable y if every truth assignment of the variables in C enables us finding out the truth value of y). Several characterization results are pointed out, complexity issues are analyzed, and some applications of both notions, including decision under incomplete knowledge and/or partial observability, and hypothesis discrimination, are sketched.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-037.pdf,
37,1998,Theorem Proving,Anytime Approximate Modal Reasoning,Fabio Massacci,"Propositional modal logics have two independent sources of complexity: unbounded logical omniscience and unbounded logical introspection. This paper discusses an approximation method to tame both of them, by merging propositional approximations with a new technique tailored for multi-modal logics. It provides both skeptical and credulous approximations (or approximation that are neither of the two). On this semantics we build an anytime proof procedure with a simple modification to classical modal tableaux. The procedure yields approximate proofs whose precision increases as we have more resources (time, space etc.) and we analyze its semantical and computational ""quality guarantees"".",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-038.pdf,
38,1998,Tractable Inference,Algorithms for Propositional KB Approximation,Yacine Boufkhad,"One of the obstacles to the effective compilation of propositional knowledge bases (KBs) using Horn approximations, as introduced by (Selman & Kautz 1991), is the lack of computationally feasible methods for generating Horn bounds. In this paper new algorithms for generating Horn Greatest Lower Bounds (GLB) that can apply to large size KBs, are presented. The approach is extended through a more general target language: the renamable Horn class. The conditions under which a renamable Horn formula is a renamable Horn GLB of a KB are established and algorithms for computing it are derived. These algorithms can be used in the other approaches based on computation of Horn or renamable lower bounds as (Boufkhad et al. 1997). The efficiency of these algorithms and the tightness with respect to the KB in terms of number of models of the bounds, are experimentally evaluated. The renamable Horn GLB proves to be closer to the KB than the Horn GLB.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-039.pdf,
39,1998,Tractable Inference,A Non-Deterministic Semantics for Tractable Inference,"James M. Crawford, David W. Etherington","Unit resolution is arguably the most useful known algorithm for tractable reasoning in propositional logic. However, devising a tractable semantics that allows unit resolution has proven to be an elusive goal. We propose a 3-valued semantics for a tractable fragment of propositional logic that is inherently non-deterministic: the denotation of a formula is not uniquely determined by the denotation of the variables it contains. We show that this semantics yields a tractable, sound and complete, decision procedure. We generalize this semantics to a family of semantics, tied to Dalal’s notion of intricacy, of increasing deductive power and computational complexity.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-040.pdf,
40,1998,Tractable Inference,Computing Intersections of Horn Theories for Reasoning with Models,"Thomas Eiter, Toshihide Ibaraki, Kazuhisa Makino","We consider computational issues in combining logical knowledge bases represented by their characteristic models; in particular, we study taking their logical intersection. We present efficient algorithms or prove intractability for the major computation problems for Horn knowledge bases. We also consider an extension of Horn theories, for which negative results are obtained. They indicate that generalizing the positive results beyond Horn theories is not immediate.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-041.pdf,
41,1998,Analysis of Search,The Branching Factor of Regular Search Spaces,"Stefan Edelkamp, Richard E. Korf","Many problems, such as the sliding-tile puzzles, generate search trees where dfferent nodes have different numbers of children, in this case depending on the position of the blank. We show how to calculate the asymptotic branching factors of such problems, and how to efficiently compute the exact numbers of nodes at a given depth. This information is important for determining the complexity ofvarious search algorithms on these problems. In addition to the sliding-tile puzzles, we also apply our technique to Rubik’s Cube. While our techniques are fairly straightforward, the literature is full of incorrect branching factors for these problems, and the errors in several incorrect methods are fairly subtle.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-042.pdf,
42,1998,Analysis of Search,Complexity Analysis of Admissible Heuristic Search,"Richard E. Korf, Michael Reid","We analyze the asymptotic time complexity of admissible heuristic search algorithms such as A*, IDA*, and depth-first branch-and-bound. Previous analyses relied on an abstract analytical model, and characterize the heuristic function in terms of its accuracy, but do not apply to real problems. In contrast, our analysis allows us to accurately predict the performance of these algorithms on problems such as the sliding-tile puzzles and Rubik’s Cube. The heuristic function is characterized simply by the distribution of heuristic values in the problem space. Contrary to conventional wisdom, our analysis shows that the asymptotic heuristic branching factor is the same as the bruteforce branching factor, and that the effect of a heuristic function is to reduce the effective depth of search, rather than the effective branching factor.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-043.pdf,
43,1998,Constraint Satisfaction Problems,On the Conversion between Non-Binary and Binary Constraint Satisfaction Problems,"Fahiem Bacchus, Peter van Beek","It is well known that any non-binary discrete constraint satisfaction problem (CSP) can be translated into an equivalent binary CSP. Two translations are known: the dual graph translation and the hidden variable translation. However, there has been little theoretical or experimental work on how well backtracking algorithms perform on these binary representations in comparison to their performance on the corresponding non-binary CSP. We present both theoretical and empirical results to help understand the tradeoffs involved. In particular, we show that translating a non-binary CSP into a binary representation can be a viable solution technique in certain circumstances. The ultimate aim of this research is to give guidance for when one should consider translating between non-binary and binary representations. Our results supply some initial answers to this question.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-044.pdf,
44,1998,Constraint Satisfaction Problems,Generalizing Partial Order and Dynamic Backtracking,Christian Bliek,"Recently, two new backtracking algorithms, dynamic backtracking ( DB ) and partial order dynamic backtracking ( PDB ) have been presented. These algorithms have the property to be additive on disjoint subproblems and yet use only polynomial space. Unlike DB , PDB only imposes a partial search order and therefore appears to have more freedom than DB to explore the search space. However, both algorithms are not directly comparable in terms of exibility. In this paper we present new backtracking algorithms that are obtained by relaxing the ordering conditions of PDB . This gives them additional exibility while still being additive on disjoint subproblems. In particular, we show that our algorithms generalize both DB and PDB.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-045.pdf,
45,1998,Constraint Satisfaction Problems,On the Computation of Local Interchangeability in Discrete Constraint Satisfaction Problems,"Berthe Y. Choueiry, Guevara Noubir","In [4], Freuder defines several types of interchangeability to capture the equivalence among the values of a variable in a discrete constraint satisfaction problem (CSP), and provides a procedure for computing one type of local interchangeability. In this paper, we first extend this procedure for computing a weak form of local interchangeability. Second, we show that the modified procedure can be used to generate a conjunctive decomposition of the CSP by localizing, in the CSP, independent subproblems. Third, for the case of constraints of mutual exclusion, we show that locally interchangeable values can be computed in a straight-forward manner, and that the only possible type of local interchangeability is the one that induces locally independent subproblems. Finally, we give hints on how to exploit these results in practice, establish a lattice that relates some types of interchangeability, and identify directions for future research.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-046.pdf,
46,1998,Constraint Satisfaction Problems,Supermodels and Robustness,"Matthew L. Ginsberg, Andrew J. Parkes, Amitabha Roy","When search techniques are used to solve a practical problem, the solution produced is often brittle in the sense that small execution difficulties can have an arbitrarily large effect on the viability of the solution. The AI community has responded to this difficulty by investigating the development of ""robust problem solvers"" that are intended to be proof against this difficulty. We argue that robustness is best cast not as a property of the problem solver, but as a property of the solution. We introduce a new class of models for a logical theory, called supermodels, that captures this idea. Supermodels guarantee that the model in question is robust, and allow us to quantify the degree to which it is so. We investigate the theoretical properties of supermodels, showing that finding supermodels is typically of the same theoretical complexity as finding models. We provide a general way to modify a logical theory so that a model of the modified theory is a supermodel of the original. Experimentally, we show that the supermodel problem exhibits phase transition behavior similar to that found in other satisfiability work.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-047.pdf,
47,1998,Constraint Satisfaction Problems,"""Squeaky Wheel"" Optimization","David E. Joslin, David P. Clements","We describe a general approach to optimization which we term ""Squeaky Wheel"" Optimization ( swo ). In SWO , a greedy algorithm is used to construct a solution which is then analyzed to find the trouble spots, i.e., those elements, that, if improved, are likely to improve the objective function score. That analysis is used to generate new priorities that determine the order in which the greedy algorithm constructs the next solution. This Construct/Analyze/Prioritize cycle continues until some limit is reached, or an acceptable solution is found. SWO can be viewed as operating on two search spaces: solutions and prioritizations. Successive solutions are only indirectly related, via the re-prioritization that results from analyzing the prior solution. Similarly, successive prioritizations are generated by constructing and analyzing solutions. This ""coupled search"" has some interesting properties, which we discuss. We report encouraging experimental results on two domains, scheduling problems that arise in fiber-optic cable manufacturing, and graph coloring problems. The fact that these domains are very different supports our claim that swo is a general technique for optimization.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-048.pdf,
48,1998,Constraint Satisfaction Problems,Reversible DAC and Other Improvements for Solving Max-CSP,"Javier Larrosa, Pedro Meseguer, Thomas Schiex, Gérard Verfaillie","Following the work of R. Wallace on Max-CSP, later improved by J. Larrosa and P. Meseguer, we tested a number of possible improvements of the usage of directed arc consistency for the partial forward checking algorithm (PFC). The main improvement consists in exploiting a non standard form of DAC, called reversible DAC where each constraint is exploited in a direction which is not necessarily determined by the variable ordering and can change dynamically during the search. Other improvements include: (i) avoiding some constraint checks when forward-checking by exploiting the constraint checks performed during DAC preprocessing (ii) using a dynamic variable ordering during the search, (iii) maintaining the directed arc-consistency counts during the search as values get deleted. These improvements have been assessed empirically on random CSP instances. Some of them lead to very large performance gains with respect to the initial algorithm.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-049.pdf,
49,1998,Constraint Satisfaction Problems,Branch and Bound Algorithm Selection by Performance Prediction,"Lionel Lobjois, Michel Lemaître","We propose a method called Selection by Performance Prediction (SPP) which allows one, when faced with a particular problem instance, to select a Branch and Bound algorithm from among several promising ones. This method is based on Knuth’s sampling method which estimates the efficiency of a backtrack program on a particular instance by iteratively generating random paths in the search tree. We present a simple adaptation of this estimator in the field of combinatorial optimization problems, more precisely for an extension of the maximal constraint satisfaction framework. Experiments both on random and strongly structured instances show that, in most cases, the proposed method is able to select, from a candidate list, the best algorithm for solving a given instance.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-050.pdf,
50,1998,Constraint Satisfaction Problems,A Fast Algorithm for the Bound Consistency of Alldiff Constraints,Jean-Francois Puget,"Some n-ary constraints such as the alldiff constraints arise naturally in real life constraint satisfaction problems (CSP). General purpose filtering algorithms could be applied to such constraints. By taking the semantics of the constrain into account, it is possible to design more efficient filtering algorithms. When the domains of the variables are totally ordered (e.g. all values are integers), then filtering based on bound consistency may be very useful. We present in this paper a filtering algorithm for the alldiff constraint based on bound consistency whose running time complexity is very low. More precisely, for a constraint involving n varialbles, the time complexity of the algorithm is O(nlog(n)) which improves previously published results. The implementation of this algorithm is discussed, and we give some experimental results that probe its practical utility.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-051.pdf,
51,1998,Constraint Satisfaction Problems,Using Arc Weights to Improve Iterative Repair,"John Thornton, Abdul Sattar","One of the surprising findings from the study of CNF satisfiability in the 1990’s has been the success of iterative repair techniques, and in particular of weighted iterative repair. However, attempts to improve weighted iterative repair have either produced marginal benefits or rely on domain specific heuristics. This paper introduces a new extension of constraint weighting called Arc Weighting Iterative Repair, that is applicable outside the CNF domain and can significantly improve the performance of constraint weighting. The new weighting strategy extends constraint weighting by additionally weighting the connections or arcs between constraints. These arc weights represent increased knowledge of the search space and can be used to guide the search more efficiently. The main aim of the research is to develop an arc weighting algorithm that creates more benefit than overhead in reducing moves in the search space. Initial empirical tests indicate the algorithm does reduce search steps and times for a selection of CNF and CSP problems.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-052.pdf,
52,1998,Constraint Satisfaction Problems,An Integer Local Search Method with Application to Capacitated Production Planning,"Joachim P. Walser, Ramesh Iyer, Narayan Venkatasubramanyan","Production planning is an important task in manufacturing systems. We consider a real-world capacitated lot-sizing problem (CLSP) from the process industry. Because the problem requires discrete lot-sizes, domain-specific methods from the literature are not directly applicable. We therefore approach the problem with WSAT (OIP), a new domain-independent heuristic for integer optimization which generalizes the Walksat algorithm. WSAT (OIP) performs stochastic tabu search and operates on over-constrained integer programs. We empirically compare WSAT (OIP) to a state-of-the-art mixed integer programming branch-and-bound solver (CPLEX 4.0) on real problem data. We find that integer local search is considerably more robust than MIP branch-and-bound in finding feasible solutions in limited time, and branch-and-bound can only solve a sub-class of the CLSP with discrete lot-sizes. With respect to production cost, both methods find solutions of similar quality.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-053.pdf,
53,1998,Constraint Satisfaction Problems,Extending GENET to Solve Fuzzy Constraint Satisfaction Problems,"Jason H. Y. Wong, Ho-fung Leung","Despite much research that has been done on constraint satisfaction problems (CSP’s), the framework is sometimes in exible and the results are not very satisfactory when applied to real-life problems. With the incorporation of the concept of fuzziness, fuzzy constraint satisfaction problems (FCSP’s) have been exploited. FCSP’s model real-life problems better by allowing individual constraints to be either fully or partially satisfied. GENET, which has been shown to be efficient and effective in solving certain traditional CSP’s, is extended to handle FCSP’s. Through transforming FCSP’s into 0 - 1 integer programming problems, we display the equivalence between the underlying working mechanism of fuzzy GENET and the discrete Lagrangian method. Simulator of fuzzy GENET for single-processor machines is implemented. Benchmarking results confirm its feasibility in tackling CSP’s and exibility in dealing with over-constrained problems.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-054.pdf,
54,1998,Constraint Satisfaction Problems--Local Search,Local Search for Statistical Counting,Olivier Bailleux,"In this paper, statistical counting is introduced in the context of stochastic local search. From a sample of trajectories by independent local search computations, it is shown that interesting statistical information can be actually extracted about the search space, most notably an unbiased estimate of the number of solutions. Computational results for random #SAT instances are provided.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-055.pdf,
55,1998,Constraint Satisfaction Problems--Local Search,A Tractable Walsh Analysis of SAT and its Implications for Genetic Algorithms,"Soraya Rana, Robert B. Heckendorn, Darrell Whitley",Walsh Transforms measure all sources of nonlinear interactions for functions that have a bit representation. There can be exponentially many nonlinear interactions and exactly computing all Walsh coefficients is usually intractable for non-trivial functions. In this paper we will show that SAT problems evaluated as MAXSAT functions have a highly restricted set of nonzero Walsh coefficients and those coefficients can be computed in linear time with respect to the number of clauses. This analysis suggests why standard simple genetic algorithms should perform poorly on MAXSAT problems.,https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-206.pdf,
56,1998,Constraint Satisfaction Problems--Understanding Intractability,Hard Problems for CSP Algorithms,David G. Mitchell,"We prove exponential lower bounds on the running time of many algorithms for Constraint Satisfaction, by giving a simple family of instances on which they always take exponential time. Although similar lower bounds for most of these algorithms have been shown before, we provide a uniform treatment which illustrates a powerful general approach and has stronger implications for the practice of algorithm design.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-056.pdf,
57,1998,Constraint Satisfaction Problems--Understanding Intractability,The Constrainedness Knife-Edge,Toby Walsh,"A general rule of thumb is to tackle the hardest part of a search problem first. Many heuristics therefore try to branch ont he most constrained variable. To test their effectiveness at this, we measure the constrainedness of a problem during search. We run experiments in several different domains, using both random and non-random problems. In each case, we observe a constrainedness ""knife-edge"" in which critically constrained problems tend to remain critically constrained. We show that this knife-edge is predicted by a theoretical lower-bound calculation. We also observe a very simple scaling with problem size for various properties measured during search including the ratio of clauses to variables, and the average clause size. Finally, we use this picture of search to propose some branching heuristics for propositional satisfiability.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-057.pdf,
58,1998,Heuristic Search,Heuristic Search in Cyclic AND / OR Graphs,"Eric A. Hansen, Shlomo Zilberstein","Heuristic search algorithms can find solutions that take the form of a simple path (A*), a tree or an acyclic graph (AO*). We present anovel generalization of heuristic search (called LAO*) that can find solutions with loops, that is, solutions that take the form of a cyclic graph. We show that it can be used to solve Markov decision problems without evaluating the entire state space, giving it an advantage over dynamic-programming algorithms such as policy iteration and value iteration as an approach to stochastic planning.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-058.pdf,
59,1998,Heuristic Search,Single-Agent Search in the Presence of Deadlocks,"Andreas Junghanns, Jonathan Schaeffer","Single-agent search is a powerful tool for solving a variety of applications. Most of the application domains used to explore single-agent search techniques have the property that if you start with a solvable state, at no time in the search can you reach a state that is unsolvable. In this paper we address the implications that arise when state transitions can lead to unsolvable (deadlock) states. Deadlock states are partially responsible for the failure of our attempts to solve positions in the game of Sokoban. In this paper, we introduce pattern search, a real-time learning algorithm that identifies the minimal conditions (pattern) necessary for a deadlock, and applies that knowledge to eliminate provably irrelevant parts of the search tree. Identification of deadlock patterns is equivalent to correcting the heuristic lower bound of a position to infinity. Generalizing pattern searches to find arbitrary lower bound increases yields a powerful new search enhancement. In the game of Sokoban, pattern searches result in a 15-fold reduction of the cost of each additional IDA* iteration.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-059.pdf,
60,1998,Heuristic Search,Complete Anytime Beam Search,Weixiong Zhang,"Beam search executes a search method, such as best-first search or depth-first search, but may abandon nonpromising search avenues in order to reduce complexity. Although it has existed for more than two decades and has been applied to many real-world problems, beam search still suffers from the drawback of possible termination with no solution or a solution of unsatisfactory quality. In this paper, we first propose a domain-independent heuristic for node pruning, and a method to reduce the possibility that beam search will fail. We then develop a complete beam search algorithm. The new algorithm can not only find an optimal solution, but can also reach better solutions sooner than its underlying search method. We apply complete beam search to the maximum boolean satisfiability and the symmetric and asymmetric Traveling Salesman Problems. Our experimental results show that the domain-independent pruning heuristic is effective and the new algorithm significantly improves the performance of its underlying search algorithm.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-060.pdf,
61,1998,Random Approaches to Search,Boosting Combinatorial Search through Randomization,"Carla P. Gomes, Bart Selman, Henry Kautz","Unpredictability in the running time of complete search procedures can often be explained by the phenomenon of ""heavy-tailed cost distributions"", meaning that at any time during the experiment there is a non-negligible probability of hitting a problem that requires exponentially more time to solve than any that has been encountered before (Gomes et al. 1998a). We present a general method for introducing controlled randomization into complete search algorithms. The ""boosted"" search methods provably eliminate heavy-tails to the right of the median. Furthermore, they can take advantage of heavy-tails to the left of the median (that is, a non-negligible chance of very short runs) to dramatically shorten the solution time. We demonstrate speedups of several orders of magnitude for state-of-the-art complete search procedures running on hard, real-world problems.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-061.pdf,
62,1998,Random Approaches to Search,Which Search Problems Are Random?,Tad Hogg,"The typical difficulty of various NP-hard problems varies with simple parameters describing their structure. This behavior is largely independent of the search algorithm, but depends on the choice of problem ensemble. A given problem instance belongs to many different ensembles, so applying these observations to individual problems requires identifying which ensemble is most appropriate for predicting its search behavior, e.g., cost or solubility. To address this issue, we introduce a readily computable measure of randomness for search problems called ""approximate entropy."" This new measure is better suited to search than other approaches, such as algorithmic complexity and information entropy. Experiments with graph coloring and 3-SAT show how this measure can be applied.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-062.pdf,
63,1998,Search and Limited Resources,A* with Bounded Costs,"Brian Logan, Natasha Alechina","A key assumption of all problem-solving approaches based on utility theory, including heuristic search, is that we can as-sign a utility or cost to each state. This in turn requires that all criteria of interest can be reduced to a common ratio scale. However, many real-world problems are difficult or impossible to formulate in terms of minimising a single criterion, and it is often more natural to express problem requirements in terms of a set of constraints which a solution should satisfy. In this paper, we present a generalisation of the A- search algorithm, A- with bounded costs ( ABC ), which searches for a solution which best satisfies a set of prioritised soft constraints, and show that, given certain reasonable assumptions about the constraints, the algorithm is both complete and optimal. We briefly describe a route planner based on ABC and illustrate the advantages of our approach in a simple route planning problem.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-063.pdf,
64,1998,Search and Limited Resources,Stochastic Node Caching for Memory-Bounded Search,"Teruhisa Miura, Toru Ishida","Linear-space search algorithms such as IDA* (Iterative Deepening A*) cache only those nodes on the current search path, but may revisit the same node again and again. This causes IDA* to take an impractically long time to find a solution. In this paper, we propose a simple and effective algorithm called Stochastic Nude Caching (SNC) for reducing the number of revisits. SNC caches a node with the best estimate, which is currently known of the minimum estimated cost from the node to the goal node. Unlike previous related research such as MREC, SNC caches nodes selectively, based on a fixed probability. We demonstrate that SNC can effectively reduce the number of revisits compared to MREC, especially when the state-space forms a lattice.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-064.pdf,
65,1998,Search Control in Theorem Proving,A Feature-Based Learning Method for Theorem Proving,Matthias Fuchs,"Automated reasoning or theorem proving essentially amounts to solving search problems. Despite significant progress in recent years theorem provers still have many shortcomings. The use of machine-learning techniques is acknowledged as promising, but difficult to apply in the areal of theorem proving. We propose here to learn search-guiding heuristics by employing features in a simple yet effective manner. Features are used to adapt a heuristic to a solved source problem. The adapted heuristic can then be utilized profitably for solving related target problems. Experiments have demonstrated that the approach not only allows for significant speed-ups, but also makes it possible to prove problems that were out of reach before.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-065.pdf,
66,1998,Search Control in Theorem Proving,Learning Investment Functions for Controlling the Utility of Control Knowledge,"Oleg Ledeniov, Shaul Markovitch","The utility problem occurs when the cost of the acquired knowledge outweighs its benefits. When the learner acquires control knowledge for speeding up a problem solver, the benefit is the speedup gained due to the better control, and the cost is the added time required by the control procedure due to the added knowledge. Previous work in this area was mainly concerned with the costs of matching control rules. The solutions to this kind of utility problem involved some kind of selection mechanism to reduce the number of control rules. In this work we deal with a control mechanism that carries very high cost regardless of the particular knowledge acquired. We propose to use in such cases explicit reasoning about the economy of the control process. The solution includes three steps. First, the control procedure must be converted to anytime procedure. Second, a resource-investment function should be acquired to learn the expected return in speedup time for additional control time. Third, the function is used to determine a stopping condition for the anytime procedure. We have implemented this framework within the context of a program for speeding up logic inference by subgoal ordering. The control procedure utilizes the acquired control knowledge to find efficient subgoal ordering. The cost of ordering, however, may outweigh its benefit. Resource investment functions are used to cut-off ordering when the future net return is estimated to be negative.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-066.pdf,
67,1998,Uncertainty Search and Optimization,Fast Probabilistic Modeling for Combinatorial Optimization,"Shumeet Baluja, Scott Davies","Probabilistic models have recently been utilized for the optimization of large combinatorial search problems. However, complex probabilistic models that attempt to capture interparameter dependencies can have prohibitive computational costs. The algorithm presented in this paper, termed COMIT, provides a method for using probabilistic models in conjunction with fast search techniques. We show how COMIT can be used with two very different fast search algorithms: hillclimbing and Population-based incremental learning (PBIL). The resulting algorithms maintain many of the benefits of probabilistic modeling, with far less computational expense. Extensive empirical results are provided; COMIT has been successfully applied to jobshop scheduling, traveling salesman, and knapsack problems. This paper also presents a review of probabilistic modeling for combinatorial optimization.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-067.pdf,
68,1998,Uncertainty Search and Optimization,Highest Utility First Search Across Multiple Levels of Stochastic Design,"Louis Steinberg, J. Storrs Hall, Brian D. Davison","Many design problems are solved using multiple levels of abstraction, where a design at one level has combinatorially many children at the next level. A stochastic optimization methods, such as simulated annealing, genetic algorithms and multi-start hill climbing, is often used in such cases to generate the children of a design. This gives rise to a search tree for the overall problem characterized by a large branching factor, objects at different levels that are hard to compare, and a child-generator that is too expensive to run more than a few times at each level. We present the Highest Utility First Search (HUFS) control algorithm for searching such trees. HUFS is based on an estimate we derive for the expected utility of starting the design process from any given design alternative, where utility reflects both the intrinsic value of the final result and the cost in computing resources it will take to get that result. We also present an empirical study applying HUFS to the problem of VLSI module placement, in which HUFS demonstrates significantly better performance than the common ""waterfall"" control method.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-068.pdf,
69,1998,Evolvable Hardware,Evolvable Hardware Chip for High Precision Printer Image Compression,"Hidenori Sakanashi, Mehrdad Salami, Masaya Iwata, Shogo Nakaya, Tsukasa Yamauchi, Takeshi Inuo, Nobuki Kajihara, Tetsuya Higuchi","This paper describes a data compression chip for the high-precision electrophotographic printer using Evolvable Hardware (EHW). EHW is a new hardware paradigm which combines Genetic Algorithm (GA) and reconfigurable hardware technology such as FPGA (Field Programmable Gate Array). In EHW, GA is used to search for the most desirable hardware structure to a given task. If the task requirement changes, GA is invoked to get a better hardware structure and EHW is reconfigured that way. In data compression, EHW is used to implement the most adequate compression method directly in hardware according to the characteristics of the target image. The EHW-based compression chip attains approximately twice the compression compared with the international standard called JBIG. This chip is the first EHW-chip to lead to a commercial product.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-069.pdf,
70,1998,Game Playing,Opponent Modeling in Poker,"Darse Billings, Denis Papp, Jonathan Schaeffer, Duane Szafron","Poker is an interesting test-bed for artificial intelligence research. It is a game of imperfect knowledge, where multiple competing agents must deal with risk management, agent modeling, unreliable information and deception, much like decision-making applications in the real world. Agent modeling is one of the most difficult problems in decision-making applications and in poker it is essential to achieving high performance. This paper describes and evaluates Loki, a poker program capable of observing its opponents, constructing opponent models and dynamically adapting its play to best exploit patterns in the opponents’ play.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-070.pdf,
71,1998,Game Playing,Finding Optimal Strategies for Imperfect Information Games,"Ian Frank, David Basin, Hitoshi Matsubara","We examine three heuristic algorithms for games with imperfect information: Monte-carlo sampling, and two new algorithms we call vector minimaxing and payoff-reduction minimaxing. We compare these algorithms theoretically and experimentally, using both simple game trees and a large database of problems from the game of Bridge. Our experiments show that the new algorithms both out-perform Monte-carlo sampling, with the superiority of payoff-reduction minimaxing being especially marked. On the Bridge problem set, for example, Monte-carlo sampling only solves 66% of the problems, whereas payoff-reduction minimaxing solves over 95%. This level of performance was even good enough to allow us to discover five errors in the expert text used to generate the test database.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-071.pdf,
72,1998,Information Extraction,Learning to Extract Symbolic Knowledge from the World Wide Web,"Mark Craven, Dan DiPasquo, Dayne Freitag, Andrew McCallum, Tom Mitchell, Kamal Nigam, Seán Slattery","The World Wide Web is a vast source of information accessible to computers, but understandable only to humans. The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web. Such a knowledge base would enable much more effective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving. Our approach is to develop a trainable information extraction system that takes two inputs: an ontology defining the classes and relations of interest, and a set of training data consisting of labeled regions of hypertext representing instances of these classes and relations. Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web. This paper describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-072.pdf,
73,1998,Information Extraction,Information Extraction from HTML: Application of a General Machine Learning Approach,Dayne Freitag,"Because the World Wide Web consists primarily of text, information extraction is central to any effort that would use the Web as a resource for knowledge discovery. We show how information extraction can be cast as a standard machine learning problem, and argue for the suitability of relational learning in solving it. The implementation of a general-purpose relational learner for information extraction, SRV , is described. In contrast with earlier learning systems for information extraction, SRV makes no assumptions about document structure and the kinds of information available for use in learning extraction patterns. Instead, structural and other information is supplied as input in the form of an extensible token-oriented feature set. We demonstrate the effectiveness of this approach by adapting SRV for use in learning extraction rules for a domain consisting of university course and research project pages sampled from the Web. Making SRV Web-ready only involves adding several simple HTML-specific features to its basic feature set.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-073.pdf,
74,1998,Information Extraction,Towards Text Knowledge Engineering,"Udo Hahn, Klemens Schnattinger","We introduce a methodology for automating the maintenance of domain-specific taxonomies based on natural language text understanding. A given ontology is incrementally updated as new concepts are acquired from real-world texts. The acquisition process is centered around the linguistic and conceptual ""quality"" of various forms of evidence underlying the generation and refinement of concept hypotheses. On the basis of the quality of evidence, concept hypotheses are ranked according to credibility and the most credible ones are selected for assimilation into the domain knowledge base.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-074.pdf,
75,1998,Information Extraction,Answering Questions for an Organization Online,"Vladimir A. Kulyukin, Kristian J. Hammond, Robin D. Burke","The World Wide Web continues to challenge organizations to make online access to their expertise convenient for their clients. One means of expertise access that many clients find convenient in everyday life is asking natural language questions of the organization. To support it online, we developed an approach to building organization-embedded question-answering intermediaries, called Information Exchange systems. These systems use their knowledge of the organization’s structure to answer the clients’ questions and to acquire new expertise from the organization’s experts. Our approach uses techniques of hierarchical and predictive indexing, combined term weighting, abstraction-based retrieval, and negative evidence acquisition. We illustrate our approach with the Chicago Information Exchange system, an Information Exchange application embedded in one university’s computer science department.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-075.pdf,
76,1998,Integrated AI Systems,BIG: A Resource-Bounded Information Gathering Agent,"Victor Lesser, Bryan Horling, Frank Klassner, Anita Raja, Thomas Wagner, Shelley XQ. Zhang","Effective information gathering on the WWW is a complex task requiring planning, scheduling, text processing, and interpretation-style reasoning about extracted data to resolve inconsistencies and to refine hypotheses about the data. This paper describes the rationale, architecture, and implementation of a next generation information gathering system - a system that integrates several areas of AI research under a single research umbrella. The goal of this system is to exploit the vast number of information sources available today on the NII including a growing number of digital libraries, independent news agencies, government agencies, as well as human experts providing a variety of services. The large number of information sources and their different levels of accessibility, reliability and associated costs present a complex information gathering coordination problem. Our solution is an information gathering agent, BIG, that plans to gather information to support a decision process, reasons about the resource trade-offs of different possible gathering approaches, extracts information from both unstructured and structured documents, and uses the extracted information to refine its search and processing activities.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-076.pdf,
77,1998,Intelligent Environments,Design Principles for Intelligent Environments,Michael H. Coen,"This paper describes design criteria for creating highly embedded, interactive spaces that we call Intelligent Environments. The motivation for building these systems is to bring computation into the real, physical world to support what is traditionally considered non-computational activity. We describe an existing prototype space, known as the Intelligent Room, which was created to experiment with different forms of natural, multimodal human-computer interaction. We discuss design decisions encountered while creating the Intelligent Room and how the experiences gained during its use have shaped the creation of its successor.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-077.pdf,
78,1998,Intelligent Environments,Cooperating with People: The Intelligent Classroom,David Franklin,"People frequently complain that it is too difficult to figure out how to get computers to do what they want. However, with a computer system that actually tries to understand what its users are doing, people can interact in ways that are more natural to them. We have been developing a system, the Intelligent Classroom, that does exactly this. The Intelligent Classroom uses cameras and microphones to sense a speaker’s actions and then infers his intentions from those actions. Finally, it uses these intentions to decide what to do to best cooperate with the speaker. In the Intelligent Classroom, the speaker need not worry about how to operate the Classroom; he may simply go about his lecture and trust the Classroom to assist him at the appropriate moments.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-078.pdf,
79,1998,Planning and Problem Solving,Integrating AI Components for a Military Planning Application,"Marie A. Bienkowski, Louis J. Hoebel","We integrated three mature AI reasoning systems and several legacy military systems in order to provide human planners with advanced capabilities in a military planning domain. The integration demonstrates the operation of a diverse set of AI applications that present a unified system to a human planner in a realistic and meaningful context. We began with an operational planning support system and integrated into it AI components that filled technology gaps. These components, drawn from AI research laboratories, were a generative planner, a temporal reasoner and plan visualizer, and a knowledge-based plan critiquer. The resulting system uses a shared representation to support plan authoring, consistency checking, feasibility analysis, replanning, and visibility into the plan. The support provided is flexible and user controlled. We describe lessons learned from the simultaneous application and integration of mature AI research software, and for individual system components, primarily as they relate to the representation of plans.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-079.pdf,
80,1998,Planning and Problem Solving,TRIPS: An Integrated Intelligent Problem-Solving Assistant,"George Ferguson, James F. Allen","We discuss what constitutes an integrated system in AI, and why AI researchers should be interested in building and studying them. Taking integrated systems to be ones that integrate a variety of components in order to perform some task from start to finish, we believe that such systems (a) allow us to better ground our theoretical work in actual tasks, and (b) provide an opportunity for much-needed evaluation based on task performance. We describe one particular integrated system we have developed that supports spoken-language dialogue to collaboratively solve planning problems. We discuss how the integrated system provides key advantages for helping both our work in natural language dialogue processing and in interactive planning and problem solving, and consider the opportunities such an approach affords for the future.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-080.pdf,
81,1998,Concepts and Context,Knowledge Intensive Exception Spaces,"Sarabjot S. Anand, David W. Patterson, John G. Hughes","In this paper we extend the concept of exception spaces as defined by Cost and Salzberg (Cost and Salzberg, 1993), in the context of exemplar-based reasoning. Cost et al. defined exception spaces based on the goodness, in terms of performance, of an exemplar. While this is straightforward when using exemplars for classification problems, such a definition does not exist for regression problems. Thus, firstly we define a measure of goodness of an exemplar. We then use this measure of goodness to compare the effectiveness of exception spaces with a variant that we introduce, called Knowledge Intensive Exception Spaces or KINS. KINS remove the restriction on the geometric shape of exception spaces as defined by Cost et al. We provide a rationale for KINS and use a data set from the domain of colorectal cancer to support our hypothesis that KINS are a useful extension to exception spaces.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-081.pdf,
82,1998,Concepts and Context,Probabilistic Frame-Based Systems,"Daphne Koller, Avi Pfeffer","Two of the most important threads of work in knowledge representation today are frame-based representation systems (FRS’s) and Bayesian networks (BNs). FRS’s provide an excellent representation for the organizational structure of large complex domains, but their applicability is limited because of their inability to deal with uncertainty and noise. BNs provide an intuitive and coherent probabilistic representation of our uncertainty, but are very limited in their ability to handle complex structured domains. In this paper, we provide a language that cleanly integrates these approaches, preserving the advantages of both. Our approach allows us to provide natural and compact definitions of probability models for a class, in a way that is local to the class frame. These models can be instantiated for any set of interconnected instances, resulting in a coherent probability distribution over the instance properties. Our language also allows us to represent important types of uncertainty that cannot be accomodated within the framework of traditional BNs: uncertainty over the set of entities present in our model, and uncertainty about the relationships between these entities. We provide an inference algorithm for our language via a reduction to inference in standard Bayesian networks. We describe an implemented system that allows most of the main frame systems in existence today to annotate their knowledge bases with probabilistic information, and to use that information in answering probabilistic queries.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-082.pdf,
83,1998,Fuzzy Logic,Logical Representation and Computation of Optimal Decisions in a Qualitative Setting,"Didier Dubois, Daniel Le Berre, Henri Prade, Régis Sabbadin","This paper describes a logical machinery for computing decisions based on an ATMS procedure, where the available knowledge on the state of the world is described by a possibilistic propositional logic base (i.e., a collection of logical statements associated with qualitative certainty levels). The preferences of the user are also described by another possibilistic logic base whose formula weights are interpreted in terms of priorities and formulas express goals. Two attitudes are allowed for the decision maker: a pessimistic uncertainty-averse one and an optimistic one. The computed decisions are in agreement with a qualitative counterpart to classical expected utility theory for decision under uncertainty.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-083.pdf,
84,1998,Fuzzy Logic,A Fuzzy Description Logic,Umberto Straccia,"Description Logics (DLs, for short) allow reasoning about individuals and concepts, i.e. set of individuals with common properties. Typically, DLs are limited to dealing with crisp, well defined concepts. That is, concepts for which the problem whether an individual is an instance of it is a yes/no question. More often than not, the concepts encountered in the real world do not have a precisely defined criteria of membership: we may say that an individual is an instance of a concept only to a certain degree, depending on the individual’s properties. Concepts of this kind are rather vague than precise. As fuzzy logic directly deals with the notion of vagueness and imprecision, it offers an appealing foundation for a generalisation of DLs to vague concepts. In this paper we present a general fuzzy DL, which combines fuzzy logic with DLs. We define its syntax, semantics and present constraint propagation calculi for reasoning in it.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-084.pdf,
85,1998,Knowledge Base Design,OKBC: A Programmatic Foundation for Knowledge Base Interoperability,"Vinay K. Chaudhri, Adam Farquhar, Richard Fikes, Peter D. Karp, James P. Rice","The technology for building large knowledge bases (KBs) is yet to witness a breakthrough so that a KB can be constructed by the assembly of prefabricated knowledge components. Knowledge components include both pieces of domain knowledge (for example, theories of economics or fault diagnosis) and KB tools (for example, editors and theorem provers). Most of the current KB development tools can only manipulate knowledge residing in the knowledge representation system (KRS) for which the tools were originally developed. Open Knowledge Base Connectivity (OKBC) is an application programming interface for accessing KRSs, and was developed to enable the construction of reusable KB tools. OKBC improves upon its predecessor, the Generic Frame Protocol (GFP), in several significant ways. OKBC can be used with a much larger range of systems because its knowledge model supports an assertional view of a KRS. OKBC provides an explicit treatment of entities that are not frames, and it has a much better way of controlling inference and specifying default values. OKBC can be used on practically any platform because it supports network transparency and has implementations for multiple programming languages. In this paper, we discuss technical design issues faced in the development of OKBC, highlight how OKBC improves upon GFP, and report on practical experiences in using it.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-085.pdf,
86,1998,Knowledge Base Design,Usability Issues in Knowledge Representation Systems,"Deborah L. McGuinness, Peter F. Patel-Schneider","The amount of use a knowledge representation system receives depends on more than just the theoretical suitability of the system. Some critical determiners of usage have to do with issues related to the representation formalism of the system, some have to do with non-representational issues of the system itself, and some might be most appropriately labeled public relations. We rely on over eight years of industrial application experiences using a particular family of knowledge representation systems based on description logics to identify and describe usability issues that were mandatory for our application successes.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-086.pdf,
87,1998,Knowledge Base Design,Representing Scientific Experiments: Implications for Ontology Design and Knowledge Sharing,"Natalya Fridman Noy, Carole D. Hafner","As part of the development of knowledge sharing technology, it is necessary to consider a variety of domains and tasks in order to ensure that the shared framework is widely applicable. This paper describes an ontology design project in experimental molecular biology, focusing on extensions to previous ontological models and frame-based formalisms that allow us to handle problems in the representation of experimental science knowledge.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-087.pdf,
88,1998,Representation of Action,An Action Language Based on Causal Explanation: Preliminary Report,"Enrico Giunchiglia, Vladimir Lifschitz","Action languages serve for describing changes that are caused by performing actions. We define a new action language C, based on the theory of causal explanation proposed recently by McCain and Turner, and illustrate its expressive power by applying it to a number of examples. The mathematical results presented in the paper relate C to the Baral--Gelfond theory of concurrent actions.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-088.pdf,
89,1998,Representation of Action,Abductive Planning with Sensing,Matthew Stone,"In abductive planning, plans are constructed as reasons for an agent to act: plans are demonstrations in logical theory of action that a goal will result assuming that given actions occur successfully. This paper shows how to construct plans abductively for an agent that can sense the world to augment its partial information. We use a formalism that explicitly refers not only to time but also to the information on which the agent deliberates. Goals are reformulated to represent the successive stages of deliberation and action the agent follows in carrying out a course of action, while constraints on assumed actions ensure that an agent at each step performs a specific action selected for its known effects. The result is a simple formalism that can directly inform extensions to implemented planners.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-089.pdf,
90,1998,Robotics,A Formal Methodology for Verifying Situated Agents,"Matthew Stone, Phan Minh Dung","In this paper, we develop a formal methodology for verifying situated agents. The methodology consists of two elements, a specification language for specifying the agent capabilities to execute its actions in dynamic environments and a repertoire of proof methods by which the correctness of an agent, relative to its capabilities, can be formally verified.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-090.pdf,
91,1998,Robotics,An Algebra for Cyclic Ordering of 2D Orientations,"Amar Isli, Anthony G. Cohn","We define an algebra of ternary relations for cyclic ordering of 2D orientations. The algebra (1) is a refinement of the CYCORD theory; (2) contains 24 atomic relations, hence 2^24 general relations, of which the usual CYCORD relation is a particular relation; and (3) is NP-complete, which is not surprising since the CYCORD theory is. We then provide: (1) a constraint propagation algorithm for the algebra, which we show is polynomial, and complete for a subclass inculding all atomic relations; (2) a proof that another subclass, expressing only information on parallel orientations, is NP-complete; and (3) a solution search algorithm for a general problem expressed in the algebra.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-091.pdf,
92,1998,Time and Representation,The Temporal Analysis of Chisholm’s Paradox,"Leendert W. N. van der Torre, Yao-Hua Tan","Deontic logic, the logic of obligations and permissions, is plagued by several paradoxes that haveto be understood before deontic logic can be used as a knowledge representation language. In this paper we extend the temporal analysis of Chisholm’s paradox using a deontic logic that combines temporal and preferential notions.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-092.pdf,
93,1998,Time and Representation,Temporal Reasoning with Qualitative and Quantitative Information about Points and Durations,"Rattana Wetprasit, Abdul Sattar","A duration is known as a time distance between two point events. This relationship has recently been formalized as the point duration network (PDN) in (Navarrete and Marin 1997). However, only the qualitative information about points and durations was considered. This paper presents an augmented point duration network (APDN) to represent both qualitative and quantitative information about point events. We further extend APDN to capture quantitative information about durations. We propose algorithms to solve reasoning tasks such as determining satisfiability of the network, and finding a consistent scenario with minimal domains. Thus, we present an expressively richer framework than the existing ones to handle both qualitative and quantitative information about points as well as durations.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-093.pdf,
94,1998,Learning,Iterated Phantom Induction: A Little Knowledge Can Go a Long Way,"Mark Brodie, Gerald DeJong","We advance a knowledge-based learning method that augments conventional generalization to permit concept acquisition in failure domains. These are domains in which learning must proceed exclusively with failure examples that are relatively uninformative for conventional methods. A domain theory is used to explain and then systematically perturb the observed failures so that they can be treated as if they were positive training examples. The concept induced from these ""phantom"" examples is exercised in the world, yielding additional observations, and the process repeats. Surprisingly, an accurate concept can often be learned even if the phantom examples are themselves failures and the domain theory is only imprecise and approximate. We investigate the behavior of the method in a stylized air-hockey domain which demands a nonlinear decision concept. Learning is shown empirically to be robust in the face of degraded domain knowledge. An interpretation is advanced which indicates that the information available from a plausible qualitative domain theory is sufficient for robust successful learning.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-094.pdf,
95,1998,Learning,SUSTAIN: A Model of Human Category Learning,"Bradley C. Love, Douglas L. Medin","SUSTAIN (Supervised and Unsupervised STratified Adaptive Incremental Network) is a network model of human category learning. SUSTAIN is a three layer model where learning between the first two layers is unsupervised, while learning between the top two layers is supervised. SUSTAIN clusters inputs in an unsupervised fashion until it groups input patterns inappropriately (as signaled by the supervised portion of the network). When such an error occurs, SUSTAIN alters its architecture, recruiting a new unit that is tuned to correctly classify the exception. Units recruited to capture exceptions can evolve into prototypes/attractors/rules in their own right. SUSTAIN’s adaptive architecture allows it to master simple classification problems quickly, while still retaining the capacity to learn difficult mappings. SUSTAIN also adjusts its sensitivity to input dimensions during the course of learning, paying more attention to dimensions relevant to the classification task. Shepard, Hovland, and Jenkins’s (1961) challenging category learning data is fit successfully by SUSTAIN. Other applications of SUSTAIN are discussed. SUSTAIN is compared to other classification models.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-095.pdf,
96,1998,Genetic Algorithm Applications,Optimal 2D Model Matching Using a Messy Genetic Algorithm,J. Ross Beveridge,"A Messy Genetic Algorithm is customized to find optimal many-to-many matches for 2D line segment models. The Messy GA is a variant upon the Standard Genetic Algorithm in which chromosome length can vary. Consequently, population dynamics can be made to drive a relatively efficient and robust search for larger and better matches. Run-times for the Messy GA are as much as an order of magnitude smaller than for random starts local search. When compared to a faster Key-Feature Algorithm, the Messy Genetic Algorithm more reliably finds optimal matches. Empirical results are presented for both controlled synthetic and real world line matching problems.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-096.pdf,
97,1998,Genetic Algorithm Applications,Learning Cooperative Lane Selection Strategies for Highways,"David E. Moriarty, Pat Langley","This paper presents a novel approach to traffic management by coordinating driver behaviors. Current traffic management systems do not consider lane organization of the cars and only affect traffic flows by controlling traffic signals or ramp meters. However, drivers should be able to increase traffic throughput and more consistently maintain desired speeds by selecting lanes intelligently. We pose the problem of intelligent lane selection as a challenging and potentially rewarding problem for artificial intelligence, and we propose a methodology that uses supervised and reinforcement learning to form distributed control strategies. Initial results are promising and demonstrate that intelligent lane selection can better approximate desired speeds and reduce the total number of lane changes.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-097.pdf,
98,1998,Inductive Learning,Boosting in the Limit: Maximizing the Margin of Learned Ensembles,"Adam J. Grove, Dale Schuurmans","The ""minimum margin"" of an ensemble classifier on a given training set is, roughly speaking, the smallest vote it gives to any correct training label. Recent work has shown that the Adaboost algorithm is particularly effective at producing ensembles with large minimum margins, and theory suggests that this may account for its success at reducing generalization error. We note, however, that the problem of finding good margins is closely related to linear programming, and we use this connection to derive and test new ""LPboosting"" algorithms that achieve better minimum margins than Adaboost. However, these algorithms do not always yield better generalization performance. In fact, more often the opposite is true. We report on a series of controlled experiments which show that no simple version of the minimum-margin story can be complete. We conclude that the crucial question as to why boosting works so well in practice, and how to further improve upon it, remains mostly open. Some of our experiments are interesting for another reason: we show that Adaboost sometimes does overfit--eventually. This may take a very long time to occur, however, which is perhaps why this phenomenon has gone largely unnoticed.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-098.pdf,
99,1998,Inductive Learning,Boosting Classifiers Regionally,Richard Maclin,"This paper presents a new algorithm for Boosting the performance of an ensemble of classifiers. In Boosting, a series of classifiers is used to predict the class of data where later members of the series concentrate on training data that is incorrectly predicted by earlier members. To make a prediction about a new pattern, each classifier predicts the class of the pattern and these predictions are then combined. In standard Boosting, the predictions are combined by weighting the predictions by a term related to the accuracy of the classifier on the training data. This approach ignores the fact that later classifiers focus on small subsets of the patterns and thus may only be good at classifying similar patterns. In RegionBoost, this problem is addressed by weighting each classifier’s predictions by a factor measuring how well that classifier performs on similar patterns. In this paper we examine several methods for determining how well a classifier performs on similar patterns. Empirical tests indicate RegionBoost produces gains in performance for some data sets and has little effect on others.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-099.pdf,
100,1998,Inductive Learning,Robust Classification Systems for Imprecise Environments,"Foster Provost, Tom Fawcett","In real-world environments it is usually difficult to specify target operating conditions precisely. This uncertainty makes building robust classification systems problematic. We show that it is possible to build a hybrid classifier that will perform at least as well as the best available classifier for any target conditions. This robust performance extends across a wide variety of comparison frameworks, including the optimization of metrics such as accuracy, expected cost, lift, precision, recall, and workforce utilization. In some cases, the performance of the hybrid can actually surpass that of the best known classifier. The hybrid is also efficient to build, to store, and to update. Finally, we provide empirical evidence that a robust hybrid classifier is needed for many real-world problems.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-100.pdf,
101,1998,Learning about People,Recommendation as Classification: Using Social and Content-Based Information in Recommendation,"Chumki Basu, Haym Hirsh, William Cohen","Recommendation systems make suggestions about artifacts to a user. For instance, they may predict whether a user would be interested in seeing a particular movie. Social recomendation methods collect ratings of artifacts from many individuals, and use nearest-neighbor techniques to make recommendations to a user concerning new artifacts. However, these methods do not use the significant amount of other information that is often available about the nature of each artifact --such as cast lists or movie reviews, for example. This paper presents an inductive learning approach to recommendation that is able to use both ratings information and other forms of information about each artifact in predicting user preferences. We show that our method outperforms an existing social-filtering method in the domain of movie recommendations on a dataset of more than 45,000 movie ratings collected from a community ofover 250 users.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-101.pdf,
102,1998,Learning about People,Learning to Predict User Operations for Adaptive Scheduling,"Melinda T. Gervasio, Wayne Iba, Pat Langley","Mixed-initiative systems present the challenge of finding an effective level of interaction between humans and computers. Machine learning presents a promising approach to this problem in the form of systems that automatically adapt their behavior to accommodate different users. In this paper, we present an empirical study of learning user models in an adaptive assistant for crisis scheduling. We describe the problem domain and the scheduling assistant, then present an initial formulation of the adaptive assistant’s learning task and the results of a baseline study. After this, we report the results of three subsequent experiments that investigate the effects of problem reformulation and representation augmentation. The results suggest that problem reformulation leads to significantly better accuracy without sacrificing the usefulness of the learned behavior. The studies also raise several interesting issues in adaptive assistance for scheduling.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-102.pdf,
103,1998,Learning about People,Adaptive Web Sites: Automatically Synthesizing Web Pages,"Mike Perkowitz, Oren Etzioni","The creation of a complex web site is a thorny problem in user interface design. In IJCAI '97, we challenged the AI community to address this problem by creating adaptive web sites: sites that automatically improve their organization and presentation by mining visitor access data collected in Web server logs. In this paper we introduce our own approach to this broad challenge. Specifically, we investigate the problem of index page synthesis | the automatic creation of pages that facilitate a visitor’s navigation of a Web site. First, we formalize this problem as a clustering problem and introduce a novel approach to clustering, which we call cluster mining: Instead of attempting to partition the entire data space into disjoint clusters, we search for a small number of cohesive (and possibly overlapping) clusters. Next, we present PageGather, a cluster mining algorithm that takes Web server logs as input and outputs the contents of candidate index pages. Finally, we show experimentally that PageGather is both faster (by a factor of three) and more effective than traditional clustering algorithms on this task. Our experiment relies on access logs collected over a month from an actual web site.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-103.pdf,
104,1998,Learning from Sequences,Feature Generation for Sequence Categorization,"Daniel Kudenko, Haym Hirsh","The problem of sequence categorization is to generalize from a corpus of labeled sequences procedures for accurately labeling future unlabeled sequences. The choice of representation of sequences can have a major impact on this task, and in the absence of background knowledge a good representation is often not known and straightforward representations are often far from optimal. We propose a feature generation method (called FGEN) that creates Boolean features that check for the presence or absence of heuristically selected collections of subsequences. We show empirically that the representation computed by FGEN improves the accuracy of two commonly used learning systems (C4.5 and Ripper) when the new features are added to existing representations of sequence data. We show the superiority ofFGEN across a range of tasks selected from three domains: DNA sequences, Unix command sequences, and English text.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-104.pdf,
105,1998,Learning from Sequences,Concepts from Time Series,"Michael T. Rosenstein, Paul R. Cohen","This paper describes a way of extracting concepts from streams of sensor readings. In particular, we demonstrate the value of attractor reconstruction techniques for transforming time series into clusters of points. These clusters, in turn, represent perceptual categories with predictive value to the agent/environment system. We also discuss the relationship between categories and concepts, with particular emphasis on class membership and predictive inference.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-105.pdf,
106,1998,Reinforcement Learning,The Dynamics of Reinforcement Learning in Cooperative Multiagent Systems,"Caroline Claus, Craig Boutilier","Reinforcement learning can provide a robust and natural means for agents to learn how to coordinate their action choices in multiagent systems. We examine some of the factors that can influence the dynamics of the learning process in such a setting. We first distinguish reinforcement learners that are unaware of (or ignore) the presence of other agents from those that explicitly attempt to learn the value of joint actions and the strategies of their counterparts. We study (a simple form of) Q-learning in cooperative multiagent systems under these two perspectives, focusing on the influence of that game structure and exploration strategies on convergence to (optimal and suboptimal) Nash equilibria. We then propose alternative optimistic exploration strategies that increase the likelihood of convergence to an optimal equilibrium.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-106.pdf,
107,1998,Reinforcement Learning,Applying Online Search Techniques to Continuous-State Reinforcement Learning,"Scott Davies, Andrew Y. Ng, Andrew Moore","In this paper, we describe methods for efficiently computing better solutions to control problems in continuous state spaces. We provide algorithms that exploit online search to boost the power of very approximate value functions discovered by traditional reinforcement learning techniques. We examine local searches, where the agent performs a finite-depth lookahead search, and global searches, where the agent performs a search for a trajectory all the way from the current state to a goal state. The key to the success of the local methods lies in taking a value function, which gives a rough solution to the hard problem of finding good trajectories from every single state, and combining that with online search, which then gives an accurate solution to the easier problem of finding a good trajectory specifically from the current state. The key to the success of the global methods lies in using aggressive state-space search techniques such as uniform-cost search and A*, tamed into a tractable form by exploiting neighborhood relations and trajectory constraints that arise from continuous-space dynamic control.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-107.pdf,
108,1998,Reinforcement Learning,Bayesian Q-Learning,"Richard Dearden, Nir Friedman, Stuart Russell","A central problem in learning in complex environments is balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classical notion of Value of Information-- the expected improvement in future decision quality that might arise from the information acquired by exploration. Estimating this quantity requires an assessment of the agent’s uncertainty about its current value estimates for states. In this paper, we adopt a Bayesian approach to maintaining this uncertain information. We extend Watkins’ Q-learning by maintaining and propagating probability distributions over the Q-values. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation. We establish the convergence properties of our algorithm and show experimentally that it can exhibit substantial improvements over other well-known model-free exploration strategies.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-108.pdf,
109,1998,Reinforcement Learning,Tree Based Discretization for Continuous State Space Reinforcement Learning,"William R. B. Uther, Manuela M. Veloso","Reinforcement learning is an effective technique for learning action policies in discrete stochastic environments, but its efficiency can decay exponentially with the size of the state space. In many situations significant portions of a large state space may be irrelevant to a specific goal and can be aggregated into a few, relevant, states. The U Tree algorithm generates a tree based state discretization that efficiently finds the relevant state chunks of large propositional domains. In this paper, we extend the U Tree algorithm to challenging domains with a continuous state space for which there is no initial discretization. This Continuous U Tree algorithm transfers traditional regression tree techniques to reinforcement learning. We have performed experiments in a variety of domains that show that Continuous U Tree effectively handles large continuous state spaces. In this paper, we report on results in two domains, one gives a clear visualization of the algorithm and another empirically demonstrates an effective state discretization in a simple multi-agent environment.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-109.pdf,
110,1998,Grammar and Language,A Sampling-Based Heuristic for Tree Search Applied to Grammar Induction,"Hugues Juillé, Jordan B. Pollack","In the field of Operation Research and Artificial Intelligence, several stochastic search algorithms have been designed based on the theory of global random search (Zhigljavsky 1991). Basically, those techniques iteratively sample the search space with respect to a probability distribution which is updated according to the result of previous samples and some predefined strategy. Genetic Algorithms (GAs) (Goldberg 1989) or Greedy Randomized Adaptive Search Procedures (GRASP) (Feo and Resende 1995) are two particular instances of this paradigm. In this paper, we present SAGE, a search algorithm based on the same fundamental mechanisms as those techniques. However, it addresses a class of problems for which it is difficult to design transformation operators to perform local search because of intrinsic constraints in the definition of the problem itself. For those problems, a procedural approach is the natural way to construct solutions, resulting in a state space represented as a tree or a DAG. The aim of this paper is to describe the underlying heuristics used by SAGE to address problems belonging to that class. The performance of SAGE is analyzed on the problem of grammar induction and its successful application to problems from the recent Abbadingo DFA learning competition is presented.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-110.pdf,
111,1998,Grammar and Language,Ambiguity and Constraint in Mathematical Expression Recognition,"Erik G. Miller, Paul A. Viola","The problem of recognizing mathematical expressions differs significantly from the recognition of standard prose. While in prose significant constraints can be put on the interpretation of a character by the characters immediately preceding and following it, few such simple constraints are present in a mathematical expression. In order to make the problem tractable, effective methods of recognizing mathematical expressions will need to put intelligent constraints on the possible interpretations. The authors present preliminary results on a system for the recognition of both handwritten and typeset mathematical expressions. While previous systems perform character recognition out of context, the current system maintains ambiguity of the characters until context can be used to disambiguate the interpretation. In addition, the system limits the number of potentially valid interpretations by decomposing the expressions into a sequence of compatible convex regions. The system uses A-star to search for the best possible interpretation of an expression. We provide a new lower bound estimate on the cost to goal that improves performance significantly.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-111.pdf,
112,1998,Learning in Natural Language,Learning to Classify Text from Labeled and Unlabeled Documents,"Kamal Nigam, Andrew McCallum, Sebastian Thrun, Tom Mitchell","In many important text classification problems, acquiring class labels for training documents is costly, while gathering large quantities of unlabeled data is cheap. This paper shows that the accuracy of text classifiers trained with a small number of labeled documents can be improved by augmenting this small training set with a large pool of unlabeled documents. We present a theoretical argument showing that, under common assumptions, unlabeled data contain information about the target function. We then introduce an algorithm for learning from labeled and unlabeled text based on the combination of Expectation-Maximization with a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents; it then trains a new classifier using the labels for all the documents, and iterates to convergence. Experimental results, obtained using text from three different realworld tasks, show that the use of unlabeled data reduces classification error by up to 33%.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-112.pdf,
113,1998,Learning in Natural Language,Knowledge Lean Word--Sense Disambiguation,"Ted Pedersen, Rebecca Bruce",We present a corpus-based approach toward sense disambiguation that only requires information that can be automatically extracted from untagged text. We use unsupervised techniques to estimate the parameters of a model describing the conditional distribution of the sense group given the known contextual features. Both the EM algorithm and Gibbs Sampling are evaluated to determine which is most appropriate for our data. We compare their disambiguation accuracy in an experiment with thirteen different words and three feature sets. Gibbs Sampling results in small but consistent improvement in disambiguation accuracy over the EM algorithm.,https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-113.pdf,
114,1998,Learning in Natural Language,Learning to Resolve Natural Language Ambiguities: A Unified Approach,Dan Roth,"We analyze a few of the commonly used statistics based and machine learning algorithms for natural language disambiguation tasks and observe that they can be recast as learning linear separators in the feature space. Each of the methods makes a priori assumptions, which it employs, given the data, when searching for its hypothesis. Nevertheless, as we show, it searches a space that is as rich as the space of all linear separators. We use this to build an argument for a data driven approach which merely searches for a good linear separator in the feature space, without further assumptions on the domain or a specific problem. We present such an approach - a sparse network of linear separators, utilizing the Winnow learning algorithm - and show how to use it in a variety ofambiguity resolution problems. The learning approach presented is attribute-efficient and, therefore, appropriate for domains having very large number of attributes. In particular, we present an extensive experimental comparison of our approach with other methods on several well studied lexical disambiguation tasks such as context-sensitive spelling correction, prepositional phrase attachment and part of speech tagging. In all cases we show that our approach either outperforms other methods tried for these tasks or performs comparably to the best.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-114.pdf,
115,1998,Natural Language Generation,Generating Inference-Rich Discourse through Revisions of RST-Trees,Helmut Horacek,"The majority of generation systems to date are able to communicate information only by uttering it explicitly. Rhetorical Structure Theory (RST), one of the most frequently used discourse theories for text planning in natural language generation, does not support more flexibility either, because it ignores implicit rhetorical relations and accepts only one prominent relation between clauses. In formal systems, however, the underlying information is represented in a very detailed way, which requires easily inferable parts to be left implicit for producing natural and comprehensible discourse. In order to improve the quality of texts generated from fine-grained semantic specifications, we present an approach that successively revises an explicit text plan by introducing addressee dependent short-cuts and communicatively justified reorganizations. Text plan revisions include the compactification of state-action and reasoning sequences, the omission of redundant conditions, and the reorganization of arguments for presentation purposes. Our techniques enable us to generate shorter and better understandable texts from detailed representations, as in formal systems, especially in deduction systems.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-115.pdf,
116,1998,Natural Language Generation,Machine Learning of Generic and User-Focused Summarization,"Inderjeet Mani, Eric Bloedorn","A key problem in text summarization is finding a salience function which determines what information in the source should be included in the summary. This paper describes the use of machine learning on a training corpus of documents and their abstracts to discover salience functions which describe what combination of features is optimal for a given summarization task. The method addresses both ""generic"" and user-focused summaries.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-116.pdf,
117,1998,Natural Language Generation--Argumentation,Hermes: Supporting Argumentative Discourse in Multi-Agent Decision Making,"Nikos Karacapilidis, Dimitris Papadias","This paper describes Hermes , a system that enhances group decision making by providing an argumentation framework to the agents involved. The system organizes the existing knowledge in a discussion graph, which consists of issues, alternatives, positions and preference relations. Argumentation is performed through a set of discourse acts which trigger appropriate procedures for the propagation of information in the graph. Hermes is able to handle incomplete, qualitative and inconsistent information, and provides mechanisms for weighing arguments.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-117.pdf,
118,1998,Natural Language Generation--Argumentation,Bayesian Reasoning in an Abductive Mechanism for Argument Generation and Analysis,"Ingrid Zukerman, Richard McConachy, Kevin B. Korb","Our argumentation system, NAG, uses Bayesian networks in a user model and in a normative model to assemble and as- sess arguments which balance persuasiveness with normative correctness. Attentional focus is simulated in both models to select relevant subnetworks for Bayesian propagation. The subnetworks are expanded in an iterative abductive process until argumentative goals are achieved in both models, when the argument is presented to the user.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-118.pdf,
119,1998,Nonmonotonic Reasoning,Fixpoint 3-Valued Semantics for Autoepistemic Logic,"Marc Denecker, K. U. Leuven, Victor Marek, Miroslaw Truszczynski","The paper presents a constructive 3-valued semantics for autoepistemic logic (AEL). We introduce a derivation operator and define the semantics as its least xpoint. The semantics is 3-valued in the sense that, for some formulas, the least fixpoint does not specify whether they are believed or not. We show that complete fixpoints of the derivation operator correspond to Moore’s stable expansions. In the case of modal representations of logic programs our least fixpoint semantics expresses well-founded semantics or 3-valued Fitting-Kunen semantics (depending on the embedding used). We show that, computationally, our semantics is simpler than the semantics proposed by Moore (assuming that the polynomial hierarchy does not collapse).",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-119.pdf,
120,1998,Nonmonotonic Reasoning,Experimenting with Power Default Reasoning,"Eric Klavins, William C. Rounds, Guo-Qiang Zhang","In this paper we explore the computational aspects of Propositional Power Default Reasoning (PDR), a form of non-monotonic reasoning in which the underlying logic is Kleene’s 3-valued propositional logic. PDR leads to a concise meaning of the problem of skeptical entailment which has better complexity characteristics than the usual formalisms (co-NP(3)-Complete instead of II^P2 -Complete). We take advantage of this in an implementation called powdef to encode and solve hard graph problems and explore randomly generated instances of skeptical entailment.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-120.pdf,
121,1998,Nonmonotonic Reasoning,Reducing Query Answering to Satisfiability in Nonmonotonic Logics,Riccardo Rosati,"We propose a unifying view of negation as failure, integrity constraints, and epistemic queries in nonmonotonic reasoning. Specifically, we study the relationship between satisfiability and logical implication in nonmonotonic logics, showing that, in many nonmonotonic formalisms, it is possible and easy to reduce logical implication to satisfiability. This result not only allows for establishing new complexity results for the satisfiability problem in nonmonotonic logics, but also establishes a clear relationship between the studies on epistemic queries and integrity constraints in monotonic knowledge bases with the work on negation by default in nonmonotonic reasoning and logic programming. From the perspective of the design of knowledge representation systems, such a reduction allows for defining both a simple method for answering epistemic queries in knowledge bases with nonmonotonic abilities, and a procedure for identifying integrity constraints in the knowledge base, which can be employed for optimizing reasoning in such systems.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-121.pdf,
122,1998,Planning,Improving Big Plans,"Neal Lesh, Nathaniel Martin, James Allen","Past research on assessing and improving plans in domains that contain uncertainty has focused on analytic techniques that are exponential in the length of the plan. Little work has been done on choosing from among the many ways in which a plan can be improved. We present the Improve algorithm which simulates the execution of large, probabilistic plans. Improve runs a data mining algorithm on the execution traces to pinpoint defects in the plan that most often lead to plan failure. Finally, Improve applies qualitative reasoning and plan adaptation algorithms to modify the plan to correct these defects. We have tested Improve on plans containing over 250 steps in an evacuation domain, produced by a domain-specific scheduling routine. In these experiments, the modidied plans have over a 15% higher probability of achieving their goal than the original plan.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-122.pdf,
123,1998,Planning,Controlling Communication in Distributed Planning Using Irrelevance Reasoning,"Michael Wolverton, Marie desJardins","Efficient and effective distributed planning requires careful control over how much information the planning agents broadcast to one another. Sending too little information could result in incorrect plans, while sending too much information could overtax the distributed planning system’s resources (bandwidth and computational power). Ideally, distributed planning systems would have an efficient technique for filtering a large amount of irrelevant information from the message stream while retaining all the relevant messages. This paper describes an approach to controlling information distribution among planning agents using irrelevance reasoning (Levy and Sagiv 1993). In this approach, each planning agent maintains a data structure encoding the planning effects that could potentially be relevant to each of the other agents, and uses this structure to decide which of the planning effects that it generates will be sent to other agents. We describe an implementation of this approach within a distributed version of the SIPE-2 planner. Our experiments with this implementation show two important benefits of the approach: first, a noticeable speedup of the distributed planners; second|and, we argue, more importantly--a substantial reduction in message traffic.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-123.pdf,
124,1998,Frameworks for Plan Generation,Automatic OBDD-Based Generation of Universal Plans in Non-Deterministic Domains,"Alessandro Cimatti, Marco Roveri, Paolo Traverso","Most real world environments are non-deterministic. Automatic plan formation in non-deterministic domains is, however, still an open problem. In this paper we present a practical algorithm for the automatic generation of solutions to planning problems in non-deterministic domains. Our approach has the following main features. First, the planner generates Universal Plans. Second, it generates plans which are guaranteed to achieve the goal in spite of non-determinism, if such plans exist. Otherwise, the planner generates plans which encode iterative trial-and-error strategies (e.g. try to pick up ablock until succeed), which are guaranteed to achieve the goal under the assumption that if there is a non-deterministic possibility for the iteration to terminate, this will not be ignored forever. Third, the implementation of the planner is based on symbolic model checking techniques which have been designed to explore efficiently large state spaces. The implementation exploits the compactness of OBDDs (Ordered Binary Decision Diagrams) to express in a practical way universal plans of extremely large size.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-124.pdf,
125,1998,Frameworks for Plan Generation,Hybrid Planning for Partially Hierarchical Domains,"Subbarao Kambhampati, Amol Mali, Biplav Srivastava","Hierarchical task network and action-based planning approaches have traditionally been studied separately. In many domains, human expertise in the form of hierarchical reduction schemas exists, but is incomplete. In such domains, hybrid approaches that use both HTN and action-based planning techniques are needed. In this paper, we extend our previous work on refinement planning to include hierarchical planning. Specifically, we provide a generalized plan-space refinement that is capable of handling non-primitive actions. The generalization provides a principled way of handling partially hierarchical domains, while preserving systematicity, and respecting the user-intent inherent in the reduction schemas. Our general account also puts into perspective the many surface differences between the HTN and action-based planners, and could support the transfer of progress between HTN and action-based planning approaches.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-125.pdf,
126,1998,Graph Plan,Conformant Graphplan,"David E. Smith, Daniel S. Weld","Planning under uncertainty is a difficult task. If sensory information is available, it is possible to do contingency planning - that is, develop plans where certain branches are executed conditionally, based on the outcome of sensory actions. However, even without sensory information, it is often possible to develop useful plans that succeed no matter which of the allowed states the world is actually in. We refer to this type of planning as conformant planning. Few conformant planners have been built, partly because conformant planning requires the ability to reason about disjunction. In this paper we describe Conformant Graphplan (CGP), a Graphplan-based planner that develops sound (non-contingent) plans when faced with uncertainty in the initial conditions and in the outcome of actions. The basic idea is to develop separate plan graphs for each possible world. This requires some subtle changes to both the graph expansion and solution extraction phases of Graphplan. In particular, the solution extraction phase must consider the unexpected side effects of actions in other possible worlds, and must confront any undesirable effects. We show that CGP performs significantly better than two previous (probabilistic) conformant planners.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-126.pdf,
127,1998,Graph Plan,Extending Graphplan to Handle Uncertainty and Sensing Actions,"Daniel S. Weld, Corin R. Anderson, David E. Smith","If an agent does not have complete information about the world-state, it must reason about alternative possible states of the world and consider whether any of its actions can reduce the uncertainty. Agents controlled by a contingent planner seek to generate a robust plan, that accounts for and handles all eventualities, in advance of execution. Thus a contingent plan may include sensing actions which gather information that is later used to select between different plan branches. Unfortunately, previous contingent planners suffered defects such as confused semantics, incompleteness, and inefficiency. In this paper we describe SGP, a descendant of Graphplan that solves contingent planning problems. SGP distinguishes between actions that sense the value of an unknown proposition from those that change its value. SGP does not suffer from the forms of incompleteness displayed by CNLP and Cassandra. Furthermore, SGP is relatively fast.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-127.pdf,
128,1998,Plan Efficiency,Inferring State Constraints for Domain-Independent Planning,"Alfonso Gerevini, Lenhart Schubert","We describe some new preprocessing techniques that enable faster domain-independent planning. The first set of techniques is aimed at inferring state constraints from the structure of planning operators and the initial state. Our methods consist of generating hypothetical state constraints by inspection of operator effects and preconditions, and checking each hypothesis against all operators and the initial conditions. Another technique extracts (supersets of) predicate domains from sets of ground literals obtained by Graphplan-like forward propagation from the initial state. Our various techniques are implemented in a package called DISCOPLAN. We show preliminary results on the effectiveness of adding computed state constraints and predicate domains to the specification of problems for SAT-based planners such as SATPLAN or MEDIC. The results suggest that large speedups in planning can be obtained by such automated methods, potentially obviating the need for adding hand-coded state constraints.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-128.pdf,
129,1998,Plan Efficiency,Analyzing External Conditions to Improve the Efficiency of HTN Planning,"Reiko Tsuneto, James Hendler, Dana Nau","One difficulty with existing theoretical work on HTN planning is that it does not address some of the planning constructs that are commonly used in HTN planners for practical applications. Although such constructs can make it difficult to ensure the soundness and completeness of HTN planning, they are important because they can greatly improve the efficiency of planning in practice. In this paper, we describe a way to achieve some of the advantages of such constructs while preserving soundness and completeness, through the use of what we will call external conditions. We describe how to detect some kinds of external conditions automatically by preprocessing the planner’s knowledge base, and how to use this knowledge to improve the efficiency of the planner’s refinement strategy. We present experimental results showing that by making use of external conditions as described here, an HTN planner can be significantly more efficient and scale better to large problems.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-129.pdf,
130,1998,Plan Execution,"Managing Multiple Tasks in Complex, Dynamic Environments",Michael Freed,"Sketchy planners are designed to achieve goals in realistically complex, time-pressured, and uncertain task environments. However, the ability to manage multiple, potentially interacting tasks in such environments requires extensions to the functionality these systems typically provide. This paper identifies a number of factors affecting how interacting tasks should be prioritized, interrupted, and resumed, and then describes a sketchy planner called APEX that takes account of these factors when managing multiple tasks.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-130.pdf,
131,1998,Plan Execution,Maintaining Consistency in Hierarchical Reasoning,"Robert E. Wray, III, John Laird","We explore techniques for maintaining consistency in reasoning when employing dynamic hierarchical task decompositions. In particular, we consider the difficulty of maintaining consistency when an agent nonmonotonically modifies an assumption in one level of the task hierarchy and that assumption depends upon potentially dynamic assertions higher in the hierarchy. The hypothesis of our work is that reasoning maintenance can be extended to hierarchical systems such that consistency is maintained across all levels of the hierarchy. We introduce two novel extensions to standard reason mamtenance approaches, assumptzon justification and dynamac hierarchical justification, both of which provide the necessary capabilities. The key difference between the two methods is whether a particular assumption (assumption justification) or an entire level of the hierarchy (dynamic hierarchical justification) is disabled when an inconsistency is found. Our investigations suggest that dynamic hierarchical justification has advantages over assumption justification, especially when the task decomposition is well-constructed. Agents using dynamic hierarchical justification also compare favorably to agents using less complete methods for reasoning consistency, improving the reactivity of hierarchical architectures while eliminating the need for knowledge that otherwise would be required to maintain reasoning consistency.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-131.pdf,
132,1998,Plan Recognition,Acquisition of Abstract Plan Descriptions for Plan Recognition,Mathias Bauer,"While most plan recognition systems make use of a plan library that contains the set of available plan hypotheses, little effort has been devoted to the question of how to create such a library. This problem is particularly difficult to deal with when only little domain knowledge is available---a common situation when e.g. developing a help system for an already existing software system. This paper describes how operational decompositions of plans can be extracted from a set of sample action sequences, thus providing the basis for automating the acquisition of plan libraries. Efficient algorithms for the approximation of optimal decompositions and experimental results supporting their feasibility are presented.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-132.pdf,
133,1998,Plan Recognition,Needles in a Haystack: Plan Recognition in Large Spatial Domains Involving Multiple Agents,"Mark Devaney, Ashwin Ram","While plan recognition research has been applied to a wide variety of problems, it has largely made identical assumptions about the number of agents participating in the plan, the observability of the plan execution process, and the scale of the domain. We describe a method for plan recognition in a real-world domain involving large numbers of agents performing spatial maneuvers in concert under conditions of limited observability. These assumptions differ radically from those traditionally made in plan recognition and produce a problem which combines aspects of the fields of plan recognition, pattern recognition, and object tracking. We describe our initial solution which borrows and builds upon research from each of these areas, employing a pattern-directed approach to recognize individual movements and generalizing these to produce inferences of large-scale behavior.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-133.pdf,
134,1998,Planning as Satisfiability,"Act, and the Rest Will Follow: Exploiting Determinism in Planning as Satisfiability","Enrico Giunchiglia, Alessandro Massarotto, Roberto Sebastiani","In this paper we focus on Planning as Satisfiability (SAT). We build from the simple consideration that the values of uents at a certain time point derive deterministically from the initial situation and the sequence of actions performed till that point. Thus, the choice of actions to perform is the only source of nondeterminism. This is a rather trivial consideration, but which has important positive consequences if implemented in current planners via SAT. In fact, it produces a dramatic size reduction of the space of the truth assignments searched in by the SAT decider used to solve the final SAT problem. To justify this claim, we repeat many of the experiments reported in (Ernst, Millstein, and Weld 1997), and show that the CPU time requested to solve a problem can go down up to 4 orders of magnitude.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-134.pdf,
135,1998,Planning as Satisfiability,Using Caching to Solve Larger Probabilistic Planning Problems,"Stephen M. Majercik, Michael L. Littman","Probabilistic planning algorithms seek effective plans for large, stochastic domains. maxplan is a recently developed algorithm that converts a planning problem into an E-Majsat problem, an NP^PP-complete problem that is essentially a probabilistic version of Sat , and draws on techniques from Boolean satisfiability and dynamic programming to solve the E-Majsat problem. This solution method is able to solve planning problems at state-of-the-art speeds, but it depends on the ability to store a value for each CNF subformula encountered in the solution process and is therefore quite memory intensive; searching for moderate-size plans even on simple problems can exhaust memory. This paper presents two techniques, based on caching, that overcome this problem without significant performance degradation. The first technique uses an LRU cache to store a fixed number of subformula values. The second technique uses a heuristic based on a measure of subformula difficulty to selectively save the values of only those subformulas whose values are sufficiently difficult to compute and are likely to be reused later in the solution process. We report results for both techniques on a stochastic test problem.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-135.pdf,
136,1998,Human-Robot Interaction,Alternative Essences of Intelligence,"Rodney A. Brooks, Cynthia Breazeal (Ferrell), Robert Irie, Charles C. Kemp, Matthew Marjanovic, Brian Scassellati, Matthew M. Williamson","We present a novel methodology for building human-like artificially intelligent systems. We take as a model the only existing systems which are universally accepted as intelligent: humans. We emphasize building intelligent systems which are not masters of a single domain, but, like humans, are adept at performing a variety of complex tasks in the real world. Using evidence from cognitive science and neuroscience, we suggest four alternative essences of intelligence to those held by classical AI. These are the parallel themes of development, social interaction, embodiment, and integration. Following a methodology based on these themes, we have built a physical humanoid robot. In this paper we present our methodology and the insights it affords for facilitating learning, simplifying the computation underlying rich behavior, and building systems that can scale to more complex tasks in more challenging environments.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-136.pdf,
137,1998,Human-Robot Interaction,Eye Finding via Face Detection for a Foveated Active Vision System,Brian Scassellati,"Eye finding is the first step toward building a machine that can recognize social cues, like eye contact and gaze direction, in a natural context. In this paper, we present a real-time implementation of an eye finding algorithm for a foveated active vision system. The system uses a motion-based prefilter to identify potential face locations. These locations are analyzed for faces with a template-based algorithm developed by Sinha (1996). Detected faces are tracked in real time, and the active vision system saccades to the face using a learned sensorimotor mapping. Once gaze has been centered on the face, a high-resolution image of the eye can be captured from the foveal camera using a self-calibrated peripheral-to-foveal mapping. We also present a performance analysis of Sinha’s ratio template algorithm on a standard set of static face images. Although this algorithm performs relatively poorly on static images, this result is a poor indicator of real-time performance of the behaving system. We find that our system finds eyes in 94% of a set of behavioral trials. We suggest that alternate means of evaluating behavioral systems are necessary.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-137.pdf,
138,1998,Human-Robot Interaction,Template-Based Recognition of Pose and Motion Gestures on a Mobile Robot,"Stefan Waldherr, Sebastian Thrun, Roseli Romero, Dimitris Margaritis","For mobile robots to assist people in everyday life, they must be easy to instruct. This paper describes a gesture-based interface for human robot interaction, which enables people to instruct robots through easy-to-performarmgestures. Such gestures might be static pose gestures, which involve only a specific configuration of the person’s arm, or they might be dynamic motion gestures (such as waving). Gestures are recognized in real-time at approximate frame rate, using a hybrid approach that integrates neural networks and template matching. A fast, color-based tracking algorithm enables the robot to track and follow a person reliably through office environments with drastically changing lighting conditions. Results are reported in the context of an interactive clean-up task, where a person guides the robot to specific locations that need to be cleaned, and the robot picks up trash which it then delivers to the nearest trash-bin.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-138.pdf,
139,1998,Robot Navigation,Position Estimation for Mobile Robots in Dynamic Environments,"Dieter Fox, Wolfram Burgard, Sebastian Thrun, Armin B. Cremers","For mobile robots to be successful, they have to navigate safely in populated and dynamic environments. While recent research has led to a variety of localization methods that can track robots well in static environments, we still lack methods that can robustly localize mobile robots in dynamic environments, in which people block the robot’s sensors for extensive periods of time or the position of furniture may change. This paper proposes extensions to Markov localization algorithms enabling them to localize mobile robots even in densely populated environments. Two different filters for determining the ""believability"" of sensor readings are employed. These filters are designed to detect sensor readings that are corrupted by humans or unexpected changes in the environment. The technique was recently implemented and applied as part of an installation, in which a mobile robot gave interactive tours to visitors of the ""Deutsches Museum Bonn."" Extensive empirical tests involving datasets recorded during peak traffic hours in the museum demonstrate that this approach is able to accurately estimate the robot’s position in more than 98% of the cases even in such highly dynamic environments.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-139.pdf,
140,1998,Robot Navigation,Integrating Topological and Metric Maps for Mobile Robot Navigation: A Statistical Approach,"Sebastian Thrun, Jens-Steffen Gutmann, Dieter Fox, Wolfram Burgard, Benjamin J. Kuipers","The problem of concurrent mapping and localization has received considerable attention in the mobile robotics community. Existing approaches can largely be grouped into two distinct paradigms: topological and metric. This paper proposes a method that integrates both. It poses the mapping problemas a statistical maximum likelihood problem, and devises an efficient algorithm for search in likelihood space. It presents an novel mapping algorithm that integrates two phases: a topological and a metric mapping phase. The topological mapping phase solves a global position alignment problem between potentially indistinguishable, significant places. The subsequent metric mapping phase produces a fine-grained metric map of the environment in floating-point resolution. The approach is demonstrated empirically to scale up to large, cyclic, and highly ambiguous environments.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-140.pdf,
141,1998,Sound Understanding,The Role of Data Reprocessing in Complex Acoustic Environments,"Frank Klassner, Victor Lesser, Hamid Nawab","The Integrated Processing and Understanding of Signals (IPUS) architecture is a general blackboard framework for structuring bidirectional interaction between front-end signal processing algorithms (SPAs) and signal understanding processes. To date, reported work on the architecture has focused on proof-of-concept demonstrations of how well a sound-understandingtestbed (SUT) basedon IPUS could use small libraries of sound models and small sets of SPAs to analyze acoustic scenarios. In this paper we evaluate how well the architecture scales up to more complex environments. We describe knowledge-representation and control-strategy issues involved in scaling up an IPUS-based SUT for use with a library of 40 sound models, and present empirical evaluation that shows (a) the IPUS data reprocessing paradigm can increase interpretation accuracy by 25% - 50% in complex scenarios, and (b) the benefit increases with increasing complexity of the environment.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-141.pdf,
142,1998,Sound Understanding,Sound Ontology for Computational Auditory Scence Analysis,"Tomohiro Nakatani, Hiroshi G. Okuno","This paper proposes that sound ontology should be used both as a common vocabulary for sound representation and as a common terminology for integrating various sound stream segregation systems. Since research on computational auditory scene analysis (CASA) focuses on recognizing and understanding various kinds of sounds, sound stream segregation which extracts each sound stream from a mixture of sounds is essential for CASA. Even if sound stream segregation systems use a harmonic structure of sound as a cue of segregation, it is not easy to integrate such systems because the definition of a harmonic structure differs or the precision of extracted harmonic structures differs. Therefore, sound ontology is needed as a common knowledge representation of sounds. Another problem is to interface sound stream segregation systems with applications such as automatic speech recognition systems. Since the requirement of the quality of segregated sound streams depends on applications, sound stream segregation systems must provide a flexible interface. Therefore, sound ontology is needed to fulfill the requirements imposed by them. In addition, the hierarchical structure of sound ontology provides a means of controlling top-down and bottom-up processing of sound stream segregation.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-142.pdf,
143,1998,SIGART / AAAI Doctoral Consortium Abstracts,Optimizing Information Agents by Selectively Materializing Data,Naveen Ashish,"There is currently great interest in building information gathering agents that provide integrated access to data from a number of distributed Web sources. Some of the prominent research projects in this area include The Internet Softbot, Information Manifold, InfoMaster and InfoSleuth. A critical problem faced by such agents is a high query response time, due to the fact that a lot of data from Web sources has to be retrieved, extracted, and integrated to answer queries. The aim of this work is to develop an approach for improving query response time in information agents. The resulting system is being developed as part of the Ariadne project (Knoblock et al. 1998).",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-165.pdf,
144,1998,SIGART / AAAI Doctoral Consortium Abstracts,Generating Adequate Instructions: Knowing When to Stop,Juliet C. Bourne,"Adequate instructions are essential to the correct performance of actions. An instruction is adequate if its action(s) and objects are identified sufficiently and unambiguously, given the instruction’s context. For instance, the instruction Turn the knob would be inadequate if, in the context, more than one knob or one way of turning a knob were salient. However, even if the knob and the manner of turning were uniquely identifiable, the instruction could still be inadequate since it does not tell the reader when to stop turning the knob. What is missing here is the termination information for the action, or when the performance of the action is to end. Conveying such information in automated text generation is the focus of my research. I consider how to incorporate termination information into an action representation and how to generate adequate instructions from instances of the action representation, including how tochoose appropriate expressions for conveying action termination.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-166.pdf,
145,1998,SIGART / AAAI Doctoral Consortium Abstracts,HR - Automatic Concept Formation in Finite Algebras,Simon Colton,"We are investigating how and why mathematicians invent new concepts while developing a theory, and implementing our ideas into the HR system, which automatically produces, assesses and displays concepts infinite algebras, such as finite group theory. We first determined a reason for HR to produce concepts - to classify a given set of groups up to isomorphism. Doing so would involve inventing concepts which help describe groups, so a classification can occur, and inventing concepts which help generate new examples of groups, so that improvements to the classification are necessitated, perpetuating the process. Next, we developed measures to tell us how interesting the concepts produced were (see Colton 1997). This helped us determine the kinds of concepts HR should produce and with this in mind, we implemented production rules taking one (or two) concepts as input and outputting a new concept.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-167.pdf,
146,1998,SIGART / AAAI Doctoral Consortium Abstracts,Optimizing Initial Configurations of Neural Networks for the Task of Natural Language Learning,Jaime J. Dávila,"One approach used to develop computer systems for natural language processing (NLP) is that of Artificial Neural Networks (NNs). Because of the large number of parameters a NN has (e.g. network topology, learning algorithm, transfer functions) and the way they interact, finding optimal parameter values for any particular task can be extremely difficult.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-168.pdf,
147,1998,SIGART / AAAI Doctoral Consortium Abstracts,Pragmatic Multi-Agent Learning,Andrew Garland,"Early models of procedural learning assumed actors were isolated, model-based thinkers. More recently, learning techniques have become more sophisticated as this assumption has been replaced with more realistic ones. To date, however, there has been no thorough investigation of multiple, heterogeneous, situated agents who learn from the pragmatics of their domain rather than from a model. This research focuses on this important problem and develops learning techniques that allow agents to improve their performance in a dynamic environment by learning from past run-time behavior.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-169.pdf,
148,1998,SIGART / AAAI Doctoral Consortium Abstracts,"Perception, Memory, and the Field of View Problem",William S. Gribble,"Robust control of a vision-based agent requires tight coupling between sensing and action. For mobile robots performing visually-guided navigation, this means closed-loop control of motion with respect to sensed features, landmarks, or other relevant parts of the visible environment. Real vision sensors have limited fields of view. This makes true closed-loop control with respect to an arbitrary set of landmarks impossible with practical vision systems, since only a fraction of the environment can be seen at any one time.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-170.pdf,
149,1998,SIGART / AAAI Doctoral Consortium Abstracts,Exploiting Diversity for Natural Language Processing,John C. Henderson,"The recent popularity of applying machine learning methods to computational linguistics problems has given rise to a large supply of trainable natural language processing systems. Most problems of interest have an array of off-the-shelf products or downloadable code implementing solutions of varying quality using varying techniques. The task this thesis is concerned with is developing reasonable methods for combining the outputs of a diverse set of systems which all address the same task. The hope is that if the set has a high enough initial accuracy and independently assorted errors, we can produce a more accurate solution using an appropriate combining method. In addition, there are principles that initial system developers should keep in mind which will help them create a family of diverse systems. We are also interested in developing methods for increasing the diversity of a set of systems without sacrificing accuracy, for the sake of fruitful combination. Each task we approach will warrant a separate investigation into how to combine outputs, and hope lies in discovering the principles that are common among all tasks. We don’t want to study just one learning method or task, instead we want to discover principles that can be applied universally, or to a broad class of problems.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-171.pdf,
150,1998,SIGART / AAAI Doctoral Consortium Abstracts,"Multimodal, Multilevel Selective Attention",Micheal Hewett,"Early knowledge based systems did not incorporate high-bandwidth I/O due to performance limitations of computers of that era. Today, intelligent agents and robots running on much more powerful computers can incorporate vision, sound, network, sonar and other modes of input. These additional inputs provide much more information about the environment, but bring additional problems related to control of perception. Perceptual input streams (called modes in the psychology literature) can have greatly varying bandwidth. In people, the sense of touch has a low bandwidth, while the sense of vision has a very high bandwidth. The human brain can not completely process all of the information from one high bandwidth mode, much less simultaneously process all the information available from all modes. To control the amount of perceptual input processed, humans use selective attention (Treisman 1993). Computer vision is a diffcult problem which, if solved, could provide robots with a large amount of useful information. However, visual input can not be processed effciently without using a top-down attention mechanism (Tsotsos 1987). Several computational models of visual selective attention have been developed (e.g. (Reece 1992)).",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-172.pdf,
151,1998,SIGART / AAAI Doctoral Consortium Abstracts,Learning in Markov Games with Incomplete Information,Junling Hu,"The Markov game (also called stochastic game (Filar and Vrieze 1997)) has been adopted as a theoretical framework for multiagent reinforcement learning (Littman 1994). In a Markov game, there are n agents, each facing a Markov decision process (MDP). All agents’ MDPs are correlated through their reward functions and the state transition function. As Markov decision process provides a theoretical framework for single-agent reinforcement learning, Markov games provide such a framework for multiagent reinforcement learning.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-173.pdf,
152,1998,SIGART / AAAI Doctoral Consortium Abstracts,Extending the Classification Paradigm to Temporal Domains,Mohammed Waleed Kadous,"One of the primary areas of machine learning research has been supervised concept learning--given some information about examples whose class is known, the goal is to produce a classifier which can classify examples whose class is not known. In general, research in this area has focused on situations where an object’s attributes do not change in the short term. However, in many real-world domains, such as speech, sign language, robotics and medicine, many of the classification tasks involve dynamic attributes. Furthermore, temporal properties are critical to classification. The current work involves developing a temporal classification learner that works in a variety of domains, does not require excessive amounts of data and is able to produce comprehensible descriptions of the concepts, while still having high predictive accuracy.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-174.pdf,
153,1998,SIGART / AAAI Doctoral Consortium Abstracts,Data Mining for Maintenance of Complex Systems,Sylvain Létourneau,"The operation and maintenance of modern sensor-equipped systems such as aircraft generates vast amounts of complex data. Proper use of this data to predict or explain component failures may lead to saving of several thousands of dollars, reducing the number of delays, and increasing the overall level of safety. The field of Knowledge Discovery in Databases (KDD) has delivered a variety of techniques to discover patterns from vast amounts of data. However, none of these techniques are designed to handle the diverse forms of data typically generated during the operation and maintenance of such complex systems. In this research, we study the specific issues to consider during the analysis of commercial aircraft data and propose adequate enhancements of the data mining process to handle these difficulties. The anticipated contributions of this research are related to two fundamental problems in the field of knowledge discovery in databases: i) automatic preparation of the data prior to model development, and ii) use of diverse sources of information.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-175.pdf,
154,1998,SIGART / AAAI Doctoral Consortium Abstracts,Empirical Acquisition of Word-Sense Distinctions,Tom O'Hara,"Applications currently make use of many kinds of lexical information: categorial relations (""dog"" is-a ""canine""), synonymy (""pooch"" same-as ""dog"") and word associations (""walk"" occurs-often-with ""dog""). However, an important type of information, differential is often omitted, especially in broad-coverage applications. Differentia are properties that distinguish a concept from others belonging to the same higher-level category. For instance, both beagles and wolfhounds are hounds, but the former are small, whereas the latter are quite large. Applications should incorporate differentia to provide finer word-sense distinctions and to facilitate inference of information not mentioned in the text.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-176.pdf,
155,1998,SIGART / AAAI Doctoral Consortium Abstracts,Neural Approaches to Blind Separation and Cumulant Analysis and Its Application to Diagnostics of Nuclear Power Plants,Alexei Ourmanov,"The problem concerned is to explore the possibility of using artificial intelligence techniques, namely neural networks, and design the appropriate neural network-based algorithm to detect signals of interest from multi-channel data recordings. The problem finds application in diagnostic systems of nuclear power plant with liquid-metal fast breeder. The idea of a whole approach is to make an adaptive diagnostic system of acoustic monitoring of a steam generator unit. The system is based on neural network feature extraction and pattern recognition of multi-channel acoustic signals generated by a steam generator unit. In the background noise environment the diagnostic system must detect water leaks in sodium which may occur in the steam generator unit under monitoring.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-177.pdf,
156,1998,SIGART / AAAI Doctoral Consortium Abstracts,Bayesian Reasoning for Tropical Cyclone Intensity Forecasting and Risk Analysis,Grace W. Rumantir,Improved methods for tropical cyclone (TC) intensity forecasting and risk analysis are the end products of this project leading to the wider use in the mitigation of the devastating impacts of tropical cyclones. Single-disciplined approaches in TC intensity forecasting by meteorologists and in risk analysis by social scientists so far have not seen satisfactory results. This is because of the intrinsically high degree of complexity in both the modelling and the problem domain parts of the project.,https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-178.pdf,
157,1998,SIGART / AAAI Doctoral Consortium Abstracts,Rational Multiagent Organization and Reorganization,Wayne A. Smith,"This research agenda has as its goal to apply principles from decision theory and organization theory to the problem of multiagent organization and reorganization. The result is a rational, dynamic multiagent architecture designed to address the following issues: Choosing the appropriate organizational structure (design from scratch); Choosing the appropriate reorganization (redesign); Adding, losing and moving agents within an organization; and Handling node and link failures that lead to agents becoming unavailable to the rest of the organization. The architecture will be experimentally validated with different organization sizes ranging from four to fifty agents. These experiments will determine how well a dynamic organization fares versus a static one with respect to solution quality and response time.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-179.pdf,
158,1998,SIGART / AAAI Doctoral Consortium Abstracts,A Script-Based Approach to Modifying Knowledge-Based Systems,Marcelo Tallis,"Modifying knowledge-based systems (KBSs) is a complex activity that needs to be performed very often. The occurrence of changes in a KBSs environment, requests for extending the system’s functionality, and the debugging of inadequate knowledge are some of the events that demand modifications to a KBSs. One of the difficulties of KBS modifications is that they might require the modification of several related portions of the system. Determining what portions have to be changed and how tochange them requires a deep understanding of how the elements of the KBS interact. This requirement is especially hard for users when the rationale behind the design of a KBS or the details of its implementation are unknown.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-180.pdf,
159,1998,Student Abstracts,Learning to Teach with a Reinforcement Learning Agent,Joseph E. Beck,"Intelligent tutoring systems (ITS) use artificial intelligence techniques to customize their instruction to fit the needs of each student. To do this, the system must have knowledge of the student being taught (commonly called a student model ) and a set of pedagogical rules that enable the system to follow good teaching principles. Teaching rules are commonly represented as a set of ""if-then"" production rules, where the ""if"" side is dependent on the student model, and the ""then"" side is a teaching action. For example, a rule may beof the form ""IF (the student has never been introduced to the current topic) THEN (teach the topic to the student).""",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-181.pdf,
160,1998,Student Abstracts,Genetic Search for Accurate Feature Sets,Brendan Burns,"This abstract describes a feature selection system, INDiGENT. INDiGENT has been to designed to enhance knowledge based neural networks by genetically searching the set of input features for an optimum subset. This search is designed to enable INDiGENT to make more accurate classifications. Signifigantly, INDiGENT has shown that it can obtain similar increases in accuracy as more complicated theory revision systems.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-182.pdf,
161,1998,Student Abstracts,A First Analysis of Qualitative Influences and Synergies,"Jesús Cerquides, Ramon López de Màntaras","Comprehensibility is a key characteristic for learning algorithms results to be useful in Knowledge Discovery in Databases tasks. Bayesian reasoning has been usually criticized as hard to explain and understand, but achieves high performance rates with simple constructs, as happens for instance with the Naive-Bayes classifier. Our approach can be viewed as a refinement of qualitative probabilistic networks in order to allow them to do the work probabilistic networks do, or as a way of showing that, slightly modified, Elsaesser’s explanations can be used for reasoning and prediction, achieving results similar to Bayesian reasoning, while keeping intact their interpretability.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-183.pdf,
162,1998,Student Abstracts,A New Approach to Rule Interest Measures,"Jesús Cerquides, Ramon López de Màntaras","Rule extraction is one of the main tasks in the Knowledge Discovery process. Our hypothesis is that the interestingness of rules is strongly related to statistical independence between facts. Human reasoning assumes by default statistical independence and when a rule breaks this assumption it is considered interesting. This fact has been noticed long ago and even proposed as a principle for rule-interest measures. However, to the best of our knowledge, nobody has correctly developedthis principle.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-184.pdf,
163,1998,Student Abstracts,Classification Using an Online Genetic Algorithm,Brian D. Davison,"Genetic Algorithms (GAs) purport to mimic the behavior of natural selection. Many GAs, however, try to optimize their populations by means of a static fitness function --one that is derived from performance on a fixed set of examples. We propose an architecture for an online genetic algorithm (OLGA) for classification. An OLGA differs from standard genetic algorithms in that it does not repeatedly evaluate individuals against a fixed set of training examples. Instead, it is presented with a series of training examples, one at a time, and does not retain the entire set for training.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-185.pdf,
164,1998,Student Abstracts,Plan Recognition in Complex Spatial Domains,Mark Devaney,"We are researching the problem of plan recognition in a complex real-world domain consisting of training battles conducted by actual troops at the US Army’s National Training Center. These battles involve hundreds of participants, last several days, and take place over a very large geographical area. Our task is to identify from this data repeated patterns of movement which correspond to planned ""maneuvers"" ---- coordinated activities which generate identifiable patterns of movement that can be identified and used as a basis for prediction.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-186.pdf,
165,1998,Student Abstracts,Nested Joint Probability Model for Morphological Analysis and its Grid Pruning,"Koji Fujimoto, Nobuo Inui, Yoshiyuki Kotani","In recent work on morphological analysis based on statistical models, the conditional probability of the observed i-th word wi with the i-th tag ti after the (i-1)-th tag ti-1 is defined as the product of observation symbol probability and the state transition probability (i.e. P(wi | ti) times P(ti | ti-1) ). In order to improve accuracy, we face the following problems: 1) if we build hidden state levels using stricter categories (e.g. lowest POS class, over 3-gram, or word themselves), the state transition probability matrix becomes much bigger and more sparse; 2) if we use rough categories, the reliability of statistical information becomes lower in some parts of speech; and 3) the best state level is not the same among POS category, and some heuristic knowledge is necessary to select the best state structure.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-187.pdf,
166,1998,Student Abstracts,Generalized A* for Cyclic AND/OR Graphs,Supriyo Ghose,"The A* algorithm has been the cornerstone of state-space search methods. Simultaneously, the vexing problem of cycles in AND/OR graphs has received considerable attention in recent times. We propose a generalization of A* to search AND/OR graphs that may contain cycles.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-188.pdf,
167,1998,Student Abstracts,Selection of Conflict Resolution Strategies in Dynamically Organized Sensible Agent-Based Systems,"T. H. Liu, K. S. Barber","A Multi-Agent System can be seen as a group of entities interacting to achieve individual or collective goals. In the past two decades, researchers have developed various MAS architectures, one being the Sensible Agent (SA) model (Barber 1996). Because one specific level of autonomy is not suitable for all situations in dynamic environments, SAs are equipped with the capability to reason about and switch among levels of autonomy. Typical autonomy levels (which are assigned to goals instead of agents) include: command-driven, master, consensus, and locally autonomous.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-189.pdf,
168,1998,Student Abstracts,Refinement-Based Planning as Satisfiability,Amol D. Mali,"A classical planning problem is the problem of computing a sequence of actions that transforms one state of the world into the desired. Traditional planners cast planning as a ""split and prune"" type search. These are called ""refinement planners"" since they start with a null plan and refine it by adding constraints. It has been shown recently that planning problems are far easier to solve when they are cast as model finding problems [2]. Some schemes for automated generation of the encodings of the planning problems in propositional logic have been designed [1]. However these schemes lack several of the refinements and preprocessing that traditional planners use. Since no single encoding has been shown to have the smallest size and the best performance, it is necessary to know what the space of the encodings is, to make a more exible and efficient exploration of the encodings possible.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-190.pdf,
169,1998,Student Abstracts,Goal and Responsibility Allocation in Sensible Agent-Based Systems,"Ryan McKay, K. S. Barber","A Multi-Agent System(MAS) can be seen as a group of entities interacting to achieve individual or collective goals. Communication is a central issue in this interaction between agents. Protocols such as the Contract Net Protocol(CNP) (Smith, 1980) have been proposed to address the coordination level of communication in predefined organizational structures with predefined agent interaction mechanisms. The research presented here applies and extends this protocol to address issues in one particular MAS - Sensible Agents(SA) (Barber, 1996).",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-191.pdf,
170,1998,Student Abstracts,Tutorial Response Generation in a Writing Tool for Deaf Learners of English,Lisa N. Michaud,"ICICLE (Interactive Computer Identification and Correction of Language Errors) is a tutoring system under development that instructs deaf users of American Sign Language on written English skills1 . (See (McCoy and Masterman (Michaud) 1997) for a discussion of overall system architecture.) The text generation module it will employ produces original text to instruct the user on errors found in his or her writing, tailored to the user' s understanding and learning style. The model I propose for planning this text composes it according to a four-tier response anatomy. It combines bottom-up and top-down planning approaches and takes into account a detailed representation of user language proficiency and a history of interaction with a user in order to create text that is maximally understandable and informative to the individual.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-192.pdf,
171,1998,Student Abstracts,Dependent Bigram Identification,Ted Pedersen,"Dependent bigrams are two consecutive words that occur together in a text more often than would be expected purely by chance. Identifying such bigrams is an important issue since they provide valuable clues for machine translation, word sense disambiguation, and information retrieval. A variety of significance tests have been proposed (e.g., Church et. al., 1991, Dunning, 1993, Pedersen et. al, 1996) to identify these interesting lexical pairs. In this poster I present a new statistic, minimum sensitivity, that is simple to compute and is free from the underlying distributional assumptions commonly made by significance tests.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-193.pdf,
172,1998,Student Abstracts,Raw Corpus Word Sense Disambiguation,Ted Pedersen,"A wide range of approaches have been applied to word sense disambiguation. However, most require manually crafted knowledge such as annotated text, machine readable dictionaries or thesari, semantic networks, or aligned bilingual corpora. The reliance on these knowledge sources limits portability since they generally exist only for selected domains and languages. This poster presents a corpus{based approach where multiple usages of an ambiguous word are divided into a specified number of sense groups based strictly on features that are automatically obtained from the immediately surrounding raw text.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-194.pdf,
173,1998,Student Abstracts,Discourse learning: Dialogue Act Tagging with Transformation-Based Learning,Ken Samuel,"My central goal is to compute dialogue acts automatically. A dialogue act is a concise abstraction of a speaker’s intention, such as SUGGEST and REQUEST. Recognizing dialogue acts is critical to understanding at the discourse level, and dialogue acts can also be useful for other applications, such as resolving ambiguity in speech recognition. But, often, a dialogue act cannot be directly inferred from a literal reading of an utterance.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-195.pdf,
174,1998,Student Abstracts,Estimating the Expected Error of Empirical Minimizers for Model Selection,"Tobias Scheffer, Thorsten Joachims","Model selection is considered the problem of choosing a hypothesis language which provides an optimal balance between low empirical error and high structural complexity. In this Abstract, we discuss the intuition of a new, very efficient approach to model selection. Our approach is inherently Bayesian, but instead of using priors on target functions or hypotheses, we talk about priors on error values--which leads us to a new mathematical characterization of the expected true error.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-196.pdf,
175,1998,Student Abstracts,Pluto: Managing Multistrategy Learning through Planning,"Gordon T. Shippey, J. William Murdock, Ashwin Ram","Multistrategy learning systems are systems that employ multiple methods to solve learning problems. In many multistrategy systems, either the user or the system selects a single method to use on the current problem. At best, such a selection-type multistrategy learning system can solve the union of the problems solvable by the individual learning methods it contains. Another strategy is to have the user specify a sequence of methods to apply to a particular problem at compile time. Both of these strategies fails to tap the full potential of the learning methods under their control.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-197.pdf,
176,1998,Student Abstracts,A Framework for Reinforcement Learning on Real Robots,"William D. Smart, Leslie Pack Kaelbling","Robots are now being used in complex, unstructured environments, performing ever more sophisticated tasks. As the task and environmental complexity increases, the need for effective learning on such systems is becoming more and more apparent. Robot programmers often find it difficult to articulate their knowledge of how to perform a given task in a form suitable for robots to use. Even when they can, the limitations of robot sensors and actuators might render their intuitions less effective. Also, it is often not possible to anticipate (and code for) all environments in which the robot might find itself having to perform a certain task. Therefore, it seems useful to have the robot be able to learn to act, in the hope of overcoming these two difficulties.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-198.pdf,
177,1998,Student Abstracts,Handling Inconsistency for Multi-Source Integration,"Sheila Tejada, Craig A. Knoblock, Steven Minton","The overwhelming amount of information sources now available through the internet has increased the need to combine or integrate the data retrieved from these sources in an intelligent and efficient manner. A desirable approach for information integration would be to have a single interface, like the SIMS information broker [1], which allows access to multiple information sources. An example application is to retrieve all the menus of restaurants from Joe' s Favorite Restaurants site which have been rated highly by the Department of Health.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-199.pdf,
178,1998,Student Abstracts,Emotion-Based Agents,"Rodrigo M. M. Ventura, Carlos A. Pinto-Ferreira","To survive in a dynamic and rich environment, human beings have to process very complex stimuli in real time. Whatever artificial system satisfying the challenge of achieving a similar performance in complex data handling, ought to incorporate mechanisms specifically conceived to perform efficiently. We hypothesize that such efficiency oriented systems process stimuli - simultaneously - under two different perspectives: a cognitive, elaborative - which allows them to understand what is happening and what they know about the world, and a perceptual, immediate - which permits them to react quickly and decide adequately in circumstances demanding urgent action. Hence, from the very same complex stimulus, two sets of facets are extracted: one, mostly directed to recognition and reasoning purposes, and another, aiming at assigning degrees of threat, danger, pleasure, and so on, to the current situation, constructing what we call a vector of desirability.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-200.pdf,
179,1998,Student Abstracts,DL-$elect: A Decision-List-Based Data-Mining System,Karl Weinmeister,"The application of machine-learning algorithms to the financial markets has been increasing in popularity in recent years. The majority of systems that have been created for the purpose of selecting stocks have utilized neural-network techniques. Our research has dealt with the feasibility of inductive logic approaches and the creation of a decision-list-based data-mining system, DL-$elect.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-201.pdf,
180,1998,Student Abstracts,Ensuring Reasoning Consistency in Hierarchical Architectures,"Robert E. Wray, III, John Laird","Ensuring reasoning consistency in agents employing hierarchical task decompositions can be difficult. We have identified two specific problems that result when hierarchical context changes during problem solving. First, the agent can be too responsive, taking an external act or making an internal derivation before higher context is fully elaborated. Second, the agent can fail to respond to a change in the context, leaving a local level ""unsituated"" with respect to the higher contexts. In both cases, an agent’s assertions of knowledge in a local level can be inconsistent with the larger context, leading potentially to irrational behavior.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-202.pdf,
181,1998,Student Abstracts,Building Agents from Shared Ontologies through Apprenticeship Multistrategy Learning,"Kathryn Wright, Mihai Boicu, Seok Won Lee, Gheorghe Tecuci","The goal of this research is to create the Disciple Learning Agent Shell for efficient development of personal agents, that relies on importing ontologies from existing repositories using the Open Knowledge Base Connectivity (OKBC) protocol and on teaching the agents to perform various tasks through apprenticeship and multistrategy learning.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-203.pdf,
182,1998,Student Abstracts,Development of Outdoor Navigation for a Robotic Wheelchair System,Holly A. Yanco,"The goal of this research is the creation of a complete robotic wheelchair system to be used by people unable to drive standard powered wheelchairs. A complete robotic wheelchair system must be able to navigate indoor and outdoor environments and should switch automatically between navigation modes. For the system to be useful, it must be easily customized for the specific access methods required for each user. This abstract focuses on the vision system being developed for outdoor navigation and the method for selecting whether to use the indoor navigation mode or the outdoor navigation mode. A report of the indoor navigation mode and user interface can be found in (Yanco In press).",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-204.pdf,
183,1998,Invited Talk,Structured Probabilistic Models: Bayesian Networks and Beyond,Daphne Koller,"Two of the most important threads of work in knowledge representation today are frame-based representation systems (FRS’s) and Bayesian networks (BNs). FRS’s provide an excellent representation for the organizational structure of large complex domains, but their applicability is limited because of their inability to deal with uncertainty and noise. BNs provide an intuitive and coherent probabilistic representation of our uncertainty, but are very limited in their ability to handle complex structured domains. In this paper, we provide a language that cleanly integrates these approaches, preserving the advantages of both. Our approach allows us to provide natural and compact definitions of probability models for a class, in a way that is local to the class frame. These models can be instantiated for any set of interconnected instances, resulting in a coherent probability distribution over the instance properties. Our language also allows us to represent important types of uncertainty that cannot be accomodated within the frame-work of traditional BNs: uncertainty over the set of entities present in our model, and uncertainty about the relationships between these entities. We provide an inference algorithm for our language via a reduction to inference in standard Bayesian networks. We describe an implemented system that allows most of the main frame systems in existence today to annotate their knowledge bases with probabilistic information, and to use that information in answering probabilistic queries.",https://aaai.org/Library/AAAI/1998/../../../Papers/AAAI/1998/AAAI98-205.pdf,
