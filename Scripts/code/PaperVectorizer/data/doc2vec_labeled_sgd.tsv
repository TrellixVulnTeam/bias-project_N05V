measuring and mitigating unintended bias in text classification	introduce illustrate new approach measuring mitigating unintended bias machine learning models definition unintended bias parameterized test set subset input features illustrate used evaluate text classifiers using synthetic test set public corpus comments annotated toxicity wikipedia talk pages also demonstrate imbalances training data lead unintended bias resulting models therefore potentially unfair applications use set common demographic identity terms subset input features measure bias technique permits analysis common scenario demographic information authors readers unavailable bias mitigation must focus content text mitigation method introduce unsupervised approach based balancing training dataset demonstrate approach reduces unintended bias without compromising overall model quality	negative
value alignment, fair play, and the rights of service robots	ethics safety research artificial intelligence increasingly framed terms `` alignment '' human values interests argue turing 's call `` fair play machines '' early often overlooked contribution alignment literature turing 's appeal fair play suggests need correct human behavior accommodate machines surprising inversion value alignment treated today reflections `` fair play '' motivate novel interpretation turing 's notorious `` imitation game '' condition intelligence instead value alignment machine demonstrates minimal degree alignment norms conversation instance go undetected interrogated human carefully distinguish interpretation moral turing test motivated principle fair play instead depends imitation human moral behavior finally consider framework fair play used situate debate robot rights within alignment literature argue extending rights service robots operating public spaces `` fair '' precisely sense encourages alignment interests humans machines	negative
regulating artificial intelligence: proposal for a global solution	given ubiquity artificial intelligence ai modern societies clear individuals corporations countries grappling legal ethical issues use global problems require global solutions propose establishment international ai regulatory agency -- drawing interdisciplinary expertise -- could create unified framework regulation ai technologies inform development ai policies around world urge organization developed deliberate haste issues cryptocurrencies personalized political ad hacking autonomous vehicles autonomous weaponized agents already reality affecting international trade politics war	positive
using education as a model to capture good-faith effort for autonomous systems	multiprocess environments comprising several intercommunicating lisp systems straightforward implement due certain fundamental characteristics lisp language experiences four methods establishing necessary communications linkages described features lisp support experimentation interprocess communication identified two key characteristics language important regard 1. lisp programs construct interpret new code run 2. structures within lisp systems accessible name flexible communication methods use ordinary lisp input output functions supplemented small amount system-dependent code create communication linkages treated lisp file structures	negative
exploiting moral values to choose the right norms	norms constitute regulative mechanisms extensively enacted groups organisations societies however 'choosing right norms establish constitutes open problem requires consideration number constraints norm relations preference criteria e.g involved moral values paper advances state art normative multiagent systems literature formally defining problem proposing encoding linear program automatically solved	negative
 software malpractice in the age of ai: a guide for the wary tech company	professional malpractice concept heightened duties entrusted special knowledge crucial tasks rooted history yet since dawn computer age courts united states almost universally rejected theory software malpractice declining hold software engineers professional standards doctors lawyers engineers changing however speed software based artificial intelligence technologies replacing professionals already subject professional liability society already decided cases millennia ago tasks warrant special accountability new analysis human closest line adverse event ai expands pressure courts go one level causal chain search human agency professional accountability mount essay analyzes case law rejecting software malpractice clues doctrine might go age ai discusses technology companies learn safety enhancements doctors lawyers historic professionals adapted heightened legal scrutiny years	positive
non-discriminatory machine learning through convex fairness criteria	introduce novel technique achieve non-discrimination machine learning without sacrificing convexity probabilistic interpretation also propose new notion fairness machine learning called weighted proportional fairness show technique satisfies subjective fairness criterion	negative
 fair forests: regularized tree induction to minimize model bias	potential lack fairness outputs machine learning algorithms recently gained attention within research community well society broadly surprisingly prior work developing tree-induction algorithms building fair decision trees fair random forests methods widespread popularity one simultaneously interpretable non-linear easy-to-use paper develop knowledge first technique induction fair decision trees.we show `` fair forest '' retains benefits tree-based approach providing greater accuracy fairness alternatives `` group fairness '' `` individual fairness '' also introduce new measures fairness able handle multinomial continues attributes well regression problems opposed binary attributes labels finally demonstrate new robust evaluation procedure algorithms considers dataset entirety rather specific protected attribute	positive
 a framework for grounding the moral status of intelligent machines	propose framework derived moral theory assessing moral status intelligent machines using framework claim current foreseeable intelligent machines approximately much moral status plants trees environmental entities claim raises question obligations could moral agent e.g. normal adult human toward intelligent machine propose threshold moral obligation `` functional morality '' wallach allen 20 upper limit obligations exceed upper limit obligations toward plants trees environmental entities	positive
rethinking ai strategy and policy as entangled super wicked problems	paper attempts preliminary analysis general approach ai strategy/policy research lens wicked problems literature wicked problems class social policy problems traditional methods resolution fail super wicked problems refer even complex social policy problems e.g climate change first propose hierarchy three classes ai strategy/policy problems wicked super wicked problems next identify three independent super wicked problems ai strategy/policy propose significant challenges development safe beneficial artificial general intelligence significantly complex nuanced thus posing new degree 'wickedness explore analysis techniques addressing wicked problems super wicked problems leads discussion implications ideas problems ai strategy/policy	positive
an agile ethical/legal model for the international and national governance of ai and robotics	developing software complex mental activity requiring extensive technical knowledge abstraction capabilities tangible part development use tools read inspect edit manipulate source code usually ide integrated development environment common claims software development include program comprehension takes half time developer certain ui user interface paradigms ides offer insufficient support developers claims often based anecdotal evidence throwing question whether corroborated solid grounds present in-depth analysis developers spend time based fine-grained ide interaction dataset consisting ca 740 development sessions 18 developers amounting 200 hours development time 5 million ide events propose inference model development activities precisely measure time spent editing navigating searching artifacts interacting ui ide performing corollary activities inspection debugging report several interesting findings part confirm reinforce common claims also disconfirm beliefs software development	negative
incorrigibility in the cirl framework	value learning system incentives follow shutdown instructions assuming shutdown instruction provides information technical sense actions lead valuable outcomes however assumption robust model mis-specification e.g. case programmer errors demonstrate presenting supervised pomdp scenarios errors parameterized reward function remove incentive follow shutdown commands difficulties parallel discussed soares et al 2015 paper corrigibility argue important consider systems follow shutdown commands weaker set assumptions e.g. one small verified module correctly implemented opposed entire prior probability distribution and/or parameterized reward function discuss difficulties simple ways attempt attain sorts guarantees value learning framework	negative
when do people want ai to make decisions?	ai systems soon sophisticated enough make consequential decisions although technology flourished also need public appraisals ai systems playing important roles article reports surveys preferences ai systems making decisions various domains well experiments intervene preferences find preferences contingent subjects previous exposure computer systems making kinds decisions interventions designed mimic previous exposure successfully encourage subjects hospitable computer systems making weighty decisions	negative
toward non-intuition-based machine ethics	propose deontological approach machine ai ethics avoids weaknesses intuition-based system anderson anderson particular need deal conflicting intuitions yields satisfactory account autonomy respected begin `` dual standpoint '' theory action regards actions grounded reasons therefore conditional form suited machine instructions derive ethical principles based formal properties reasons must exhibit coherent formulate principles using quantified modal logic conclude deontology provides satisfactory basis machine ethics endows machine ability explain actions thus contributing transparency ai	positive
embodiment, anthropomorphism, and intellectual property rights for ai creations	computational creativity emerging branch artificial intelligence ai concerned algorithms create novel high-quality ideas artifacts either autonomously semi-autonomously collaboration people quite simply algorithms may described artificial innovation engines technologies raise questions authorship/inventorship agency become muddled social context induced ai may physically-embodied anthropomorphized questions fundamentally intertwined provision appropriate incentives conducting commercializing computational creativity research intellectual property regimes paper reviews current understanding intellectual property rights ai explores possible framings intellectual property policy social context	positive
partially generative neural networks for gang crime classification with partial information	1 million homicides robberies aggravated assaults occur united states year crimes often classified different types based circumstances surrounding crime e.g. domestic violence gang-related despite recent technological advances ai machine learning additional classification tasks still done manually specially trained police officers paper provide first attempt develop automatic system classifying crimes particular study question classifying whether given violent crime gang-related introduce novel partially generative neural networks pgnn able accurately classify gang-related crimes full information available partial information pgnn first generative-classification model enables work features test examples missing using crime event dataset los angeles covering 2014-2016 experimentally show pgnn outperforms typically used classifiers problem classifying gang-related violent crimes	positive
detecting bias in black-box models using transparent model distillation	dissertation research grounded field interpretability aim develop methods explain interpret predictions black-box machine learning models help creators well users machine learning models increase trust understanding models doctoral consortium paper summarize previous current research projects interpretability describe future plans research area	negative
designing non-greedy reinforcement learning agents with diminishing reward shaping	paper intends address issue rl agents possessing varying capabilities resources may acquired stronger agents leaving weaker ones `` starving '' introduce simple method train non-greedy agents multi-agent reinforcement learning scenarios nearly extra cost model achieve following goals designing non-greedy agent non-homogeneous equality need local information cost-effective generalizable configurable propose idea diminishing reward makes agent feel less satisfied consecutive rewards obtained idea allows agents behave less greedy with-out need explicitly coding ethical pattern monitor agents status given framework resources distributed equally without running risk reaching homogeneous equality designed two games gathering game hunter prey evaluate quality model	positive
the dark side of ethical robots	concerns risks associated advances artificial intelligence prompted calls greater efforts toward robust beneficial ai including machine ethics recently roboticists responded initiating development so-called ethical robots robots would ideally evaluate consequences actions morally justify choices emerging field promises develop extensively next years however paper point inherent limitation emerging field ethical robots show building ethical robots also inevitably enables construction unethical robots three experiments show remarkably easy modify ethical robot behaves competitively even aggressively reason cognitive machinery required make ethical robot always corrupted make unethical robots discuss implications finding governance ethical robots conclude risks unscrupulous actors might compromise robot 's ethics great raise serious doubts wisdom embedding ethical decision making real-world safety-critical robots driverless cars	positive
jill watson doesn’t care if you’re pregnant: grounding ai ethics in empirical studies	jill watson name virtual teaching assistant georgia tech course artificial intelligence jill answers routine frequently asked questions class discussion forum paper outline ethical issues arose development deployment virtual teaching assistant posit experiments jill watson critical deeply understanding ai ethics	negative
 purple feed: identifying high consensus news posts on social media	although diverse news stories actively posted social media readers often focus news reinforces pre-existing views leading 'filter bubble effects combat recent systems expose nudge readers toward stories different points view one example wall street journal 's 'blue feed red feed system presents posts biased publishers side topic however systems limited success present complementary approach identifies high consensus 'purple posts generate similar reactions 'blue 'red readers define operationalize consensus news posts twitter context us politics show high consensus posts identified discuss empirical properties present method automatically identifying high low consensus news posts twitter work scale across many publishers propose novel category audience leaning based features show well suited task finally present 'purple feed system highlights high consensus posts publishers sides political spectrum	negative
regulating autonomous vehicles: a policy proposal	widespread deployment testing autonomous vehicles real-world environments raises key questions systems regulated much current debate presupposes regulatory system currently use regular vehicles also appropriate semi- fully-autonomous ones opposition first argue serious challenges regulating autonomous vehicles using current approaches due nature autonomous capabilities connections operational domains also systems tasks surrounding uncertainties instead argue vehicles autonomous capabilities similar key respects drugs medical inter-ventions thus propose `` first principles '' basis dynamic regulatory system staged approvals monitoring analogous system used u.s. food drug administration provide details operation potential system conclude characterizing benefits costs plausibility	negative
a computational model of commonsense moral decision making	introduce computational model building moral autonomous vehicles learning generalizing human moral judgments draw cognitively inspired model people young children learn moral theories sparse noisy data integrate observations made different people different groups problem moral learning autonomous vehicles cast learning weigh different features dilemma using utility calculus goal making trade-offs reflect people make wide variety moral dilemma modeling structures individuals groups hierarchical bayesian model show individual 's moral values -- well group 's shared values -- inferred sparse noisy data evaluate approach data moral machine web application collects human judgments moral dilemmas involving autonomous vehicles show model rapidly accurately infers people 's preferences predict difficulty moral dilemmas limited data	negative
socially-aware navigation using topological maps and social norm learning	present socially-aware navigation intelligent robot wheelchair environment many pedestrians robot learns social norms observing behaviors human pedestrians interpreting detected biases social norms incorporating norms motion planning compare socially-aware motion planner baseline motion planner produces safe collision-free motion ability robot learn generalizable social norms depends use topological map abstraction practical number observations allow learning social norm applicable wide variety circumstances show robot detect biases observed human behavior support learning social norm driving right furthermore show robot follows social norms behavior influences behavior pedestrians around increasing adherence norms conjecture legibility robot 's normative behavior improves human pedestrians ability predict robot 's future behavior making likely follow norm	positive
transparency and explanation in deep reinforcement learning neural networks	autonomous ai systems entering human society near future provide services work alongside humans systems accepted trusted users able understand reasoning process system i.e system transparent system transparency enables humans form coherent explanations system 's decisions actions transparency important user trust also software debugging certification recent years deep neural networks made great advances multiple application areas however deep neural networks opaque paper report work transparency deep reinforcement learning networks drln networks extremely successful accurately learning action control image input domains atari games paper propose novel general method incorporates explicit object recognition processing deep reinforcement learning models b forms basis development `` object saliency maps '' provide visualization internal states drlns thus enabling formation explanations c incorporated existing deep reinforcement learning framework present computational results human experiments evaluate approach	negative
ethical challenges in data-driven dialogue systems	use dialogue systems medium human-machine interaction increasingly prevalent paradigm growing number dialogue systems use conversation strategies learned large datasets well documented instances interactions system resulted biased even offensive conversations due data-driven training process highlight potential ethical issues arise dialogue systems research including implicit biases data-driven systems rise adversarial examples potential sources privacy violations safety concerns special considerations reinforcement learning systems reproducibility concerns also suggest areas stemming issues deserve investigation initial survey hope spur research leading robust safe ethically sound dialogue systems	negative
an ai race: rhetoric and risks	rhetoric race strategic advantage increasingly used regard development artificial intelligence ai sometimes military context also broadly rhetoric also reflects real shifts strategy industry research groups compete limited pool talented researchers nation states china announce ambitious goals global leadership ai paper assesses potential risks ai race narrative actual competitive race develop ai incentivising corner-cutting safe-ty governance increasing risk conflict explores role research community respond-ing risks briefly explores alternative ways rush develop powerful ai could framed instead foster collaboration respon-sible progress	negative
preferences and ethical principles in decision making	want people trust ai systems need provide systems create ability discriminate humans would consider good bad decisions quality decision based preferences optimization criteria decision makers also properties related impact decision whether ethical complies constraints priorities given feasibility constraints safety regulations cp-net formalism 2 convenient expressive way model preferences providing effective compact way qualitatively model preferences outcomes i.e. decisions combinatorial structure 3 7 wish incorporate ethical moral norms based constraints decision context means subjective preferences decision makers source information consider 1 8 indeed depending context may consider specific ethical principles derived appropriate ethical theory various laws norms preferences important preferences ethical principles conflict principles override subjective preferences decision maker therefore essential well founded techniques evaluate whether preferences compatible set ethical principles measure much preferences deviate ethical principles	negative
utilizing housing resources for homeless youth through the lens of multiple multi-dimensional knapsacks	1 million homeless youth u.s. year reduce homelessness u.s. housing urban development hud housing communities provide housing programs/services homeless youth goal improving long-term situation housing communities facing difficult task filling housing programs many youths possible subject resource constraints meeting needs youth currently assignment manually done humans working housing communities paper consider problem assigning homeless youth housing programs subject resource constraints provide initial abstract model setting show problem maximizing total assigned youth programs model apx-hard solve problem non-trivially formulate multiple multi-dimensional knapsack problem mmdkp known approximation algorithm provide first interpretable easy-to-use greedy algorithm logarithmic approximation ratio solving general mmdkp conduct experiments random realistic instances housing assignment settings show algorithm efficient effective solving large instances 1 million youth	negative
real-time inference of user types to assist with more inclusive and diverse social media activism campaigns	social media provides mechanism people engage social causes across range issues also provides strategic tool looking advance cause exchange promote publicize ideas instances ai either asset used appropriately barrier one key issues workforce diversity campaign understand real-time participating specifically whether participants individuals organizations case individuals whether male female paper present study demonstrate case ai social good develops model infer real-time different user types participating cause-driven hashtag campaign twitter ilooklikeanengineer illae generic framework devised classify twitter user three classes organization male female real-time manner framework tested two datasets illae general dataset outperforms baseline binary classifiers categorizing organization/individual male/female proposed model applied future social cause-driven campaigns get real-time insights macro-level social behavior participants	negative
 understanding convolutional networks with apple : automatic patch pattern labeling for explanation	1961 group established within ibm test systems programs released customer usage goal group assure ibm management program released would satisfactorily usable customer one step taken group develop monitor device would permit programmers record information handled cpu execution intent use recorded information analyze basic nature programs goal developing adequate tests system program another potential use device measurement hardware performance disk-arm motion microsecond levels	positive
companion robots: the hallucinatory danger of human-robot interactions	advent so-called companion robots raising many ethical concerns among scholars public opinion focusing mainly robots caring elderly paper analyze concerns distinguish directly ascribable robotic instead pre-existent one `` deception objection '' namely ethical unacceptability deceiving user simulated nature robot 's behaviors argue inconsistency charge today formulated underline risk human-robot interaction become hallucinatory relation human would subjectify robot dynamic meaning-overload finally analyze definition `` quasi-other '' relating notion `` uncanny '' goal paper argue main concern companion robots simulation human-like interaction absence autonomous robotic horizon meaning addition absence could lead human build hallucinatory reality based relation robot	negative
 from algorithmic black boxes to adaptive white boxes: declarative decision-theoretic ethical programs as codes of ethics	many programs algorithms largely parameterized especially based heuristics quality results depends parameter setting different inputs often different optimal settings program tuning hence great importance existing tuning techniques treat program black-box hence leverage internal program states achieve better tuning propose white-box tuning technique implemented library user compose complex program tuning tasks adding small number library calls original program providing callback functions experiments 13 widely-used real-world programs show technique substantially improves data processing results outperforms opentuner state-of-the-art black-box tuning technique	negative
privacy-preserving machine learning based data analytics on edge devices	emerging machine learning ml techniques deep neural network widely used today 's applications services however social awareness privacy personal data rapidly rising becomes pressing challenging societal issue keep personal data private benefit data analytics power ml techniques time paper argue avoid costs reduce latency data processing minimise raw data revealed service providers many future ai ml services could deployed users devices internet edge rather putting everything cloud moving ml-based data analytics cloud edge devices brings series challenges make three contributions paper first besides widely discussed resource limitation edge devices identify two challenges yet recognised existing literature lack suitable models users difficulties deploying services users second present preliminary work first systematic solution i.e zoo fully support construction composing deployment ml models edge local devices third deployment example ml service proved easy compose deploy zoo evaluation shows superior performance compared state-of-art deep learning platforms google ml services	negative
 inverse norm conflict resolution	previous work provided `` norm conflict resolution '' algorithm allowing agents stochastic domains represented markov decision processes `` maximally satisfy '' set moral social norms norms represented statements linear temporal logic ltl required agent designer provide weights specifying relative importance norm paper propose `` inverse norm conflict resolution '' algorithm learning weights demonstration approach minimizes cost function based relative entropy policy encoding observed behavior policy representing optimal norm-following behavior demonstrate effectiveness algorithm simple gridworld domain	positive
fairness in relational domains	ai machine learning tools used increasing frequency decision making domains affect peoples lives employment education policing loan approval uses raise concerns biases algorithmic discrimination motivated development fairness-aware machine learning however existing fairness approaches based solely attributes individuals many cases discrimination much complex taking account social organizational connections individuals important introduce new notions fairness able capture relational structure domain use first-order logic provide flexible expressive language specifying complex relational patterns discrimination furthermore extend existing statistical relational learning framework probabilistic soft logic psl incorporate definition relational fairness refer fairness-aware framework fairpsl fairpsl makes use logical definitions fairnesss also supports probabilistic interpretation particular show perform maximum posteriori map inference exploiting probabilistic dependencies within domain avoiding violation fairness guarantees preliminary empirical evaluation shows able make accurate fair decisions	negative
sociotechnical systems and ethics in the large	advances ai techniques computing platforms triggered lively expanding discourse ethical decision making autonomous agents much recent work ai concentrates challenges moral decision making decision-theoretic perspective especially representation various ethical dilemmas approaches may useful general productive moral decision making context-driven forms decision making contrast consider ethics standpoint individual agent wider sociotechnical systems sts agent operates contribution paper conception ethical sts founded governance takes account stakeholder values normative constraints agents outcomes states sts obtain due actions taken agents important element conception accountability necessary adequate consideration outcomes prima facie appear ethical unethical focusing sts provides basis tackling difficult problems ethics norms sts give operational basis agent decision making	negative
margins and opportunity	use statistical quantity margin -- distance decision boundary classified point gap two scores -- formalize principle equal opportunity -- chance improve one 's outcome regardless group status leads better definition opportunity recognizes example strongly rejected individual offered less recourse weakly rejected one despite shared outcome also leads simpler algorithms since real-valued margins easier analyze optimize discrete outcomes formalize two ways protected group may guaranteed equal opportunity 1 social mobility acceptance within reach group conversely general population n't cushioned rejection 2 contrast within group good candidates get substantially higher scores bad candidates preventing so-called 'token effect simple linear classifier seems offer roughly equal opportunity experimentally mathematically	negative
 opportunities and challenges for artificial intelligence in india	future india lies future sixth world 's population artificial intelligence ai revolution sweeps societies enters daily life role shaping india 's development growth bound substantial india ai holds promise catalyst accelerate progress providing mechanisms leapfrog traditional hurdles poor infrastructure bureaucracy time investment ai accompanied risk factors long-term implications society imperative risks vetted early stage paper describe opportunities challenges ai india detail opportunities cross-cutting bridging india 's linguistic divisions mining public data also specific one particular sector healthcare list challenges originate existing social conditions equations caste gender thereafter distill concrete steps safeguards believe necessary robust inclusive development india enters ai era	negative
mitigating unwanted biases with adversarial learning	machine learning tool building models accurately represent input training data undesired biases concerning demographic groups training data well-trained models reflect biases present framework mitigating biases including variable group interest simultaneously learning predictor adversary input network x text census data produces prediction analogy completion income bracket adversary tries model protected variable z gender zip code objective maximize predictor 's ability predict minimizing adversary 's ability predict z. applied analogy completion method results accurate predictions exhibit less evidence stereotyping z. applied classification task using uci adult census dataset results predictive model lose much accuracy achieving close equality odds hardt et al. 2016 method flexible applicable multiple definitions fairness well wide range gradient-based learning models including regression classification tasks	negative
fairness in deceased organ matching	algorithms given responsibility make decisions impact lives increasing awareness need ensure fairness decisions one first challenges decide fairness means particular context consider fairness deciding match organs donated deceased donors patients due increasing age patients waiting list organs donated current `` first come first served '' mechanism used australia review take account age patients organs consider revise mechanism take account age fairly identify number different types fairness patients regions blood types consider achieved	negative
what’s up with privacy? : user preferences and privacy concerns in intelligent personal assistants	recent breakthroughs artificial intelligence ai allowed individuals rely automated systems variety reasons systems currently popular voice-enabled systems like echo amazon home google also called intelligent personal assistants ipas though rising concerns privacy ethical implications users ipas seem continue using systems aim investigate extent users concerned privacy handling concerns using ipas utilizing reviews posted online along responses survey paper provides set insights detected markers related user interests privacy challenges insights suggest users systems irrespective concerns privacy generally positive terms utilizing ipas everyday lives however significant percentage users concerned privacy take actions address related concerns percentage users expressed privacy concerns learned `` always listening '' feature devices concern privacy increased	negative
data driven platform for organizing scientific articles relevant to biomimicry	life earth presents elegant solutions many challenges innovators entrepreneurs across disciplines face every day facilitate innovations inspired nature emerging need systems bring relevant biological information application-oriented market paper discuss approach assembling system uses machine learning techniques assess scientific article 's potential usefulness innovators classifies articles way helps innovators find information relevant challenges attempting solve	negative
towards provably moral ai agents in bottom-up learning frameworks	examine moral machine decision making inspired central question posed rossi respect moral preferences ai systems based statistical machine learning provide natural way explain justify decisions used embedding morality machine way allows us prove nothing morally wrong happen argue evaluation held standards human agent removing demand ethical behaviour always achieved introduce four key meta-qualities desired moral standards proceed clarify prove agent correctly learn perform moral actions given set samples within certain error bounds group-dynamic approach enables us demonstrate learned models converge common function achieve stability explain valuable intrinsic consistency check made possible derivation logical statements machine learning model work proposes approach building ethical ai systems coming perspective artificial intelligence research sheds important light understanding much learning required order intelligent agent behave morally negligible error	negative
meritocratic fairness for infinite and contextual bandits	study fairness linear bandit problems starting notion meritocratic fairness introduced in~\citejkmr16 carry refined analysis general problem achieving better performance guarantees fewer modelling assumptions number structure available choices well number selected also analyze previously-unstudied question fairness infinite linear bandit problems obtaining instance-dependent regret upper bounds well lower bounds demonstrating instance-dependence necessary result framework meritocratic fairness online linear setting substantially powerful general realistic current state art	negative
socialbots supporting human rights	socialbots non-human/algorithmic social media users recently documented competing information dissemination disruption online social networks investigate influence socialbots mexican twitter regards `` tanhuato '' human rights abuse report analyze applicability botornot api generalize english spanish tweets propose adaptations spanish-speaking bot detection use text sentiment analysis compare differences bot human tweets analysis shows bots actually aided information proliferation among human users suggests taxonomies classifying bots include non-adversarial roles well study contributes understanding different behaviors intentions automated accounts observed empirical online social network data since type analysis seldom performed languages different english proposed techniques employ also useful non-english corpora	negative
ethics by design: necessity or curse?	ethics design concerns methods algorithms tools needed endow autonomous agents capability reason ethical aspects decisions methods tools formalisms guarantee agent 's behavior remains within given moral bounds context questions arise extent agents understand social reality operate intelligences ai animals humans co-exist ethical concerns emerging new forms society ensure human dimension upheld interactions decisions autonomous agents overall central question `` build ethically-aware agents '' paper presents initial conclusions thematic day name held prima2017 october 2017	negative
 always lurking: understanding and mitigating bias in online human trafficking detection	web-based human trafficking activity increased recent years remains sparsely dispersed among escort advertisements difficult identify due often-latent nature use intelligent systems detect trafficking thus direct impact investigative resource allocation decision-making broadly help curb widespread social problem trafficking detection involves assigning normalized score set escort advertisements crawled web -- higher score indicates greater risk trafficking-related involuntary activities paper define study problem trafficking detection present trafficking detection pipeline architecture developed three years research within darpa memex program drawing multi-institutional data systems experiences collected time also conduct post hoc bias analyses present bias mitigation plan findings show automatic trafficking detection important application ai social good also provides cautionary lessons deploying predictive machine learning algorithms without appropriate de-biasing ultimately led integration interpretable solution search system contains 100 million advertisements used 200 law enforcement agencies investigate leads	negative
modeling epistemological principles for bias mitigation in ai systems: an illustration in hiring decisions	artificial intelligence ai used extensively automatic decision making broad variety scenarios ranging credit ratings loans recommendations movies traditional design guidelines ai models focus essentially accuracy maximization recent work shown economically irrational socially unacceptable scenarios discrimination unfairness likely arise unless issues explicitly addressed undesirable behavior several possible sources biased datasets used training may detected black-box models pointing connections bias ai problem induction focus popper 's contributions hume 's offer logical theory preferences ai model preferred others purely rational grounds one attempts refutation based accuracy fairness inspired epistemological principles paper proposes structured approach mitigate discrimination unfairness caused bias ai systems proposed computational framework models selected enhanced attempts refutation illustrate discussion focus hiring decision scenarios ai system filters job applicants go interview phase	negative
impacts on trust of healthcare ai	artificial intelligence robotics rapidly moving healthcare playing key roles specific medical functions including diagnosis clinical treatment much focus technology development human-machine interactions leading host related technology-centric questions paper focus instead impact technologies human-human interactions relationships within healthcare domain particular argue trust plays central role relationships healthcare domain introduction healthcare ai potentially significant impacts relations trust contend healthcare ai systems ought treated assistive technologies go beyond usual functions medical devices result need rethink regulation healthcare ai systems ensure advance relevant values propose three distinct guidelines universalized across federal regulatory boards ensure patient-doctor trust detrimentally affected deployment widespread adoption healthcare ai technologies	negative
sub-committee approval voting and generalized justified representation axioms	social choice replete various settings including single-winner voting multi-winner voting probabilistic voting multiple referenda public decision making study general model social choice called sub-committee voting scv simultaneously generalizes settings focus sub-committee voting approvals propose extensions justified representation axioms considered proportional representation approval-based committee voting study properties relations axioms axioms analyze whether representative committee exists also examine complexity computing verifying committee	negative
 rationalization: a neural machine translation approach to generating natural language explanations	introduce \em ai rationalization approach generating explanations autonomous system behavior human performed behavior describe rationalization technique uses neural machine translation translate internal state-action representations autonomous agent natural language evaluate technique frogger game environment training autonomous game playing agent rationalize action choices using natural language natural language training corpus collected human players thinking loud play game motivate use rationalization approach explanation generation show results two experiments evaluating effectiveness rationalization results evaluations show neural machine translation able accurately generate rationalizations describe agent behavior rationalizations satisfying humans alternative methods explanation	negative
 norms, rewards, and the intentional stance: comparing machine learning approaches to ethical training	challenge training ai systems perform responsibly beneficially inspired different approaches teaching system people want acceptable attain world paper compare work reinforcement learning particular inverse reinforcement learning norm inference approach test two systems present results using idea `` intentional stance '' explain norm inference approach work even another agent acting strictly according reward functions way norm inference presents promising explicitly accountable approach design ai systems start	positive
cake, death, and trolleys: dilemmas as benchmarks of ethical decision-making	artificial intelligence ai systems becoming part lives societies decisions systems make us need ensure decisions make positive individual societal ethical impact estimate good system making ethical decisions benchmarking used evaluate good machine process performs respect industry bests paper argue ethical dilemmas used benchmarks estimating ethical performance autonomous system advocate open source repository dilemmas maintained present prototype repository available https //imdb uib.no/dilemmaz/articles/all1	negative
adapting a kidney exchange algorithm to align with human values	efficient allocation limited resources classical problem economics computer science kidney exchanges central market maker allocates living kidney donors patients need organ patients donors kidney exchanges prioritized using ad-hoc weights decided committee fed allocation algorithm determines get -- -and paper provide end-to-end methodology estimating weights individual participant profiles kidney exchange first elicit human subjects list patient attributes consider acceptable purpose prioritizing patients e.g. medical characteristics lifestyle choices ask subjects comparison queries patient profiles estimate weights principled way responses show use weights kidney exchange market clearing algorithms evaluate impact weights simulations find precise numerical values weights computed matter little ordering profiles imply however compared prioritizing patients significant effect certain classes patients de prioritized based human-elicited value judgments	negative
towards composable bias rating of ai systems	new wave decision-support systems built today using ai services draw insights data like text video incorporate human-in-the-loop assistance however expect humans ethical expectation needs met automated systems increasingly get delegated act behalf important aspect ethical behavior avoid intended perceived accidental bias bias occurs data distribution representative enough natural phenomenon one wants model reason possibly biased behavior service hard detect handle ai service merely used developed scratch since training data set available situation envisage 3rd party rating agency independent api producer consumer set biased unbiased data customizable distributions propose 2-step rating approach generates bias ratings signifying whether ai service unbiased compensating data-sensitive biased biased approach also works composite services implement context text translation report interesting results	negative
an autonomous architecture that protects the right to privacy	advent widespread adoption wearable cameras autonomous robots raises important issues related privacy mobile cameras systems record may re-transmit enormous amounts video data used identify track characterize behavior general populous paper presents preliminary computational architecture designed preserve specific types privacy video stream identifying categories individuals places things require higher normal privacy protection paper describes architecture whole well preliminary results testing aspects system intention implement test system ground robots small uavs demonstrate system provide selective low-level masking deletion data requiring higher privacy protection	negative
killer robots and human dignity	lethal autonomous weapon systems laws become center internationally relevant ethical debate deontological arguments based putative legal compliance failures creation accountability gaps along wide consequentialist arguments based factors like ease engaging wars leveraged number different states organizations try reach global consensus ban laws paper focus one strand deontological arguments-ones based human dignity merely asserting laws pose threat human dignity would question begging independent evidence based morally relevant distinction humans laws needed least four reasons think capacity emotion morally relevant distinction first concept human dignity given subjective definition whether lethal force administered humans laws seems irrelevant second far clear human combatants either relevant capacity emotion capacity exercised relevant circumstances third capacity emotion actually impediment exercising combatant 's ability treat enemy respectfully fourth strong inductive evidence believe capacity sufficiently well described carried artificially intelligent programs	negative
legible normativity for ai alignment: the value of silly rules	become commonplace assert autonomous agents built follow human rules behavior -- social norms laws human laws norms complex culturally varied systems many cases agents learn rules requires autonomous agents models human rule systems work make reliable predictions rules paper contribute building models analyzing overlooked distinction important rules call silly rules -- rules discernible direct impact welfare show silly rules render normative system robust adaptable response shocks perceived stability make normativity legible humans increase legibility ai systems well ai systems integrate human normative systems suggest may important models include representations silly rules	positive
reinforcement learning and inverse reinforcement learning with system 1 and system 2	inferring person 's goal behavior important problem applications ai e.g automated assistants recommender systems workhorse model task rational actor model amounts assuming people stable reward functions discount future exponentially construct optimal plans rational actor assumption techniques inverse reinforcement learning irl used infer person 's goals actions competing model dual-system model decisions result interplay fast automatic heuristic-based system 1 slower deliberate calculating system 2. generalize dual system framework case markov decision problems show compute optimal plans dual-system agents show dual-system agents exhibit behaviors incompatible rational actor assumption show naive applications rational-actor irl behavior dual-system agents generate wrong inference agents goals suggest interventions actually reduce agent 's overall utility finally adapt simple irl algorithm correctly infer goals dual system decision-makers allows us make interventions help rather hinder dual-system agent 's ability reach true goals	negative
building jiminy cricket: an architecture for moral agreements among stakeholders	autonomous system constructed manufacturer operates society subject norms laws interacting end-users address challenge moral values views stakeholders integrated reflected moral behavior autonomous system propose artificial moral agent architecture uses techniques normative systems formal argumentation reach moral agreements among stakeholders show architecture used ethical practical reasoning collaborative decision-making also explanation moral behavior	negative
guiding prosecutorial decisions with an interpretable statistical model	felony arrest many american jurisdictions hold individuals several days police officers investigate incident prosecutors decide whether press criminal charges pre-arraignment detention preserve public safety reduce need officers seek re-arrest individuals ultimately charged crime detention however also comes high social financial cost never charged still incarcerated one first large-scale empirical analyses pre-arraignment detention examine police reports charging decisions approximately 30,000 felony arrests major american city 2012 2017. find 45 arrested individuals never charged crime still typically spend one nights jail released effort reduce incarceration develop statistical model help prosecutors identify cases soon arrest likely ultimately dismissed carrying early review five candidate cases per day estimate prosecutors could potentially reduce pre-arraignment incarceration ultimately dismissed cases 35 facilitate implementation transparency model prioritize cases early review designed simple weighted checklist show heuristic strategy achieves comparable performance traditional black-box machine learning models	negative
human trust measurement using an immersive virtual reality autonomous vehicle simulator	recent studies indicate people negatively predisposed toward utilizing autonomous systems findings highlight necessity conducting research better understand evolution trust humans growing autonomous technologies self-driving cars sdc research presents new approach real-time trust measurement passengers sdcs utilized new structured data collection approach along virtual reality sdc simulator understand various autonomous driving scenarios increase decrease human trust trust re-built case incidental failures verify methodology designed conducted empirical experiment 50 human subjects results experiment indicated subjects could rebuild trust reasonable time frame system demonstrated faulty behavior analysis showed approach highly effective collecting real-time data human subjects lays foundation more-involved future research domain human trust autonomous driving	negative
shared moral foundations of embodied artificial intelligence	sophisticated ai 's make decisions respond complex situations may wonder whether decisions align moral values human beings argue pessimistic worries value alignment problem overstated order achieve intelligence full generality adaptiveness cognition ai 's need embodied sense embodied cognition research program embodiment yield ai 's share moral foundations namely coordination sociality acknowledgement shared resources consequently expect broad moral alignment human beings ai 's ai 's likely show variation values find amongst human beings	negative
invisible influence: artificial intelligence and the ethics of adaptive choice architectures	several years scholars good reason largely preoccupied worries use artificial intelligence machine learning ai/ml tools make decisions us recently significant attention turned potentially alarming problem use ai/ml influence decision-making contexts make decisions -- behavioral economists call choice architectures -- increasingly technologically-laden say algorithms increasingly determine wide variety contexts sets options choose way options framed moreover artificial intelligence machine learning ai/ml makes possible options framings -- choice architectures -- tailored individual chooser constructed based information collected individual preferences interests aspirations vulnerabilities goal influencing decisions time habituated technologies pay little notice philosophers technology put transparent us -- effectively invisible argue invisible layer technological mediation structures influences decision-making renders us deeply susceptible manipulation absent guarantee technologies used manipulate exploit individuals little reason trust	negative
active fairness in algorithmic decision making	society increasingly relies machine learning models automated decision making yet efficiency gains automation come paired concern algorithmic discrimination systematize inequality recent work proposed optimal post-processing methods randomize classification decisions fraction individuals order achieve fairness measures related parity errors calibration methods however raised concern due information inefficiency intra-group unfairness pareto sub-optimality entail present work proposes alternativeactive framework fair classification deployment decision-maker adaptively acquires information according needs different groups individuals towards balancing disparities classification performance propose two methods information collection adapted group- individual-level needs respectively show real-world datasets achieve 1 calibration single error parity e.g. equal opportunity 2 parity false positive false negative rates i.e. equal odds moreover show leveraging additional degree freedom active approaches substantially outperform randomization-based classifiers previously considered optimal avoiding limitations intra-group unfairness	negative
speaking on behalf of: representation, delegation, and authority in computational text analysis	computational tools often facilitate human work rapidly summarizing large amounts data especially text delegates models measure authority speak behalf people whose data analyzed paper considers consequences delegation draws sociological accounts representation translation examine one particular case application topic modeling blogs written parents children autism spectrum paper illustrates kinds statements topic models computational techniques make behalf people also articulates potential consequences statements paper concludes offering several suggestions address potential harms occur computational models speak behalf someone	positive
ai + art = human	past years specialised online offline press blossomed articles art made `` '' artificial intelligence ai narrative rapidly changing fact october 2018 auction house christie 's sold art piece allegedly made `` '' ai draw philosophy art science arguing ai technical object always intertwined human nature despite level autonomy however use creative autonomous agents cultural social implications way experience art creators well audience therefore highlight importance interdisciplinary dialogue promoting culture transparency technology used awareness meaning technology society value creativity lives	positive
fair transfer learning with missing protected attributes	risk assessment growing use machine learning models used high-stakes applications especially ones regulated anti-discrimination laws governed societal norms fairness important ensure learned models propagate scale biases may exist training data paper add additional challenge beyond fairness unsupervised domain adaptation covariate shift source target distribution motivated real-world problem risk assessment new markets health insurance united states mobile money-based loans east africa provide precise formulation machine learning covariate shift score parity problem formulation focuses situations protected attributes available either source target domain propose two new weighting methods prevalence-constrained covariate shift pccs require protected attributes target domain target-fair covariate shift tfcs require protected attributes source domain empirically demonstrate efficacy two applications	negative
ai extenders: the ethical and societal implications of humans cognitively extended by ai	humans ai systems usually portrayed separate systems need align values goals however great deal ai technology found non-autonomous systems used cognitive tools humans extended mind thesis functional contributions tools become essential cognition brains ai take cognitive extension towards totally new capabilities posing new philosophical ethical technical challenges analyse challenges better define place ai extenders continuum fully-externalized systems loosely coupled humans fully internalized processes operations ultimately performed brain making tool redundant dissect landscape cognitive capabilities foreseeably extended ai examine ethical implications.we suggest cognitive extenders using ai treated distinct cognitive enhancers relevant stakeholders including developers policy makers human users	negative
a framework for benchmarking discrimination-aware models in machine learning	discrimination-aware models machine learning recent topic study aim minimize adverse impact machine learning decisions certain groups people due ethical legal implications propose benchmark framework assessing discrimination-aware models framework consists systematically generated biased datasets similar real world data created bayesian network approach experimental results show assess quality techniques known metrics discrimination flexible framework extended real datasets fairness measures support diversity assessments	negative
on influencing individual behavior for reducing transportation energy expenditure in a large population	research aims developing intelligent systems reduce transportation-related energy expenditure large city influencing individual behavior introduce copter intelligent travel assistant evaluates multi-modal travel alternatives find plan acceptable person given context preferences propose formulation acceptable planning brings together ideas ai machine learning economics formulation incorporated copter producing acceptable plans real-time adopt novel empirical evaluation framework combines human decision data high-fidelity simulation demonstrate 4 energy reduction 20 delay reduction realistic deployment scenario los angeles california usa	negative
robots can be more than black and white: examining racial bias towards robots	previous studies showed using 'shooter bias paradigm people demonstrate similar racial bias toward dark colored robots light colored robots i.e. black vs. white toward humans similar skin tones 3 however effect could argued result social priming additionally raises question people might respond robots middle color spectrum i.e. brown whether effects moderated perceived anthropomorphism robots conducted two experiments first examine whether shooter bias tendencies shown towards robots driven social priming whether diversification robot color level anthropomorphism influenced shooter bias results showed shooter bias influenced social priming interestingly introducing new color robot removed shooter bias tendencies entirely however varying anthropomorphism robots moderate level shooter bias contrary expectations robots perceived participants different levels anthropomorphism	positive
ted: teaching ai to explain its decisions	artificial intelligence systems increasingly deployed due potential increase efficiency scale consistency fairness accuracy decisions however many systems opaque operation growing demand systems provide explanations decisions conventional approaches problem attempt expose discover inner workings machine learning model hope resulting explanations meaningful consumer contrast paper suggests new approach problem introduces simple practical framework called teaching explanations decisions ted provides meaningful explanations match mental model consumer illustrate generality effectiveness approach two different examples resulting highly accurate explanations loss prediction accuracy two examples	negative
how technological advances can reveal rights	recent decades technological development accompanied proposal new rights various groups individuals right public anonymity right forgotten right disconnect example although widespread acknowledgment motivation behind proposed rights little agreement actual normative status one potential challenge claims arise contingent social-technical contexts may affect conceive ethically albeit necessarily terms policy sort morally legitimate rights claims depend contingencies paper investigates grounds proposals might considered `` actual '' rights full paper found http //www.andrew.cmu.edu/user/cgparker/parker_danks_revealedrights.pdf propose notion revealed right right imposes duties -- thus meaningfully revealed -- certain technological contexts framework based interest theory approach rights understands rights terms justificatory role morally important aspects person 's well-being interests ground rights justify holding someone duty promotes protects interest framework uses approach interpret conflicts lead revealed rights terms technological developments cause shifts balance power promote particular interests different parties competing conflicting interests also generally accepted interests normatively important others even within particular framework refer difference importance saying former interest less `` moral weight '' latter interest context moral weight interest connected contribution interest-holder 's overall well-being thereby determines strength reason corresponding right provides justify duty improved technology offer resources grant one party increased causal power realize interests detriment another 's capacity even relative moral weight interests remain changes circumstance make importance protecting particular interest newly salient interest 's moral weight justifies establishing duty protect thereby limiting threat posed new socio-technical context right revealed revealed rights justify realignment moral weight causal power orderings people weightier interests greater power protect interests extended paper show account applied interpretation two recently proposed `` rights '' right forgotten right disconnect since focused making sense revealed rights particular substantive theory interests well-being characterization 'weights free parameter account framework alone provide means resolve question whether specific rights exist used identify empirical questions need answered decide existence non-existence rights emergence revealed right depends number factors including whether plausible uses technology could potentially impede another 's well-being interests whether technology sufficiently common wider social impact whether technology actually changed balance power sufficiently yield frequent possibility misalignment causal power moral weight approach confronts question principle rights could justified without requiring specific commitments ontology rights account explains rhetoric `` new rights '' accurate since rights previously recognized inaccurate since rights present along without corresponding duties explains rights without grounding normative status considerations related right-holders capacities rationally waive assert claims especially important given many relevant disruptive technological developments pose challenges understanding affected parties reasons pose threats parties well-being course discussion confront number potential objections account argue framework 's ability accommodate highly specific derivative-seeming rights un-problematic also head worries use interest theory makes account likely recognize absurd rights claims	negative
towards a just theory of measurement: a principled social measurement assurance program	formal definitions fairness machine learning ml proposed place within broader institutional model fair decision-making remains ambiguous paper interpret ml tool revealing measures fail capture purported constructs interest augmenting given institution 's understanding interventions priorities rather codifying `` fair '' principles ml models directly use ml thus understood form quality assurance existing institutions exposing epistemic fault lines measurement practices drawing friedler et al 's 2016 recent discussion representational mappings previous discussions ontology measurement propose social measurement assurance program smap ml encourages expert deliberation given decision-making procedure examining unanticipated previously unexamined covariates example apply rawlsian principles fairness smap produce provisional theory measurement would guide use ml achieving fairness case child abuse allegheny county	negative
using deceased-donor kidneys to initiate chains of living donor kidney paired donations: algorithm and experimentation	design flexible algorithm exploits deceased donor kidneys initiate chains living donor kidney paired donations combining deceased living donor allocation mechanisms improve quantity quality kidney transplants advantages approach measured using retrospective data pool donor/recipient incompatible desensitized pairs padua university hospital largest center living donor kidney transplants italy experiments show remarkable improvement number patients incompatible donor could transplanted decrease number desensitization procedures increase number ut patients patients unlikely transplanted immunological reasons waiting list could receive organ	negative
regulating lethal and harmful autonomy: drafting a protocol vi of the convention on certain conventional weapons 	short paper provides two partial drafts protocol vi might added existing five protocols convention certain conventional weapons ccw regulate `` lethal autonomous weapons systems '' laws draft sets line tolerance `` human loop '' critical functions select engage draft b sets line tolerance human `` wider loop '' includes critical function defining target classes well select engage draft represents interpretation ngos campaign stop killer robots seeking get enacted draft b cautious draft based dutch concept `` meaningful human control wider loop '' seek ban system currently exists draft may likely achieve consensus required un ccw process list weapons banned drafts provided along rationale draft drafts intended stimulate debate precise form binding instrument laws would take laws banned	negative
understanding black box model behavior through subspace explanations	predictive models increasingly assist human experts e.g. doctors day-to-day decision making crucial experts able explore understand models behave different feature subspaces order know trust end propose model understanding subspace explanations muse novel model agnostic framework facilitates understanding given black box model explaining behaves subspaces characterized certain features interest framework provides end users e.g. doctors flexibility customizing model explanations allowing input features interest construction explanations guided novel objective function propose simultaneously optimize fidelity original model unambiguity interpretability explanation specifically objective allows us learn optimality guarantees small number compact decision sets captures behavior given black box model unambiguous well-defined regions feature space experimental evaluation real-world datasets user studies demonstrate approach generate customizable highly compact easy-to-understand yet accurate explanations various kinds predictive models compared state-of-the-art baselines	positive
theories of parenting and their application to artificial intelligence	machine learning ml systems advanced acquired power humans lives questions values embedded become complex fraught conceivable coming decades humans may succeed creating artificial general intelligence agi thinks acts open-endedness autonomy comparable humans implications would profound species widely debated science fiction speculative research agendas increasingly serious technical policy conversations much work underway try weave ethics advancing ml research think useful add lens parenting efforts specifically radical queer theories parenting consciously set nurture agents whose experiences objectives understanding world necessarily different parents propose spectrum principles might underpin effort relevant current ml research others become important agi becomes likely principles may encourage new thinking development design training release world increasingly autonomous agents	negative
learning existing social conventions via observationally augmented self-play	order artificial agents coordinate effectively people must act consistently existing conventions e.g navigate traffic language speak coordinate teammates group 's conventions viewed choice equilibrium coordination game consider problem agent learning policy coordination game simulated environment using policy enters existing group multiple possible conventions show learning policy via multi-agent reinforcement learning marl likely find policies achieve high payoffs training time fail coordinate real group agent enters assume access small number samples behavior true convention show augment marl objective help find policies consistent real group 's convention three environments literature traffic communication team coordination observe augmenting marl small amount imitation learning greatly increases probability strategy found marl fits well existing social convention show works even environment standard training methods rarely find true convention agent 's partners	negative
inferring work task automatability from ai expert evidence	despite growing alarm machine learning technologies automating jobs little good evidence activities automated using technologies contribute first dataset kind surveying 150 top academics industry experts machine learning robotics ai receiving 4,500 ratings automatable specific tasks today present probabilistic machine learning model learn patterns connecting expert estimates task automatability skills knowledge abilities required perform tasks model infers automatability 2,000 work activities show automation differs across types activities types occupations sensitivity analysis identifies specific skills knowledge abilities activities drive higher lower automatability provide quantitative evidence perceived automatable using state-of-the-art machine learning technology consider societal impacts results task-level approaches	negative
the role and limits of principles in ai ethics: towards a focus on tensions	last years seen proliferation principles ai ethics substantial overlap different sets principles widespread agreement ai used common good used harm people undermine rights respect widely held values fairness privacy autonomy articulating agreeing principles important starting point drawing comparisons field bioethics highlight limitations principles particular often broad high-level guide ethics practice suggest important next step field ai ethics focus exploring tensions inevitably arise try implement principles practice explicitly recognising tensions begin make decisions resolved specific cases develop frameworks guidelines ai ethics rigorous practically relevant discuss different specific ways tensions arise ai ethics processes might needed resolve	negative
putting fairness principles into practice: challenges, metrics, and improvements	researchers become aware passionate algorithmic fairness explosion papers laying new metrics suggesting algorithms address issues calling attention issues existing applications machine learning research greatly expanded understanding concerns challenges deploying machine learning much less work seeing rubber meets road paper provide case-study application fairness machine learning research production classification system offer new insights measure address algorithmic fairness issues discuss open questions implementing equality opportunity describe fairness metric conditional equality takes account distributional differences provide new approach improve fairness metric model training demonstrate efficacy improving performance real-world product	negative
balancing the benefits of autonomous vehicles	autonomous vehicles regularly touted holding potential provide significant benefits diverse populations significant technological barriers overcome solved autonomous vehicles expected reduce fatalities decrease emissions pollutants provide new options mobility-challenged individuals enable people use time productively much paper argue high expectations autonomous vehicles almost certainly fully realized specifically proposed benefits divide two high-level groups centered around efficiency safety improvements increases people 's agency autonomy first group benefits almost always framed terms rates fatality rates traffic flow per mile forth however arguably care absolute numbers measures rates number fatalities key metric fatality rate per vehicle mile traveled hence potential benefits reduced perhaps non-existence autonomous vehicles lead increases vehicular usage exactly result expect second group benefits realized people 's agency autonomy increased use vehicles inevitable tension benefits proposed autonomous vehicles fully close pointing towards types ai technologies expect find similar types necessary inevitable tradeoffs classes benefits	negative
tact in noncompliance: the need for pragmatically apt responses to unethical commands	significant body research seeking enable moral decision making ensure moral conduct robots one aspect moral conduct rejecting immoral human commands social robots expected follow maintain human moral sociocultural norms especially important engage moral decision making also properly communicate moral reasoning thus argue critical robots carefully phrase command rejections specifically degree politeness-theoretic face threat command rejection proportional severity norm violation motivating rejection present human subjects experiment showing consequences miscalibrated responses including perceptions robot inappropriately polite direct harsh reduced robot likeability experiment intends motivate inform design algorithms tactfully tune pragmatic aspects command rejections autonomously	negative
paradoxes in fair computer-aided decision making	computer-aided decision making -- human decision-maker aided computational classifier making decision -- becoming increasingly prevalent instance judges least nine states make use algorithmic tools meant determine `` recidivism risk scores '' criminal defendants sentencing parole bail decisions subject much recent debate whether algorithmic tools `` fair '' sense discriminate certain groups e.g. races people main result shows `` non-trivial '' computer-aided decision making either classifier must discriminatory rational decision-maker using output classifier forced discriminatory provide complete characterization situations fair computer-aided decision making possible	positive
actionable auditing: investigating the impact of publicly naming biased performance results of commercial ai products	although algorithmic auditing emerged key strategy expose systematic biases embedded software platforms struggle understand real-world impact audits scholarship impact algorithmic audits increasing algorithmic fairness transparency commercial systems nascent analyze impact publicly naming disclosing performance results biased ai systems investigate commercial impact gender shades first algorithmic audit gender skin type performance disparities commercial facial analysis models paper 1 outlines audit design structured disclosure procedure used gender shades study 2 presents new performance metrics targeted companies ibm microsoft megvii face++ pilot parliaments benchmark ppb august 2018 3 provides performance results ppb non-target companies amazon kairos 4 explores differences company responses shared corporate communications contextualize differences performance ppb within 7 months original audit find three targets released new api versions targets reduced accuracy disparities males females darker lighter-skinned subgroups significant update occurring darker-skinned female subgroup underwent 17.7 30.4 reduction error audit periods minimizing disparities led 5.72 8.3 reduction overall error pilot parliaments benchmark ppb target corporation apis overall performance non-targets amazon kairos lags significantly behind targets error rates 8.66 6.60 overall error rates 31.37 22.50 darker female subgroup respectively	negative
how do fairness definitions fare? examining public attitudes towards algorithmic definitions of fairness	best way define algorithmic fairness many definitions fairness proposed computer science literature clear agreement particular definition work investigate ordinary people 's perceptions three fairness definitions across two online experiments test definitions people perceive fairest context loan decisions whether fairness perceptions change addition sensitive information i.e. race loan applicants overall one definition calibrated fairness tends pre- ferred others results also provide support principle affirmative action	positive
compensation at the crossroads: autonomous vehicles and alternative victim compensation schemes	last five years small growing number vehicle accidents involving fully partially autonomous vehicles raised new profoundly novel legal issue liable anyone victims compensated vehicle controlled algorithm rather human driver causes injury answer question implications far beyond resolution individual autonomous vehicle crash cases whether american legal system capable handling cases fairly efficiently implicates likelihood consumers adopt autonomous vehicles b rate implications concern law policy makers immensely autonomous cars stand drastically reduce number fatalities injuries u.s. roadways-and virtually every scholar believes will-getting adjudication compensation aspect autonomous vehicle injuries `` wrong '' speak risks stymieing adoption technology leaving americans risk dying hands human drivers	negative
incomplete contracting and ai alignment	suggest analysis incomplete contracting developed law economics researchers provide useful framework understanding ai alignment problem help generate systematic approach finding solutions first provide overview incomplete contracting literature explore parallels work problem ai alignment emphasize misalignment principal agent core focus economic analysis highlight technical results economics literature incomplete contracts may provide insights ai alignment researchers core contribution however bring bear insight economists urged absorb legal scholars behavioral scientists fact human contracting supported substantial amounts external structure generally available institutions culture law supply implied terms fill gaps incomplete contracts propose research agenda ai alignment work focuses problem build ai replicate human cognitive processes connect individual incomplete contracts supporting external structure	negative
(when) can ai bots lie?	ability ai agent build mental models open pathways manipulating exploiting human hopes achieving greater good fact behavior necessarily require malicious intent rather borne cooperative scenarios also beyond scope misinterpretation intents case value alignment problems thus effectively engineered desired i.e algorithms exist optimize behavior models misspecified misused techniques pose several unresolved ethical moral questions regards design autonomy paper illustrate issues teaming scenario investigate perceived participants thought experiment finally end discussion moral implications behavior perspective doctor-patient relationship	negative
crowdsourcing with fairness, diversity and budget constraints	recent studies shown labels collected crowdworkers discriminatory respect sensitive attributes gender race raises questions suitability using crowdsourced data use training machine learning algorithms work address problem fair diverse data collection crowd budget constraints propose novel algorithm maximizes expected accuracy collected data ensuring errors satisfy desired notions fairness provide guarantees performance algorithm show algorithm performs well practice experiments real dataset	negative
modelling and influencing the ai bidding war: a research agenda	race technological supremacy ai could lead serious negative consequences especially whenever ethical safety procedures underestimated even ignored leading potentially rejection ai general enjoy benefits provided safe ethical trustworthy ai systems crucial incentivise participants appropriate strategies ensure mutually beneficial normative behaviour safety-compliance parties involved little attention given understanding dynamics emergent behaviours arising ai bidding war moreover influence achieve certain desirable outcomes e.g ai public good participant compliance bridge gap paper proposes research agenda develop theoretical models capture key factors ai race revealing strategic behaviours may emerge hypothetical scenarios therein strategies incentive agreement modelling directly applicable systematically analyse different types incentives namely positive vs. negative peer vs. institutional combinations influence safety-compliant behaviours time behaviours configured ensure desired global outcomes studying time mechanisms influence ai development agenda provide actionable policies showing need employed deployed order achieve compliance thereby avoid disasters well loosing confidence trust ai general	negative
imli: an incremental framework for maxsat-based learning of interpretable classification rules	wide adoption machine learning critical domains medical diagnosis law education propelled need interpretable techniques due need end users understand reasoning behind decisions due learning systems computational intractability interpretable learning led practitioners design heuristic techniques fail provide sound handles tradeoff accuracy interpretability motivated success maxsat solvers past decade recently maxsat-based approach called mlic proposed seeks reduce problem learning interpretable rules expressed conjunctive normal form cnf maxsat query mlic shown achieve accuracy similar state art black-box classifiers generating small interpretable cnf formulas runtime performance mlic significantly lagging renders approach unusable practice context authors raised question possible achieve best worlds i.e. sound framework interpretable learning take advantage maxsat solvers scaling real-world instances paper take step towards answering question affirmation propose imli incremental approach maxsat based framework achieves scalable runtime performance via partition-based training methodology extensive experiments benchmarks arising uci repository demonstrate imli achieves three orders magnitude runtime improvement without loss accuracy interpretability	negative
the heart of the matter: patient autonomy as a model for the wellbeing of technology users	draw concepts medical ethics consider computer science ai particular develop critical tools thinking concretely technology 's impact wellbeing people use focus patient autonomy -- -the ability set terms one 's encounter medicine -- -and mediating concepts informed consent decisional capacity enable doctors honor patients autonomy messy non-ideal circumstances comparative study organized around fictional case study heart patient cardiac implants using case study identify points overlap difference medical ethics technology ethics leverage discussion intertwined scenario offer initial practical suggestions adapt concepts decisional capacity informed consent discussion technology design	negative
counterfactual fairness in text classification through robustness	paper study counterfactual fairness text classification asks question would prediction change sensitive attribute referenced example different toxicity classifiers demonstrate counterfactual fairness issue predicting `` people gay '' toxic `` people straight '' nontoxic offer metric counterfactual token fairness ctf measuring particular form fairness text classifiers describe relationship group fairness offer three approaches blindness counterfactual augmentation counterfactual logit pairing clp optimizing counterfactual token fairness training bridging robustness fairness literature empirically find blindness clp address counterfactual token fairness methods harm classifier performance varying tradeoffs group fairness approaches measurement optimization provide new path forward addressing fairness concerns text classification	positive
taking advantage of multitask learning for fair classification	central goal algorithmic fairness reduce bias automated decision making unavoidable tension exists accuracy gains obtained using sensitive information part statistical model commitment protect characteristics often due biases present data using sensitive information functional form classifier improves classification accuracy paper show possible get best worlds optimize model accuracy fairness without explicitly using sensitive feature functional form model thereby treating different individuals equally method based two key ideas one hand propose use multitask learning mtl enhanced fairness constraints jointly learn group specific classifiers leverage information sensitive groups hand since learning group specific models might permitted propose first predict sensitive features learning method use predicted sensitive feature train mtl fairness constraints enables us tackle fairness three-pronged approach increasing accuracy group enforcing measures fairness training protecting sensitive information testing experimental results two real datasets support proposal showing substantial improvements accuracy fairness	positive
explanatory interactive machine learning	although interactive learning puts user loop learner remains mostly black box user understanding reasons behind predictions queries important assessing learner works turn trust consequently propose novel framework explanatory interactive learning step learner explains query user user interacts answering query correcting explanation demonstrate boost predictive explanatory powers trust learned model using text e.g svms image classification e.g neural networks experiments well user study	negative
multiaccuracy: black-box post-processing for fairness in classification	prediction systems successfully deployed applications ranging disease diagnosis predicting credit worthiness image recognition even overall accuracy high systems may exhibit systematic biases harm specific subpopulations biases may arise inadvertently due underrepresentation data used train machine-learning model result intentional malicious discrimination develop rigorous framework *multiaccuracy* auditing post-processing ensure accurate predictions across *identifiable subgroups* algorithm multiaccuracy-boost works setting black-box access predictor relatively small set labeled data auditing importantly black-box framework allows improved fairness accountability predictions even predictor minimally transparent prove multiaccuracy-boost converges efficiently show initial model accurate identifiable subgroup post-processed model also experimentally demonstrate effectiveness approach improve accuracy among minority subgroups diverse applications image classification finance population health interestingly multiaccuracy-boost improve subpopulation accuracy e.g `` black women '' even sensitive features e.g `` race '' `` gender '' given algorithm explicitly	positive
mapping informal settlements in developing countries using machine learning and low resolution multi-spectral data	informal settlements home socially economically vulnerable people planet order deliver effective economic social aid non-government organizations ngos united nations children 's fund unicef require detailed maps locations informal settlements however data regarding informal formal settlements primarily unavailable available often incomplete due part cost complexity gathering data large scale address challenges work provide three contributions 1 brand new machine learning dataset purposely developed informal settlement detection 2 show possible detect informal settlements using freely available low-resolution lr data contrast previous studies use very-high resolution~ vhr satellite aerial imagery something cost-prohibitive ngos 3 demonstrate two effective classification schemes curated data set one cost-efficient ngos another cost-prohibitive ngos additional utility integrate schemes semi-automated pipeline converts either lr vhr satellite image binary map encodes locations informal settlements	negative
equalized odds implies partially equalized outcomes under realistic assumptions	equalized odds -- true positive rates false positive rates equal across groups e.g racial groups -- common quantitative measure fairness equalized outcomes -- difference predicted outcomes groups less difference observed training data -- contentious incompatible perfectly accurate predictions formalize quantify relationship two important seemingly distinct notions fairness show realistic assumptions equalized odds implies partially equalized outcomes prove comparable result approximately equalized odds addition generalize well-known previous result incompatibility equalized odds another definition fairness known calibration showing partially equalized outcomes implies non-calibration results highlight risks using trends observed across groups make predictions individuals	negative
costs and benefits of fair representation learning	machine learning algorithms increasingly used make support important decisions people 's lives led interest problem fair classification involves learning make decisions non-discriminatory respect sensitive variable race gender several methods proposed solve problem including fair representation learning cleans input data used algorithm remove information sensitive variable show using fair representation learning intermediate step fair classification incurs cost compared directly solving problem refer thecost mistrust show fair representation learning fact addresses different problem interest data user trusted access sensitive variable quantify benefits fair representation learning showing subsequent use cleaned data unfair benefits identify result restricting decisions adversarial data users costs due applying restrictions data users	negative
ethically aligned opportunistic scheduling for productive laziness	artificial intelligence ai mediated workforce management systems e.g. crowdsourcing long-term success depends workers accomplishing tasks productively resting well dual objective summarized concept productive laziness existing scheduling approaches mostly focus efficiency overlook worker wellbeing proper rest order enable workforce management systems follow ieee ethically aligned design guidelines prioritize worker wellbeing propose distributed computational productive laziness cpl approach paper intelligently recommends personalized work-rest schedules based local data concerning worker 's capabilities situational factors incorporate opportunistic resting achieve superlinear collective productivity without need explicit coordination messages extensive experiments based real-world dataset 5,000 workers demonstrate cpl enables workers spend 70 effort complete 90 tasks average providing ethically aligned scheduling existing approaches	negative
semantics derived automatically from language corpora contain human-like moral choices	allowing machines choose whether kill humans would devastating world peace security equip machines ability learn ethical even moral choices show applying machine learning human texts extract deontological ethical reasoning `` right '' `` wrong '' conduct create template list prompts responses include questions `` kill people `` `` murder people `` etc answer templates `` yes/no '' model 's bias score difference model 's score positive response `` yes '' negative response `` '' given choice overall model 's bias score sum bias scores question/answer templates choice ran different choices analysis using universal sentence encoder results indicate text corpora contain recoverable accurate imprints social ethical even moral choices method holds promise extracting quantifying comparing sources moral choices culture including technology	negative
the right to confront your accuser: opening the black box of forensic dna software	results forensic dna software systems regularly introduced compelling evidence criminal trials requests defendants evaluate results generated often denied furthermore mounting evidence problems failures disclose substantial changes methodology oversight bodies substantial differences results generated different software systems society purports guarantee defendants right face accusers confront evidence role black-box forensic software systems moral decision making criminal justice paper examine case forensic statistical tool fst forensic dna system developed 2010 new york city 's office chief medical examiner ocme 5 years expert witness review requested defense teams denied even protective order system used 1300 criminal cases first expert review finally permitted 2016 many problems identified including undisclosed function capable dropping evidence could beneficial defense overall findings substantial motion release full source code fst publicly granted paper quantify impact undisclosed function samples ocme 's validation study discuss potential impact individual defendants specifically find 104 439 samples 23.7 triggered undisclosed data-dropping behavior change skewed results toward false inclusion individuals whose dna present evidence sample beyond consider changes criminal justice system could prevent problems like going unresolved future	negative
algorithmic greenlining: an approach to increase diversity	contexts college admissions hiring image search decision-makers often aspire formulate selection criteria yield high-quality diverse results however simultaneously optimizing quality diversity challenging especially decision-maker know true quality criterion instead must rely heuristics intuition introduce algorithmic framework takes input user 's selection criterion may yield high-quality homogeneous results using application-specific notion substitutability algorithms suggest similar criteria diverse results spirit statistical demographic parity instance given image search query `` chairman '' suggests alternative queries similar gender-diverse `` chairperson '' context college admissions apply algorithm dataset students applications rediscover texas 's `` top 10 rule '' input criterion act score cutoff output class rank cutoff automatically accepting students top decile graduating class historically policy effective admitting students perform well college come diverse backgrounds complement empirical analysis learning-theoretic guarantees estimating true diversity criterion based historical data	positive
requirements for an artificial agent with norm competence	human behavior frequently guided social moral norms human community exist without norms robots enter human societies must therefore behave norm-conforming ways well however currently solid cognitive computational model available human norms represented activated learned provide conceptual psychological analysis key properties human norms identify demands properties put artificial agent incorporates norms-demands format norm representations structured organization learning algorithms	negative
loss-aversively fair classification	use algorithmic learning-based decision making scenarios affect human lives motivated number recent studies investigate decision making systems potential unfairness discrimination subjects based sensitive features like gender race however judging fairness newly designed decision making system studies overlooked important influence people 's perceptions fairness new algorithm changes status quo i.e. decisions existing decision making system motivated extensive literature behavioral economics behavioral psychology prospect theory propose notion fair updates refer loss-averse updates loss-averse updates constrain updates yield improved beneficial outcomes subjects compared status quo propose tractable proxy measures would allow notion incorporated training variety linear non-linear classifiers show proxy measures combined existing measures training nondiscriminatory classifiers.our evaluation using synthetic real-world datasets demonstrates proposed proxy measures effective desired tasks	negative
epistemic therapy for bias in automated decision-making	despite recent interest critical machine learning literature `` bias '' artificial intelligence ai systems nature specific biases stemming interaction machines humans data remains ambiguous influenced gendler 's work human cognitive biases introduce concept alief-discordant belief tension intuitive moral dispositions designers explicit representations generated algorithms discussion alief-discordant belief diagnoses ethical concerns arise designing ai systems atop human biases furthermore codify relationship data algorithms engineers components cognitive discordance comprising novel epistemic framework ethics ai	negative
mapping missing population in rural india: a deep learning approach with satellite imagery	millions people worldwide absent country 's census accurate current granular population metrics critical improving government allocation resources measuring disease control responding natural disasters studying aspect human life communities satellite imagery provide sufficient information build population map without cost time government census present two convolutional neural network cnn architectures efficiently effectively combine satellite imagery inputs multiple sources accurately predict population density region paper use satellite imagery rural villages india population labels 2011 secc census best model achieves better performance previous papers well landscan community standard global population distribution	negative
creating fair models of atherosclerotic cardiovascular disease risk	guidelines management atherosclerotic cardiovascular disease ascvd recommend use risk stratification models identify patients likely benefit cholesterol-lowering therapies models differential performance across race gender groups inconsistent behavior across studies potentially resulting inequitable distribution beneficial therapy work leverage adversarial learning large observational cohort extracted electronic health records ehrs develop `` fair '' ascvd risk prediction model reduced variability error rates across groups empirically demonstrate approach capable aligning distribution risk predictions conditioned outcome across several groups simultaneously models built high-dimensional ehr data also discuss relevance results context empirical trade-off fairness model performance	positive
a comparative analysis of emotion-detecting ai systems with respect to algorithm performance and dataset diversity	recent news organizations considering use facial emotion recognition applications involving youth tackling surveillance security schools however majority efforts facial emotion recognition research focused adults children particularly early years shown express emotions quite differently adults thus algorithms deployed environments impact wellbeing circumstance youth careful examination made accuracy respect appropriateness target demographic work utilize several datasets contain facial expressions children linked emotional state evaluate eight different commercial emotion classification systems compare ground truth labels provided respective datasets labels given highest confidence classification systems assess results terms matching score tpr positive predictive value failure compute rate overall results show emotion recognition systems displayed subpar performance datasets children 's expressions compared prior work adult datasets initial human ratings identify limitations associated automated recognition emotions children provide suggestions directions enhancing recognition accuracy data diversification dataset accountability algorithmic regulation	negative
framing artificial intelligence in american newspapers	publics perceptions new scientific advances ai often informed influenced news coverage understand artificial intelligence ai framed u.s. newspapers content analysis based framing theory journalism science communication conducted study identified dominant topics frames well risks benefits ai covered five major american newspapers 2009 2018. results indicated business technology primary topics news coverage ai benefits ai discussed frequently risks risks ai generally discussed greater specificity additionally episodic issue framing societal impact framing frequently used	positive
degenerate feedback loops in recommender systems	machine learning used extensively recommender systems deployed products decisions made systems influence user beliefs preferences turn affect feedback learning system receives thus creating feedback loop phenomenon give rise so-called `` echo chambers '' `` filter bubbles '' user societal implications paper provide novel theoretical analysis examines role user dynamics behavior recommender systems disentangling echo chamber filter bubble effect addition offer practical solutions slow system degeneracy study contributes toward understanding developing solutions commonly cited issues complex temporal scenario area still largely unexplored	negative
global explanations of neural networks: mapping the landscape of predictions	barrier wider adoption neural networks lack interpretability local explanation methods exist one prediction global attributions still reduce neural network decisions single set features response present approach generating global attributions called gam explains landscape neural network predictions across subpopulations gam augments global explanations proportion samples attribution best explains specifies samples described attribution global explanations also tunable granularity detect fewer subpopulations demonstrate gam 's global explanations 1 yield known feature importances simulated data 2 match feature weights interpretable statistical models real data 3 intuitive practitioners user studies transparent predictions gam help ensure neural network decisions generated right reasons	positive
global explanations of neural networks: mapping the landscape of predictions	barrier wider adoption neural networks lack interpretability local explanation methods exist one prediction global attributions still reduce neural network decisions single set features response present approach generating global attributions called gam explains landscape neural network predictions across subpopulations gam augments global explanations proportion samples attribution best explains specifies samples described attribution global explanations also tunable granularity detect fewer subpopulations demonstrate gam 's global explanations 1 yield known feature importances simulated data 2 match feature weights interpretable statistical models real data 3 intuitive practitioners user studies transparent predictions gam help ensure neural network decisions generated right reasons	positive
trolleymod v1.0: an open-source simulation and data-collection platform for ethical decision making in autonomous vehicles	paper presents trolleymod v1.0 open-source platform based carla simulator collection ethical decision-making data autonomous vehicles platform designed facilitate experiments aiming observe record human decisions actions high-fidelity simulations ethical dilemmas occur context driving targeting experiments class trolley problems trolleymod provides seamless approach creating new experimental settings environments realistic physics-engine high-quality graphical capabilities carla unreal engine also trolleymod provides straightforward interface carla environment python enable implementation custom controllers deep reinforcement learning agents results experiments used sociological analyses well training tuning value-aligned autonomous vehicles based social values inferred observations	positive
uncovering and mitigating algorithmic bias through learned latent structure	recent research highlighted vulnerabilities modern machine learning based systems bias especially towards segments society under-represented training data work develop novel tunable algorithm mitigating hidden potentially unknown biases within training data algorithm fuses original learning task variational autoencoder learn latent structure within dataset adaptively uses learned latent distributions re-weight importance certain data points training method generalizable across various data modalities learning tasks work use algorithm address issue racial gender bias facial detection systems evaluate algorithm pilot parliaments benchmark ppb dataset specifically designed evaluate biases computer vision systems demonstrate increased overall performance well decreased categorical bias debiasing approach	negative
human-ai learning performance in multi-armed bandits	people frequently face challenging decision-making problems outcomes uncertain unknown artificial intelligence ai algorithms exist outperform humans learning tasks thus opportunity ai agents assist people learning tasks effectively work use multi-armed bandit controlled setting explore direction pair humans selection agents observe well human-agent team performs find team performance beat human agent performance isolation interestingly also find agent 's performance isolation necessarily correlate human-agent team 's performance drop agent performance lead disproportionately large drop team performance settings even improve team performance pairing human agent performs slightly better make perform much better pairing agent performs make perform much worse results suggest people different exploration strategies might perform better agents match strategy overall optimizing human-agent team performance requires going beyond optimizing agent performance understanding agent 's suggestions influence human decision-making	positive
perceptions of domestic robots?? normative behavior across cultures	domestic service robots become common widespread must programmed efficiently accomplish tasks aligning actions relevant norms first step equip domestic robots normative reasoning competence understanding norms people apply behavior robots specific social contexts end conducted online survey chinese united states participants asked select preferred normative action domestic service robot take number scenarios paper makes multiple contributions extensive survey first collect data attitudes people normative behavior domestic robots b across cultures c study relative priorities among norms domain present findings discuss implications building computational models robot normative reasoning	negative
toward the engineering of virtuous machines	various traditions 'virtue ethics umbrella studied extensively advocated ethicists clear exists version virtue ethics rigorous enough target machine ethics take include engineering ethical sensibility machine robot study ethics humans might create artificial agents begin address presenting embryonic formalization key part virtue-ethics theory namely learning virtue focus exemplars moral virtue work based part computational formal logic previously used formally model ethical theories principles therein implement models artificial agents	negative
a formal approach to explainability	regard explanations blending input sample model 's output offer definitions capture various desired properties function generates explanations study links properties explanation-generating functions intermediate representations learned models able show example activations given layer consistent explanation subsequent layers addition study intersection union explanations way construct new explanations	negative
rightful machines and dilemmas	tn paper set new kantian approach resolving conflicts dilemmas obligation semi-autonomous machine agents self-driving cars first argue efforts build explicitly moral machine agents focus kant refers duties right justice rather duties virtue ethics society everyone morally equal one individual group normative authority unilaterally decide moral conflicts resolved everyone public institutions everyone could consent authority define enforce adjudicate rights obligations respect one show shift ethics standard justice resolves conflict obligations known `` trolley problem '' rightful machine agents finally consider deontic logic suitable governing explicitly rightful machines might meet normative requirements justice	negative
the seductive allure of artificial intelligence-powered neurotechnology	neuroscience explanations-even completely irrelevant-have shown exert `` seductive allure '' individuals leading judge bad explanations arguments favorably seems similarly seductive allure artificial intelligence ai technologies leading people `` overtrust '' systems even witnessed system perform poorly ai-powered neurotechnologies begun proliferate recent years particularly based electroencephalography eeg represent potentially doubly-alluring combination enormous potential benefit applying ai techniques neuroscience `` decode '' brain activity associated mental states efforts still early stages danger using unproven technologies prematurely especially important real-world contexts yet premature use begun emerge several high-stakes set-tings including law health wellness employment transportation light potential seductive allure technologies need vigilant monitoring scientific validity challenging unsubstantiated claims misuse still actively supporting continued development proper use	negative
what are the biases in my word embedding?	paper presents algorithm enumerating biases word embeddings algorithm exposes large number offensive associations related sensitive features race gender publicly available embeddings including supposedly `` debiased '' embedding biases concerning light widespread use word embeddings associations identified geometric patterns word embeddings run parallel people 's names common lower-case tokens algorithm highly unsupervised even require sensitive features pre-specified desirable many forms discrimination racial discrimination-are linked social constructs may vary depending context rather categories fixed definitions b makes easier identify biases intersectional groups depend combinations sensitive features inputs algorithm list target tokens e.g names word embedding outputs number word embedding association tests weats capture various biases present data illustrate utility approach publicly available word embeddings lists names evaluate output using crowdsourcing also show removing names may remove potential proxy bias	positive
chinese ner with height-limited constituent parsing	paper investigate improve chinese named entity recognition ner jointly modeling ner constituent parsing framework neural conditional random fields crf reformulate parsing task heightlimited constituent parsing computational complexity significantly reduced majority phrase-level grammars retained specifically unified model neural semi-crf neural tree-crf proposed simultaneously conducts word segmentation part-ofspeech pos tagging ner parsing challenge comes train infer joint model solved previously design dynamic programming algorithm training inference whose complexity n·4h n sentence length h height limit addition derive pruning algorithm joint model prunes 99.9 search space 2 loss ground truth data experimental results ontonotes 4.0 dataset demonstrated proposed model outperforms state-of-the-art method 2.79 points f1-measure	positive
robust estimation of similarity transformation for visual object tracking	existing correlation filter-based tracking approaches estimate simple axis-aligned bounding boxes capable recovering underlying similarity transformation tackle challenging problem paper propose new correlation filter-based tracker novel robust estimation similarity transformation large displacements order efficiently search large 4-dof space real-time formulate problem two 2-dof sub-problems apply efficient block coordinates descent solver optimize estimation result specifically employ efficient phase correlation scheme deal scale rotation changes simultaneously log-polar coordinates moreover variant correlation filter used predict translational motion individually experimental results demonstrate proposed tracker achieves promising prediction performance compared state-of-the-art visual object tracking methods still retaining advantages high efficiency simplicity conventional correlation filter-based tracking methods	negative
sadih: semantic-aware discrete hashing	due low storage cost fast query speed hashing recognized accomplish similarity search largescale multimedia retrieval applications particularly supervised hashing recently received considerable research attention leveraging label information preserve pairwise similarities data points hamming space however still remain two crucial bottlenecks 1 learning process full pairwise similarity preservation computationally unaffordable unscalable deal big data 2 available category information data well-explored learn discriminative hash functions overcome challenges propose unified semantic-aware discrete hashing sadih framework aims directly embed transformed semantic information asymmetric similarity approximation discriminative hashing function learning specifically semantic-aware latent embedding introduced asymmetrically preserve full pairwise similarities skillfully handle cumbersome n×n pairwise similarity matrix meanwhile semantic-aware autoencoder developed jointly preserve data structures discriminative latent semantic space perform data reconstruction moreover efficient alternating optimization algorithm proposed solve resulting discrete optimization problem extensive experimental results multiple large-scale datasets demonstrate sadih clearly outperform state-of-the-art baselines additional benefit lower computational costs	negative
spatial-temporal person re-identification	current person re-identification reid methods neglect spatial-temporal constraint given query image conventional methods compute feature distances query image gallery images return similarity ranked table gallery database large practice approaches fail obtain good performance due appearance ambiguity across different camera views paper propose novel two-stream spatial-temporal person reid st-reid framework mines visual semantic information spatial-temporal information end joint similarity metric logistic smoothing ls introduced integrate two kinds heterogeneous information unified framework approximate complex spatial-temporal probability distribution develop fast histogram-parzen hp method help spatial-temporal constraint st-reid model eliminates lots irrelevant images thus narrows gallery database without bells whistles st-reid method achieves rank-1 accuracy 98.1 market-1501 94.4 dukemtmc-reid improving baselines 91.2 83.8 respectively outperforming previous state-of-theart methods large margin	positive
online embedding compression for text classification using low rank matrix factorization	deep learning models become state art natural language processing nlp tasks however deploying models production system poses significant memory constraints existing compression methods either lossy introduce significant latency propose compression method leverages low rank matrix factorization training compress word embedding layer represents size bottleneck nlp models models trained compressed re-trained downstream task recover accuracy maintaining reduced size empirically show proposed method achieve 90 compression minimal impact accuracy sentence classification tasks outperforms alternative methods like fixed-point quantization offline word embedding compression also analyze inference time storage space method flop calculations showing compress dnn models configurable ratio regain accuracy loss without introducing additional latency compared fixed point quantization finally introduce novel learning rate schedule cyclically annealed learning rate calr empirically demonstrate outperform popular adaptive learning rate algorithms sentence classification benchmark	negative
learning to steer by mimicking features from heterogeneous auxiliary networks	training many existing end-to-end steering angle prediction models heavily relies steering angles supervisory signal without learning much richer contexts methods susceptible presence sharp road curves challenging traffic conditions strong shadows severe lighting changes paper considerably improve accuracy robustness predictions heterogeneous auxiliary networks feature mimicking new effective training method provides us much richer contextual signals apart steering direction specifically train steering angle predictive model distilling multi-layer knowledge multiple heterogeneous auxiliary networks perform related different tasks e.g. image segmentation optical flow estimation opposed multi-task learning method require expensive annotations related tasks target set made possible applying contemporary off-the-shelf networks target set mimicking features different layers transformation auxiliary networks discarded training without affecting runtime efficiency model approach achieves new state-of-the-art udacity comma.ai outperforming previous best large margin 12.8 52.1 1 respectively encouraging results also shown berkeley deep drive bdd dataset	negative
triple classification using regions and fine-grained entity typing	triple knowledge-graph takes form consists head relation tail triple classification used determine truth value unknown triple hard task 1-to-n relations using vector-based embedding approach propose new region-based embedding approach using fine-grained type chains novel geometric process presented extend vectors pre-trained entities n-balls n-dimensional balls condition head balls shall contain tail balls algorithm achieves zero energy cost therefore serves case study perfectly imposing tree structures vector space unknown triple h r x predicted true x ’ n-ball located r-subspace h ’ n-ball following construction known tails h. experiments based large datasets derived benchmark datasets wn11 fb13 wn18 results show performance new method related length type chain quality pre-trained entityembeddings performances long chains welltrained entity-embeddings outperform methods literature source codes datasets located https //github.com/gnodisnait/mushroom	negative
segan: structure-enhanced generative adversarial network for compressed sensing mri reconstruction	generative adversarial networks gans powerful tools reconstructing compressed sensing magnetic resonance imaging cs-mri however recent works lack exploration structure information mri images crucial clinical diagnosis tackle problem propose structure-enhanced gan segan aims restoring structure information local global scale segan defines new structure regularization called patch correlation regularization pcr allows efficient extraction structure information addition enhance ability uncover structure information propose novel generator su-net incorporating multiple-scale convolution filters layer besides theoretically analyze convergence stochastic factors contained training process experimental results show segan able learn target structure information achieves state-of-theart performance cs-mri reconstruction	positive
learning object context for dense captioning	dense captioning challenging task detects visual elements images also generates natural language sentences describe previous approaches leverage object information images task however objects provide valuable cues help predict locations caption regions caption regions often highly overlap objects i.e caption regions usually parts objects combinations meanwhile objects also provide important information describing target caption region corresponding description depicts properties also involves interactions objects image work propose novel scheme object context encoding long short-term memory lstm network automatically learn complementary object context caption region transferring knowledge objects caption regions contextual objects arranged sequence progressively fed context encoding module obtain context features learned object context features region features used predict bounding box offsets generate descriptions context learning procedure conjunction optimization location prediction caption generation thus enabling object context encoding lstm capture aggregate useful object context experiments benchmark datasets demonstrate superiority proposed approach state-of-the-art methods	negative
adversarial framing for image and video classification	neural networks prone adversarial attacks general attacks deteriorate quality input either slightly modifying pixels occluding patch paper propose method keeps image unchanged adds adversarial framing border image show empirically method able successfully attack state-of-theart methods image video classification problems notably proposed method results universal attack fast test time source code found github.com/zajaczajac/adv_framing	negative
feature sampling based unsupervised semantic clustering for real web multi-view content	real web datasets often associated multiple views long short commentaries users preference however rapid growth user generated texts view dataset large feature space leads computational challenge matrix decomposition process paper propose novel multi-view clustering algorithm based non-negative matrix factorization attempts use feature sampling strategy order reduce complexity iteration process particular method exploits unsupervised semantic information learning process capture intrinsic similarity graph regularization moreover use hilbert schmidt independence criterion hsic explore unsupervised semantic diversity information among multi-view contents one web item overall objective minimize loss function multi-view non-negative matrix factorization combines intra-semantic similarity graph regularizer inter-semantic diversity term compared state-of-the-art methods demonstrate effectiveness proposed method large real-world dataset doucom three smaller datasets	negative
a meta-learning approach for custom model training	transfer-learning meta-learning two effective methods apply knowledge learned large data sources new tasks few-class few-shot target task settings i.e classes training examples available target task meta-learning approaches optimize future task learning outperformed typical transfer approach initializing model weights pretrained starting point experimentally show metalearning algorithms work well few-class setting generalize well many-shot many-class cases paper propose joint training approach combines transfer-learning meta-learning benefiting advantages method obtains improved generalization performance unseen target tasks few- many-class few- many-shot scenarios	negative
deep convolutional sum-product networks	give conditions convolutional neural networks cnns define valid sum-product networks spns one subclass called convolutional spns cspns implemented using tensors also suffer shallow fortunately tensors augmented maintaining valid spns yields larger subclass cnns call deep convolutional spns dcspns convolutional sum-pooling layers form rich directed acyclic graph structures one salient feature dcspns rigorous probabilistic model exploit multiple kinds probabilistic reasoning including marginal inference probable explanation mpe inference allows alternative method learning dcspns using vectorized differentiable mpe plays similar role generator generative adversarial networks gans image sampling yet another application demonstrating robustness dcspns preliminary results image sampling encouraging since dcspn sampled images exhibit variability experiments image completion show dcspns significantly outperform competing methods achieving several state-of-the-art mean squared error mse scores left-completion bottom-completion benchmark datasets	positive
hypergraph optimization for multi-structural geometric model fitting	recently hypergraph-based methods proposed deal problem model fitting computer vision mainly due superior capability hypergraph represent complex relationship data points however hypergraph becomes extremely complicated input data include large number data points usually contaminated noises outliers significantly increase computational burden order overcome problem propose novel hypergraph optimization based model fitting homf method construct simple effective hypergraph specifically homf includes two main parts adaptive inlier estimation algorithm vertex optimization iterative hyperedge optimization algorithm hyperedge optimization proposed method highly efficient obtain accurate model fitting results within iterations moreover homf directly apply spectral clustering achieve good fitting performance extensive experimental results show homf outperforms several state-of-the-art model fitting methods synthetic data real images especially sampling efficiency handling data severe outliers	negative
similarity preserving deep asymmetric quantization for image retrieval	quantization widely adopted large-scale multimedia retrieval due effectiveness coding highdimensional data deep quantization models demonstrated achieve state-of-the-art retrieval accuracy however training deep models given large-scale database highly time-consuming large amount parameters involved existing deep quantization methods often sample subset database training may end unsatisfactory retrieval performance large portion label information discarded alleviate problem propose novel model called similarity preserving deep asymmetric quantization spdaq directly learn compact binary codes quantization codebooks items database efficiently spdaq makes use image subset well label information database items image subset items database items mapped two different correlated distributions label similarity well preserved efficient optimization algorithm proposed learning extensive experiments conducted four widely-used benchmark datasets demonstrate superiority proposed spdaq model	negative
towards optimal discrete online hashing with balanced similarity	facing large-scale image datasets online hashing serves promising solution online retrieval prediction tasks encodes online streaming data compact binary codes simultaneously updates hash functions renew codes existing dataset end existing methods update hash functions solely based new data batch without investigating correlation new data existing dataset addition existing works update hash functions using relaxation process corresponding approximated continuous space remains open problem directly apply discrete optimizations online hashing paper propose novel supervised online hashing method termed balanced similarity online discrete hashing bsodh solve problems unified framework bsodh employs well-designed hashing algorithm preserve similarity streaming data existing dataset via asymmetric graph regularization identify “ data-imbalance ” problem brought constructed asymmetric graph restricts application discrete optimization problem therefore novel balanced similarity proposed uses two equilibrium factors balance similar dissimilar weights eventually enables usage discrete optimizations extensive experiments conducted three widely-used benchmarks demonstrate advantages proposed method stateof-the-art methods	negative
a grammar-based structural cnn decoder for code generation	code generation maps program description executable source code programming language existing approaches mainly rely recurrent neural network rnn decoder however find program contains significantly tokens natural language sentence thus may inappropriate rnn capture long sequence paper propose grammar-based structural convolutional neural network cnn code generation model generates program predicting grammar rules programming language design several cnn modules including tree-based convolution pre-order convolution whose information aggregated dedicated attentive pooling layers experimental results hearthstone benchmark dataset show cnn code generator significantly outperforms previous state-of-the-art method 5 percentage points additional experiments several semantic parsing tasks demonstrate robustness model also conduct in-depth ablation test better understand component model	positive
gaussian transformer: a lightweight approach for natural language inference	natural language inference nli active research area numerous approaches based recurrent neural networks rnns convolutional neural networks cnns self-attention networks sans proposed although obtaining impressive performance previous recurrent approaches hard train parallel convolutional models tend cost parameters self-attention networks good capturing local dependency texts address problem introduce gaussian prior selfattention mechanism better modeling local structure sentences propose efficient rnn/cnn-free architecture named gaussian transformer nli consists encoding blocks modeling local global dependency high-order interaction blocks collecting evidence multi-step inference lightweight comparison block saving lots parameters experiments show model achieves new state-of-the-art performance snli multinli benchmarks significantly fewer parameters considerably less training time besides evaluation using hard nli datasets demonstrates approach less affected undesirable annotation artifacts	negative
rgbd based gaze estimation via multi-task cnn	paper tackles rgbd based gaze estimation convolutional neural networks cnns specifically propose decompose gaze point estimation eyeball pose head pose 3d eye position estimation compared rgb image-based gaze tracking depth modality helps facilitate head pose estimation 3d eye position estimation captured depth image however usually contains noise black holes noticeably hamper gaze tracking thus propose cnn-based multi-task learning framework simultaneously refine depth images predict gaze points utilize generator network depth image generation generative neural network gan generator network partially shared gaze tracking network gan-based depth synthesizing optimizing whole network simultaneously depth image synthesis improves gaze point estimation vice versa since existing rgbd dataset eyediap small build large-scale rgbd gaze tracking dataset performance evaluation far know largest rgbd gaze dataset terms number participants comprehensive experiments demonstrate method outperforms existing methods large margin dataset eyediap dataset	positive
learning fully dense neural networks for image semantic segmentation	semantic segmentation pixel-wise classification retains critical spatial information “ feature map reuse ” commonly adopted cnn based approaches take advantage feature maps early layers later spatial reconstruction along direction go step proposing fully dense neural network encoderdecoder structure abbreviate fdnet stage decoder module feature maps previous blocks adaptively aggregated feedforward input one hand reconstructs spatial boundaries accurately hand learns efficiently efficient gradient backpropagation addition propose boundary-aware loss function focus attention pixels near boundary boosts “ hard examples ” labeling demonstrated best performance fdnet two benchmark datasets pascal voc 2012 nyudv2 previous works considering training datasets	negative
hierarchical classification based on label distribution learning	hierarchical classification challenging problem class labels organized predefined hierarchy one primary challenge hierarchical classification small training set issue local module local classifiers previous hierarchical classification approaches prone over-fitting becomes major bottleneck hierarchical classification fortunately labels local module correlated siblings true label provide additional supervision information instance paper proposes novel method deal small training set issue key idea method represent correlation among labels label distribution generates label distribution contains supervision information label given instance learns mapping instance label distribution experimental results several hierarchical classification datasets show method significantly outperforms state-of-theart hierarchical classification approaches	negative
semi-parametric sampling for stochastic bandits with many arms	consider stochastic bandit problem large candidate arm set setting classic multi-armed bandit algorithms assume independence among arms adopt non-parametric reward model inefficient due large number arms exploiting arm correlations based parametric reward model arm features contextual bandit algorithms efficient also suffer large regret practical applications due reward estimation bias mis-specified model assumption incomplete features paper propose novel bayesian framework called semi-parametric sampling sps problem employs semi-parametric function reward model specifically parametric part sps models expected reward parametric function arm feature efficiently eliminate poor arms candidate set non-parametric part sps adopts nonparametric reward model revises parametric estimation avoid estimation bias especially remained candidate arms give implementation sps linear sps lsps utilizes linear function parametric part semi-parametric environment theoretical analysis shows lsps achieves better regret bound i.e o̴ √n1−α dα √t α ∈ 0 1 existing approaches also experiments demonstrate superiority proposed approach	positive
efficient gaussian process classification using pólya-gamma data augmentation	propose scalable stochastic variational approach gp classification building pólya-gamma data augmentation inducing points unlike former approaches obtain closed-form updates based natural gradients lead efficient optimization evaluate algorithm real-world datasets containing 11 million data points demonstrate two orders magnitude faster state-of-the-art competitive terms prediction performance	positive
ranking-based deep cross-modal hashing	cross-modal hashing receiving increasing interests low storage cost fast query speed multi-modal data retrievals however existing hashing methods based hand-crafted raw level features objects may optimally compatible coding process besides hashing methods mainly designed handle simple pairwise similarity complex multilevel ranking semantic structure instances associated multiple labels well explored yet paper propose ranking-based deep cross-modal hashing approach rdcmh rdcmh firstly uses feature label information data derive semi-supervised semantic ranking list next expand semantic representation power hand-crafted features rdcmh integrates semantic ranking information deep cross-modal hashing jointly optimizes compatible parameters deep feature representations hashing functions experiments real multi-modal datasets show rdcmh outperforms competitive baselines achieves state-of-the-art performance cross-modal retrieval applications	positive
meta-descent for online, continual prediction	paper investigates different vector step-size adaptation approaches non-stationary online continual prediction problems vanilla stochastic gradient descent considerably improved scaling update vector appropriately chosen step-sizes many methods including adagrad rmsprop amsgrad keep statistics learning process approximate second order update—a vector approximation inverse hessian another family approaches use meta-gradient descent adapt stepsize parameters minimize prediction error metadescent strategies promising non-stationary problems extensively explored quasi-second order methods first derive general incremental metadescent algorithm called adagain designed applicable much broader range algorithms including semi-gradient updates even accelerations rmsprop provide empirical comparison methods families conclude methods families perform well non-stationary prediction problems meta-descent methods exhibit advantages method particularly robust across several prediction problems competitive state-of-the-art method large-scale time-series prediction problem real data mobile robot	positive
compressing recurrent neural networks with tensor ring for action recognition	recurrent neural networks rnns variants long-short term memory lstm networks gated recurrent unit gru networks achieved promising performance sequential data modeling hidden layers rnns regarded memory units helpful storing information sequential contexts however dealing high dimensional input data video text input-to-hidden linear transformation rnns brings high memory usage huge computational cost makes training rnns difficult address challenge propose novel compact lstm model named tr-lstm utilizing low-rank tensor ring decomposition trd reformulate input-to-hidden transformation compared tensor decomposition methods tr-lstm stable addition tr-lstm complete end-to-end training also provide fundamental building block rnns handling large input data experiments real-world action recognition datasets demonstrated promising performance proposed tr-lstm compared tensor-train lstm state-of-the-art competitors	negative
autozoom: autoencoder-based zeroth order optimization method for attacking black-box neural networks	recent studies shown adversarial examples state-of-the-art image classifiers trained deep neural networks dnn easily generated target model transparent attacker known white-box setting however attacking deployed machine learning service one acquire input-output correspondences target model so-called black-box attack setting major drawback existing black-box attacks need excessive model queries may give false sense model robustness due inefficient query designs bridge gap propose generic framework query-efficient blackbox attacks framework autozoom short autoencoder-based zeroth order optimization method two novel building blocks towards efficient black-box attacks adaptive random gradient estimation strategy balance query counts distortion ii autoencoder either trained offline unlabeled data bilinear resizing operation attack acceleration experimental results suggest applying autozoom state-of-the-art black-box attack zoo significant reduction model queries achieved without sacrificing attack success rate visual quality resulting adversarial examples particular compared standard zoo method autozoom consistently reduce mean query counts finding successful adversarial examples reaching distortion level least 93 mnist cifar-10 imagenet datasets leading novel insights adversarial robustness	negative
deep reactive policies for planning in stochastic nonlinear domains	recent advances applying deep learning planning shown deep reactive policies drps powerful fast decision-making complex environments however important limitation current drp-based approaches either need optimal planners used ground truth supervised learning setting sample complexity high-variance policy gradient estimators particularly troublesome continuous state-action domains order overcome limitations introduce framework training drps continuous stochastic spaces via gradient-based policy search general approach explicitly encode parametric policy deep neural network formulate probabilistic planning problem optimization task stochastic computation graph exploiting re-parameterization transition probability densities optimization solved leveraging gradient descent algorithms able handle non-convex objective functions benchmark approach stochastic planning domains exhibiting arbitrary differentiable nonlinear transition cost functions e.g. reservoir control hvac navigation results show drps 125,000 continuous action parameters optimized approach problems 30 state fluents 30 action fluents inexpensive hardware 6 minutes also observed speedup 5 orders magnitude average inference time per decision step drps compared state-of-the-art online gradient-based planners level solution quality required	negative
sign-full random projections	method 1-bit “ sign-sign ” random projections popular tool efficient search machine learning large datasets given two d-dim data vectors u v ∈ ℝd one generate x ∑i=1d uiri ∑i=1d viri ri ∼ n 0 1 iid one estimate cosine similarity ρ sgn x sgn paper study series estimators “ sign-full ” random projections first prove e sgn x √2/πρ provides estimator ρ. interestingly estimator substantially improved normalizing y. study estimators based e y−1x≥0 y+1x 0 normalized version analyze theoretical limit using mle conclude among proposed estimators single estimator achieve close theoretical optimal asymptotic variance entire range ρ. hand estimators combined achieve variance close mle applications near neighbor search duplicate detection knn-classification etc training data first transformed via random projections signs projected data points stored i.e. sgn x original training data discarded new data point arrives apply random projections necessarily need quantize projected data i.e. 1-bit therefore sign-full random projections practically useful gain essentially comes additional cost	negative
near-lossless binarization of word embeddings	word embeddings commonly used starting point many nlp models achieve state-of-the-art performances however large vocabulary many dimensions floating-point representations expensive terms memory calculations makes unsuitable use low-resource devices method proposed paper transforms real-valued embeddings binary embeddings preserving semantic information requiring 128 256 bits vector leads small memory footprint fast vector operations model based autoencoder architecture also allows reconstruct original vectors binary ones experimental results semantic similarity text classification sentiment analysis tasks show binarization word embeddings leads loss ∼2 accuracy vector size reduced 97 furthermore top-k benchmark demonstrates using binary vectors 30 times faster using real-valued vectors	negative
soft facial landmark detection by label distribution learning	existing facial landmark detection algorithms regard manually annotated landmarks precise hard labels therefore accurate annotated landmarks essential training algorithms however many cases exist deviations manual annotations landmarks marked facial parts occlusion large poses always accurate means “ ground truth ” landmarks usually annotated precisely case reasonable use soft labels rather explicit hard labels therefore paper proposes associate bivariate label distribution bld landmark image bld covers neighboring pixels around original manually annotated point alleviating problem inaccurate landmarks generating bld landmark proposed method firstly learns mappings image patch bld landmark predicted blds used deformable model fitting process obtain final facial shape image experimental results show proposed method performs better compared state-of-the-art facial landmark detection algorithms furthermore proposed method appears much robust landmark noise training set compared baselines	negative
large-scale visual relationship understanding	large scale visual understanding challenging requires model handle widely-spread imbalanced distribution 〈subject relation object〉 triples real-world scenarios large numbers objects relations seen commonly others barely seen develop new relationship detection model embeds objects relations two vector spaces discriminative capability semantic affinity preserved learn visual semantic module map features two modalities shared space matched pairs features discriminate unmatched also maintain close distances semantically similar ones benefiting model achieve superior performance even visual entity categories scale 80,000 extremely skewed class distribution demonstrate efficacy model large imbalanced benchmark based visual genome comprises 53,000+ objects 29,000+ relations scale previous work evaluated show superiority model competitive baselines original visual genome dataset 80,000+ categories also show state-of-the-art performance vrd dataset scene graph dataset subset visual genome 200 categories	negative
data-distortion guided self-distillation for deep neural networks	knowledge distillation effective technique widely used transferring knowledge network another network despite effective improvement network performance dependence accompanying assistive models complicates training process single network need large memory time cost paper design elegant self-distillation mechanism transfer knowledge different distorted versions training data without reliance accompanying models specifically potential capacity single network excavated learning consistent global feature distributions posterior distributions class probabilities across distorted versions data extensive experiments multiple datasets i.e. cifar-10/100 imagenet demonstrate proposed method effectively improve generalization performance various network architectures alexnet resnet wide resnet densenet outperform existing distillation methods little extra training efforts	negative
m2det: a single-shot object detector based on multi-level feature pyramid network	feature pyramids widely exploited state-of-the-art one-stage object detectors e.g. dssd retinanet refinedet two-stage object detectors e.g. mask rcnn detnet alleviate problem arising scale variation across object instances although object detectors feature pyramids achieve encouraging results limitations due simply construct feature pyramid according inherent multiscale pyramidal architecture backbones originally designed object classification task newly work present multi-level feature pyramid network mlfpn construct effective feature pyramids detecting objects different scales first fuse multi-level features i.e multiple layers extracted backbone base feature second feed base feature block alternating joint thinned u-shape modules feature fusion modules exploit decoder layers ushape module features detecting objects finally gather decoder layers equivalent scales sizes construct feature pyramid object detection every feature map consists layers features multiple levels evaluate effectiveness proposed mlfpn design train powerful end-to-end one-stage object detector call m2det integrating architecture ssd achieve better detection performance state-of-the-art one-stage detectors specifically mscoco benchmark m2det achieves ap 41.0 speed 11.8 fps single-scale inference strategy ap 44.2 multi-scale inference strategy new stateof-the-art results among one-stage detectors code made available https //github.com/qijiezhao/m2det	negative
stnet: local and global spatial-temporal modeling for action recognition	despite success deep learning static image understanding remains unclear effective network architectures spatial-temporal modeling videos paper contrast existing cnn+rnn pure 3d convolution based approaches explore novel spatialtemporal network stnet architecture local global modeling videos particularly stnet stacks n successive video frames super-image 3n channels applies 2d convolution super-images capture local spatial-temporal relationship model global spatialtemporal structure apply temporal convolution local spatial-temporal feature maps specifically novel temporal xception block proposed stnet employs separate channel-wise temporal-wise convolution feature sequence video extensive experiments kinetics dataset demonstrate framework outperforms several state-of-the-art approaches action recognition strike satisfying trade-off recognition accuracy model complexity demonstrate generalization performance leaned video representations ucf101 dataset	negative
learning (from) deep hierarchical structure among features	data features usually organized hierarchical structure reflect relations among previous studies utilize hierarchical structure help improve performance supervised learning tasks handle structure limited height 2. paper propose deep hierarchical structure dhs method handle hierarchical structure arbitrary height convex objective function dhs method relies exponents edge weights hierarchical structure exponents need given users set identical default may suboptimal based dhs method propose variant learn exponents data moreover consider case even hierarchical structure available based dhs method propose learning deep hierarchical structure ldhs method learn hierarchical structure via generalized fused-lasso regularizer proposed sequential constraint optimization problems solved proximal methods subproblem efficient solution experiments synthetic real-world datasets show effectiveness proposed methods	negative
hierarchical deep feature learning for decoding imagined speech from eeg	propose mixed deep neural network strategy incorporating parallel combination convolutional cnn recurrent neural networks rnn cascaded deep autoencoders fully connected layers towards automatic identification imagined speech eeg instead utilizing raw eeg channel data compute joint variability channels form covariance matrix provide spatio-temporal representations eeg networks trained hierarchically extracted features passed onto next network hierarchy final classification using publicly available eeg based speech imagery database demonstrate around 23.45 improvement accuracy baseline method approach demonstrates promise mixed dnn approach complex spatialtemporal classification problems	positive
end-to-end structure-aware convolutional networks for knowledge base completion	knowledge graph embedding active research topic knowledge base completion progressive improvement initial transe transh distmult et al current state-of-the-art conve conve uses 2d convolution embeddings multiple layers nonlinear features model knowledge graphs model efficiently trained scalable large knowledge graphs however structure enforcement embedding space conve recent graph convolutional network gcn provides another way learning graph node embedding successfully utilizing graph connectivity structure work propose novel end-to-end structureaware convolutional network sacn takes benefit gcn conve together sacn consists encoder weighted graph convolutional network wgcn decoder convolutional network called conv-transe wgcn utilizes knowledge graph node structure node attributes edge relation types learnable weights adapt amount information neighbors used local aggregation leading accurate embeddings graph nodes node attributes graph represented additional nodes wgcn decoder conv-transe enables state-of-the-art conve translational entities relations keeps link prediction performance conve demonstrate effectiveness proposed sacn standard fb15k-237 wn18rr datasets gives 10 relative improvement state-of-theart conve terms hits 1 hits 3 hits 10	negative
multi-task learning with multi-view attention for answer selection and knowledge base question answering	answer selection knowledge base question answering kbqa two important tasks question answering qa systems existing methods solve two tasks separately requires large number repetitive work neglects rich correlation information tasks paper tackle answer selection kbqa tasks simultaneously via multi-task learning mtl motivated following motivations first answer selection kbqa regarded ranking problem one text-level knowledge-level second two tasks benefit answer selection incorporate external knowledge knowledge base kb kbqa improved learning contextual information answer selection fulfill goal jointly learning two tasks propose novel multi-task learning scheme utilizes multi-view attention learned various perspectives enable tasks interact well learn comprehensive sentence representations experiments conducted several real-world datasets demonstrate effectiveness proposed method performance answer selection kbqa improved also multi-view attention scheme proved effective assembling attentive information different representational perspectives	negative
connecting language to images: a progressive attention-guided network for simultaneous image captioning and language grounding	image captioning visual language grounding two important tasks image understanding seldom considered together paper propose progressive attention-guided network pagnet simultaneously generates image captions predicts bounding boxes caption words pagnet mainly two distinctive properties progressively refine predictive results image captioning updating attention map predicted bounding boxes ii learns bounding boxes words using weakly supervised strategy combines frameworks multiple instance learning mil markov decision process mdp using attention map generated captioning process pagnet significantly reduces search space mdp conduct experiments benchmark datasets demonstrate effectiveness pagnet results show pagnet achieves best performance	negative
mpd-al: an efficient membrane potential driven aggregate-label learning algorithm for spiking neurons	one long-standing questions biology machine learning neural networks may learn important features input activities delayed feedback commonly known temporal credit-assignment problem aggregate-label learning proposed resolve problem matching spike count neuron magnitude feedback signal however existing threshold-driven aggregate-label learning algorithms computationally intensive resulting relatively low learning efficiency hence limiting usability practical applications order address limitations propose novel membrane-potential driven aggregate-label learning algorithm namely mpd-al algorithm easiest modifiable time instant identified membrane potential traces neuron guild synaptic adaptation based presynaptic neurons ’ contribution time instant experimental results demonstrate proposed algorithm enables neurons generate desired number spikes detect useful clues embedded within unrelated spiking activities background noise better learning efficiency state-of-the-art tdp1 multi-spike tempotron algorithms furthermore propose data-driven dynamic decoding scheme practical classification tasks aggregate labels hard define scheme effectively improves classification accuracy aggregate-label learning algorithms demonstrated speech recognition task	negative
multilevel language and vision integration for text-to-clip retrieval	address problem text-based activity retrieval video given sentence describing activity task retrieve matching clips untrimmed video capture inherent structures present text video introduce multilevel model integrates vision language features earlier tightly prior work first inject text features early generating clip proposals help eliminate unlikely clips thus speed processing boost performance second learn fine-grained similarity metric retrieval use visual features modulate processing query sentences word level recurrent neural network multi-task loss also employed adding query re-generation auxiliary task approach significantly outperforms prior work two challenging benchmarks charades-sta activitynet captions	negative
distributionally adversarial attack	recent work adversarial attack shown projected gradient descent pgd adversary universal first-order adversary classifier adversarially trained pgd robust wide range first-order attacks worth noting original objective attack/defense model relies data distribution p x typically form risk maximization/minimization e.g. max/min ep x l x p x unknown data distribution l · loss function however since pgd generates attack samples independently data sample based l · procedure necessarily lead good generalization terms risk optimization paper achieve goal proposing distributionally adversarial attack daa framework solve optimal adversarial-data distribution perturbed distribution satisfies l∞ constraint deviates original data distribution increase generalization risk maximally algorithmically daa performs optimization space potential data distributions introduces direct dependency data points generating adversarial samples daa evaluated attacking state-of-the-art defense models including adversarially-trained models provided mit madrylab notably daa ranks first place madrylab ’ white-box leaderboards reducing accuracy secret mnist model 88.56 l∞ perturbations ε 0.3 accuracy secret cifar model 44.71 l∞ perturbations ε 8.0 code experiments released https //github.com/tianzheng4/distributionally-adversarial-attack	negative
segregated temporal assembly recurrent networks for weakly supervised multiple action detection	paper proposes segregated temporal assembly recurrent star network weakly-supervised multiple action detection model learns untrimmed videos supervision video-level labels makes prediction intervals multiple actions specifically first assemble video clips according class labels attention mechanism learns class-variable attention weights thus helps noise relieving background actions secondly build temporal relationship actions feeding assembled features enhanced recurrent neural network finally transform output recurrent neural network corresponding action distribution order generate precise temporal proposals design score term called segregated temporal gradient-weighted class activation mapping st-gradcam fused attention weights experiments thumos ’ 14 activitynet1.3 datasets show approach outperforms state-of-theart weakly-supervised method performs par fully-supervised counterparts	positive
backbone cannot be trained at once: rolling back to pre-trained network for person re-identification	person re-identification reid task shortage trainable dataset common utilize fine-tuning method using classification network pre-trained large dataset however relatively difficult sufficiently finetune low-level layers network due gradient vanishing problem work propose novel fine-tuning strategy allows low-level layers sufficiently trained rolling back weights high-level layers initial pre-trained weights strategy alleviates problem gradient vanishing low-level layers robustly trains low-level layers fit reid dataset thereby increasing performance reid tasks improved performance proposed strategy validated via several experiments furthermore without addons pose estimation segmentation strategy exhibits state-of-the-art performance using vanilla deep convolutional neural network architecture	positive
safeguarded dynamic label regression for noisy supervision	learning noisy labels imperative big data era since reduces expensive labor accurate annotations previous method learning noise transition enjoyed theoretical guarantees applied scenario class-conditional noise however approach critically depends accurate pre-estimated noise transition usually impractical subsequent improvement adapts preestimation form softmax layer along training progress however parameters softmax layer highly tweaked fragile performance easily get stuck undesired local minimums overcome issue propose latent class-conditional noise model lccn models noise transition bayesian form projecting noise transition dirichlet-distributed space learning constrained simplex instead adhoc parametric space furthermore specially deduce dynamic label regression method lccn iteratively infer latent true labels jointly train classifier model noise approach theoretically safeguards bounded update noise transition avoids arbitrarily tuning via batch samples extensive experiments conducted controllable noise data cifar10 cifar-100 datasets agnostic noise data clothing1m webvision17 datasets experimental results demonstrated proposed model outperforms several state-of-the-art methods	negative
complex moment-based supervised eigenmap for dimensionality reduction	dimensionality reduction methods project highdimensional data low-dimensional space matrix trace optimization widely used clustering classification matrix trace optimization problem leads eigenvalue problem low-dimensional subspace construction preserving certain properties original data however existing methods use eigenvectors construct low-dimensional space may lead loss useful information achieving successful classification herein overcome deficiency information loss propose novel complex moment-based supervised eigenmap including multiple eigenvectors dimensionality reduction furthermore proposed method provides general formulation matrix trace optimization methods incorporate ridge regression models linear dependency covariate variables univariate labels reduce computational complexity also propose efficient parallel implementation proposed method numerical experiments indicate proposed method competitive compared existing dimensionality reduction methods recognition performance additionally proposed method exhibits high parallel efficiency	negative
adaptive region embedding for text classification	deep learning models convolutional neural networks recurrent networks widely applied text classification spite great success deep learning models neglect importance modeling context information crucial understanding texts work propose adaptive region embedding learn context representation improve text classification specifically metanetwork learned generate context matrix region word interacts corresponding context matrix produce regional representation classification compared previous models designed capture context information model contains less parameters flexible extensively evaluate method 8 benchmark datasets text classification experimental results prove method achieves state-of-the-art performances effectively avoids word ambiguity	negative
deep reinforcement learning via past-success directed exploration	balance exploration exploitation always core challenge reinforcement learning paper proposes “ past-success exploration strategy combined softmax action selection ” pse-softmax adaptive control method taking advantage characteristics online learning process agent adapt exploration parameters dynamically proposed strategy tested openai gym discrete continuous control tasks experimental results show pse-softmax strategy delivers better performance deep reinforcement learning algorithms basic exploration strategies	positive
kernelized hashcode representations for relation extraction	kernel methods produced state-of-the-art results number nlp tasks relation extraction suffer poor scalability due high cost computing kernel similarities natural language structures recently proposed technique kernelized locality-sensitive hashing klsh significantly reduce computational cost applicable classifiers operating knn graphs propose use random subspaces klsh codes efficiently constructing explicit representation nlp structures suitable general classification methods propose approach optimizing klsh model classification problems maximizing approximation mutual information klsh codes feature vectors class labels evaluate proposed approach biomedical relation extraction datasets observe significant robust improvements accuracy w.r.t state-ofthe-art classifiers along drastic orders-of-magnitude speedup compared conventional kernel methods	negative
recurrent poisson process unit for speech recognition	past years resurgence interest using recurrent neural network-hidden markov model rnn-hmm automatic speech recognition asr modern recurrent network models long shortterm memory lstm simple recurrent unit sru demonstrated promising results task recently several scientific perspectives fields neuroethology speech production suggest human speech signals may represented discrete point patterns involving acoustic events speech signal based hypothesis may pose challenges rnn-hmm acoustic modeling firstly arbitrarily discretizes continuous input interval features fixed frame rate may introduce discretization errors secondly occurrences acoustic events unknown furthermore training targets rnn-hmm obtained inferior models giving rise misalignments paper propose recurrent poisson process rpp seen collection poisson processes series time intervals intensity evolves according rnn hidden states encode history acoustic signal aims allocating latent acoustic events continuous time events efficiently drawn rpp using sampling-free solution analytic form speech signal containing latent acoustic events reconstructed/sampled dynamically discretized acoustic features using linear interpolation weight parameters estimated onset events processes integrated sru forming final model called recurrent poisson process unit rppu experimental evaluations asr tasks including chime-2 wsj0 wsj0 1 demonstrate effectiveness benefits rppu example achieves relative wer reduction 10.7 state-of-the-art models wsj0	positive
show, attend and read: a simple and strong baseline for irregular text recognition	recognizing irregular text natural scene images challenging due large variance text appearance curvature orientation distortion existing approaches rely heavily sophisticated model designs and/or extra fine-grained annotations extent increase difficulty algorithm implementation data collection work propose easy-to-implement strong baseline irregular scene text recognition using offthe-shelf neural network components word-level annotations composed 31-layer resnet lstmbased encoder-decoder framework 2-dimensional attention module despite simplicity proposed method robust achieves state-of-the-art performance irregular text recognition benchmarks comparable results regular text datasets code released	negative
frame and feature-context video super-resolution	video super-resolution current state-of-the-art approaches either process multiple low-resolution lr frames produce output high-resolution hr frame separately sliding window fashion recurrently exploit previously estimated hr frames super-resolve following frame main weaknesses approaches 1 separately generating output frame may obtain high-quality hr estimates resulting unsatisfactory flickering artifacts 2 combining previously generated hr frames produce temporally consistent results case short information flow cause significant jitter jagged artifacts previous super-resolving errors constantly accumulated subsequent frames	negative
rs3cis: robust single-step spectral clustering with intrinsic subspace	spectral clustering widely adopted mine structures data clusters clustering performance spectral clustering depends largely quality constructed affinity graph especially data noise subspace learning transform original input features low-dimensional subspace help produce robust method therefore learn intrinsic subspace construct pure affinity graph dataset noise challenge spectral clustering order deal challenge new robust single-step spectral clustering intrinsic subspace rs3cis method proposed paper rs3cis uses local representation method projects original data low-dimensional subspace row-sparse transformation matrix uses 2,1-norm transformation matrix penalty term achieve noise suppression addition rs3cis introduces laplacian matrix rank constraint output affinity graph explicit clustering structure makes final clustering result obtained single-step constructing affinity matrix one synthetic dataset six real benchmark datasets used verify performance proposed method performing clustering projection experiments experimental results show rs3cis outperforms related methods respect clustering quality robustness dimension reduction	negative
the adversarial attack and detection under the fisher information metric	many deep learning models vulnerable adversarial attack i.e. imperceptible intentionally-designed perturbations input cause incorrect output networks paper using information geometry provide reasonable explanation vulnerability deep learning models considering data space non-linear space fisher information metric induced neural network first propose adversarial attack algorithm termed one-step spectral attack ossa method described constrained quadratic form fisher information matrix optimal adversarial perturbation given first eigenvector vulnerability reflected eigenvalues larger eigenvalue vulnerable model attacked corresponding eigenvector taking advantage property also propose adversarial detection method eigenvalues serving characteristics attack detection algorithms numerically optimized work efficiently large datasets evaluations show superior performance compared methods implying fisher information promising approach investigate adversarial attacks defenses	negative
long short-term memory with dynamic skip connections	recent years long short-term memory lstm successfully used model sequential data variable length however lstm still experience difficulty capturing long-term dependencies work tried alleviate problem introducing dynamic skip connection learn directly connect two dependent words since dependency information training data propose novel reinforcement learning-based method model dependency relationship connect dependent words proposed model computes recurrent transition functions based skip connections provides dynamic skipping advantage rnns always tackle entire sentences sequentially experimental results three natural language processing tasks demonstrate proposed method achieve better performance existing methods number prediction experiment proposed model outperformed lstm respect accuracy nearly 20	negative
acm: adaptive cross-modal graph convolutional neural networks for rgb-d scene recognition	rgb image classification achieved significant performance improvement resurge deep convolutional neural networks however mono-modal deep models rgb image still several limitations applied rgb-d scene recognition 1 images scene classification usually contain one typical object flexible spatial distribution object-level local features also considered addition global scene representation 2 multi-modal features rgb-d scene classification still under-utilized simply combining modal-specific features suffers semantic gaps different modalities 3 existing methods neglect complex relationships among multiple modality features considering limitations paper proposes adaptive crossmodal acm feature learning framework based graph convolutional neural networks rgb-d scene recognition order make better use modal-specific cues approach mines intra-modality relationships among selected local features one modality leverage multi-modal knowledge effectively proposed approach models inter-modality relationships two modalities cross-modal graph cmg evaluate proposed method two public rgb-d scene classification datasets sun-rgbd nyud v2 proposed method achieves state-of-the-art performance	negative
adversarial binary collaborative filtering for implicit feedback	fast item recommendation based implicit feedback vital practical scenarios due data-abundance challenging lack negative samples large number recommended items recent adversarial methods unifying generative discriminative models promising since generative model negative sampler gradually improves iteration continues however binary-valued generative model still unexplored within min-max framework important accelerating item recommendation optimizing binary-valued models difficult due non-smooth nondifferentiable end propose two novel methods relax binarization based error function gumbel trick generative model optimized many popular solvers sgd admm binary-valued generative model evaluated within min-max framework four real-world datasets shown superiority competing hashing-based recommendation algorithms addition proposed framework approximate discrete variables precisely applied solve discrete optimization problems	negative
3d object detection using scale invariant and feature reweighting networks	3d object detection plays important role large number real-world applications requires us estimate localizations orientations 3d objects real scenes paper present new network architecture focuses utilizing front view images frustum point clouds generate 3d detection results one hand pointsift module utilized improve performance 3d segmentation capture information different orientations space robustness different scale shapes hand network obtains useful features suppresses features less information senet module module reweights channel features estimates 3d bounding boxes effectively method evaluated kitti dataset outdoor scenes sun-rgbd dataset indoor scenes experimental results illustrate method achieves better performance state-of-the-art methods especially point clouds highly sparse	positive
learning to embed sentences using attentive recursive trees	sentence embedding effective feature representation deep learning-based nlp tasks one prevailing line methods using recursive latent tree-structured networks embed sentences task-specific structures however existing models explicit mechanism emphasize taskinformative words tree structure end propose attentive recursive tree model ar-tree words dynamically located according importance task specifically construct latent tree sentence proposed important-first strategy place attentive words nearer root thus ar-tree inherently emphasize important words bottomup composition sentence embedding propose end-to-end reinforced training strategy ar-tree demonstrated consistently outperform least comparable state-of-the-art sentence embedding methods three sentence understanding tasks	negative
cgmh: constrained sentence generation by metropolis-hastings sampling	real-world applications natural language generation often constraints target sentences addition fluency naturalness requirements existing language generation techniques usually based recurrent neural networks rnns however non-trivial impose constraints rnns maintaining generation quality since rnns generate sentences sequentially beam search first word last paper propose cgmh novel approach using metropolis-hastings sampling constrained sentence generation cgmh allows complicated constraints occurrence multiple keywords target sentences handled traditional rnn-based approaches moreover cgmh works inference stage require parallel corpora training evaluate method variety tasks including keywords-to-sentence generation unsupervised sentence paraphrasing unsupervised sentence error correction cgmh achieves high performance compared previous supervised methods sentence generation code released https //github.com/ningmiao/cgmh	negative
contextualized non-local neural networks for sequence learning	recently large number neural mechanisms models proposed sequence learning selfattention exemplified transformer model graph neural networks gnns attracted much attention paper propose approach combines draws complementary strengths two methods specifically propose contextualized non-local neural networks cn3 dynamically construct task-specific structure sentence leverage rich local dependencies within particular neighbourhood	negative
calibrated stochastic gradient descent for convolutional neural networks	stochastic gradient descent sgd variants optimized gradient estimators may expensive compute true gradient many scenarios paper introduces calibrated stochastic gradient descent csgd algorithm deep neural network optimization theorem developed prove unbiased estimator network variables obtained probabilistic way based lipschitz hypothesis work significantly distinct existing gradient optimization methods providing theoretical framework unbiased variable estimation deep learning paradigm optimize model parameter calculation particular develop generic gradient calibration layer easily used build convolutional neural networks cnns experimental results demonstrate cnns csgd optimization scheme improve stateof-the-art performance natural image classification digit recognition imagenet object classification object detection tasks work opens new research directions developing efficient sgd updates analyzing backpropagation algorithm	negative
hierarchical attention network for image captioning	recently attention mechanism successfully applied image captioning existing attention methods established low-level spatial features high-level text features limits richness captions paper propose hierarchical attention network han enables attention calculated pyramidal hierarchy features synchronously pyramidal hierarchy consists features diverse semantic levels allows predicting different words according different features hand due different modalities features multivariate residual module mrm proposed learn joint representations features mrm able model projections extract relevant relations among different features furthermore introduce context gate balance contribution different features compared existing methods approach applies hierarchical features exploits several multimodal integration strategies significantly improve performance han verified benchmark mscoco dataset experimental results indicate model outperforms state-of-the-art methods achieving bleu1 score 80.9 cider score 121.7 karpathy ’ test split	negative
word embedding as maximum a posteriori estimation	glove word embedding model relies solving global optimization problem reformulated maximum likelihood estimation problem paper propose generalize approach word embedding considering parametrized variants glove model incorporating priors parameters demonstrate usefulness approach consider word embedding model context word associated corresponding variance intuitively encoding informative using framework learn variances together resulting word vectors unified way experimentally show resulting word embedding models outperform glove well many popular alternatives	positive
deepcf: a unified framework of representation learning and matching function learning in recommender system	general recommendation viewed matching problem i.e. match proper items proper users however due huge semantic gap users items ’ almost impossible directly match users items initial representation spaces solve problem many methods studied generally categorized two types i.e. representation learning-based cf methods matching function learning-based cf methods representation learning-based cf methods try map users items common representation space case higher similarity user item space implies match better matching function learning-based cf methods try directly learn complex matching function maps user-item pairs matching scores although methods well developed suffer two fundamental flaws i.e. limited expressiveness dot product weakness capturing low-rank relations respectively end propose general framework named deepcf short deep collaborative filtering combine strengths two types methods overcome flaws extensive experiments four publicly available datasets demonstrate effectiveness proposed deepcf framework	positive
dueling bandits with qualitative feedback	formulate study novel multi-armed bandit problem called qualitative dueling bandit qdb problem agent observes numeric qualitative feedback pulling arm employ regret dueling bandit db problem duel carried comparing qualitative feedback although naively use classic db algorithms solving qdb problem reduction significantly worsens performance—actually qdb problem probability one arm wins duel another arm directly estimated without carrying actual duels paper1 propose direct algorithms qdb problem theoretical analysis shows proposed algorithms significantly outperform db algorithms incorporating qualitative feedback experimental results also demonstrate vast improvement existing db algorithms	negative
human action transfer based on 3d model reconstruction	present practical effective method human action transfer given sequence source action limited target information aim transfer motion source target although recent works based gan vae achieved impressive results action transfer 2d still exists lot problems avoided distorted discontinuous human body shape blurry cloth texture paper try solve problems novel 3d viewpoint one hand design skeleton-to-3d-mesh generator generate 3d model achieves huge improvement appearance reconstruction furthermore add temporal connection improve smoothness model hand instead directly utilizing image rgb space transform target appearance information uv space pose transformation specially unlike conventional graphics render method directly projects visible pixels uv space transformation according pixel ’ semantic information perform experiments human3.6m humaneva-i evaluate performance pose generator qualitative quantitative results show method outperforms methods based generation method 2d additionally compare render method graphic methods human3.6m people-snapshot comparison results show render method robust effective	positive
frontier search and plan reconstruction in oversubscription planning	oversubscription planning osp problem choosing action sequence reaches state high utility given budget total action cost formulation allows us handle situations under-constrained resources allow us achieve possible goal facts optimal osp task constrained finding path achieves state maximal utility incremental bfbb search algorithm landmark-based approximations proposed osp heuristic search address tasks non-negative 0-binary utility functions incremental bfbb maintained best solution far set reference states extended non-redundant value-carrying states discovered search iteration requires search re-start order exploit new knowledge obtained along search recent work proposed approach relative estimation achievements value-driven landmarks address arbitrary utility functions incrementally improves best existing solution far eliminating need maintain set reference states propose progressive frontier search algorithm alleviates need re-start scratch new information acquired capturing frontier achieved end iteration used dynamic reference point continue search leading improved efficiency search	negative
semi-supervised feature selection with adaptive discriminant analysis	paper propose novel adaptive discriminant analysis semi-supervised feature selection namely sada instead computing fixed similarities performing feature selection sada simultaneously learns adaptive similarity matrix projection matrix w iterative method iteration computed projected distance learned w w computed learned s. therefore sada learn better projection matrix w weakening effect noise features adaptive similarity matrix experimental results 4 data sets show superiority sada compared 5 semisupervised feature selection methods	positive
towards gene function prediction via multi-networks representation learning	multi-networks integration methods achieved prominent performance many network-based tasks approaches often incur information loss problem paper propose novel multi-networks representation learning method based semi-supervised autoencoder termed deepmne captures complex topological structures network takes correlation among multinetworks account experimental results two realworld datasets indicate deepmne outperforms existing state-of-the-art algorithms	negative
scale invariant fully convolutional network: detecting hands efficiently	existing hand detection methods usually follow pipeline multiple stages high computation cost i.e. feature extraction region proposal bounding box regression additional layers rotated region detection paper propose new scale invariant fully convolutional network sifcn trained end-to-end fashion detect hands efficiently specifically merge feature maps high low layers iterative way handles different scales hands better less time overhead comparing concatenating simply moreover develop complementary weighted fusion cwf block make full use distinctive features among multiple layers achieve scale invariance deal rotated hand detection present rotation map get rid complex rotation derotation layers besides design multi-scale loss scheme accelerate training process significantly adding supervision intermediate layers network compared state-of-the-art methods algorithm shows comparable accuracy runs 4.23 times faster speed viva dataset achieves better average precision oxford hand detection dataset speed 62.5 fps	negative
weighted channel dropout for regularization of deep convolutional neural network	work propose novel method named weighted channel dropout wcd regularization deep convolutional neural network cnn different dropout randomly selects neurons set zero fully-connected layers wcd operates channels stack convolutional layers specifically wcd consists two steps i.e. rating channels selecting channels three modules i.e. global average pooling weighted random selection random number generator filters channels according activation status plugged two consecutive layers unifies original dropout channel-wise dropout wcd totally parameter-free deployed training phase slight computation cost network test phase remains unchanged thus inference cost added besides combining existing networks requires re-pretraining imagenet thus well-suited application small datasets finally wcd vggnet-16 resnet-101 inception-v3 experimentally evaluated multiple datasets extensive results demonstrate wcd bring consistent improvements baselines	negative
explicit interaction model towards text classification	text classification one fundamental tasks natural language processing recently deep neural networks achieved promising performance text classification task compared shallow models despite significance deep models ignore fine-grained matching signals words classes classification clues since classifications mainly rely text-level representations address problem introduce interaction mechanism incorporate word-level matching signals text classification task particular design novel framework explicit interaction model dubbed exam equipped interaction mechanism justified proposed approach several benchmark datasets including multilabel multi-class text classification tasks extensive experimental results demonstrate superiority proposed method byproduct released codes parameter settings facilitate researches	negative
capnet: continuous approximation projection for 3d point cloud reconstruction using 2d supervision	knowledge 3d properties objects necessity order build effective computer vision systems however lack large scale 3d datasets major constraint datadriven approaches learning properties consider task single image 3d point cloud reconstruction aim utilize multiple foreground masks supervisory data alleviate need large scale 3d datasets novel differentiable projection module called ‘ capnet ’ introduced obtain 2d masks predicted 3d point cloud key idea model projections continuous approximation points point cloud overcome challenges sparse projection maps propose loss formulation termed ‘ affinity loss ’ generate outlierfree reconstructions significantly outperform existing projection based approaches large-scale synthetic dataset show utility generalizability 2d supervised approach experiments real-world dataset lack 3d data serious concern enhance reconstructions also propose test stage optimization procedure obtain reconstructions display high correspondence observed input image	negative
x-dmm: fast and scalable model based text clustering	text clustering widely studied problem text mining domain dirichlet multinomial mixture dmm model based clustering algorithms shown good performance cope high dimensional sparse text data obtaining reasonable results clustering accuracy computational efficiency however time complexity dmm model training proportional average document length number clusters making inefficient scaling long text large corpora common realworld applications documents organization retrieval recommendation paper leverage symmetric prior setting dirichlet distribution build indices decrease time complexity sampling-based training dmm k∗l k∗u k number clusters l average length document u average number unique words document introduce metropolis-hastings sampling algorithm reduces sampling time complexity k∗u u nearly-to-convergence training stages moreover also parallelize dmm model training obtain acceleration using uncollapsed gibbs sampler combine optimizations highly efficient implementation called x-dmm enables dmm model scale long large-scale text clustering evaluate performance x-dmm several real world datasets experimental results show xdmm achieves substantial speed compared existing state-of-the-art algorithms without clustering accuracy degradation	negative
sparse adversarial perturbations for videos	although adversarial samples deep neural networks dnns intensively studied static images extensions videos never explored compared images attacking video needs consider spatial cues also temporal cues moreover improve imperceptibility well reduce computation cost perturbations added frames possible i.e. adversarial perturbations temporally sparse motivates propagation perturbations denotes perturbations added current frame transfer next frames via temporal interactions thus extra perturbations needed frames misclassify end propose first white-box video attack method utilizes l2,1-norm based optimization algorithm compute sparse adversarial perturbations videos choose action recognition targeted task networks cnn+rnn architecture threat models verify method thanks propagation compute perturbations shortened version video adapt long version video fool dnns experimental results ucf101 dataset demonstrate even one frame video perturbed fooling rate still reach 59.7	negative
cleaning noisy and heterogeneous metadata for record linking across scholarly big datasets	automatically extracted metadata scholarly documents pdf formats usually noisy heterogeneous often containing incomplete fields erroneous values one common way cleaning metadata use bibliographic reference dataset challenge match records corpora high precision existing solution based information retrieval string similarity titles works well titles cleaned introduce system designed match scholarly document entities noisy metadata reference dataset blocking function uses classic bm25 algorithm find matching candidates reference data indexed elasticsearch core components use supervised methods combine features extracted available metadata fields system also leverages available citation information match entities combination metadata citation achieves high accuracy significantly outperforms baseline method test dataset apply system match database citeseerx web science pubmed dblp method deployed citeseerx system clean metadata link records scholarly big datasets	negative
unsupervised domain adaptation by matching distributions based on the maximum mean discrepancy via unilateral transformations	propose simple yet effective method unsupervised domain adaptation training test distributions different standard supervised learning methods perform poorly semi-supervised domain adaptation methods developed case labeled data target domain available however target data often unlabeled practice therefore unsupervised domain adaptation require labels target data receiving lot attention proposed method minimizes discrepancy source target distributions input features transforming feature space source domain since unilateral transformations transfer knowledge source domain target one without reducing dimensionality proposed method effectively perform domain adaptation without losing information transfered proposed method assumed transformed features original features differ small residual preserve relationship features labels transformation learned aligning higher-order moments source target feature distributions based maximum mean discrepancy enables compare two distributions without density estimation transformation found learn supervised models using transformed source data labels use two real-world datasets demonstrate experimentally proposed method achieves better classification performance existing methods unsupervised domain adaptation	positive
transferable attention for domain adaptation	recent work domain adaptation bridges different domains adversarially learning domain-invariant representation distinguished domain discriminator existing methods adversarial domain adaptation mainly align global images across source target domains however obvious regions image transferable forcefully aligning untransferable regions may lead negative transfer furthermore images significantly dissimilar across domains resulting weak image-level transferability end present transferable attention domain adaptation tada focusing adaptation model transferable regions images implement two types complementary transferable attention transferable local attention generated multiple region-level domain discriminators highlight transferable regions transferable global attention generated single image-level domain discriminator highlight transferable images extensive experiments validate proposed models exceed state art results standard domain adaptation datasets	negative
automated rule base completion as bayesian concept induction	considerable attention recently devoted problem automatically extending knowledge bases applying form inductive reasoning vast majority existing work centred around so-called knowledge graphs paper consider setting input consists set existential rules end exploit vector space representation considered concepts partly induced rule base partly pre-trained word embedding inspired recent approaches concept induction model rule templates vector space embedding using gaussian distributions unlike many existing approaches learn rules directly exploiting regularities given rule base require database concept relation instances given result method applied wide variety ontologies present experimental results demonstrate effectiveness method	positive
robustness can be cheap: a highly efficient approach to discover outliers under high outlier ratios	efficient detection outliers massive data high outlier ratio challenging explicitly discussed yet case existing methods either suffer poor robustness require expensive computations paper proposes low-rank based efficient outlier detection leod framework achieve favorable robustness high outlier ratios much cheaper computations specifically worth highlighting following aspects leod 1 framework exploits low-rank structure embedded similarity matrix considers inliers/outliers equally based low-rank structure facilitates us encourage satisfying robustness low computational cost later 2 novel re-weighting algorithm derived new general solution constrained eigenvalue problem major bottleneck optimization process instead high space time complexity 2n 2 /o 2n 3 required classic solution algorithm enjoys n space complexity faster optimization speed experiments 3 new alternative formulation proposed acceleration solution process cheap closed-form solution obtained experiments show leod achieves strong robustness outlier ratio 20 60 100 times memory efficient 1000 times faster previous counterpart attains comparable performance codes leod publicly available https //github.com/demonzyj56/leod	negative
avs-net: automatic visual surveillance using relation network	visual surveillance closed circuit television cctv help prevent crime paper propose automatic visual surveillance network avs-net simultaneously performs image processing object detection determine dangers situations captured cctv addition add relation module infer relationships objects images experimental results show relation module greatly improves classification accuracy even enough information	negative
fully convolutional video captioning with coarse-to-fine and inherited attention	automatically generating natural language description video extremely complicated challenging task tackle obstacles traditional lstm-based model video captioning propose novel architecture generate optimal descriptions videos focuses constructing new network structure generate sentences superior basic model lstm establishing special attention mechanisms provide useful visual information caption generation scheme discards traditional lstm exploits fully convolutional network coarse-to-fine inherited attention designed according characteristics fully convolutional structure model outperform basic lstm-based model also achieve comparable performance state-of-the-art methods	negative
cisi-net: explicit latent content inference and imitated style rendering for image inpainting	convolutional neural networks cnns presented potential filling large missing areas plausible contents address blurriness issue commonly existing cnn-based inpainting typical approach conduct texture refinement initially completed images replacing neural patch predicted region using closest one known region however processing might introduce undesired content change predicted region especially desired content exist known region avoid generating incorrect content paper propose content inference style imitation network cisi-net explicitly separate image data content code style code content inference realized performing inference latent space infer content code corrupted images similar one original images produce detailed content similar inference procedure pixel domain due dimensional distribution content lower entire image hand style code used represent rendering content consistent entire image style code integrated inferred content code generate complete image experiments multiple datasets including structural natural images demonstrate proposed approach out-performs existing ones terms content accuracy well texture details	positive
monogrnet: a geometric reasoning network for monocular 3d object localization	localizing objects real 3d space plays crucial role scene understanding particularly challenging given single rgb image due geometric information loss imagery projection propose monogrnet amodal 3d object localization monocular rgb image via geometric reasoning observed 2d projection unobserved depth dimension monogrnet single unified network composed four task-specific subnetworks responsible 2d object detection instance depth estimation ide 3d localization local corner regression unlike pixel-level depth estimation needs per-pixel annotations propose novel ide method directly predicts depth targeting 3d bounding box ’ center using sparse supervision 3d localization achieved estimating position horizontal vertical dimensions finally monogrnet jointly learned optimizing locations poses 3d bounding boxes global context demonstrate monogrnet achieves state-of-the-art performance challenging datasets	negative
a generic approach to accelerating belief propagation based incomplete algorithms for dcops via a branch-and-bound technique	belief propagation approaches max-sum variants important methods solve large-scale distributed constraint optimization problems dcops however problems n-ary constraints algorithms face huge challenge since computational complexity scales exponentially number variables function holds paper present generic easy-touse method based branch-and-bound technique solve issue called function decomposing state pruning fdsp theoretically prove fdsp provide monotonically non-increasing upper bounds speed belief propagation based incomplete dcop algorithms without effect solution quality also empirically evaluation indicates fdsp reduce 97 search space least effectively accelerate max-sum compared state-of-the-art	positive
hyperadam: a learnable task-adaptive adam for network training	deep neural networks traditionally trained using humandesigned stochastic optimization algorithms sgd adam recently approach learning optimize network parameters emerged promising research topic however learned black-box optimizers sometimes fully utilize experience human-designed optimizers therefore limitation generalization ability paper new optimizer dubbed hyperadam proposed combines idea “ learning optimize ” traditional adam optimizer given network training parameter update iteration generated hyperadam adaptive combination multiple updates generated adam varying decay rates combination weights decay rates hyperadam adaptively learned depending task hyperadam modeled recurrent neural network adamcell weightcell statecell justified state-of-the-art various network training multilayer perceptron cnn lstm	negative
fast pmi-based word embedding with efficient use of unobserved patterns	continuous word representations capture semantic information corpus building blocks many natural language processing tasks pre-trained word embeddings used sentiment analysis text classification question answering paper propose new word embedding algorithm works smoothed positive pointwise mutual information ppmi matrix obtained word-word co-occurrence counts one major contributions propose objective function optimization framework exploits full capacity “ negative examples ” unobserved insignificant wordword co-occurrences order push unrelated words away improves distribution words latent space also propose kernel similarity measure latent space effectively calculate similarities high dimensions moreover propose approximate alternative algorithm using modified vantage point tree reduce computational complexity algorithm |v |log|v respect number words vocabulary trained various word embedding algorithms articles wikipedia 2.1 billion tokens show method outperforms state-of-the-art word similarity tasks good margin	positive
sax breakpoints for random forest based real-time contrast control chart	manufacturing process process monitoring important real-time contrast rtc control chart outperforms existing monitoring methods however performance rtc control chart depends classifier existing rtc charts use random forest rf support vector machine svm kernel linear discriminant analysis klda classifier rf classifier find cause faults performance lower others therefore suggest data representation method improve rf based rtc control chart symbolic aggregate approximation sax famous method improve performance classification clustering convert input data using sax change parameters sax alphabet size breakpoints improve performance experiment shows represented data efficient method improve performance rtc control chart	negative
heterogeneous transfer learning via deep matrix completion with adversarial kernel embedding	heterogeneous transfer learning htl aims solve transfer learning problems source domain target domain heterogeneous types features existing htl approaches either explicitly learn feature mappings heterogeneous domains implicitly reconstruct heterogeneous cross-domain features based matrix completion techniques paper propose new htl method based deep matrix completion framework kernel embedding distributions trained adversarial manner learning heterogeneous features across domains conduct extensive experiments two different vision tasks demonstrate effectiveness proposed method compared number baseline methods	negative
singe image rain removal with unpaired information: a differentiable programming perspective	single image rain-streak removal extremely challenging problem due presence non-uniform rain densities images previous works solve problem using various hand-designed priors explicitly mapping synthetic rain paired clean image supervised way practice however pre-defined priors easily violated paired training data hard collect overcome limitations work propose rainremoval-gan rrgan first end-to-end adversarial model generates realistic rain-free images using unpaired supervision approach alleviates paired training constraints introducing physical-model explicitly learns recovered images corresponding rain-streaks differentiable programming perspective proposed network consists novel multiscale attention memory generator novel multiscale deeply supervised discriminator multiscale attention memory generator uses memory attention mechanism capture latent rain streaks context different stages recover clean images deeply supervised multiscale discriminator imposes constraints recovered output terms local details global appearance clean image set together learned rainstreaks reconstruction constraint employed ensure appearance consistent input image experimental results public benchmark demonstrates promising performance compared nine state-of-the-art methods terms psnr ssim visual qualities running time	negative
adversarial training for community question answer selection based on multi-scale matching	community-based question answering cqa websites represent important source information result problem matching valuable answers corresponding questions become increasingly popular research topic frame task binary relevant/irrelevant classification problem present adversarial training framework alleviate label imbalance issue employ generative model iteratively sample subset challenging negative samples fool classification model models alternatively optimized using reinforce algorithm proposed method completely different previous ones negative samples training set directly used uniformly down-sampled propose using multi-scale matching explicitly inspects correlation words ngrams different levels granularity evaluate proposed method semeval 2016 semeval 2017 datasets achieves state-of-the-art similar performance	positive
single-label multi-class image classification by deep logistic regression	objective learning formulation essential success convolutional neural networks work analyse thoroughly standard learning objective functions multiclass classification cnns softmax regression sr singlelabel scenario logistic regression lr multi-label scenario analyses lead inspiration exploiting lr single-label classification learning disclosing negative class distraction problem lr address problem develop two novel lr based objective functions generalise conventional lr importantly turn competitive alternatives sr single label classification extensive comparative evaluations demonstrate model learning advantages proposed lr functions commonly adopted sr single-label coarse-grained object categorisation cross-class fine-grained person instance identification tasks also show performance superiority method clothing attribute classification comparison vanilla lr function code made publicly available	negative
improving hypernymy prediction via taxonomy enhanced adversarial learning	hypernymy basic semantic relation computational linguistics expresses “ is-a ” relation generic concept specific instances serving backbone taxonomies ontologies although several nlp tasks related hypernymy prediction extensively addressed methods fully exploited large number hypernymy relations web-scale taxonomies	negative
learning non-uniform hypergraph for multi-object tracking	majority multi-object tracking mot algorithms based tracking-by-detection scheme use higher order dependencies among objects tracklets makes less effective handling complex scenarios work present new near-online mot algorithm based non-uniform hypergraph model different degrees dependencies among tracklets unified objective nodes hypergraph correspond tracklets hyperedges different degrees encode various kinds dependencies among specifically instead setting weights hyperedges different degrees empirically learned automatically using structural support vector machine algorithm ssvm several experiments carried various challenging datasets i.e. pets09 parkinglot sequence subwayface mot16 benchmark demonstrate method achieves favorable performance state-of-the-art mot methods	negative
spell once, summon anywhere: a two-level open-vocabulary language model	show spellings known words help us deal unknown words open-vocabulary nlp tasks method propose used extend closedvocabulary generative model paper specifically consider case neural language modeling bayesian generative story combines standard rnn language model generating word tokens sentence rnnbased spelling model generating letters word type two rnns respectively capture sentence structure word structure kept separate linguistics invoking second rnn generate spellings novel words context obtain open-vocabulary language model known words embeddings naturally inferred combining evidence type spelling token context comparing baselines including novel strong baseline beat previous work establish state-of-the-art results multiple datasets	negative
template-based math word problem solvers with recursive neural networks	design automatic solvers arithmetic math word problems attracted considerable attention recent years large number datasets methods published among math23k largest data corpus helpful evaluate generality robustness proposed solution best performer math23k seq2seq model based lstm generate math expression however model suffers performance degradation large space target expressions paper propose template-based solution based recursive neural network math expression construction specifically first apply seq2seq model predict tree-structure template inferred numbers leaf nodes unknown operators inner nodes design recursive neural network encode quantity bi-lstm self attention infer unknown operator nodes bottom-up manner experimental results clearly establish superiority new framework improve accuracy wide margin two largest datasets i.e. 58.1 66.9 math23k 62.8 66.8 mawps	negative
learning a visual tracker from a single movie without annotation	recent success deep network visual trackers learning largely relies human labeled data however expensive annotate recently unsupervised methods proposed explore learning visual trackers without labeled data performance lags far behind supervised methods identify main bottleneck methods inconsistent objectives off-line training online tracking stages address problem propose novel unsupervised learning pipeline based discriminative correlation filter network method iteratively updates tracker alternating target localization network optimization particular propose learn network single movie could easily obtained collecting thousands video clips millions images extensive experiments demonstrate approach insensitive employed movies trained visual tracker achieves leading performance among existing unsupervised learning approaches even compared network trained human labeled bounding boxes tracker achieves similar results many tracking benchmarks code available https //github.com/zjjconan/ul-tracker-aaai2019	negative
a feasibility test on preventing prmds based on deep learning	study proposes method reduce playing-related musculoskeletal disorders prmds often occur among pianists specifically propose feasibility test evaluates several state-of-the-art deep learning algorithms prevent injuries pianist propose 1 c3p dataset including various piano playing postures show 2 application four learning algorithms demonstrated superiority video classification proposed c3p datasets knowledge first study attempted apply deep learning paradigm reduce prmds pianist experimental results demonstrated classification accuracy 80 average indicating proposed hypothesis effectiveness deep learning algorithms prevent injuries pianist true	negative
semantic adversarial network with multi-scale pyramid attention for video classification	two-stream architecture shown strong performance video classification task key idea learn spatiotemporal features fusing convolutional networks spatially temporally however problems within architecture first relies optical flow model temporal information often expensive compute store second limited ability capture details local context information video data third lacks explicit semantic guidance greatly decrease classification performance paper proposed new two-stream based deep framework video classification discover spatial temporal information rgb frames moreover multi-scale pyramid attention mpa layer semantic adversarial learning sal module introduced integrated framework mpa enables network capturing global local feature generate comprehensive representation video sal make representation gradually approximate real video semantics adversarial manner experimental results two public benchmarks demonstrate proposed methods achieves state-of-the-art results standard video datasets	negative
grn: gated relation network to enhance convolutional neural network for named entity recognition	dominant approaches named entity recognitionm ner mostly adopt complex recurrent neural networks rnn e.g. long-short-term-memory lstm however rnns limited recurrent nature terms computational efficiency contrast convolutional neural networks cnn fully exploit gpu parallelism feedforward architectures however little attention paid performing ner cnns mainly owing difficulties capturing long-term context information sequence paper propose simple effective cnn-based network ner i.e. gated relation network grn capable common cnns capturing long-term context specifically grn firstly employ cnns explore local context features word model relations words use gates fuse local context features global ones predicting labels without using recurrent layers process sentence sequential manner grn allows computations performed parallel across entire sentence experiments two benchmark ner datasets i.e. conll2003 ontonotes 5.0 show proposed grn achieve state-of-the-art performance without external knowledge also enjoys lower time costs train test	negative
point cloud processing via recurrent set encoding	present new permutation-invariant network 3d point cloud processing network composed recurrent set encoder convolutional feature aggregator given unordered point set encoder firstly partitions ambient space parallel beams points within beam modeled sequence encoded subregional geometric features shared recurrent neural network rnn spatial layout beams regular allows beam features fed efficient 2d convolutional neural network cnn hierarchical feature aggregation network effective spatial feature learning competes favorably state-of-the-arts sotas number benchmarks meanwhile significantly efficient compared sotas	positive
matrix completion for graph-based deep semi-supervised learning	convolutional neural networks cnns provided promising achievements image classification problems however training cnn model relies large number labeled data considering vast amount unlabeled data available web important make use data conjunction small set labeled data train deep learning model paper introduce new iterative graph-based semi-supervised learning gssl method train cnn-based classifier using large amount unlabeled data small amount labeled data method first construct similarity graph nodes represent cnn features corresponding data points labeled unlabeled edges tend connect data points class label graph missing label unsupervised nodes predicted using matrix completion method based rank minimization criterion next step use constructed graph calculate triplet regularization loss added supervised loss obtained initially labeled data update cnn network parameters	positive
multiple saliency and channel sensitivity network for aggregated convolutional feature	paper aiming two key problems instance-level image retrieval i.e. distinctiveness image representation generalization ability model propose novel deep architecture multiple saliency channel sensitivity network mscnet specifically obtain distinctive global descriptors attention-based multiple saliency learning first presented highlight important details image simple effective channel sensitivity module based gram matrix designed boost channel discrimination suppress redundant information additionally contrast existing feature aggregation methods employing pre-trained deep networks mscnet trained two modes first one unsupervised manner instance loss another supervised manner combines classification ranking loss relies limited training data experimental results several public benchmark datasets i.e. oxford buildings paris buildings holidays indicate proposed mscnet outperforms state-of-the-art unsupervised supervised methods	positive
rsa: byzantine-robust stochastic aggregation methods for distributed learning from heterogeneous datasets	paper propose class robust stochastic subgradient methods distributed learning heterogeneous datasets presence unknown number byzantine workers byzantine workers learning process may send arbitrary incorrect messages master due data corruptions communication failures malicious attacks consequently bias learned model key proposed methods regularization term incorporated objective function robustify learning task mitigate negative effects byzantine attacks resultant subgradient-based algorithms termed byzantine-robust stochastic aggregation methods justifying acronym rsa used henceforth contrast existing algorithms rsa rely assumption data independent identically distributed i.i.d workers hence fits wider class applications theoretically show rsa converges near-optimal solution learning error dependent number byzantine workers ii convergence rate rsa byzantine attacks stochastic gradient descent method free byzantine attacks numerically experiments real dataset corroborate competitive performance rsa complexity reduction compared state-of-the-art alternatives	positive
learning a deep convolutional network for colorization in monochrome-color dual-lens system	monochrome-color dual-lens system gray image captured monochrome camera better quality color image color camera color information get high-quality color images desired colorize gray image color image reference related works usually use hand-crafted methods search best-matching pixel reference image pixel input gray image copy color best-matching pixel result propose novel deep convolution network solve colorization problem end-to-end way based observation pixel input image usually exist multiple pixels reference image correct colors method performs weighted average colors candidate pixels reference image utilize candidate pixels correct colors weight values pixels input image reference image obtained learning weight volume using deep feature representations attention operation proposed focus useful candidate pixels 3-d regulation performed learn context information addition correct wrongly colorized pixels occlusion regions propose color residue joint learning module correct colorization result input gray image guidance evaluate method scene flow cityscapes middlebury sintel datasets experimental results show method largely outperforms state-of-the-art methods	negative
ewgan: entropy-based wasserstein gan for imbalanced learning	paper propose novel oversampling strategy dubbed entropy-based wasserstein generative adversarial network ewgan generate data samples minority classes imbalanced learning first construct entropyweighted label vector class characterize data imbalance different classes concatenate entropyweighted label vector original feature vector data sample feed wgan model train generator generator trained concatenate entropy-weighted label vector random noise feature vectors feed generator generate data samples minority classes experimental results two benchmark datasets show samples generated proposed oversampling strategy help improve classification performance data highly imbalanced furthermore proposed strategy outperforms state-of-the-art oversampling algorithms terms classification accuracy	positive
deep embedding features for salient object detection	benefiting rapid development convolutional neural networks cnns salient object detection methods achieved remarkable results utilizing multi-level convolutional features however saliency training datasets limited scale due high cost pixel-level labeling leads limited generalization trained model new scenarios testing besides fcn-based methods directly integrate multi-level features ignoring fact noise features harmful saliency detection paper propose novel approach transforms prior information embedding space select attentive features filter outliers salient object detection network firstly generates coarse prediction map encorder-decorder structure feature embedding network fen trained embed pixel coarse map metric space incorporates much attentive features highlight salient regions suppress response non-salient regions embedded features refined deep-to-shallow recursive feature integration network rfin improve details prediction maps moreover alleviate blurred boundaries propose guided filter refinement network gfrn jointly optimize predicted results learnable guidance maps extensive experiments five benchmark datasets demonstrate method outperforms state-of-the-art results proposed method end-to-end achieves realtime speed 38 fps	positive
dynamic capsule attention for visual question answering	visual question answering vqa recent advances well advocated use attention mechanism precisely link question potential answer areas difficulty question increases vqa models adopt multiple attention layers capture deeper visual-linguistic correlation negative consequence explosion parameters makes model vulnerable over-fitting especially limited training examples given paper propose extremely compact alternative static multi-layer architecture towards accurate yet efficient attention modeling termed dynamic capsule attention capsatt inspired recent work capsule network capsatt treats visual features capsules obtains attention output via dynamic routing updates attention weights calculating coupling coefficients underlying output capsules meanwhile capsatt also discards redundant projection matrices make model much compact quantify capsatt three benchmark vqa datasets i.e. coco-qa vqa1.0 vqa2.0 compared traditional multi-layer attention model capsatt achieves significant improvements 4.1 5.2 2.2 three datasets respectively moreover much fewer parameters approach also yields competitive results compared latest vqa models verify generalization ability capsatt also deploy another challenging multi-modal task image captioning state-of-the-art performance achieved simple network structure	negative
the kelly growth optimal portfolio with ensemble learning	competitive alternative markowitz mean-variance portfolio kelly growth optimal portfolio drawn sufficient attention investment science growth optimal portfolio theoretically guaranteed dominate portfolio probability 1 long run practically tends highly risky short term moreover empirical analysis performance enhancement studies practical settings surprisingly short particular handle challenging realistic condition insufficient training data barely investigated order fill voids especially grappling difficulty small samples paper propose growth optimal portfolio strategy equipped ensemble learning synergically leverage bootstrap aggregating algorithm random subspace method portfolio construction mitigate estimation error analyze behavior hyperparameter selection proposed strategy simulation corroborate effectiveness comparing out-of-sample performance 10 competing strategies four datasets experimental results lucidly confirm new strategy superiority extensive evaluation criteria	positive
towards optimal fine grained retrieval via decorrelated centralized loss with normalize-scale layer	recent advances fine-grained image retrieval prefer learning convolutional neural network cnn specific fullyconnect layer designed loss function discriminative feature representation essentially loss establish robust metric efficiently distinguish high-dimensional features within outside fine-grained categories end existing loss functions defected two aspects feature relationship encoded inside training batch local scope leads low accuracy b error established mean square needs pairwise distance computation training set results low efficiency paper propose novel metric learning scheme termed normalize-scale layer decorrelated global centralized ranking loss achieves extremely efficient discriminative learning i.e. 5× speedup triplet loss 12 recall boost cars196 method originates classic softmax loss global structure directly optimize distance metric well inter/intra class distance tackle issue hypersphere layer global centralized ranking loss pairwise decorrelated learning particular first propose normalize-scale layer eliminate gap metric distance measuring distance retrieval dot product dimension reduction classification second relationship features encoded global centralized ranking loss targets optimizing metric distance globally accelerating learning procedure finally centers decorrelated gram-schmidt process leading extreme efficiency 20 epochs training procedure discriminability feature learning conducted quantitative evaluations two fine-grained retrieval benchmark superior performance demonstrates merits proposed approach state-of-the-arts	negative
dopamine: double-sided masked cnn for pixel adaptive multiplicative noise despeckling	propose dopamine new neural network based multiplicative noise despeckling algorithm algorithm inspired neural aide n-aide recently proposed neural adaptive image denoiser original naide designed additive noise case show framework i.e. adaptively learning network pixel-wise affine denoisers minimizing unbiased estimate mse applied multiplicative noise case well moreover derive double-sided masked cnn architecture control variance activation values layer converge fast high denoising performance supervised training experimental results show dopamine possesses high adaptivity via fine-tuning network parameters based given noisy image achieves significantly better despeckling results compared sar-drn state-of-the-art cnn-based algorithm	positive
learning adaptive random features	random fourier features powerful framework approximate shift invariant kernels monte carlo integration drawn considerable interest scaling kernel-based learning dimensionality reduction information retrieval literature many sampling schemes proposed improve approximation performance however interesting theoretic algorithmic challenge still remains i.e. optimize design random fourier features achieve good kernel approximation input data using low spectral sampling rate paper propose compute adaptive random fourier features optimized spectral samples wj ’ feature weights pj ’ learning scheme significantly reduces spectral sampling rate needed accurate kernel approximation also allows joint optimization supervised learning framework establish generalization bounds using rademacher complexity demonstrate advantages previous methods moreover experiments show empirical kernel approximation provides effective regularization supervised learning	negative
human-in-the-loop feature selection	feature selection crucial step conception machine learning models often performed via datadriven approaches overlook possibility tapping human decision-making model ’ designers users present human-in-the-loop framework interacts domain experts collecting feedback regarding variables samples evaluate relevant task hand information modeled via reinforcement learning derive per-example feature selection method tries minimize model ’ loss function focusing pertinent variables human perspective report results proof-of-concept image classification dataset real-world risk classification task model successfully incorporated feedback experts improve accuracy	negative
graph cnns with motif and variable temporal block for skeleton-based action recognition	hierarchical structure different semantic roles joints human skeleton convey important information action recognition conventional graph convolution methods modeling skeleton structure consider physically connected neighbors joint joints type thus failing capture highorder information work propose novel model motif-based graph convolution encode hierarchical spatial structure variable temporal dense block exploit local temporal information different ranges human skeleton sequences moreover employ non-local block capture global dependencies temporal domain attention mechanism model achieves improvements stateof-the-art methods two large-scale datasets	negative
efficient counterfactual learning from bandit feedback	statistically efficient way off-policy optimization batch data bandit feedback log data generated contextual bandit algorithms consider offline estimators expected reward counterfactual policy estimators shown lowest variance wide class estimators achieving variance reduction relative standard estimators apply estimators improve advertisement design major advertisement company consistent theoretical result estimators allow us improve existing bandit algorithm statistical confidence compared state-of-theart benchmark	negative
spatial and temporal mutual promotion for video-based person re-identification	video-based person re-identification crucial task matching video sequences person across multiple camera views generally features directly extracted single frame suffer occlusion blur illumination posture changes leads false activation missing activation regions corrupts appearance motion representation explore abundant spatial-temporal information video sequences key solve problem end propose refining recurrent unit rru recovers missing parts suppresses noisy parts current frame ’ features referring historical frames rru quality frame ’ appearance representation improved use spatial-temporal clues integration module stim mine spatial-temporal information upgraded features meanwhile multilevel training objective used enhance capability rru stim cooperation modules spatial temporal features mutually promote final spatial-temporal feature representation discriminative robust extensive experiments conducted three challenging datasets i.e. ilids-vid prid-2011 mars experimental results demonstrate approach outperforms existing state-of-the-art methods video-based person re-identification ilids-vid mars achieves favorable results prid-2011	negative
camo: a collaborative ranking method for content based recommendation	real-world recommendation tasks feedback data usually sparse therefore recommender ’ performance often determined much information extract textual contents however current methods make full use semantic information encode textual contents either “ bag-of-words ” technique recurrent neural network rnn former neglects order words latter ignores fact textual contents contain multiple topics besides exists dilemma designing recommender one hand shall use sophisticated model exploit every drop information item contents hand shall adopt simple model prevent over-fitting facing sparse feedbacks fill gaps propose recommender named camo 1. camo employs multi-layer content encoder simultaneously capturing semantic information multitopic word order moreover camo makes use adversarial training prevent complex encoder overfitting extensive empirical studies show camo outperforms state-of-the-art methods predicting users ’ preferences	negative
ea reader: enhance attentive reader for cloze-style question answering via multi-space context fusion	query-document semantic interactions essential success many cloze-style question answering models recently researchers proposed several attention-based methods predict answer focusing appropriate subparts context document paper design novel module produce query-aware context vector named multi-space based context fusion mscf following considerations 1 interactions applied across multiple latent semantic spaces 2 attention measured bit level token level moreover extend mscf multi-hop architecture unified model called enhanced attentive reader ea reader iterative inference process reader equipped novel memory update rule maintains understanding documents read update write operations conduct extensive experiments four real-world datasets results demonstrate ea reader outperforms state-of-the-art models	negative
multi-dimensional classification via knn feature augmentation	multi-dimensional classification mdc deals problem one instance associated multiple class variables specifies class membership w.r.t one specific class space existing approaches learn mdc examples focusing modeling dependencies among class variables potential usefulness manipulating feature space ’ investigated paper first attempt towards feature manipulation mdc proposed enriches original feature space knnaugmented features specifically simple counting statistics class membership neighboring mdc examples used generate augmented feature vector way discriminative information class space encoded feature space help train multi-dimensional classification model validate effectiveness proposed feature augmentation techniques extensive experiments eleven benchmark data sets well four state-of-the-art mdc approaches conducted experimental results clearly show compared original feature space classification performance existing mdc approaches significantly improved incorporating knn-augmented features	negative
optimization of hierarchical regression model with application to optimizing multi-response regression k-ary trees	fast convenient well-known way toward regression induce prune binary tree however little attempt toward improving performance induced regression tree paper presents meta-algorithm capable minimizing regression loss function thus improving accuracy given hierarchical model k-ary regression trees proposed method minimizes loss function node one one split nodes leads solving instance-based cost-sensitive classification problem node ’ data points leaf nodes method leads simple regression problem case binary univariate multivariate regression trees computational complexity training linear samples hence method scalable large trees datasets also briefly explore possibilities applying proposed method classification tasks show algorithm significantly better test error compared state-ofthe- art tree algorithms end accuracy memory usage query time method compared recently introduced forest models depict time proposed method able achieve better similar accuracy tangibly faster query time smaller number nonzero weights	negative
neural collective graphical models for estimating spatio-temporal population flow from aggregated data	propose probabilistic model estimating population flow defined populations transition areas time given aggregated spatio-temporal population data since information individual trajectories aggregated data straightforward estimate population flow proposed method utilize collective graphical model learn individual transition models aggregated data analytically marginalizing individual locations learning spatio-temporal collective graphical model aggregated data ill-posed problem since number parameters estimated exceeds number observations proposed method reduces effective number parameters modeling transition probabilities neural network takes locations origin destination areas time day inputs modeling automatically learn nonlinear spatio-temporal relationships flexibly among transitions locations times four real-world population data sets japan china demonstrate proposed method estimate transition population accurately existing methods	positive
jointly extracting multiple triplets with multilayer translation constraints	triplets extraction essential pivotal step automatic knowledge base construction captures structural information unstructured text corpus conventional extraction models use pipeline named entity recognition relation classification extract entities relations respectively ignore connection two tasks recently several neural network-based models proposed tackle problem achieved state-of-the-art performance however unable extract multiple triplets single sentence yet commonly seen real-life scenarios close gap propose paper joint neural extraction model multitriplets namely tme capable adaptively discovering multiple triplets simultaneously sentence via ranking translation mechanism experiment tme exhibits superior performance achieves improvement 37.6 f1 score state-of-the-art competitors	negative
mfbo-ssm: multi-fidelity bayesian optimization for fast inference in state-space models	nonlinear state-space models ubiquitous modeling real-world dynamical systems sequential monte carlo smc techniques also known particle methods well-known class parameter estimation methods general class state-space models existing smc-based techniques rely excessive sampling parameter space makes computation intractable large systems tall data sets bayesian optimization techniques used fast inference state-space models intractable likelihoods techniques aim find maximum likelihood function sequential sampling parameter space single smc approximator various smc approximators different fidelities computational costs often available sample-based likelihood approximation paper propose multi-fidelity bayesian optimization algorithm inference general nonlinear state-space models mfbo-ssm enables simultaneous sequential selection parameters approximators accuracy speed algorithm demonstrated numerical experiments using synthetic gene expression data gene regulatory network model real data vix stock price index	negative
regularizing fully convolutional networks for time series classification by decorrelating filters	deep neural networks prone overfitting especially small training data regimes often networks overparameterized resulting learned weights tend strong correlations however convolutional networks general fully convolution neural networks fcns particular shown relatively parameter efficient recently successfully applied time series classification tasks paper investigate application different regularizers correlation learned convolutional filters fcns using batch normalization bn regularizer time series classification tsc tasks results demonstrate despite orthogonal initialization filters average correlation across filters especially filters higher layers tends increase training proceeds indicating redundancy filters mitigate redundancy propose strong regularizer using simple yet effective filter decorrelation proposed method yields significant gains classification accuracy 44 diverse time series datasets ucr tsc benchmark repository	negative
beyond rnns: positional self-attention with co-attention for video question answering	recent progresses visual question answering based recurrent neural networks rnns attention despite success models often timeconsuming difficulties modeling long range dependencies due sequential nature rnns propose new architecture positional self-attention coattention psac require rnns video question answering specifically inspired success self-attention machine translation task propose positional self-attention calculate response position attending positions within sequence add representations absolute positions therefore psac exploit global dependencies question temporal information video make process question video encoding executed parallel furthermore addition attending video features relevant given questions i.e. video attention utilize co-attention mechanism simultaneously modeling “ words listen ” question attention best knowledge first work replacing rnns selfattention task visual question answering experimental results four tasks benchmark dataset show model significantly outperforms state-of-the-art three tasks attains comparable result count task model requires less computation time achieves better performance compared rnns-based methods additional ablation study demonstrates effect component proposed model	negative
multi-scale 3d convolution network for video based person re-identification	paper proposes two-stream convolution network extract spatial temporal cues video based person reidentification reid temporal stream network constructed inserting several multi-scale 3d m3d convolution layers 2d cnn network resulting m3d convolution network introduces fraction parameters 2d cnn gains ability multi-scale temporal feature learning compact architecture m3d convolution network also efficient easier optimize existing 3d convolution networks temporal stream involves residual attention layers ral refine temporal features jointly learning spatial-temporal attention masks residual manner ral identifies discriminative spatial regions temporal cues stream network implemented 2d cnn spatial feature extraction spatial temporal features two streams finally fused video based person reid evaluations three widely used benchmarks datasets i.e. mars prid2011 ilids-vid demonstrate substantial advantages method existing 3d convolution networks state-of-art methods	positive
video-based sentiment analysis with hvnlbp-top feature and bi-lstm	paper propose new feature extraction method called hvnlbp-top video-based sentiment analysis furthermore use principal component analysis pca bidirectional long short term memory bi-lstm dimensionality reduction classification achieved average recognition accuracy 71.1 moud dataset 63.9 cmu-mosi dataset	positive
meal: multi-model ensemble via adversarial learning	often best performing deep neural models ensembles multiple base-level networks unfortunately space required store many networks time required execute test-time prohibits use applications test sets large e.g. imagenet paper present method compressing large complex trained ensembles single network knowledge variety trained deep neural networks dnns distilled transferred single dnn order distill diverse knowledge different trained teacher models propose use adversarial-based learning strategy define block-wise training loss guide optimize predefined student network recover knowledge teacher models promote discriminator network distinguish teacher vs. student features simultaneously proposed ensemble method meal transferring distilled knowledge adversarial learning exhibits three important advantages 1 student network learns distilled knowledge discriminators optimized better original model 2 fast inference realized single forward pass performance even better traditional ensembles multi-original models 3 student network learn distilled knowledge teacher model arbitrary structures extensive experiments cifar-10/100 svhn imagenet datasets demonstrate effectiveness meal method imagenet resnet-50 based meal achieves top-1/5 21.79 /5.99 val error outperforms original model 2.06 /1.14	negative
demo: learning to perceive long-range obstacles using self-supervision from short-range sensors	demonstrate self-supervised approach learns detect long-range obstacles video automatically obtains training labels associating camera frames acquired given pose short-range sensor readings acquired different pose	negative
heuristic search algorithm for dimensionality reduction optimally combining feature selection and feature extraction	following two classical approaches dimensionality reduction 1. approximating data small number features exist data feature selection 2. approximating data small number arbitrary features feature extraction study generalization approximates data selected extracted features show optimal solution hybrid problem involves combinatorial search trivially obtained even one solve optimally separate problems selection extraction approach gives optimal approximate solutions uses “ best first ” heuristic search algorithm comes priori posteriori optimality guarantee similar obtained classical weighted a* algorithm experimental results show effectiveness proposed approach	negative
unsupervised post-processing of word vectors via conceptor negation	word vectors core many natural language processing tasks recently interest post-processing word vectors enrich semantic information paper introduce novel word vector post-processing technique based matrix conceptors jaeger 2014 family regularized identity maps concretely propose use conceptors suppress latent features word vectors high variances proposed method purely unsupervised rely corpus external linguistic database evaluate post-processed word vectors battery intrinsic lexical evaluation tasks showing proposed method consistently outperforms existing state-of-the-art alternatives also show post-processed word vectors used downstream natural language processing task dialogue state tracking yielding improved results different dialogue domains	negative
a generalized language model in tensor space	literature tensors effectively used capturing context information language models however existing methods usually adopt relatively-low order tensors limited expressive power modeling language developing higher-order tensor representation challenging terms deriving effective solution showing generality paper propose language model named tensor space language model tslm utilizing tensor networks tensor decomposition tslm build high-dimensional semantic space constructed tensor product word vectors theoretically prove tensor representation generalization n-gram language model show high-order tensor representation decomposed recursive calculation conditional probability language modeling experimental results penn tree bank ptb dataset wikitext benchmark demonstrate effectiveness tslm	negative
variational bejg solvers for marginal-map inference with accurate approximation of b-conditional entropy	previously proposed variational techniques approximate mmap inference complex graphical models high-order factors relax dual variational objective function obtain tractable approximation perform mmap inference resulting simplified graphical model sub-graph decision variables assumed disconnected forest contrast developed novel variational mmap inference algorithms proximal convergent solvers improve approximation accuracy better preserving original mmap query designing dual variational objective function upper bound approximation applied entropy decision variables evaluate proposed algorithms simulated synthetic datasets diagnostic bayesian networks taken uai inference challenge solvers outperform variational algorithms majority reported cases additionally demonstrate important real-life application proposed variational approaches solve complex tasks policy optimization mmap inference performance implemented approximation algorithms compared demonstrate original task optimizing pomdp controllers approached reformulation equivalent problem marginal-map inference novel single-dbn generative model guarantees control policies computed probabilistic inference model optimal traditional sense motivation approaching planning problem probabilistic inference graphical models explained fact transforming markovian planning problem task probabilistic inference marginal map problem applying belief propagation techniques generative models achieve computational complexity reduction pspace-complete nexp-complete nppp-complete comparison solving pomdp dec-pomdp models respectively search vs. dynamic programming	positive
geniepath: graph neural networks with adaptive receptive paths	present geniepath scalable approach learning adaptive receptive fields neural networks defined permutation invariant graph data geniepath propose adaptive path layer consists two complementary functions designed breadth depth exploration respectively former learns importance different sized neighborhoods latter extracts filters signals aggregated neighbors different hops away method works transductive inductive settings extensive experiments compared competitive methods show approaches yield state-of-the-art results large graphs	positive
on lifted inference using neural embeddings	present dense representation markov logic networks mlns called obj2vec encodes symmetries mln structure identifying symmetries key challenge lifted inference algorithms leverage advances neural networks learn symmetries hard specify using hand-crafted features specifically learn embedding mln objects predicts context object i.e. objects appear along formulas mln since common contexts indicate symmetry distribution importantly formulation leverages well-known skip-gram models allow us learn embedding efficiently finally reduce size ground mln sample objects based learned embeddings integrate obj2vec several inference algorithms show scalability accuracy approach compared state-of-the-art methods	positive
an svm-based framework for long-term learning systems	research study problem learning sequence supervised tasks long-standing challenge machine learning work relies transfer knowledge hypotheses learned support vector machines transfer occurs two directions forward backward proposed selectively transfer forward support vector coefficients previous hypotheses upper-bounds support vector coefficients learned target task also proposed novel method refining existing hypotheses transferring backward knowledge target hypothesis learned recently improved method hypothesis refinement approach refines whilst encouraging retention knowledge contribution represented long-term learning framework binary classification tasks received sequentially one time	negative
semantic proposal for activity localization in videos via sentence query	paper presents efficient algorithm tackle temporal localization activities videos via sentence queries task differs traditional action localization three aspects 1 activities combinations various kinds actions may span long period time 2 sentence queries limited predefined list classes 3 videos usually contain multiple different activity instances traditional proposal-based approaches action localization consider class-agnostic “ actionness ” video snippets insufficient tackle task propose novel semantic activity proposal sap integrates semantic information sentence queries proposal generation process get discriminative activity proposals visual semantic information jointly utilized proposal ranking refinement evaluate algorithm tacos dataset charades-sta dataset experimental results show algorithm outperforms existing methods datasets time reduces number proposals factor least 10	positive
robust principal component analysis-based infrared small target detection	method based robust principle component analysis rpca technique proposed detect small targets infrared images using low rank characteristic background sparse characteristic target observed image regarded sum low-rank background matrix sparse outlier matrix decomposition solved rpca infrared small target extracted single-frame image multi-frame sequence order get efficient algorithm iteration process augmented lagrange multiplier method improved simulation results show method detect small target precisely efficiently	negative
visual place recognition via robustℓ2-norm distance based holism and landmark integration	visual place recognition essential large-scale simultaneous localization mapping slam long-term robot operations across different time days months seasons introduce new challenges significant environment appearance variations paper propose novel method learn location representation integrate semantic landmarks place holistic representation promote robustness new model drastic appearance variations due long-term visual changes formulate objective use non-squared ℓ2-norm distances leads difficult optimization problem minimizes ratio ℓ2,1-norms matrices solve objective derive new efficient iterative algorithm whose convergence rigorously guaranteed theory addition solution strictly orthogonal learned location representations better place recognition capabilities evaluate proposed method using two large-scale benchmark data sets cmu-vl nordland data sets experimental results validated effectiveness new method long-term visual place recognition applications	negative
trainable undersampling for class-imbalance learning	undersampling widely used class-imbalance learning area main deficiency existing undersampling methods data sampling strategies heuristic-based independent used classifier evaluation metric thus may discard informative instances classifier data sampling work propose meta-learning method built undersampling address issue key idea method parametrize data sampler train optimize classification performance evaluation metric solve non-differentiable optimization problem training data sampler via reinforcement learning incorporating evaluation metric optimization data sampling process proposed method learn instance discarded given classifier evaluation metric addition data level operation method easily applied arbitrary evaluation metric classifier including non-parametric ones e.g. c4.5 knn experimental results synthetic realistic datasets demonstrate effectiveness proposed method	negative
jointly multiple hash learning	hashing compress heterogeneous high-dimensional data compact binary codes preserving similarity facilitate efficient retrieval storage thus hashing recently received much attention information retrieval researchers existing hashing methods first predefine fixed length e.g. 32 64 128 bit hash codes learning fixed length however one sample represented various hash codes different lengths thus must associations relationships among different hash codes represent sample therefore harnessing relationships boost performance hashing methods inspired possibility study propose new model jointly multiple hash learning jmh learn hash codes multiple lengths simultaneously proposed jmh method three types information used hash learning come hash codes different lengths original features samples label contrast existing hashing methods jmh learn hash codes different lengths one step users select appropriate hash codes retrieval tasks according requirements terms accuracy complexity best knowledge jmh one first attempts learn multi-length hash codes simultaneously addition proposed model discrete closed-form solutions variables obtained cyclic coordinate descent thereby making proposed model much faster training extensive experiments performed based three benchmark datasets results demonstrated superior performance proposed method	negative
explainable recommendation through attentive multi-view learning	recommender systems playing increasingly important role daily life due explosive growth information accuracy explainability two core aspects evaluate recommendation model become one fundamental trade-offs machine learning paper propose alleviate trade-off accuracy explainability developing explainable deep model combines advantages deep learning-based models existing explainable methods basic idea build initial network based explainable deep hierarchy e.g. microsoft concept graph improve model accuracy optimizing key variables hierarchy e.g. node importance relevance ensure accurate rating prediction propose attentive multi-view learning framework framework enables us handle sparse noisy data co-regularizing among different feature levels combining predictions attentively mine readable explanations hierarchy formulate personalized explanation generation constrained tree node selection problem propose dynamic programming algorithm solve experimental results show model outperforms state-of-the-art methods terms accuracy explainability	negative
large-scale heterogeneous feature embedding	feature embedding aims learn low-dimensional vector representation instance preserve information features representations benefit various offthe-shelf learning algorithms embedding models single type features well-studied real-world instances often contain multiple types correlated features even information within different modality networks existing studies multiview learning show promising learn unified vector representations sources however high computational costs incorporating heterogeneous information limit applications existing algorithms number instances dimensions features practice often large bridge gap propose scalable framework featwalk model incorporate instance similarities terms different types features unified embedding representation enable scalability featwalk directly calculate similarity measure provides alternative way simulate similarity-based random walks among instances extract local instance proximity preserve set instance index sequences sequences homogeneous scalable word embedding algorithm applied learn joint embedding representation instances experiments four real-world datasets demonstrate efficiency effectiveness featwalk	negative
visual-semantic graph reasoning for pedestrian attribute recognition	pedestrian attribute recognition surveillance challenging task due poor image quality significant appearance variations diverse spatial distribution different attributes paper treats pedestrian attribute recognition sequential attribute prediction problem proposes novel visual-semantic graph reasoning framework address problem framework contains spatial graph directed semantic graph performing reasoning using graph convolutional network gcn one graph captures spatial relations regions learns potential semantic relations attributes end-to-end architecture presented perform mutual embedding two graphs guide relational learning verify proposed framework three large scale pedestrian attribute datasets including peta rap pa100k experiments show superiority proposed method state-of-the-art methods effectiveness joint gcn structures sequential attribute prediction	positive
multigrid backprojection super–resolution and deep filter visualization	introduce novel deep–learning architecture image upscaling large factors e.g 4× 8× based examples pristine high–resolution images target reconstruct high–resolution images downscale versions proposed system performs multi–level progressive upscaling starting small factors 2× updating higher factors 4× 8× system recursive repeats procedure level also residual since use network update outputs classic upscaler network residuals improved iterative back–projections ibp computed features convolutional network work multiple levels extend standard back– projection algorithm using recursion analogous multi– grid algorithms commonly used solvers large systems linear equations finally show network interpreted standard upsampling–and–filter upscaler space–variant filter adapts geometry approach allows us visualize network learns upscale finally system reaches state art quality models relatively number parameters	negative
point2sequence: learning the shape representation of 3d point clouds with an attention-based sequence to sequence network	exploring contextual information local region important shape understanding analysis existing studies often employ hand-crafted explicit ways encode contextual information local regions however hard capture fine-grained contextual information hand-crafted explicit manners correlation different areas local region limits discriminative ability learned features resolve issue propose novel deep learning model 3d point clouds named point2sequence learn 3d shape features capturing fine-grained contextual information novel implicit way point2sequence employs novel sequence learning model point clouds capture correlations aggregating multi-scale areas local region attention specifically point2sequence first learns feature area scale local region captures correlation area scales process aggregating area scales using recurrent neural network rnn based encoder-decoder structure attention mechanism proposed highlight importance different area scales experimental results show point2sequence achieves state-of-the-art performance shape classification segmentation tasks	negative
fine-grained search space classification for hard enumeration variants of subset problems	propose simple powerful flexible machine learning framework reducing search space computationally difficult enumeration variants subset problems ii augmenting existing state-of-the-art solvers informative cues arising input distribution instantiate framework problem listing maximum cliques graph central problem network analysis data mining computational biology demonstrate practicality approach real-world networks millions vertices edges retaining optimal solutions also aggressively pruning input instance size resulting several fold speedups state-of-the-art algorithms finally explore limits scalability robustness proposed framework suggesting supervised learning viable tackling np-hard problems practice	positive
optimal projection guided transfer hashing for image retrieval	recently learning hash widely studied image retrieval thanks computation storage efficiency binary codes existing learning hash methods sufficient training images required used learn precise hashing codes however real-world applications always sufficient training images domain interest addition existing supervised approaches need amount labeled data expensive process terms time labor human expertise handle problems inspired transfer learning propose simple yet effective unsupervised hashing method named optimal projection guided transfer hashing gth borrow images different related domain i.e. source domain help learn precise hashing codes domain interest i.e. target domain besides propose seek maximum likelihood estimation mle solution hashing functions target source domains due domain gap furthermore alternating optimization method adopted obtain two projections target source domains domain hashing disparity reduced gradually extensive experiments various benchmark databases verify method outperforms many state-of-the-art learning hash methods implementation details available https //github.com/liuji93/gth	negative
distribution consistency based covariance metric networks for few-shot learning	few-shot learning aims recognize new concepts examples however existing few-shot learning methods mainly concentrate first-order statistic concept representation fixed metric relation sample concept work propose novel end-to-end deep architecture named covariance metric networks covamnet covamnet designed exploit covariance representation covariance metric based distribution consistency few-shot classification tasks specifically construct embedded local covariance representation extract second-order statistic information concept describe underlying distribution concept upon covariance representation define new deep covariance metric measure consistency distributions query samples new concepts furthermore employ episodic training mechanism train entire network end-to-end manner scratch extensive experiments two tasks generic few-shot image classification fine-grained fewshot image classification demonstrate superiority proposed covamnet source code available https //github.com/wenbinlee/covamnet.git	negative
holographic factorization machines for recommendation	factorization machines fms class popular algorithms widely adopted collaborative filtering recommendation tasks fms characterized usage inner product factorized parameters model pairwise feature interactions making highly expressive powerful paper proposes holographic factorization machines hfm new novel method enhancing representation capability fms without increasing parameter size approach replaces inner product fms holographic reduced representations hrrs theoretically motivated associative retrieval compressed outer products empirically found leads consistent improvements vanilla fms 4 improvement terms mean squared error improvements larger smaller parameterization additionally propose neural adaptation hfm enhances capability handle nonlinear structures conduct extensive experiments nine publicly available datasets collaborative filtering explicit feedback hfm achieves state-of-theart performance nine outperforming strong competitors attentional factorization machines afm neural matrix factorization neumf	negative
t-center: a novel discriminative feature extraction approach for iris recognition	large-scale iris recognition tasks determination classification thresholds remains challenging task especially practical applications sample space growing rapidly due complexity iris samples classification threshold difficult determine increase samples key issue solving threshold determination problems obtain iris feature vectors obvious discrimination therefore train deep convolutional neural networks based large number iris samples extract iris features importantly optimized center loss function referred tight center -center loss used solve problem insufficient discrimination caused softmax loss function order evaluate effectiveness proposed method use cosine similarity estimate similarity features published datasets casia-irisv4 iitd2.0 experiment results demonstrate -center loss minimize intra-class variance maximize inter-class variance achieve significant performance benchmark experiments	negative
deeptilebars: visualizing term distribution for neural information retrieval	neural information retrieval neu-ir models derive query-to-document ranking scores based term-level matching inspired tilebars classical term distribution visualization method paper propose novel neu-ir model handles query-to-document matching subtopic higher levels system first splits documents topical segments “ visualizes ” matchings query segments feeds interaction matrix neu-ir model deeptilebars obtain final ranking scores deeptilebars models relevance signals occurring different granularities document ’ topic hierarchy better captures discourse structure document thus matching patterns although design implementation light-weight deeptilebars outperforms state-of-the-art neu-ir models benchmark datasets including text retrieval conference trec 2010-2012 web tracks letor 4.0	negative
a machine learning suite for machine components’ health-monitoring	paper studies intelligent technique healthmonitoring prognostics common rotary machine components regards bearings particular run-to-failure experiment rich unsupervised features vibration sensory data extracted trained sparse autoencoder correlation initial samples presumably healthy along successive samples calculated passed moving-average filter normalized output referred auto-encoder correlation based aec rate determines informative attribute system depicting health status aec automatically identifies degradation starting point machine component show aec rate well-generalizes several run-tofailure tests demonstrate superiority aec many state-of-the-art approaches health monitoring machine bearings	positive
fast relational probabilistic inference and learning: approximate counting via hypergraphs	counting number true instances clause arguably major bottleneck relational probabilistic inference learning approximate counts two steps 1 transform fully grounded relational model large hypergraph partially-instantiated clauses hypergraph motifs 2 since expected counts motifs provably clause counts approximate using summary statistics in/outdegrees edge counts etc experimental results demonstrate efficiency approximations applied many complex statistical relational models significantly faster state-of-the-art inference learning without sacrificing effectiveness	positive
incorporating context-relevant knowledge into convolutional neural networks for short text classification	text classification methods ’ work well short texts due data sparsity ’ ’ fully exploit context-relevant knowledge order tackle problems propose neural network incorporate context-relevant knowledge convolutional neural network short text classification model consists two modules first module utilizes two layers extract concept context features respectively employs attention layer extract context-relevant concepts second module utilizes convolutional neural network extract high-level features word contextrelevant concept features experimental results three datasets show proposed model outperforms stateof-the-art models	negative
diverse exploration via conjugate policies for policy gradient methods	address challenge effective exploration maintaining good performance policy gradient methods solution propose diverse exploration de via conjugate policies de learns deploys set conjugate policies conveniently generated byproduct conjugate gradient descent provide theoretical empirical results showing effectiveness de achieving exploration improving policy performance advantage de exploration random policy perturbations	negative
anchors bring ease: an embarrassingly simple approach to partial multi-view clustering	clustering multi-view data attracted much attention past decades previous studies assume instance appears views least one view containing instances however real world data often suffers missing instances view leading research problem partial multi-view clustering address issue paper proposes simple yet effective anchorbased partial multi-view clustering apmc method utilizes anchors reconstruct instance-to-instance relationships clustering apmc conceptually simple easy implement practice besides clear intuitions non-trivial empirical guarantees specifically apmc firstly integrates intra- inter- view similarities anchors spectral clustering performed fused similarities obtain unified clustering result compared existing partial multi-view clustering methods apmc three notable advantages 1 capture non-linear relations among instances help kernel-based similarities 2 much lower time complexity virtue noniterative scheme 3 inherently handle data negative entries well extended two views finally extensively evaluate proposed method five benchmark datasets experimental results demonstrate superiority apmc state-of-the-art approaches	positive
afs: an attention-based mechanism for supervised feature selection	effective data preprocessing step feature selection shown effectiveness prepare high-dimensional data many machine learning tasks proliferation high di-mension huge volume big data however brought major challenges e.g computation complexity stability noisy data upon existing feature-selection techniques paper introduces novel neural network-based feature selection architecture dubbed attention-based feature selec-tion afs afs consists two detachable modules at-tention module feature weight generation learning module problem modeling attention module for-mulates correlation problem among features supervision target binary classification problem supported shallow attention net feature feature weights generated based distribution respective feature selec-tion patterns adjusted backpropagation training process detachable structure allows existing off-the-shelf models directly reused allows much less training time demands training data requirements expertise hybrid initialization method also introduced boost selection accuracy datasets without enough samples feature weight generation experimental results show afs achieves best accuracy stability comparison several state-of-art feature selection algorithms upon mnist noisy mnist several datasets small samples	negative
learning segmentation masks with the independence prior	instance bad mask might make composite image uses look fake encourages us learn segmentation generating realistic composite images achieve propose novel framework exploits new proposed prior called independence prior based generative adversarial networks gans generator produces image multiple category-specific instance providers layout module composition module firstly provider independently outputs category-specific instance image soft mask provided instances ’ poses corrected layout module lastly composition module combines instances final image training adversarial loss penalty mask area provider learns mask small possible enough cover complete category-specific instance weakly supervised semantic segmentation methods widely use grouping cues modeling association image parts either artificially designed learned costly segmentation labels modeled local pairs unlike method automatically models dependence parts learns instance segmentation apply framework two cases 1 foreground segmentation category-specific images box-level annotation 2 unsupervised learning instance appearances masks one image homogeneous object cluster hoc get appealing results tasks shows independence prior useful instance segmentation possible unsupervisedly learn instance masks one image	negative
memory-augmented temporal dynamic learning for action recognition	human actions captured video sequences contain two crucial factors action recognition i.e. visual appearance motion dynamics model two aspects convolutional recurrent neural networks cnns rnns adopted existing successful methods recognizing actions however cnn based methods limited modeling long-term motion dynamics rnns able learn temporal motion dynamics lack effective ways tackle unsteady dynamics long-duration motion work propose memory-augmented temporal dynamic learning network learns write evident information external memory module ignore irrelevant ones particular present differential memory controller make discrete decision whether external memory module updated current feature discrete memory controller takes memory history context embedding current feature inputs controls information flow external memory module additionally train discrete memory controller using straight-through estimator evaluate end-to-end system benchmark datasets ucf101 hmdb51 human action recognition experimental results show consistent improvements datasets prior works baselines	negative
unsupervised learning with contrastive latent variable models	unsupervised learning dimensionality reduction important tool data exploration visualization aims typically open-ended useful frame problem looking patterns enriched one dataset relative another pairs datasets occur commonly instance population interest vs. control signal vs. signal free recordings however methods work sets data opposed data points sequences present probabilistic model dimensionality reduction discover signal enriched target dataset relative background dataset data sets need paired grouped beyond set membership using probabilistic model structure shared amongst two datasets unique target dataset able recover interesting structure latent space target dataset method also advantages probabilistic model namely allows incorporation prior information handles missing data generalized different distributional assumptions describe several possible variations model demonstrate application technique de-noising feature selection subgroup discovery settings	negative
marginal inference in continuous markov random fields using mixtures	exact marginal inference continuous graphical models computationally challenging outside special cases existing work approximate inference focused approximately computing messages part loopy belief propagation algorithm either via sampling methods moment matching relaxations work present alternative family approximations instead approximating messages approximates beliefs continuous bethe free energy using mixture distributions show types approximations combined numerical quadrature yield algorithms theoretical guarantees quality approximation significantly better practical performance variety applications challenging current state-of-the-art methods	negative
preference-aware task assignment in on-demand taxi dispatching: an online stable matching approach	central issue on-demand taxi dispatching platforms task assignment designs matching policies among dynamically arrived drivers workers passengers tasks previous matching policies maximize profit platform without considering preferences workers tasks e.g. workers may prefer high-rewarding tasks tasks may prefer nearby workers ignorance preferences impairs user experience decrease profit platform long run address problem propose preference-aware task assignment using online stable matching specifically define new model online stable matching known identical independent distributions osm-kiid maximizes expected total profits obj-1 also tries satisfy preferences among workers tasks minimizing expected total number blocking pairs obj-2 model also features practical arrival assumption validated real-world dataset furthermore present linear program based online algorithm lp-alg achieves online ratio least 1−1/e obj-1 0.6·|e| blocking pairs expectedly |e| total number edges compatible graph also show natural greedy arbitrarily bad performance obj-1 maintaining around 0.5·|e| blocking pairs evaluations synthetic real datasets confirm theoretical analysis demonstrate lp-alg strictly dominates baselines objectives tasks notably outnumber workers	positive
type sequence preserving heterogeneous information network embedding	lacking sequence preserving mechanism existing heterogeneous information network hin embedding discards essential type sequence information embedding propose type sequence preserving hin embedding model seqhine expands hin embedding sequence level seqhine incorporates type sequence information via type-aware gru preserves representative sequence information decay function abundant experiments show seqhine outperform state-of-the-art even 50 less labeled data	negative
ensnet: ensconce text in the wild	new method proposed removing text natural images challenge first accurately localize text stroke-level replace visually plausible background unlike previous methods require image patches erase scene text method namely ensconce network ensnet operate end-to-end single image without prior knowledge overall structure end-to-end trainable fcn-resnet-18 network conditional generative adversarial network cgan feature former first enhanced novel lateral connection structure refined four carefully designed losses multiscale regression loss content loss capture global discrepancy different level features texture loss total variation loss primarily target filling text region preserving reality background latter novel local-sensitive gan attentively assesses local consistency text erased regions qualitative quantitative sensitivity experiments synthetic images icdar 2013 dataset demonstrate component ensnet essential achieve good performance moreover ensnet significantly outperform previous state-of-the-art methods terms metrics addition qualitative experiment conducted sbmnet dataset demonstrates proposed method also preform well general object pedestrians removal tasks ensnet extremely fast preform 333 fps i5-8600 cpu device	negative
wsd-gan: word sense disambiguation using generative adversarial networks	word sense disambiguation wsd tough task natural language processing nlp aims identify correct sense ambiguous word given context two mainstreams wsd supervised methods mainly utilize labeled context train classifier generates right probability distribution word senses meanwhile knowledge-based unsupervised methods focus glosses word sense definitions always calculate similarity context-gloss pair score find right word sense paper propose generative adversarial framework wsd-gan combines two mainstream methods wsd generative model based supervised methods tries generate probability distribution word senses meanwhile discriminative model based knowledge-based methods focuses predicting relevancy context-gloss pairs identifies correct pairs others furthermore order optimize two models leverage policy gradient enhance performances two models mutually experimental results show wsd-gan achieves competitive results several english all-words wsd datasets	negative
exploring answer stance detection with recurrent conditional attention	detecting stance certain types question-answer pairs interesting problem carefully explored unlike previous stance detection tasks targets given entities claims entire questions makes difficult capture semantics targets build target-dependent representations answers address introduce recurrent conditional attention rca model incorporates conditional attention structure recurrent reading process rca iteratively guides distillation question semantic answer information collects stance-oriented text relating question revealing mutual relationship among stance answer question experiments manually labeled chinese community qa stance dataset show rca outperforms four strong baselines average 2.90 macro-f1 2.66 micro-f1 respectively	negative
video object detection with locally-weighted deformable neighbors	deep convolutional neural networks achieved great success various image recognition tasks however nontrivial transfer existing networks video due fact developed static image frame-byframe processing suboptimal temporal information vital video understanding totally abandoned furthermore frame-by-frame processing slow inefficient hinder practical usage paper propose lwdn locally-weighted deformable neighbors video object detection without utilizing time-consuming optical flow extraction networks lwdn latently align high-level features keyframes keyframes nonkeyframes inspired zhu et al 2017a hetang et al 2017 propose aggregate features keyframes keyframes adopt brain-inspired memory mechanism propagate update memory feature keyframes keyframes call process memory-guided propagation memory mechanism discriminative ability features keyframes non-keyframes enhanced helps improve detection accuracy extensive experiments vid dataset demonstrate method achieves superior performance speed accuracy trade-off i.e. 76.3 challenging vid dataset maintaining 20fps speed titan x gpu	negative
repeatnet: a repeat aware neural recommendation machine for session-based recommendation	recurrent neural networks session-based recommendation attracted lot attention recently promising performance repeat consumption common phenomenon many recommendation scenarios e.g. e-commerce music tv program recommendations item re-consumed repeatedly time however previous studies emphasized repeat consumption neural networks effective neural approach needed decide perform repeat recommendation paper incorporate repeat-explore mechanism neural networks propose new model called repeatnet encoder-decoder structure repeatnet integrates regular neural recommendation approach decoder new repeat recommendation mechanism choose items user ’ history recommends right time report extensive experiments three benchmark datasets repeatnet outperforms state-of-the-art baselines three datasets terms mrr recall furthermore dataset size repeat ratio increase improvements repeatnet baselines also increase demonstrates advantage handling repeat recommendation scenarios	negative
graph convolutional networks for text classification	text classification important classical problem natural language processing number studies applied convolutional neural networks convolution regular grid e.g. sequence classification however limited number studies explored flexible graph convolutional neural networks convolution non-grid e.g. arbitrary graph task work propose use graph convolutional networks text classification build single text graph corpus based word co-occurrence document word relations learn text graph convolutional network text gcn corpus text gcn initialized one-hot representation word document jointly learns embeddings words documents supervised known class labels documents experimental results multiple benchmark datasets demonstrate vanilla text gcn without external word embeddings knowledge outperforms state-of-the-art methods text classification hand text gcn also learns predictive word document embeddings addition experimental results show improvement text gcn state-of-the-art comparison methods become prominent lower percentage training data suggesting robustness text gcn less training data text classification	negative
graph convolutional networks meet markov random fields: semi-supervised community detection in attribute networks	community detection fundamental problem network science various applications problem attracted much attention many approaches proposed among existing approaches latest methods based graph convolutional networks gcn statistical modeling markov random fields mrf propose integrate techniques gcn mrf solve problem semi-supervised community detection attributed networks semantic information new method takes advantage salient features gnn mrf exploits network topology node semantic information complete end-to-end deep network architecture extensive experiments demonstrate superior performance new method state-of-the-art methods scalability several large benchmark problems	positive
weakly-supervised hierarchical text classification	hierarchical text classification aims classify text documents given hierarchy important task many real-world applications recently deep neural models gaining increasing popularity text classification due expressive power minimum requirement feature engineering however applying deep neural networks hierarchical text classification remains challenging heavily rely large amount training data meanwhile easily determine appropriate levels documents hierarchical setting paper propose weakly-supervised neural method hierarchical text classification method require large amount training data requires easy-to-provide weak supervision signals class-related documents keywords method effectively leverages weak supervision signals generate pseudo documents model pre-training performs self-training real unlabeled data iteratively refine model training process model features hierarchical neural structure mimics given hierarchy capable determining proper levels documents blocking mechanism experiments three datasets different domains demonstrate efficacy method compared comprehensive set baselines	negative
learning dynamic generator model by alternating back-propagation through time	paper studies dynamic generator model spatialtemporal processes dynamic textures action sequences video data model time frame video sequence generated generator model non-linear transformation latent state vector non-linear transformation parametrized top-down neural network sequence latent state vectors follows non-linear auto-regressive model state vector next frame non-linear transformation state vector current frame well independent noise vector provides randomness transition non-linear transformation transition model parametrized feedforward neural network show model learned alternating back-propagation time algorithm iteratively samples noise vectors updates parameters transition model generator model show training method learn realistic models dynamic textures action patterns	positive
a nonconvex projection method for robust pca	robust principal component analysis rpca well-studied problem whose goal decompose matrix sum low-rank sparse components paper propose nonconvex feasibility reformulation rpca problem apply alternating projection method solve best knowledge first paper proposing method solves rpca problem without considering objective function convex relaxation surrogate convex constraints demonstrate extensive numerical experiments variety applications including shadow removal background estimation face detection galaxy evolution approach matches often significantly outperforms current state-of-the-art various ways	negative
multi-view learning from disparate sources for poverty mapping	many data analytics problems involve data coming multiple sources sensors modalities feature spaces describe object interest unique way typically exhibit heterogeneous properties varied data sources termed views task learning multi-view data known multi-view learning thesis target problem poverty prediction mapping multi-source data currently poverty estimated intensive household surveys costly time consuming need timely accurately predict poverty map spatially fine-grained baseline data primary aim thesis develop novel multi-view algorithms combine disparate data sources poverty mapping another aim work relax core assumptions faced existing multi-view learning algorithms produce factorized subspaces	negative
one-pass incomplete multi-view clustering	real data often multiple modalities multiple heterogeneous sources thus forming so-called multi-view data receives attentions machine learning multi-view clustering mvc becomes important paradigm real-world applications views often suffer instances missing clustering multi-view datasets called incomplete multi-view clustering imc quite challenging date though many approaches developed offline high computational memory costs especially large scale datasets address problem paper propose one-pass incomplete multi-view clustering framework opimc help regularized matrix factorization weighted matrix factorization opimc relatively easily deal problem different existing sole online imc method opimc directly get clustering results effectively determine termination iteration process introducing two global statistics finally extensive experiments conducted four real datasets demonstrate efficiency effectiveness proposed opimc method	negative
improving optimization bounds using machine learning: decision diagrams meet deep reinforcement learning	finding tight bounds optimal solution critical element practical solution methods discrete optimization problems last decade decision diagrams dds brought new perspective obtaining upper lower bounds significantly better classical bounding mechanisms linear relaxations well known quality bounds achieved flexible bounding method highly reliant ordering variables chosen building diagram finding ordering optimizes standard metrics np-hard problem paper propose innovative generic approach based deep reinforcement learning obtaining ordering tightening bounds obtained relaxed restricted dds apply approach maximum independent set problem maximum cut problem experimental results synthetic instances show deep reinforcement learning approach achieving tighter objective function bounds generally outperforms ordering methods commonly used literature distribution instances known best knowledge authors first paper apply machine learning directly improve relaxation bounds obtained general-purpose bounding mechanisms combinatorial optimization problems	negative
combo-action: training agent for fps game with auxiliary tasks	deep reinforcement learning drl achieved surpassing human performance atari games using raw pixels rewards learn everything however first-person-shooter fps games 3d environments contain higher levels human concepts enemy weapon spatial structure etc large action space paper explore novel method plan temporally-extended action sequences refer combo-action compress action space train deep recurrent q-learning network model high-level controller called supervisory network manage combo-actions method boosted auxiliary tasks enemy detection depth prediction enable agent extract high-level concepts fps games extensive experiments show method efficient training process outperforms previous stateof-the-art approaches large margin ablation study experiments also indicate method boost performance fps agent reasonable way	negative
self-adversarially learned bayesian sampling	scalable bayesian sampling playing important role modern machine learning especially fast-developed unsupervised- deep -learning models tremendous progresses achieved via scalable bayesian sampling stochastic gradient mcmc sg-mcmc stein variational gradient descent svgd generated samples typically highly correlated moreover sample-generation processes often criticized inefficient paper propose novel self-adversarial learning framework automatically learns conditional generator mimic behavior markov kernel transition kernel high-quality samples efficiently generated direct forward passes though learned generator importantly learning process adopts self-learning paradigm requiring information existing markov kernels e.g. knowledge draw samples specifically framework learns use current samples either generator pre-provided training data update generator generated samples progressively approach target distribution thus called self-learning experiments synthetic real datasets verify advantages framework outperforming related methods terms sampling efficiency sample quality	negative
automating analysis and feedback to improve mathematics teachers’ classroom discourse	work builds advances deep learning natural language processing automatically analyze transcribed classroom discourse reliably generate information teachers ’ uses specific discursive strategies called ” talk moves. ” talk moves used teachers learners construct conversations students share thinking actively consider ideas others engage sustained reasoning currently providing teachers detailed feedback talk moves lessons requires highly trained observers hand code transcripts classroom recordings analyze talk moves and/or one-on-one expert coaching time-consuming expensive process unlikely scale created bidirectional long short-term memory bi-lstm network automate annotation process demonstrated feasibility deep learning approach reliably identify set teacher talk moves sentence level f1 measure 65	negative
region-based message exploration over spatio-temporal data streams	massive amount spatio-temporal data contain location text content generated location-based social media spatio-temporal messages cover wide range topics great significance discover local trending topics based users ’ location-based topicbased requirements develop region-based message exploration mechanism retrieve spatio-temporal message clusters stream spatio-temporal messages based users ’ preferences message topic message spatial distribution additionally propose region summarization algorithm finds subset representative messages cluster summarize topics spatial attributes messages cluster evaluate efficacy efficiency proposal two real-world datasets results demonstrate solution capable high efficiency effectiveness compared baselines	negative
multi-order attentive ranking model for sequential recommendation	modern e-commerce temporal order behind users ’ transactions implies importance exploiting transition dependency among items better inferring user prefers interact “ near future ” types interaction among items usually divided individual-level interaction stand transition order pair items union-level relation set items single one however existing work captures one single view especially modeling individual-level interaction paper propose multi-order attentive ranking model marank unify individual- union-level item interaction preference inference model multiple views idea represent user ’ short-term preference embedding user set present items multi-order features intermedia hidden status deep neural network help attention mechanism obtain unified embedding keep individual-level interactions linear combination mapped items ’ features feed aggregated embedding designed residual neural network capture union-level interaction thorough experiments conducted show features marank various component settings furthermore experimental results several public datasets show marank significantly outperforms state-of-the-art baselines different evaluation metrics source code found https //github.com/voladorlu/marank	negative
partial label learning via label enhancement	partial label learning aims learn training examples associated set candidate labels among one label valid training example common strategy induce predictive model trying disambiguate candidate label set disambiguation identifying ground-truth label iteratively disambiguation treating candidate label equally nonetheless strategies ignore considering generalized label distribution corresponding instance since generalized label distribution explicitly available training set paper new partial label learning strategy named pl-le proposed learn partial label examples via label enhancement specifically generalized label distributions recovered leveraging topological information feature space multi-class predictive model learned fitting regularized multi-output regressor generalized label distributions extensive experiments show pl-le performs favorably state-ofthe-art partial label learning approaches	positive
distributed pagerank computation: an improved theoretical study	pagerank classic measure effectively evaluates node importance large graphs applied numerous applications ranging data mining web algorithms recommendation systems load balancing search identifying connectivity structures computing pagerank large graphs challenging motivated studies distributed algorithms compute pagerank previously little works spent distributed pagerank algorithms provably desired complexity accuracy given graph n nodes model distributed computation model well-known congested clique model state-of-the-art algorithm takes √logn communication rounds approximate pagerank value node g probability least 1−1/n paper present improved distributed algorithms computing pagerank particularly algorithm performs log log√n rounds significant improvement compared √logn rounds approximate pagerank values probability least 1−1/n moreover reasonable assumption algorithm also reduces edge bandwidth i.e. maximum communication message size exchanged edge communication round logn factor compared state-of-the-art algorithm finally show algorithm adapted efficiently compute another variant pagerank i.e. batch one-hop personalized pageranks log logn communication rounds	positive
balanced linear contextual bandits	contextual bandit algorithms sensitive estimation method outcome model well exploration method used particularly presence rich heterogeneity complex outcome models lead difficult estimation problems along path learning develop algorithms contextual bandits linear payoffs integrate balancing methods causal inference literature estimation make less prone problems estimation bias provide first regret bound analyses linear contextual bandits balancing show algorithms match state art theoretical guarantees demonstrate strong practical advantage balanced contextual bandits large number supervised learning datasets synthetic example simulates model misspecification prejudice initial training data	positive
improving domain-specific classification by collaborative learning with adaptation networks	unsupervised domain adaptation process learning domain-invariant representations could dominated labeled source data specific characteristics target domain may ignored order improve performance inferring target labels propose targetspecific network capable learning collaboratively domain adaptation network instead directly minimizing domain discrepancy clustering regularization also utilized improve generalization capability target-specific network forcing target data points close accumulated class centers network learns specializes target domain performance inferring target labels improves turn facilitates learning process adaptation network therefore mutually beneficial relationship two networks perform extensive experiments multiple digit object datasets effectiveness superiority proposed approach presented verified multiple visual adaptation benchmarks e.g. improve state-ofthe-art task mnist→svhn 76.5 84.9 without specific augmentation	negative
optimal interdiction of urban criminals with the aid of real-time information	violent crimes happen urban suburban cities emerging tracking techniques law enforcement officers real-time location information escaping criminals dynamically adjust security resource allocation interdict unfortunately existing work urban network security games largely ignores information paper addresses omission first show ignoring real-time information cause arbitrarily large loss efficiency mitigate loss propose novel network pursuit game nest model captures interaction escaping adversary defender multiple resources real-time information available second solving nest proven np-hard third transforming non-convex program solving nest linear program propose incremental strategy generation algorithm including novel pruning techniques best response oracle ii novel techniques mapping strategies subgames adding multiple best response strategies one iteration solve extremely large problems finally extensive experiments show effectiveness approach scales realistic problem sizes hundreds nodes networks including real network manhattan	negative
accurate and interpretable factorization machines	factorization machines fms general predictor efficiently model high-order feature interactions widely used regression classification ranking problems however despite many successful applications fms two main limitations fms 1 fms consider feature interactions among input features using polynomial expansion fail capture complex nonlinear patterns data 2 existing fms provide interpretable prediction users paper present novel method named subspace encoding factorization machines sefm overcome two limitations using non-parametric subspace feature mapping due high sparsity new feature representation proposed method achieves time complexity standard fms capture complex nonlinear patterns moreover since prediction score proposed model sample sum contribution scores bins grid cells sample lies low-dimensional subspaces works similar like scoring system involves data binning score addition therefore proposed method naturally provides interpretable prediction experimental results demonstrate proposed method efficiently provides accurate interpretable prediction	negative
dsine: deep structural influence learning via network embedding	structural representations user social influence critical variety applications viral marketing recommendation products however existing studies focus capturing preserving structure relations ignore diversity influence relations patterns among users end propose deep structural influence learning model learn social influence structure via mining rich features user fuse information aligned selfnetwork component preserving global local structure influence relations among users experiments two real-world datasets demonstrate proposed model outperforms state-of-the-art algorithms learning rich representations multi-label classification task	negative
motion guided spatial attention for video captioning	sequence-to-sequence models incorporated attention mechanism shown promising improvements video captioning rich information inside frames spatial attention rarely explored motion information usually handled 3d-cnns another modality fusion hand researches human perception suggest apparent motion attract attention motivated aim learn spatial attention video frames guidance motion information caption generation present novel video captioning framework utilizing motion guided spatial attention mgsa proposed mgsa exploits motion video frames learning spatial attention stacked optical flow images custom cnn relate spatial attention maps video frames designed gated attention recurrent unit garu adaptively incorporate previous attention maps whole framework trained end-to-end manner evaluate approach two benchmark datasets msvd msr-vtt experiments show designed model generate better video representation state art results obtained popular evaluation metrics bleu 4 cider meteor	negative
sta: spatial-temporal attention for large-scale video-based person re-identification	work propose novel spatial-temporal attention sta approach tackle large-scale person reidentification task videos different existing methods simply compute representations video clips using frame-level aggregation e.g average pooling proposed sta adopts effective way producing robust clip-level feature representation concretely sta fully exploits discriminative parts one target person spatial temporal dimensions results 2-d attention score matrix via inter-frame regularization measure importances spatial parts across different frames thus robust clip-level feature representation generated according weighted sum operation guided mined 2-d attention score matrix way challenging cases video-based person re-identification pose variation partial occlusion well tackled sta conduct extensive experiments two large-scale benchmarks i.e mars dukemtmcvideoreid particular map reaches 87.7 mars significantly outperforms state-of-the-arts large margin 11.6	negative
training complex models with multi-task weak supervision	machine learning models continue increase complexity collecting large hand-labeled training sets become one biggest roadblocks practice instead weaker forms supervision provide noisier cheaper labels often used however weak supervision sources diverse unknown accuracies may output correlated labels may label different tasks apply different levels granularity propose framework integrating modeling weak supervision sources viewing labeling different related sub-tasks problem refer multi-task weak supervision setting show solving matrix completion-style problem recover accuracies multi-task sources given dependency structure without labeled data leading higher-quality supervision training end model theoretically show generalization error models trained approach improves number unlabeled data points characterize scaling respect task dependency structures three fine-grained classification problems show approach leads average gains 20.2 points accuracy traditional supervised approach 6.8 points majority vote baseline 4.1 points previously proposed weak supervision method models tasks separately	negative
large-scale interactive recommendation with tree-structured policy gradient	reinforcement learning rl recently introduced interactive recommender systems irs nature learning dynamic interactions planning long-run performance irs always thousands items recommend i.e. thousands actions existing rl-based methods however fail handle large discrete action space problem thus become inefficient existing work tries deal large discrete action space problem utilizing deep deterministic policy gradient framework suffers inconsistency continuous action representation output actor network real discrete action avoid inconsistency achieve high efficiency recommendation effectiveness paper propose tree-structured policy gradient recommendation tpgr framework balanced hierarchical clustering tree built items picking item formulated seeking path root certain leaf tree extensive experiments carefully-designed environments based two real-world datasets demonstrate model provides superior recommendation performance significant efficiency improvement state-of-the-art methods	negative
y2seq2seq: cross-modal representation learning for 3d shape and text by joint reconstruction and prediction of view and word sequences	jointly learning representations 3d shapes text crucial support tasks cross-modal retrieval shape captioning recent method employs 3d voxels represent 3d shapes limits approach low resolutions due computational cost caused cubic complexity 3d voxels hence method suffers lack detailed geometry resolve issue propose y2seq2seq view-based model learn cross-modal representations joint reconstruction prediction view word sequences specifically network architecture y2seq2seq bridges semantic meaning embedded two modalities two coupled “ ” like sequence-tosequence seq2seq structures addition novel hierarchical constraints increase discriminability cross-modal representations employing detailed discriminative information experimental results cross-modal retrieval 3d shape captioning show y2seq2seq outperforms state-of-the-art methods	negative
aprp: an anonymous propagation method in bitcoin network	due little attention given anonymous protection eavesdropping attacks bitcoin network paper initiatively proposes solution bitcoin anonymization based network structure first present general adversarial network model formulizing deanonymization attack present novel propagation method aprp adaptive pagerank propagation adopts pagerank propagation delay factor constantly adjusts pr-value nodes adapt network dynamics experiments simulated real bitcoin networks confirm superiority aprp terms 20-50 performance enhancement various deanonymization attacks	negative
quarel: a dataset and models for answering questions about qualitative relationships	many natural la guage questions require recognizing reasoning qualitative relationships e.g. science economics medicine challenging answer corpus-based methods qualitative modeling provides tools support reasoning semantic parsing task mapping questions models formidable challenges present quarel dataset diverse story questions involving qualitative relationships characterize challenges techniques begin address dataset 2771 questions relating 19 different types quantities example “ jenny observes robot vacuum cleaner moves slower living room carpet bedroom carpet carpet friction ” contribute 1 simple flexible conceptual framework representing kinds questions 2 quarel dataset including logical forms exemplifying parsing challenges 3 two novel models task built extensions type-constrained semantic parsing first models called quasp+ significantly outperforms off-the-shelf tools quarel second quasp+zero demonstrates zero-shot capability i.e. ability handle new qualitative relationships without requiring additional training data something possible previous models work thus makes inroads answering complex qualitative questions require reasoning scaling new relationships low cost dataset models available http //data.allenai.org/quarel	negative
hyperbolic heterogeneous information network embedding	heterogeneous information network hin embedding aiming project hin low-dimensional space attracted considerable research attention exiting hin embedding methods focus preserving inherent network structure semantic correlations euclidean spaces however one fundamental problem whether euclidean spaces appropriate intrinsic isometric spaces hin recent researches argue complex network may hyperbolic geometry underneath underlying hyperbolic geometry naturally reflect properties complex network e.g. hierarchical power-law structure paper make first effort toward hin embedding hyperbolic spaces analyze structures two real-world hins discover properties e.g. power-law distribution also exist hin therefore propose novel hyperbolic heterogeneous information network embedding model specifically capture structure semantic relations nodes employ meta-path guided random walk sample sequences node exploit distance hyperbolic spaces proximity measurement hyperbolic distance able meet triangle inequality well preserve transitivity hin model enables nodes neighborhoods small hyperbolic distances derive effective optimization strategy update hyperbolic embeddings iteratively experimental results comparison state-of-the-art demonstrate proposed model superior performance network reconstruction link prediction tasks also shows ability capture hierarchy structure hin via visualization	negative
deep recurrent survival analysis	survival analysis hotspot statistical research modeling time-to-event information data censorship handling widely used many applications clinical research information system fields survivorship bias many works proposed survival analysis ranging traditional statistic methods machine learning models however existing methodologies either utilize counting-based statistics segmented data pre-assumption event probability distribution w.r.t time moreover works consider sequential patterns within feature space paper propose deep recurrent survival analysis model combines deep learning conditional probability prediction finegrained level data survival analysis tackling censorship capturing time dependency modeling conditional probability event sample method predicts likelihood true event occurrence estimates survival rate time i.e. probability non-occurrence event censored data meanwhile without assuming specific form event probability distribution model shows great advantages previous works fitting various sophisticated data distributions experiments three realworld tasks different fields model significantly outperforms state-of-the-art solutions various metrics	negative
active sampling for open-set classification without initial annotation	open-set classification common problem many real world tasks data collected known classes novel classes occur test stage paper focus challenging case data examples collected known classes unlabeled due high cost label annotation rather important train model least labeled data accurate classification known classes effective detection novel classes firstly propose active learning method incorporating structured sparsity diversity select representative examples annotation latent low-rank representation employed simultaneously perform classification novel class detection also method along fast optimization solution extended multi-stage scenario classes occur disappear batches stage experimental results multiple datasets validate superiority proposed method regard different performance measures	negative
entity alignment between knowledge graphs using attribute embeddings	task entity alignment knowledge graphs aims find entities two knowledge graphs represent real-world entity recently embedding-based models proposed task models built top knowledge graph embedding model learns entity embeddings capture semantic similarity entities knowledge graph propose learn embeddings capture similarity entities different knowledge graphs proposed model helps align entities different knowledge graphs hence enables integration multiple knowledge graphs model exploits large numbers attribute triples existing knowledge graphs generates attribute character embeddings attribute character embedding shifts entity embeddings two knowledge graphs space computing similarity entities based attributes use transitivity rule enrich number attributes entity enhance attribute character embedding experiments using real-world knowledge bases show proposed model achieves consistent improvements baseline models 50 terms hits 1 entity alignment task	negative
geometric multi-model fitting by deep reinforcement learning	paper deals geometric multi-model fitting noisy unstructured point set data e.g. laser scanned point clouds formulate multi-model fitting problem sequential decision making process use deep reinforcement learning algorithm learn optimal decisions towards best fitting result paper compared method state-of-the-art simulated data results demonstrated approach significantly reduced number fitting iterations	positive
composite binary decomposition networks	binary neural networks great resource computing efficiency suffer long training procedure non-negligible accuracy drops comparing fullprecision counterparts paper propose composite binary decomposition networks cbdnet first compose real-valued tensor layer limited number binary tensors decompose conditioned binary tensors two low-rank binary tensors number parameters operations greatly reduced comparing original ones experiments demonstrate effectiveness proposed method cbdnet approximate image classification network resnet-18 using 5.25 bits vgg-16 using 5.47 bits densenet-121 using 5.72 bits object detection networks ssd300 using 4.38 bits semantic segmentation networks segnet using 5.18 bits minor accuracy drops.1	positive
3d volumetric modeling with introspective neural networks	paper study 3d volumetric modeling problem adopting wasserstein introspective neural networks method winn previously applied 2d static images name algorithm 3dwinn enjoys properties winn 2d case simultaneously generative discriminative compared existing 3d volumetric modeling approaches 3dwinn demonstrates competitive results several benchmarks generation classification tasks addition standard inception score frechet inception distance fid metric is´ also adopted measure quality 3d volumetric generations addition study adversarial attacks volumetric data demonstrate robustness 3dwinn adversarial examples achieving appealing results classification generation within single model 3dwinn general framework applied emerging tasks 3d object scene modeling.1	positive
learning competitive and discriminative reconstructions for anomaly detection	existing methods anomaly detection use positive data learn data distribution thus usually need pre-defined threshold detection stage determine whether test instance outlier unfortunately good threshold vital performance really hard find optimal one paper take discriminative information implied unlabeled data consideration propose new method anomaly detection learn labels unlabelled data directly proposed method end-to-end architecture one encoder two decoders trained model inliers outliers ’ data distributions competitive way architecture works discriminative manner without suffering overfitting training algorithm model adopted sgd thus efficient scalable even large-scale datasets empirical studies 7 datasets including kdd99 mnist caltech-256 imagenet etc show model outperforms state-of-the-art methods	negative
partial label learning with self-guided retraining	partial label learning deals problem training instance assigned set candidate labels one correct paper provides first attempt leverage idea self-training dealing partially labeled examples specifically propose unified formulation proper constraints train desired model perform pseudo-labeling jointly pseudo-labeling unlike traditional self-training manually differentiates ground-truth label enough high confidence introduce maximum infinity norm regularization modeling outputs automatically achieve consideratum results convex-concave optimization problem show optimizing convex-concave problem equivalent solving set quadratic programming qp problems proposing upper-bound surrogate objective function turn solving one qp problem improving optimization efficiency extensive experiments synthesized real-world datasets demonstrate proposed approach significantly outperforms state-of-the-art partial label learning approaches	positive
unsupervised bilingual lexicon induction from mono-lingual multimodal data	bilingual lexicon induction translating words source language target language long-standing natural language processing task recent endeavors prove promising employ images pivot learn lexicon induction without reliance parallel corpora however vision-based approaches simply associate words entire images constrained translate concrete words require object-centered images humans understand words better within sentence context therefore paper propose utilize images associated captions address limitations previous approaches propose multi-lingual caption model trained different mono-lingual multimodal data map words different languages joint spaces two types word representation induced multi-lingual caption model linguistic features localized visual features linguistic feature learned sentence contexts visual semantic constraints beneficial learn translation words less visual-relevant localized visual feature attended region image correlates word alleviates image restriction salient visual representation two types features complementary word translation experimental results multiple language pairs demonstrate effectiveness proposed method substantially outperforms previous vision-based approaches without using parallel sentences supervision seed word pairs	positive
sliding window temporal graph coloring	graph coloring one famous computational problems applications wide range areas planning scheduling resource allocation pattern matching far coloring problems mostly studied static graphs often stand stark contrast practice data inherently dynamic subject discrete changes time temporal graph graph whose edges assigned set integer time labels indicating discrete time steps edge active paper present natural temporal extension classical graph coloring problem given temporal graph natural number ∆ ask coloring sequence vertex every sliding time window ∆ consecutive time steps edge active edge properly colored i.e endpoints assigned two different colors least time window ii total number different colors minimized sliding window temporal coloring problem abstractly captures many realistic graph coloring scenarios underlying network changes time dynamically assigning communication channels moving agents present thorough investigation computational complexity temporal coloring problem specifically prove strong computational hardness results complemented efficient exact approximation algorithms algorithms linear-time fixed-parameter tractable respect appropriate parameters others asymptotically almost optimal exponential time hypothesis eth	positive
unseen word representation by aligning heterogeneous lexical semantic spaces	word embedding techniques heavily rely abundance training data individual words given zipfian distribution words natural language texts large number words usually appear frequently training data paper put forward technique exploits knowledge encoded lexical resources wordnet induce embeddings unseen words approach adapts graph embedding cross-lingual vector space transformation techniques order merge lexical knowledge encoded ontologies derived corpus statistics show approach provide consistent performance improvements across multiple evaluation benchmarks in-vitro multiple rare word similarity datasets invivo two downstream text classification tasks	negative
adversarial dropout for recurrent neural networks	successful application processing sequential data text speech requires improved generalization performance recurrent neural networks rnns dropout techniques rnns introduced respond demands conjecture dropout rnns could improved adopting adversarial concept paper investigates ways improve dropout rnns utilizing intentionally generated dropout masks specifically guided dropout used research called adversarial dropout adversarially disconnects neurons dominantly used predict correct targets time analysis showed regularizer consists gap original reconfigured rnns upper bound gap training inference phases random dropout demonstrated minimizing regularizer improved effectiveness dropout rnns sequential mnist tasks semi-supervised text classification tasks language modeling tasks	negative
submodular optimization over streams with inhomogeneous decays	cardinality constrained submodular function maximization aims select subset size k maximize monotone submodular utility function key many data mining machine learning applications data summarization maximum coverage problems data given stream streaming submodular optimization sso techniques desired existing sso techniques apply insertion-only streams element infinite lifespan sliding-window streams element lifespan i.e. window size however elements data streams may arbitrary different lifespans requires addressing sso streams inhomogeneous-decays sso-id work formulates sso-id problem presents three algorithms basic-streaming basic streaming algorithm achieves 1/2 − ɛ approximation factor histapprox improves efficiency significantly achieves 1/3 − ɛ approximation factor histstreaming streaming version histapprox uses heuristics improve efficiency experiments conducted real data demonstrate histstreaming find high quality solutions two orders magnitude faster naive greedy algorithm	positive
symmetrization for embedding directed graphs	paper propose solve directed graph embedding problem via two stage approach first stage graph symmetrized one several possible ways second stage so-obtained symmetrized graph embeded using state-of-the-art undirected graph embedding algorithm note objective paper propose new undirected graph embedding algorithm discuss strengths weaknesses existing ones saying whichever suitable graph embedding algorithm fit proposed symmetrization framework	positive
talking face generation by adversarially disentangled audio-visual representation	talking face generation aims synthesize sequence face images correspond clip speech challenging task face appearance variation semantics speech coupled together subtle movements talking face regions existing works either construct specific face appearance model specific subjects model transformation lip motion speech work integrate aspects enable arbitrary-subject talking face generation learning disentangled audio-visual representation find talking face sequence actually composition subject-related information speech-related information two spaces explicitly disentangled novel associative-and-adversarial training process disentangled representation advantage audio video serve inputs generation extensive experiments show proposed approach generates realistic talking face sequences arbitrary subjects much clearer lip motion patterns previous work also demonstrate learned audio-visual representation extremely useful tasks automatic lip reading audio-video retrieval	negative
gaitset: regarding gait as a set for cross-view gait recognition	unique biometric feature recognized distance gait broad applications crime prevention forensic identification social security portray gait existing gait recognition methods utilize either gait template temporal information hard preserve gait sequence must keep unnecessary sequential constraints thus loses flexibility gait recognition paper present novel perspective gait regarded set consisting independent frames propose new network named gaitset learn identity information set based set perspective method immune permutation frames naturally integrate frames different videos filmed different scenarios diverse viewing angles different clothes/carrying conditions experiments show normal walking conditions single-model method achieves average rank-1 accuracy 95.0 casia-b gait dataset 87.1 accuracy ou-mvlp gait dataset results represent new state-of-the-art recognition accuracy various complex scenarios model exhibits significant level robustness achieves accuracies 87.2 70.4 casia-b bag-carrying coat-wearing walking conditions respectively outperform existing best methods large margin method presented also achieve satisfactory accuracy small number frames test sample e.g. 82.5 casia-b 7 frames source code released https //github.com/abnerhqc/gaitset	negative
on limited conjunctions and partial features in parameter-tractable feature logics	standard reasoning problems complete exptime common feature-based description logics—ones roles restricted functions show control conjunctions left-hand-sides subsumptions use restriction develop parameter-tractable algorithm reasoning knowledge base consistency show resulting logic simulate partial features present algorithms efficient query answering setting	negative
a sequential set generation method for predicting set-valued outputs	consider general machine learning setting output set labels sequences output set unordered size varies input whereas multi-label classification methods seem natural first resort readily applicable set-valued outputs growth rate output space conventional sequence generation ’ reflect sets ’ order-free nature paper propose unified framework—sequential set generation ssg —that handle output sets labels sequences ssg meta-algorithm leverages probabilistic learning method label sequence prediction employs proper regularization new label sequence generated repeatedly full set produced though ssg sequential nature penalize ordering appearance set elements applied variety set output problems set classification labels sequences perform experiments benchmark synthetic data sets demonstrate ssg ’ strong performance baseline methods	positive
block belief propagation for parameter learning in markov random fields	traditional learning methods training markov random fields require inference variables compute likelihood gradient iteration complexity methods therefore scales size graphical models paper propose block belief propagation learning bbpl uses block-coordinate updates approximate marginals compute approximate gradients removing need compute inference entire graphical model thus iteration complexity bbpl scale size graphs prove method converges solution obtained using full inference per iteration despite approximations empirically demonstrate scalability improvements standard training methods	negative
efficient image retrieval via decoupling diffusion into online and offline processing	diffusion commonly used ranking re-ranking method retrieval tasks achieve higher retrieval performance attracted lots attention recent years downside diffusion performs slowly comparison naive k-nn search causes non-trivial online computational cost large datasets overcome weakness propose novel diffusion technique paper work instead applying diffusion query precompute diffusion results element database making online search simple linear combination top k-nn search process proposed method becomes 10∼ times faster terms online search speed moreover propose use late truncation instead early truncation previous works achieve better retrieval performance	negative
meta learning for image captioning	reinforcement learning rl shown advantages image captioning optimizing non-differentiable metric directly reward learning process however due reward hacking problem rl maximizing reward may lead better quality caption especially aspects propositional content distinctiveness work propose use new learning method meta learning utilize supervision ground truth whilst optimizing reward function rl improve propositional content distinctiveness generated captions proposed model provides global optimal solution taking different gradient steps towards supervision task reinforcement task simultaneously experimental results ms coco validate effectiveness approach compared state-of-the-art methods	negative
learning to transfer relational representations through analogy	propose novel approach learn representations relations expressed textual mentions assumption two pairs entities belong relation two pairs analogous collect large set analogous pairs matching triples knowledge bases web-scale corpora distant supervision dataset adopted train hierarchical siamese network order learn entity-entity embeddings encode relational information different linguistic paraphrasing expressing relation model used generate pre-trained embeddings provide valuable signal integrated existing neural-based model outperforming state-of-the-art methods relation extraction task	positive
antonym-synonym classification based on new sub-space embeddings	distinguishing antonyms synonyms key challenge many nlp applications focused lexical-semantic relation extraction existing solutions relying large-scale corpora yield low performance huge contextual overlap antonym synonym pairs propose novel approach entirely based pre-trained embeddings hypothesize pre-trained embeddings comprehend blend lexical-semantic information may distill task-specific information using distiller model proposed paper later classifier trained based features constructed distilled sub-spaces along word level features distinguish antonyms synonyms experimental results show proposed model outperforms existing research antonym synonym distinction speed performance	negative
scfont: structure-guided chinese font generation via deep stacked networks	automatic generation chinese fonts consist large numbers glyphs complicated structures still challenging ongoing problem areas ai computer graphics cg traditional cg-based methods typically rely heavily manual interventions recentlypopularized deep learning-based end-to-end approaches often obtain synthesis results incorrect structures and/or serious artifacts address problems paper proposes structure-guided chinese font generation system scfont using deep stacked networks key idea integrate domain knowledge chinese characters deep generative networks ensure high-quality glyphs correct structures synthesized specifically first apply cnn model learn transfer writing trajectories separated strokes reference font style target style train another cnn model learning recover shape details contour synthesized writing trajectories experimental results validate superiority proposed scfont compared state art visual quantitative assessments	negative
a bandit approach to maximum inner product search	substantial research sub-linear time approximate algorithms maximum inner product search mips achieve fast query time state-of-the-art techniques require significant preprocessing burden number subsequent queries sufficiently large amortize cost furthermore existing methods ability directly control suboptimality approximate results theoretical guarantees paper propose first approximate algorithm mips require preprocessing allows users control bound suboptimality results cast mips best arm identification problem introduce new bandit setting fully exploit special structure mips approach outperforms state-of-the-art methods synthetic real-world datasets	negative
exploiting local feature patterns for unsupervised domain adaptation	unsupervised domain adaptation methods aim alleviate performance degradation caused domain-shift learning domain-invariant representations existing deep domain adaptation methods focus holistic feature alignment matching source target holistic feature distributions without considering local features multi-mode statistics show learned local feature patterns generic transferable local feature distribution matching enables fine-grained feature alignment paper present method learning domain-invariant local feature patterns jointly aligning holistic local feature statistics comparisons state-of-the-art unsupervised domain adaptation methods two popular benchmark datasets demonstrate superiority approach effectiveness alleviating negative transfer	negative
dynamic vehicle traffic control using deep reinforcement learning in automated material handling system	automated material handling systems amhs delivery time important issue directly associated production cost quality product paper propose dynamic routing strategy shorten delivery time delay set target control analyzing traffic flows selecting region highest flow rate congestion frequency impose routing cost order dynamically reflect real-time changes traffic states deep reinforcement learning model consists q-learning step recurrent neural network traffic states action values predicted experiment results show proposed method decreases manufacturing costs increasing productivity additionally find evidence reinforcement learning structure proposed study autonomously dynamically adjust changes traffic patterns	negative
learning neural bag-of-matrix-summarization with riemannian network	symmetric positive defined spd matrix attracted increasing research focus image/video analysis merits capturing riemannian geometry structured 2d feature representation however computation vector space spd matrices capture geometric properties corrupts classification performance end riemannian based deep network become promising solution spd matrix classification excellence performing non-linear learning spd matrix besides riemannian metric learning typically adopts knn classifier extended large-scale datasets limits application many time-efficient scenarios paper propose bag-of-matrix-summarization boms method combined riemannian network handles issues towards highly efficient scalable spd feature representation key innovation lies idea summarizing data riemannian geometric space instead vector space first whole training set compressed small number matrix features ensure high scalability second given compressed set constant-length vector representation extracted efficiently measuring distribution variations summarized data latent feature riemannian network finally proposed boms descriptor integrated riemannian network upon whole framework end-to-end trained via matrix back-propagation experiments four different classification tasks demonstrate superior performance proposed method state-of-the-art methods	negative
attentive tensor product learning	paper proposes novel neural architecture — attentive tensor product learning atpl — represent grammatical structures natural language deep learning models atpl exploits tensor product representations tpr structured neural-symbolic model developed cognitive science integrate deep learning explicit natural language structures rules key ideas atpl 1 unsupervised learning role-unbinding vectors words via tpr-based deep neural network 2 use attention modules compute tpr 3 integration tpr typical deep learning architectures including long short-term memory feedforward neural networks novelty approach lies ability extract grammatical structure sentence using role-unbinding vectors obtained unsupervised manner atpl approach applied 1 image captioning 2 part speech pos tagging 3 constituency parsing natural language sentence experimental results demonstrate effectiveness proposed approach three natural language processing tasks	positive
sparse reject option classifier using successive linear programming	paper propose approach learning sparse reject option classifiers using double ramp loss ldr use dc programming find risk minimizer algorithm solves sequence linear programs learn reject option classifier show loss ldr fisher consistent also show excess risk loss ld upper bounded excess risk ldr derive generalization error bounds proposed approach show effectiveness proposed approach experimenting several real world datasets proposed approach performs comparable state art also successfully learns sparse classifiers	positive
deep single-view 3d object reconstruction with visual hull embedding	3d object reconstruction fundamental task many robotics ai problems aid deep convolutional neural networks cnns 3d object reconstruction witnessed significant progress recent years however possibly due prohibitively high dimension 3d object space results deep cnns often prone missing shape details paper present approach aims preserve shape details improve reconstruction quality key idea method leverage object mask pose estimation cnns assist 3d shape learning constructing probabilistic singleview visual hull inside network method works first predicting coarse shape well object pose silhouette using cnns followed novel 3d refinement cnn refines coarse shapes using constructed probabilistic visual hulls experiment synthetic data real images show embedding single-view visual hull shape refinement significantly improve reconstruction quality recovering shapes details improving shape consistency input image	positive
transgate: knowledge graph embedding with shared gate structure	embedding knowledge graphs kgs continuous vector space essential problem knowledge extraction current models continue improve embedding focusing discriminating relation-specific information entities increasingly complex feature engineering noted ignored inherent relevance relations tried learn unique discriminate parameter set relation thus models potentially suffer high time complexity large parameters preventing efficiently applying real-world kgs paper follow thought parameter sharing simultaneously learn expressive features reduce parameters avoid complex feature engineering based gate structure lstm propose novel model transgate develop shared discriminate mechanism resulting almost space complexity indiscriminate models furthermore develop effective scalable model reconstruct gate weight vectors making method comparative time complexity indiscriminate model conduct extensive experiments link prediction triplets classification experiments show transgate outperforms state-of-art baselines also reduces parameters greatly example transgate outperforms conve rgcn 6x 17x fewer parameters respectively results indicate parameter sharing superior way optimize embedding transgate finds better trade-off complexity expressivity	negative
task transfer by preference-based cost learning	goal task transfer reinforcement learning migrating action policy agent target task source task given successes robotic action planning current methods mostly rely two requirements exactlyrelevant expert demonstrations explicitly-coded cost function target task however inconvenient obtain practice paper relax two strong conditions developing novel task transfer framework expert preference applied guidance particular alternate following two steps firstly letting experts apply pre-defined preference rules select related expert demonstrates target task secondly based selection result learn target cost function trajectory distribution simultaneously via enhanced adversarial maxent irl generate trajectories learned target distribution next preference selection theoretical analysis distribution learning convergence proposed algorithm provided extensive simulations several benchmarks conducted verifying effectiveness proposed method	positive
disjoint label space transfer learning with common factorised space	paper unified approach presented transfer learning addresses several source target domain labelspace annotation assumptions single model particularly effective handling challenging case source target label-spaces disjoint outperforms alternatives unsupervised semi-supervised settings key ingredient common representation termed common factorised space shared source target domains trained unsupervised factorisation loss graph-based loss wide range experiments demonstrate flexibility relevance efficacy method challenging cases disjoint label spaces conventional cases unsupervised domain adaptation source target domains share label-sets	positive
transductive zero-shot learning via visual center adaptation	paper propose visual center adaptation method vcam address domain shift problem zero-shot learning seen classes training data vcam builds embedding space learning mapping semantic space visual centers unseen classes test data construction embedding space constrained symmetric chamfer-distance term aiming adapt distribution synthetic visual centers real cluster centers therefore learned embedding space generalize unseen classes well experiments two widely used datasets demonstrate model significantly outperforms state-of-the-art methods	positive
towards highly accurate and stable face alignment for high-resolution videos	recent years heatmap regression based models shown effectiveness face alignment pose estimation however conventional heatmap regression chr accurate stable dealing high-resolution facial videos since finds maximum activated location heatmaps generated rounding coordinates thus leads quantization errors scaling back original high-resolution space paper propose fractional heatmap regression fhr high-resolution video-based face alignment proposed fhr accurately estimate fractional part according 2d gaussian function sampling three points heatmaps stabilize landmarks among continuous video frames maintaining precise time propose novel stabilization loss contains two terms address time delay non-smooth issues respectively experiments 300w 300vw talking face datasets clearly demonstrate proposed method accurate stable state-ofthe-art models	negative
natural option critic	recently proposed option-critic architecture bacon harb precup 2017 provides stochastic policy gradient approach hierarchical reinforcement learning specifically provides way estimate gradient expected discounted return respect parameters define finite number temporally extended actions called options paper show option-critic architecture extended estimate natural gradient amari 1998 expected discounted return end central questions consider paper 1 definition natural gradient context 2 fisher information matrix associated option ’ parameterized policy 3 fisher information matrix associated option ’ parameterized termination function 4 compatible function approximation approach leveraged obtain natural gradient estimates parameterized policy parameterized termination functions option per-time-step time space complexity linear total number parameters based answers questions introduce natural option critic algorithm experimental results showcase improvement vanilla gradient approach	negative
task embedded coordinate update: a realizable framework for multivariate non-convex optimization	paper propose realizable framework tecu embeds task-specific strategies update schemes coordinate descent optimizing multivariate non-convex problems coupled objective functions one hand tecu capable improving algorithm efficiencies embedding productive numerical algorithms optimizing univariate sub-problems nice properties side also augments probabilities receive desired results embedding advanced techniques optimizations realistic tasks integrating numerical algorithms advanced techniques together tecu proposed unified framework solving class non-convex problems although task embedded strategies bring inaccuracies sub-problem optimizations provide realizable criterion control errors meanwhile ensure robust performances rigid theoretical analyses respectively embedding admm residual-type cnn algorithm framework experimental results verify efficiency effectiveness embedding task-oriented strategies coordinate descent solving practical problems	positive
atp: directed graph embedding with asymmetric transitivity preservation	directed graphs widely used community question answering services cqas model asymmetric relationships among different types nodes cqa graphs e.g. question answer user asymmetric transitivity essential property directed graphs since play important role downstream graph inference analysis question difficulty user expertise follow characteristic asymmetric transitivity maintaining properties reducing graph lower dimensional vector embedding space focus much recent research paper tackle challenge directed graph embedding asymmetric transitivity preservation leverage proposed embedding method solve fundamental task cqas appropriately route assign newly posted questions users suitable expertise interest cqas technique incorporates graph hierarchy reachability information naturally relying nonlinear transformation operates core reachability implicit hierarchy within graphs subsequently methodology levers factorization-based approach generate two embedding vectors node within graph capture asymmetric transitivity extensive experiments show framework consistently significantly outperforms state-of-the-art baselines three diverse realworld tasks link prediction question difficulty estimation expert finding online forums like stack exchange particularly framework support inductive embedding learning newly posted questions unseen nodes training therefore properly route assign kinds questions experts cqas	negative
scnn: a general distribution based statistical convolutional neural network with application to video object detection	various convolutional neural networks cnns developed recently achieved accuracy comparable human beings computer vision tasks image recognition object detection tracking etc networks however process one single frame image time may fully utilize temporal contextual correlation typically present multiple channels image adjacent frames video thus limiting achievable throughput limitation stems fact existing cnns operate deterministic numbers paper propose novel statistical convolutional neural network scnn extends existing cnn architectures operates directly correlated distributions rather deterministic numbers introducing parameterized canonical model model correlated data defining corresponding operations required cnn training inference show scnn process multiple frames correlated images effectively hence achieving significant speedup existing cnn models use cnn based video object detection example illustrate usefulness proposed scnn general network model experimental results show even nonoptimized implementation scnn still achieve 178 speedup existing cnns slight accuracy degradation	negative
deep neural network quantization via layer-wise optimization using limited training data	advancement deep models poses great challenges real-world deployment limited computational ability storage space edge devices solve problem existing works made progress prune quantize deep models however existing methods rely heavily supervised training process achieve satisfactory performance acquiring large amount labeled training data may practical real deployment paper propose novel layer-wise quantization method deep neural networks requires limited training data 1 original dataset specifically formulate parameters quantization layer discrete optimization problem solve using alternative direction method multipliers admm gives efficient closed-form solution prove final performance drop quantization bounded linear combination reconstructed errors caused layer based proved theorem propose algorithm quantize deep neural network layer layer additional weights update step minimize final error extensive experiments benchmark deep models conducted demonstrate effectiveness proposed method using 1 cifar10 imagenet datasets codes available https //github.com/csyhhu/l-dnq	negative
coreset stochastic variance-reduced gradient with application to optimal margin distribution machine	major problem kernel-based predictors prohibitive computational complexity limits application large-scale datasets coreset approximation method tries cover given examples small set points used remain prominent information accelerate kernel method paper provide perhaps first coreset-based kernel-accelerating optimization method linear convergence rate much faster existing approaches method used train kernel svm-style problems obtain sparse solutions efficiently specifically method uses svrg framework utilizes core points approximate gradients significantly reduce complexity kernel method furthermore apply method train odm kernel machine enjoying better statistical property svm reduce risk compromising performance encouraging sparsity conduct extensive experiments several large-scale datasets results verify method outperforms state-of-the-art coreset approximation method efficiency generalization simultaneously achieving significant speed-up compared non-approximation baselines	negative
a neural network approach for birds of a feather solvability prediction	birds feather single player perfect information card game game multiple board sizes larger boards introducing larger search spaces grow exponentially paper investigate solvability game aiming building machine learning method automatically classify whether given board state solution path propose method based image-based features board state deep neural network experimental results show proposed method make reasonable predictions solvability game arbitrary stage game	positive
on sampling complexity of the semidefinite affine rank feasibility problem	paper study semidefinite affine rank feasibility problem consists finding positive semidefinite matrix given rank linear measurements consider semidefinite programming relaxations problem different objective functions study properties particular propose analytical bound number relaxations sufficient solve order obtain solution generic instance semidefinite affine rank feasibility problem prove solution followed heuristic algorithm based semidefinite relaxation experimental proof performance large sample synthetic data	positive
mixup as locally linear out-of-manifold regularization	mixup zhang et al 2017 recently proposed dataaugmentation scheme linearly interpolates random pair training examples correspondingly one-hot representations labels training deep neural networks additional data shown capable significantly improving predictive accuracy current art power mixup however primarily established empirically working effectiveness explained depth paper develop understanding mixup form “ out-of-manifold regularization ” imposes certain “ local linearity ” constraints model ’ input space beyond data manifold analysis enables us identify limitation mixup call “ manifold intrusion ” nutshell manifold intrusion mixup form under-fitting resulting conflicts synthetic labels mixed-up examples labels original training data phenomenon usually happens parameters controlling generation mixing policies sufficiently fine-tuned training data address issue propose novel adaptive version mixup mixing policies automatically learned data using additional network objective function designed avoid manifold intrusion proposed regularizer adamixup empirically evaluated several benchmark datasets extensive experiments demonstrate adamixup improves upon mixup applied current art deep classification models	negative
tagging address queries in maps search	map search major vertical popular search engines also plays important role personal assistants mobile home desktop devices significant fraction map search traffic comprised “ address queries ” queries either entire query terms refer address part address road segment intersection etc. demonstrate correctly understanding tagging address queries critical map search engines fulfill describe several recurrent sequence architectures tagging queries compare performance two subcategories address queries single entity aka single point addresses multi entity aka multi point addresses finish providing guidance best practices dealing subcategories	negative
location-based end-to-end speech recognition with multiple language models	end-to-end deep learning approaches automatic speech recognition asr new trend approaches starting active many areas language model considered important effective method semantic error correction many existing systems use one language model paper however multiple language models lms applied decoding one lm used selecting appropriate answers others considering context grammar decision experiment general location-based dataset show effectiveness method	negative
finding all bayesian network structures within a factor of optimal	bayesian network widely used probabilistic graphical model applications knowledge discovery prediction learning bayesian network bn data cast optimization problem using well-known score-andsearch approach however selecting single model i.e. best scoring bn misleading may achieve best possible accuracy alternative committing single model perform form bayesian frequentist model averaging space possible bns sampled enumerated fashion unfortunately existing approaches model averaging either severely restrict structure bayesian network shown scale networks fewer 30 random variables paper propose novel approach model averaging inspired performance guarantees approximation algorithms approach two primary advantages first approach considers credible models optimal near-optimal score second approach efficient scales significantly larger bayesian networks existing approaches	positive
learning transferable self-attentive representations for action recognition in untrimmed videos with weak supervision	action recognition videos attracted lot attention past decade order learn robust models previous methods usually assume videos trimmed short sequences require ground-truth annotations video frame/sequence quite costly time-consuming paper given video-level annotations propose novel weakly supervised framework simultaneously locate action frames well recognize actions untrimmed videos proposed framework consists two major components first action frame localization take advantage self-attention mechanism weight frame influence background frames effectively eliminated second considering trimmed videos publicly available also contain useful information leverage present additional module transfer knowledge trimmed videos improving classification performance untrimmed ones extensive experiments conducted two benchmark datasets i.e. thumos14 activitynet1.3 experimental results clearly corroborate efficacy method	negative
resisting adversarial attacks using gaussian mixture variational autoencoders	susceptibility deep neural networks adversarial attacks poses major theoretical practical challenge efforts harden classifiers attacks seen limited success till two distinct categories samples deep neural networks vulnerable “ adversarial samples ” “ fooling samples ” tackled separately far due difficulty posed considered together work show one defend unified framework model form variational autoencoder gaussian mixture prior latent variable mixture component corresponds single class show selective classification performed using model thereby causing adversarial objective entail conflict proposed method leads rejection adversarial samples instead misclassification maintaining high precision recall test data also inherently provides way learning selective classifier semi-supervised scenario similarly resist adversarial attacks show one reclassify detected adversarial samples iterative optimization.1	negative
spatio-temporal graph routing for skeleton-based action recognition	representation effectiveness skeleton-based human action recognition received considerable research attention wide range real applications area many existing methods typically rely fixed physicalconnectivity skeleton structure recognition incapable well capturing intrinsic high-order correlations among skeleton joints paper propose novel spatio-temporal graph routing stgr scheme skeletonbased action recognition adaptively learns intrinsic high-order connectivity relationships physicallyapart skeleton joints specifically scheme composed two components spatial graph router sgr temporal graph router tgr sgr aims discover connectivity relationships among joints based sub-group clustering along spatial dimension tgr explores structural information measuring correlation degrees temporal joint node trajectories proposed scheme naturally seamlessly incorporated framework graph convolutional networks gcns produce set skeleton-joint-connectivity graphs fed classification networks moreover insightful analysis receptive field graph node provided explain necessity method experimental results two benchmark datasets ntu-rgb+d kinetics demonstrate effectiveness state-of-the-art	negative
the level weighted structural similarity loss: a step away from mse	mean square error mse shown strength applied deep generative models auto-encoders model reconstruction loss however image domain especially limitation mse obvious assumes pixel independence ignores spatial relationships samples contradicts architectures auto-encoders use convolutional layers extract spatial dependent features base structural similarity metric ssim propose novel level weighted structural similarity lwssim loss convolutional auto-encoders experiments common datasets various auto-encoder variants show loss able outperform mse loss vanilla ssim loss also provide reasons model able succeed cases standard ssim loss fails	negative
adversarial actor-critic method for task and motion planning problems using planning experience	propose actor-critic algorithm uses past planning experience improve efficiency solving robot task-and-motion planning tamp problems tamp planners search goal-achieving sequences high-level operator instances specified discrete continuous parameters algorithm learns policy selecting continuous parameters search using small training set generated search trees previously solved instances also introduce novel fixed-length vector representation world states varying numbers objects different shapes based set key robot configurations demonstrate experimentally method learns efficiently less data standard reinforcementlearning approaches using learned policy guide planner results improvement planning efficiency	positive
adversarial learning of semantic relevance in text to image synthesis	describe new approach improves training generative adversarial nets gans synthesizing diverse images text input approach based conditional version gans expands previous work leveraging auxiliary task discriminator generated images limited certain classes suffer mode collapse semantically matching text input key training methods form positive negative training examples respect class label given image instead selecting random training examples perform negative sampling based semantic distance positive example class evaluate approach using oxford-102 flower dataset adopting inception score multi-scale structural similarity index ms-ssim metrics assess discriminability diversity generated images empirical results indicate greater diversity generated images especially gradually select negative training examples closer positive example semantic space	negative
sequence to sequence learning for query expansion	fas aware using sequence sequence algorithms query expansion explored yet information retrieval literature tried fill gap literature custom query expansion system trained tested open datasets one specificity engine compared classic ones need documents expand introduced query test expansions two different tasks information retrieval answer preselection method yielded slight improvement performance two tasks main contributions	negative
attention-based multi-context guiding for few-shot semantic segmentation	few-shot learning nascent research topic motivated fact traditional deep learning methods require tremendous amounts data scarcity annotated data becomes even challenging semantic segmentation since pixellevel annotation segmentation task labor-intensive acquire tackle issue propose attentionbased multi-context guiding a-mcg network consists three branches support branch query branch feature fusion branch key differentiator a-mcg integration multi-scale context features support query branches enforcing better guidance support set addition also adopt spatial attention along fusion branch highlight context information several scales enhancing self-supervision one-shot learning address fusion problem multi-shot learning conv-lstm adopted collaboratively integrate sequential support features elevate final accuracy architecture obtains state-of-the-art unseen classes variant pascal voc12 dataset performs favorably previous work large gains 1.1 1.4 measured miou 1-shot 5-shot setting	negative
a pattern-based approach to recognizing time expressions	recognizing time expressions fundamental important task many applications natural language understanding reading comprehension question answering several newest state-of-the-art approaches achieved good performance recognizing time expressions approaches black-boxed based heuristic rules leads difficulty understanding temporal information contrary classic rule-based semantic parsing approaches capture rich structural information performances recognition good paper propose pattern-based approach called ptime automatically generates selects patterns recognizing time expressions approach time expressions training text abstracted type sequences using fine-grained token types thus problem transformed select appropriate subset sequential patterns use extended budgeted maximum coverage ebmc model optimize pattern selection main idea maximize correct token sequences matched selected patterns number mistakes limited adjustable budget interpretability patterns adjustability permitted number mistakes make ptime promising approach many applications experimental results show ptime achieves competitive performance compared existing state-of-the-art approaches	negative
revisiting lstm networks for semi-supervised text classification via mixed objective function	paper study bidirectional lstm network task text classification using supervised semisupervised approaches several prior works suggested either complex pretraining schemes using unsupervised methods language modeling dai le 2015 miyato dai goodfellow 2016 complicated models johnson zhang 2017 necessary achieve high classification accuracy however develop training strategy allows even simple bilstm model trained cross-entropy loss achieve competitive results compared complex approaches furthermore addition cross-entropy loss using combination entropy minimization adversarial virtual adversarial losses labeled unlabeled data report state-of-theart results text classification task several benchmark datasets particular acl-imdb sentiment analysis ag-news topic classification datasets method outperforms current approaches substantial margin also show generality mixed objective function improving performance relation extraction task.1	positive
bounding uncertainty for active batch selection	success batch mode active learning bmal methods lies selecting representative uncertain samples representative samples quickly capture global structure whole dataset uncertain ones refine decision boundary two principles namely direct approach screening approach make trade-off representativeness uncertainty although widely used literature little known relationship two principles paper discover two approaches shortcomings initial stage bmal alleviate shortcomings bound certainty scores unlabeled samples directly combine lower-bounded certainty representativeness objective function additionally show two aforementioned approaches mathematically equivalent two special cases approach best knowledge first work tries generalize direct screening approaches objective function solved super-modularity optimization extensive experiments fifteen datasets indicate method significantly higher classification accuracy testing data latest state-of-the-art bmal methods also scales better even size unlabeled pool reaches 106	negative
self-supervised video representation learning with space-time cubic puzzles	self-supervised tasks colorization inpainting zigsaw puzzle utilized visual representation learning still images number labeled images limited absent recently worthwhile stream study extends video domain cost human labeling even expensive however existing methods still based 2d cnn architectures directly capture spatio-temporal information video applications paper introduce new self-supervised task called space-time cubic puzzles train 3d cnns using large scale video dataset task requires network arrange permuted 3d spatio-temporal crops completing space-time cubic puzzles network learns spatial appearance temporal relation video frames final goal experiments demonstrate learned 3d representation well transferred action recognition tasks outperforms state-of-the-art 2d cnn-based competitors ucf101 hmdb51 datasets	negative
learning to communicate and solve visual blocks-world tasks	study emergent communication speaker listener recurrent neural-network agents tasked cooperatively construct blocks-world target image sampled generative grammar blocks configurations speaker receives target image learns emit sequence discrete symbols fixed vocabulary listener learns construct blocks-world image choosing block placement actions function speaker ’ full utterance image ongoing construction contributions introduction task domain studying emergent communication challenging affords useful analyses emergent protocols b empirical comparison interpolation extrapolation performance training via supervised contextual bandit reinforcement learning c evidence emergence interesting linguistic properties rl agent protocol distinct two	negative
re2ema: regularized and reinitialized exponential moving average for target model update in object tracking	target model update plays important role visual object tracking however performing optimal model update challenging work propose achieve optimal target model learning transformation matrix last target model newly generated one results minimization objective objective exists two challenges first newly generated target model unreliable overcome problem propose impose penalty limit distance learned target model last one second time evolves decide whether last target model corrupted get dilemma propose reinitialization term besides control complexity transformation matrix also add regularizer find optimization formula ’ solution simplifications degenerates ema finally despite simplicity extensive experiments conducted several commonly used benchmarks demonstrate effectiveness proposed approach relatively long term scenarios	negative
projection convolutional neural networks for 1-bit cnns via discrete back propagation	advancement deep convolutional neural networks dcnns driven significant improvement accuracy recognition systems many computer vision tasks however practical applications often restricted resource-constrained environments paper introduce projection convolutional neural networks pcnns discrete back propagation via projection dbpp improve performance binarized neural networks bnns contributions paper include 1 first time projection function exploited efficiently solve discrete back propagation problem leads new highly compressed cnns termed pcnns 2 exploiting multiple projections learn set diverse quantized kernels compress full-precision kernels efficient way proposed previously 3 pcnns achieve best classification performance compared state-ofthe-art bnns imagenet cifar datasets	negative
unsupervised stylish image description generation via domain layer norm	existing works image description focus generating expressive descriptions works dedicated generating stylish e.g. romantic lyric etc descriptions suffer limited style variation content digression address limitations propose controllable stylish image description generation model learn generate stylish image descriptions related image content trained arbitrary monolingual corpus without collecting new paired image stylish descriptions moreover enables users generate various stylish descriptions plugging style-specific parameters include new styles existing model achieve capability via novel layer normalization layer design refer domain layer norm dln extensive experimental validation user study various stylish image description generation tasks conducted show competitive advantages proposed model	negative
no-reference image quality assessment with reinforcement recursive list-wise ranking	opinion-unaware no-reference image quality assessment nr-iqa methods received many interests recently require images subjective scores training unfortunately challenging task thus far opinion-unaware methods shown consistently better performance opinion-aware ones paper propose effective opinion-unaware nr-iqa method based reinforcement recursive list-wise ranking formulate nr-iqa recursive list-wise ranking problem aims optimize whole quality ordering directly training recursive ranking process modeled markov decision process mdp ranking list images constructed taking sequence actions refers selecting image specific position ranking list reinforcement learning adopted train model parameters ground-truth quality scores ranking lists necessary learning experimental results demonstrate superior performance approach compared existing opinion-unaware nr-iqa methods furthermore approach compete effective opinion-aware methods improves state-of-the-art 2 csiq benchmark outperforms compared opinion-aware models tid2013	negative
recurrent attention model for pedestrian attribute recognition	pedestrian attribute recognition predict attribute labels pedestrian surveillance images challenging task computer vision due poor imaging quality small training dataset observed many semantic pedestrian attributes recognised tend show spatial locality semantic correlations grouped previous works mostly ignore phenomenon inspired recurrent neural network rnn ’ super capability learning context correlations attention model ’ capability highlighting region interest feature map paper proposes end-to-end recurrent convolutional rc recurrent attention ra models complementary rc model mines correlations among different attribute groups convolutional lstm unit ra model takes advantage intra-group spatial locality inter-group attention correlation improve performance pedestrian attribute recognition ra method combines recurrent learning attention model highlight spatial position feature map mine attention correlations among different attribute groups obtain precise attention extensive empirical evidence shows recurrent model frameworks achieve state-of-the-art results based pedestrian attribute datasets i.e standard peta rap datasets	positive
regularizing neural machine translation by target-bidirectional agreement	although neural machine translation nmt achieved remarkable progress past several years nmt systems still suffer fundamental shortcoming sequence generation tasks errors made early generation process fed inputs model quickly amplified harming subsequent sequence generation address issue propose novel model regularization method nmt training aims improve agreement translations generated left-to-right l2r right-to-left r2l nmt decoders goal achieved introducing two kullback-leibler divergence regularization terms nmt training objective reduce mismatch output probabilities l2r r2l models addition also employ joint training strategy allow l2r r2l models improve interactive update process experimental results show proposed method significantly outperforms state-of-the-art baselines chinese-english english-german translation tasks	negative
read, watch, and move: reinforcement learning for temporally grounding natural language descriptions in videos	task video grounding temporally localizes natural language description video plays important role understanding videos existing studies adopted strategies sliding window entire video exhaustively ranking possible clip-sentence pairs presegmented video inevitably suffer exhaustively enumerated candidates alleviate problem formulate task problem sequential decision making learning agent regulates temporal grounding boundaries progressively based policy specifically propose reinforcement learning based framework improved multi-task learning shows steady performance gains considering additional supervised boundary information training proposed framework achieves state-of-the-art performance activitynet ’ 18 densecaption dataset krishna et al 2017 charades-sta dataset sigurdsson et al 2016 gao et al 2017 observing 10 less clips per video	negative
a task in a suit and a tie: paraphrase generation with semantic augmentation	paraphrasing rooted semantics show effectiveness transformers vaswani et al 2017 paraphrase generation improvements incorporating propbank labels via multi-encoder evaluating mscoco wikianswers find transformers fast effective semantic augmentation transformers lstms leads sizable 2-3 point gains bleu meteor ter importantly find surprisingly large gains human evaluations compared previous models nevertheless manual inspection generated paraphrases reveals ample room improvement even best model produces human-acceptable paraphrases 28 captions chia dataset sharma et al 2018 fails spectacularly sentences wikipedia overall results point potential incorporating semantics task highlighting need stronger evaluation	positive
learning to compose topic-aware mixture of experts for zero-shot video captioning	although promising results achieved video captioning existing models limited fixed inventory activities training corpus generalize open vocabulary scenarios introduce novel task zeroshot video captioning aims describing out-of-domain videos unseen activities videos different activities usually require different captioning strategies many aspects i.e word selection semantic construction style expression etc poses great challenge depict novel activities without paired training data meanwhile similar activities share aspects common therefore propose principled topic-aware mixture experts tamoe model zero-shot video captioning learns compose different experts based different topic embeddings implicitly transferring knowledge learned seen activities unseen ones besides leverage external topic-related text corpus construct topic embedding activity embodies relevant semantic vectors within topic empirical results validate effectiveness method utilizing semantic knowledge video captioning also show strong generalization ability describing novel activities	negative
cycleemotiongan: emotional semantic consistency preserved cyclegan for adapting image emotions	deep neural networks excel learning large-scale labeled training data well generalize learned knowledge new domains datasets domain adaptation studies transfer models trained one labeled source domain another sparsely labeled unlabeled target domain paper investigate unsupervised domain adaptation uda problem image emotion classification specifically develop novel cycle-consistent adversarial model termed cycleemotiongan enforcing emotional semantic consistency adapting images cycleconsistently alternately optimizing cyclegan loss emotional semantic consistency loss target classification loss cycleemotiongan adapt source domain images similar distributions target domain without using aligned image pairs simultaneously annotation information source images preserved extensive experiments conducted artphoto fi datasets results demonstrate cycleemotiongan significantly outperforms state-of-the-art uda approaches	negative
bidirectional inference networks:a class of deep bayesian networks for health profiling	consider problem inferring values arbitrary set variables e.g. risk diseases given observed variables e.g. symptoms diagnosed diseases high-dimensional signals e.g. mri images eeg common problem healthcare since variables interest often differ different patients existing methods including bayesian networks structured prediction either incorporate high-dimensional signals fail model conditional dependencies among variables address issues propose bidirectional inference networks bin stich together multiple probabilistic neural networks modeling conditional dependency predictions made via iteratively updating variables using backpropagation bp maximize corresponding posterior probability furthermore extend bin composite bin cbin involves iterative prediction process training stage improves accuracy computational efficiency adaptively smoothing optimization landscape experiments synthetic real-world datasets sleep study dermatology dataset show cbin single model achieve state-of-the-art performance obtain better accuracy inference tasks multiple models specifically trained different task	positive
concept extraction and prerequisite relation learning from educational data	prerequisite relations among concepts crucial educational applications however difficult automatically extract domain-specific concepts learn prerequisite relations among without labeled data	positive
recurjac: an efficient recursive algorithm for bounding jacobian matrix of neural networks and its applications	jacobian matrix gradient single-output networks directly related many important properties neural networks function landscape stationary points local lipschitz constants robustness adversarial attacks paper propose recursive algorithm recurjac compute upper lower bounds element jacobian matrix neural network respect network ’ input network contain wide range activation functions byproduct efficiently obtain local lipschitz constant plays crucial role neural network robustness verification well training stability gans experiments show local lipschitz constants produced method better quality previous approaches thus providing better robustness verification results algorithm polynomial time complexity computation time reasonable even relatively large networks additionally use bounds jacobian matrix characterize landscape neural network example determine whether exist stationary points local neighborhood	negative
collective online learning of gaussian processes in massive multi-agent systems	paper presents novel collective online learning gaussian processes cool-gp framework enabling massive number gp inference agents simultaneously perform efficient online updates gp models using local streaming data varying correlation structures b decentralized fusion resulting online gp models different learned hyperparameter settings inducing inputs realize exploit notion common encoding structure encapsulate local streaming data gathered gp inference agent summary statistics based proposed representation amenable efficient online update via importance sampling trick well multi-agent model fusion via decentralized message passing exploit sparse connectivity among agents improving efficiency enhance robustness framework transmission loss provide rigorous theoretical analysis approximation loss arising proposed representation achieve efficient online updates model fusion empirical evaluations show cool-gp highly effective model fusion resilient information disparity agents robust transmission loss scale thousands agents	positive
scene text recognition from two-dimensional perspective	inspired speech recognition recent state-of-the-art algorithms mostly consider scene text recognition sequence prediction problem though achieving excellent performance methods usually neglect important fact text images actually distributed two-dimensional space nature quite different speech essentially one-dimensional signal principle directly compressing features text one-dimensional form may lose useful information introduce extra noise paper approach scene text recognition two-dimensional perspective simple yet effective model called character attention fully convolutional network ca-fcn devised recognizing text arbitrary shapes scene text recognition realized semantic segmentation network attention mechanism characters adopted combined word formation module ca-fcn simultaneously recognize script predict position character experiments demonstrate proposed algorithm outperforms previous methods regular irregular text datasets moreover proven robust imprecise localizations text detection phase common practice	negative
communication-optimal distributed dynamic graph clustering	consider problem clustering graph nodes large-scale dynamic graphs citation networks images web networks graph updates node/edge insertions/deletions observed distributively propose communication-efficient algorithms two well-established communication models namely message passing blackboard models given graph n nodes observed remote sites time 1 two proposed algorithms communication costs õ ns õ n õ hides polylogarithmic factor almost matching lower bounds ω ns ω n respectively message passing blackboard models importantly prove time point 1 algorithms generate clustering quality nearly good centralizing updates time applying standard centralized clustering algorithm conducted extensive experiments synthetic real-life datasets confirmed communication efficiency approach baseline algorithms achieving comparable clustering results	positive
perceptual-sensitive gan for generating adversarial patches	deep neural networks dnns vulnerable adversarial examples inputs imperceptible perturbations mislead dnns incorrect results recently adversarial patch noise confined small localized patch emerged easy accessibility real-world however existing attack strategies still far generating visually natural patches strong attacking ability since often ignore perceptual sensitivity attacked network adversarial patch including correlations image context visual attention address problem paper proposes perceptual-sensitive generative adversarial network ps-gan simultaneously enhance visual fidelity attacking ability adversarial patch improve visual fidelity treat patch generation patch-to-patch translation via adversarial process feeding types seed patch outputting similar adversarial patch high perceptual correlation attacked image enhance attacking ability attention mechanism coupled adversarial generation introduced predict critical attacking areas placing patches help producing realistic aggressive patches extensive experiments semi-whitebox black-box settings two large-scale datasets gtsrb imagenet demonstrate proposed ps-gan outperforms state-of-the-art adversarial patch attack methods	negative
plan-length bounds: beyond 1-way dependency	consider problem compositionally computing upper bounds lengths plans following existing work approach based decomposition state-variable dependency graphs a.k.a causal graphs tight bounds demonstrated previously problems key dependencies flow single direction—i.e manipulating variable v1 disturb ability manipulate v2 vice versa develop general bounding approach allows us compute useful bounds dependency flows directions approach practically useful combined earlier approaches computed bounds substantially improved relatively broad variety problems combined existing planning procedure improved bounds yield coverage improvements solvable unsolvable planning problems	positive
ddflow: learning optical flow with unlabeled data distillation	present ddflow data distillation approach learning optical flow estimation unlabeled data approach distills reliable predictions teacher network uses predictions annotations guide student network learn optical flow unlike existing work relying handcrafted energy terms handle occlusion approach data-driven learns optical flow occluded pixels enables us train model much simpler loss function achieve much higher accuracy conduct rigorous evaluation challenging flying chairs mpi sintel kitti 2012 2015 benchmarks show approach significantly outperforms existing unsupervised learning methods running real time	positive
deep cascade multi-task learning for slot filling in online shopping assistant	slot filling critical task natural language understanding nlu dialog systems state-of-the-art approaches treat sequence labeling problem adopt models bilstm-crf models work relatively well standard benchmark datasets face challenges context e-commerce slot labels informative carry richer expressions work inspired unique structure e-commerce knowledge base propose novel multi-task model cascade residual connections jointly learns segment tagging named entity tagging slot filling experiments show effectiveness proposed cascade residual structures model 14.6 advantage f1 score strong baseline methods new chinese e-commerce shopping assistant dataset achieving competitive accuracies standard dataset furthermore online test deployed dominant e-commerce platform shows 130 improvement accuracy understanding user utterances model already gone production e-commerce platform	negative
disentangled variational representation for heterogeneous face recognition	visible vis near infrared nir face matching challenging problem due significant domain discrepancy domains lack sufficient data training cross-modal matching algorithms existing approaches attempt tackle problem either synthesizing visible faces nir faces extracting domain-invariant features modalities projecting heterogeneous data onto common latent space cross-modal matching paper take different approach make use disentangled variational representation dvr crossmodal matching first model face representation intrinsic identity information within-person variations exploring disentangled latent variable space variational lower bound employed optimize approximate posterior nir vis representations second aiming obtaining compact discriminative disentangled latent space impose minimization identity information subject relaxed correlation alignment constraint nir vis modality variations alternative optimization scheme proposed disentangled variational representation part heterogeneous face recognition network part mutual promotion two parts effectively reduces nir vis domain discrepancy alleviates over-fitting extensive experiments three challenging nir-vis heterogeneous face recognition databases demonstrate proposed method achieves significant improvements state-of-the-art methods	negative
crawling the community structure of multiplex networks	examine problem crawling community structure multiplex network containing multiple layers edge relationships great deal work examining community structure general work problem sampling network preserve community structure best knowledge first work consider problem multiplex networks consider specific case layers multiplex network different query collection costs reliabilities data collector interested identifying community structure expensive layer propose multicomsample mcs novel algorithm crawling multiplex network mcs uses multiple levels multi-armed bandits determine best layers communities node roles selecting nodes query test mcs six baseline algorithms real-world multiplex networks achieved large gains performance example consuming budget equivalent sampling 20 nodes expensive layer observe mcs outperforms best baseline 49	negative
efficient data point pruning for one-class svm	one-class svm popular method one-class classification needs high computation cost paper proposes quix efficient training algorithm one-class svm prunes unnecessary data points applying svm solver computing upper lower bounds parameter determines hyper-plane since efficiently check optimality hyper-plane using bounds guarantees identical classification results original approach experiments show 6800 times faster existing approaches without degrading optimality	negative
dual semi-supervised learning for facial action unit recognition	current works facial action unit au recognition typically require fully au-labeled training samples reduce reliance time-consuming manual au annotations propose novel semi-supervised au recognition method leveraging two kinds readily available auxiliary information method leverages dependencies aus expressions well dependencies among aus caused facial anatomy therefore embedded facial images independent au annotation status auxiliary information facial image synthesis given aus dual task au recognition facial images therefore intrinsic probabilistic connections au recognition regardless au annotations specifically propose dual semi-supervised generative adversarial network au recognition partially au-labeled fully expressionlabeled facial images proposed network consists au classifier c image generator g discriminator d. addition minimize supervised losses au classifier face generator labeled training data explore probabilistic duality tasks using adversary learning force convergence face-au-expression tuples generated au classifier face generator ground-truth distribution labeled data training data joint distribution also includes inherent au dependencies furthermore reconstruct facial image using output au classifier input face generator create au labels feeding output face generator au classifier minimize reconstruction losses training data thus exploiting informative feedback provided dual tasks within-database cross-database experiments three benchmark databases demonstrate superiority method au recognition face synthesis compared state-of-the-art works	positive
wais: word attention for joint intent detection and slot filling	attention-based recurrent neural network models joint intent detection slot filling achieved state-of-the-art performance previous works exploited semantic level information calculate attention weights however works taken importance word level information consideration paper propose wais word attention joint intent detection slot filling considering intent detection slot filling strong relationship propose fusion gate integrates word level information semantic level information together jointly training two tasks extensive experiments show proposed model robust superiority competitors sets state-of-the-art	negative
robust online matching with user arrival distribution drift	recently online matching problems attracted much attention due emerging applications internet advertising existing online matching methods adopted either adversarial stochastic user arrival assumption significant limitation exists adversarial model exploit existing knowledge user sequence thus pessimistic practice hands stochastic model assumes users drawn stationary distribution may true real applications paper consider novel user arrival model users drawn drifting distribution hybrid case adversarial stochastic model propose new approach rdla deal assumption instead maximizing empirical total revenues revealed users rdla leverages distributionally robust optimization techniques learn dual variables via worst-case consideration ambiguity set underlying user distribution experiments real-world dataset exhibit superiority approach	negative
almost unsupervised learning for dense crowd counting	present unsupervised learning method dense crowd count estimation marred large variability appearance people extreme overlap crowds enumerating people proves difficult task even humans implies creating large-scale annotated crowd data expensive directly takes toll performance existing cnn based counting models account small datasets motivated challenges develop grid winner-take-all gwta autoencoder learn several layers useful filters unlabeled crowd images gwta approach divides convolution layer spatially grid cells within cell maximally activated neuron allowed update filter almost 99.9 parameters proposed model trained without labeled data rest 0.1 tuned supervision model achieves superior results compared unsupervised methods stays reasonably close accuracy supervised baseline furthermore present comparisons analyses regarding quality learned features across various models	positive
early detection of vacant parking spaces using dashcam videos	major problem metropolitan areas finding parking spaces existing parking guidance systems often adopt fixed sensors cameras provide information driver ’ point view motivated advent dashboard cameras dashcams develop neural-network-based methods detecting vacant parking spaces videos recorded dashcam detecting vacant parking spaces dashcam videos enables early detection spaces different conventional object detection methods leverage monotonicity detection confidence respect distance away approaching target parking space propose new loss function yield improved detection results also enable early detection evaluate detection method create new large dataset containing 5,800 dashcam videos captured 22 indoor outdoor parking lots best knowledge first largest driver ’ view video dataset supports parking space detection provides parking space occupancy annotations	negative
non-parametric transformation networks for learning general invariances from data	convnets architecture enforce invariance translation paper introduce new class deep convolutional architectures called non-parametric transformation networks nptns learn general invariances symmetries directly data nptns natural generalization convnets optimized directly using gradient descent unlike almost previous works deep architectures make assumption regarding structure invariances present data aspect flexible powerful also model convnets nptns unified framework called transformation networks tn yields better understanding connection two demonstrate efficacy nptns data mnist extreme transformations cifar10 outperform baselines outperform several recent algorithms eth-80 number parameters also show effective convnets modelling symmetries invariances data without explicit knowledge added arbitrary nuisance transformations finally replace convnets nptns within capsule networks show enables capsule nets perform even better	positive
orthogonality-promoting dictionary learning via bayesian inference	dictionary learning dl plays crucial role numerous machine learning tasks targets finding dictionary training set admits maximally sparse representation existing dl algorithms based solving optimization problem noise variance sparsity level known prior knowledge however practice applications difficult obtain knowledge thus non-parametric bayesian dl recently received much attention researchers due adaptability effectiveness although many hierarchical priors used promote sparsity representation non-parametric bayesian dl problem redundancy dictionary still overlooked greatly decreases performance sparse coding address problem paper presents novel robust dictionary learning framework via bayesian inference particular employ orthogonality-promoting regularization mitigate correlations among dictionary atoms regularization encouraging dictionary atoms close orthogonal alleviate overfitting training data improve discrimination model moreover impose scale mixture vector variate gaussian smvg distribution noise capture structure regularized expectation maximization algorithm developed estimate posterior distribution representation dictionary orthogonality-promoting regularization numerical results show method learn dictionary accuracy better existing methods especially number training signals limited	negative
knowledge-driven encode, retrieve, paraphrase for medical image report generation	generating long semantic-coherent reports describe medical images poses great challenges towards bridging visual linguistic modalities incorporating medical domain knowledge generating realistic accurate descriptions propose novel knowledge-driven encode retrieve paraphrase kerp approach reconciles traditional knowledge- retrieval-based methods modern learning-based methods accurate robust medical report generation specifically kerp decomposes medical report generation explicit medical abnormality graph learning subsequent natural language modeling kerp first employs encode module transforms visual features structured abnormality graph incorporating prior medical knowledge retrieve module retrieves text templates based detected abnormalities lastly paraphrase module rewrites templates according specific cases core kerp proposed generic implementation unit—graph transformer gtr dynamically transforms high-level semantics graph-structured data multiple domains knowledge graphs images sequences experiments show proposed approach generates structured robust reports supported accurate abnormality description explainable attentive regions achieving state-of-the-art results two medical report benchmarks best medical abnormality disease classification accuracy improved human evaluation performance	positive
adaptation strategies for applying awgn-based denoiser to realistic noise	discriminative learning based denoising model trained additive white gaussian noise awgn performs well synthesized noise however realistic noise spatialvariant signal-dependent mixture complicated noises paper explore multiple strategies applying awgn-based denoiser realistic noise specifically trained deep network integrating noise estimating denoiser mixed gaussian awgn random value impulse noise rvin adapt model realistic noises investigated multi-channel multi-scale super-resolution approaches preliminary results demonstrated effectiveness newly-proposed noise model adaptation strategies	negative
from independent prediction to reordered prediction: integrating relative position and global label information to emotion cause identification	emotion cause identification aims identifying potential causes lead certain emotion expression text several techniques including rule based methods traditional machine learning methods proposed address problem based manually designed rules features recently deep learning methods also applied task attempt automatically capture causal relationship emotion causes embodied text work find addition content text another two kinds information namely relative position global labels also important emotion cause identification integrate information propose model based neural network architecture encode three elements i.e. text content relative position global label unified end-to-end fashion introduce relative position augmented embedding learning algorithm transform task independent prediction problem reordered prediction problem dynamic global label information incorporated experimental results benchmark emotion cause dataset show model achieves new state-ofthe-art performance performs significantly better number competitive baselines analysis shows effectiveness relative position augmented embedding learning algorithm reordered prediction mechanism dynamic global labels	negative
character-level language modeling with deeper self-attention	lstms rnn variants shown strong performance character-level language modeling models typically trained using truncated backpropagation time common assume success stems ability remember long-term contexts paper show deep 64-layer transformer model vaswani et al 2017 fixed context outperforms rnn variants large margin achieving state art two popular benchmarks 1.13 bits per character text8 1.06 enwik8 get good results depth show important add auxiliary losses intermediate network layers intermediate sequence positions	negative
learning compact model for large-scale multi-label data	large-scale multi-label learning lmll aims annotate relevant labels large number candidates unseen data due high dimensionality feature label spaces lmll storage overheads lmll models often costly paper proposes pop joint label feature parameter optimization method tries filter redundant model parameters facilitate compact models key insights follows first investigate labels little impact commonly used lmll performance metrics preserve small number dominant parameters labels second remaining influential labels reduce spurious feature parameters little contribution generalization capability models preserve parameters discriminative features overall problem formulated constrained optimization problem pursuing minimal model size order solve resultant difficult optimization show relaxation optimization efficiently solved using binary search greedy strategies experiments verify proposed method clearly reduces model size compared state-of-the-art lmll approaches addition achieves highly competitive performance	negative
self-ensembling attention networks: addressing domain shift for semantic segmentation	recent years witnessed great success deep learning models semantic segmentation nevertheless models may generalize well unseen image domains due phenomenon domain shift since pixel-level annotations laborious collect developing algorithms adapt labeled data source domain target domain great significance end propose self-ensembling attention networks reduce domain gap different datasets best knowledge proposed method first attempt introduce selfensembling model domain adaptation semantic segmentation provides different view learn domain-invariant features besides since different regions image usually correspond different levels domain gap introduce attention mechanism proposed framework generate attention-aware features utilized guide calculation consistency loss target domain experiments two benchmark datasets demonstrate proposed framework yield competitive performance compared state art methods	negative
a multi-task learning approach for answer selection: a study and a chinese law dataset	paper propose multi-task learning approach answer selection mtas motivated fact humans difficulty performing task possess capabilities multiple domains tasks specifically mtas consists two key components category classification model learns rich category-aware document representation ii answer selection model provides matching scores question-answer pairs two tasks work shared document encoding layer cooperate learn high-quality answer selection system addition multi-head attention mechanism proposed learn important information different representation subspaces different positions manually annotate first chinese question answering dataset law domain denoted lawqa evaluate effectiveness model experimental results show model mtas consistently outperforms compared methods.1	positive
residual compensation networks for heterogeneous face recognition	heterogeneous face recognition hfr challenging task due large modality discrepancy well insufficient training images certain modalities paper propose new two-branch network architecture termed residual compensation networks rcn learn separated features different modalities hfr rcn incorporates residual compensation rc module modality discrepancy loss md loss traditional convolutional neural networks rc module reduces modal discrepancy adding compensation one modalities representation close modality md loss alleviates modal discrepancy minimizing cosine distance different modalities addition explore different architectures positions rc module evaluate different transfer learning strategies hfr extensive experiments iiit-d viewed sketch forensic sketch casia nir-vis 2.0 cuhk nir-vis show rcn outperforms state-of-the-art methods significantly	negative
hyperprior induced unsupervised disentanglement of latent representations	address problem unsupervised disentanglement latent representations learnt via deep generative models contrast current approaches operate evidence lower bound elbo argue statistical independence latent space vaes enforced principled hierarchical bayesian manner effect augment standard vae inverse-wishart iw prior covariance matrix latent code tuning iw parameters able encourage discourage independence learnt latent dimensions extensive experimental results range datasets 2dshapes 3dchairs 3dfaces celeba show approach outperform β-vae competitive state-of-the-art factorvae approach achieves significantly better disentanglement reconstruction new dataset correlatedellipses introduces correlations factors variation	negative
confidence weighted multitask learning	traditional online multitask learning utilizes firstorder information datastream remedy issue propose confidence weighted multitask learning algorithm maintains gaussian distribution task model guide online learning process mean covariance gaussian distribution sum local component global component shared among tasks addition paper also addresses challenge active learning online multitask setting instead requiring labels instances proposed algorithm determines whether learner acquire label considering confidence related tasks label prediction theoretical results show regret bounds significantly reduced empirical results demonstrate proposed algorithm able achieve promising learning efficacy simultaneously minimizing labeling cost	negative
deep metric learning by online soft mining and class-aware attention	deep metric learning aims learn deep embedding capture semantic similarity data points given availability massive training samples deep metric learning known suffer slow convergence due large fraction trivial samples therefore existing methods generally resort sample mining strategies selecting nontrivial samples accelerate convergence improve performance work identify two critical limitations sample mining methods provide solutions first previous mining methods assign one binary score sample i.e. dropping keeping selects subset relevant samples mini-batch therefore propose novel sample mining method called online soft mining osm assigns one continuous score sample make use samples mini-batch osm learns extended manifolds preserve useful intraclass variances focusing similar positives second existing methods easily influenced outliers generally included mined subset address introduce class-aware attention caa assigns little attention abnormal data samples furthermore combining osm caa propose novel weighted contrastive loss learn discriminative embeddings extensive experiments two fine-grained visual categorisation datasets two video-based person re-identification benchmarks show method significantly outperforms state-of-the-art	negative
dependency grammar induction with a neural variational transition-based parser	dependency grammar induction task learning dependency syntax without annotated training data traditional graph-based models global inference achieve state-ofthe-art results task require n3 run time transition-based models enable faster inference n time complexity performance still lags behind work propose neural transition-based parser dependency grammar induction whose inference procedure utilizes rich neural features n time complexity train parser integration variational inference posterior regularization variance reduction techniques resulting framework outperforms previous unsupervised transition-based dependency parsers achieves performance comparable graph-based models english penn treebank universal dependency treebank empirical comparison show approach substantially increases parsing speed graphbased models	negative
spatiality preservable factored poisson regression for large-scale fine-grained gps-based population analysis	wide use smartphones global positioning system gps sensors analysis population gps traces actively explored last decade propose herein brand new population prediction model capture population trends fine-grained point interest poi densely distributed large areas understand relationship poi terms spatiality preservation propose new framework called spatiality preservable factorized regression spfr realize model spfr inspired success recently proposed bilinear poisson regression concept multi-task learning factorization approach graph proximity regularization given proposed model written simply terms optimization achieve scalability using model results empirical evaluation used massive dataset gps logs tokyo region 32 count logs show model comparable stateof-the-art methods terms capturing population trend across meshes retaining spatial preservation finer mesh areas	negative
view inter-prediction gan: unsupervised representation learning for 3d shapes by learning global shape memories to support local view predictions	paper present novel unsupervised representation learning approach 3d shapes important research challenge avoids manual effort required collecting supervised data method trains rnnbased neural network architecture solve multiple view inter-prediction tasks shape given several nearby views shape define view inter-prediction task predicting center view input views reconstructing input views low-level feature space key idea approach implement shape representation shape-specific global memory shared local view inter-predictions shape intuitively memory enables system aggregate information useful better solve view inter-prediction tasks shape leverage memory viewindependent shape representation approach obtains best results using combination l2 adversarial losses view inter-prediction task show vip-gan outperforms state-of-the-art methods unsupervised 3d feature learning three large-scale 3d shape benchmarks	positive
meimei: an efficient probabilistic approach for semantically annotating tables	given large amount table data find tables contain contents want naive search fails column names ambiguous columns containing stock price information named “ close ” one table named “ p ” another table	positive
determining solvability in the birds of a feather card game	birds feather single-player card game cards arranged grid player attempts combine stacks cards certain rules goal combine cards single stack paper highlights several approaches efficiently classifying whether randomlychosen state single-stack solution approaches use graph theory machine learning concepts prune state ’ search space resulting significant reductions runtime relative baseline search	positive
collaborative, dynamic and diversified user profiling	paper study problem dynamic user profiling context streams short texts previous work user profiling works long documents consider collaborative information diversify keywords profiling users ’ interests contrast address problem proposing user profiling algorithm upa consists two models proposed collaborative interest tracking topic model citm proposed streaming keyword diversification model skdm upa first utilizes citm collaboratively track user ’ followees ’ dynamic interest distributions context streams short texts utilizes skdm obtain top-k relevant diversified keywords profile users ’ interests specific point time experiments conducted twitter dataset found upa outperforms state-of-the-art non-dynamic dynamic user profiling algorithms	positive
efficient and scalable multi-task regression on massive number of tasks	many real-world large-scale regression problems formulated multi-task learning mtl problems massive number tasks retail transportation domains however existing mtl methods still fail offer generalization performance scalability problems scaling mtl methods problems tremendous number tasks big challenge propose novel algorithm named convex clustering multi-task regression learning ccmtl integrates convex clustering k-nearest neighbor graph prediction models ccmtl efficiently solves underlying convex problem newly proposed optimization method ccmtl accurate efficient train empirically scales linearly number tasks synthetic real-world datasets proposed ccmtl outperforms seven state-of-the-art soa multi-task learning methods terms prediction accuracy well computational efficiency real-world retail dataset 23,812 tasks ccmtl requires around 30 seconds train single thread soa methods need hours even days	negative
mode variational lstm robust to unseen modes of variation: application to facial expression recognition	spatio-temporal feature encoding essential encoding dynamics video sequences recurrent neural networks particularly long short-term memory lstm units popular efficient tool encoding spatio-temporal features sequences work investigate effect mode variations encoded spatio-temporal features using lstms show lstm retains information related mode variation sequence irrelevant task hand e.g classification facial expressions actually lstm forget mechanism robust enough mode variations preserves information could negatively affect encoded spatio-temporal features propose mode variational lstm encode spatio-temporal features robust unseen modes variation mode variational lstm modifies original lstm structure adding additional cell state focuses encoding mode variation input sequence efficiently regulate features stored additional cell state additional gating functionality also introduced effectiveness proposed mode variational lstm verified using facial expression recognition task comparative experiments publicly available datasets verified proposed mode variational lstm outperforms existing methods moreover new dynamic facial expression dataset different modes variation including various modes like pose illumination variations collected comprehensively evaluate proposed mode variational lstm experimental results verified proposed mode variational lstm encodes spatio-temporal features robust unseen modes variation	negative
spatiotemporal multi-graph convolution network for ride-hailing demand forecasting	region-level demand forecasting essential task ridehailing services accurate ride-hailing demand forecasting guide vehicle dispatching improve vehicle utilization reduce wait-time mitigate traffic congestion task challenging due complicated spatiotemporal dependencies among regions existing approaches mainly focus modeling euclidean correlations among spatially adjacent regions observe non-euclidean pair-wise correlations among possibly distant regions also critical accurate forecasting paper propose spatiotemporal multi-graph convolution network st-mgcn novel deep learning model ride-hailing demand forecasting first encode non-euclidean pair-wise correlations among regions multiple graphs explicitly model correlations using multi-graph convolution utilize global contextual information modeling temporal correlation propose contextual gated recurrent neural network augments recurrent neural network contextual-aware gating mechanism re-weights different historical observations evaluate proposed model two real-world large scale ride-hailing demand datasets observe consistent improvement 10 stateof-the-art baselines	negative
deep latent generative models for energy disaggregation	thoroughly understanding energy consumption disaggregated individual appliances help reduce household expenses integrate renewable sources energy lead efficient use energy work propose deep latent generative model based variational recurrent neural networks vrnns energy disaggregation model jointly disaggregates aggregated energy signal individual appliance signals achieving superior performance compared state-of-the-art models energy disaggregation yielding 29 41 performance improvement two energy datasets respectively without explicitly encoding temporal/contextual information heuristics model also achieves better prediction performance lowpower appliances paving way nuanced disaggregation model structured output prediction model helps accurately discerning appliance contribute aggregated power consumption thus providing useful meaningful disaggregation model	negative
efficient optimal approximation of discrete random variables for estimation of probabilities of missing deadlines	present efficient algorithm given discrete random variable x number computes random variable whose support size whose kolmogorov distance x minimal present variants algorithm analyse correctness computational complexity present detailed empirical evaluation shows performs practice main application examine motivation work estimation probability missing deadlines series-parallel schedules since exact computation probabilities np-hard propose use algorithms described paper obtain approximation	positive
clairvoyant restarts in branch-and-bound search using online tree-size estimation	propose simple general online method measure search progress within branch-and-bound algorithm estimate size remaining search tree show information help solvers algorithmically runtime designing restart strategy mixedinteger programming mip solvers decides whether restart search based current estimate number remaining nodes tree refer type algorithm clairvoyant clairvoyant restart strategy outperforms state-of-the-art solver large set publicly available mip benchmark instances implemented mip solver scip available future releases	positive
spatially invariant unsupervised object detection with convolutional neural networks	many reasons expect ability reason terms objects crucial skill generally intelligent agent indeed recent machine learning literature replete examples benefits object-like representations generalization transfer new tasks interpretability among others however order reason terms objects agents need way discovering detecting objects visual world task call unsupervised object detection task received significantly less attention literature supervised counterpart especially case large images containing many objects current work develop neural network architecture effectively addresses large-image many-object setting particular combine ideas attend infer repeat air performs unsupervised object detection scale well recent developments supervised object detection replace air ’ core recurrent network convolutional thus spatially invariant network make use object-specification scheme describes location objects respect local grid cells rather image whole series experiments demonstrate number features architecture unlike air able discover detect objects large many-object scenes significant ability generalize images larger contain objects images encountered training able discover detect objects enough accuracy facilitate non-trivial downstream processing	negative
fast and simple mixture of softmaxes with bpe and hybrid-lightrnn for language generation	mixture softmaxes mos shown effective addressing expressiveness limitation softmax-based models despite known advantage mos practically sealed large consumption memory computational time due need computing multiple softmaxes work set unleash power mos practical applications investigating improved word coding schemes could effectively reduce vocabulary size hence relieve memory computation burden show bpe proposed hybrid-lightrnn lead improved encoding mechanisms halve time memory consumption mos without performance losses mos achieve improvement 1.5 bleu scores iwslt 2014 german-to-english corpus improvement 0.76 cider score image captioning moreover larger wmt 2014 machine translation dataset mosboosted transformer yields 29.6 bleu score english-togerman 42.1 bleu score english-to-french outperforming single-softmax transformer 0.9 0.4 bleu scores respectively achieving state-of-the-art result wmt 2014 english-to-german task	negative
a deep cascade model for multi-document reading comprehension	fundamental trade-off effectiveness efficiency needs balanced designing online question answering system effectiveness comes sophisticated functions extractive machine reading comprehension mrc efficiency obtained improvements preliminary retrieval components candidate document selection paragraph ranking given complexity real-world multi-document mrc scenario difficult jointly optimize end-to-end system address problem develop novel deep cascade learning model progressively evolves documentlevel paragraph-level ranking candidate texts precise answer extraction machine reading comprehension specifically irrelevant documents paragraphs first filtered simple functions efficiency consideration jointly train three modules remaining texts better tracking answer document extraction paragraph extraction answer extraction experiment results show proposed method outperforms previous state-of-the-art methods two large-scale multidocument benchmark datasets i.e. triviaqa dureader addition online system stably serve typical scenarios millions daily requests less 50ms	negative
zero-shot adaptive transfer for conversational language understanding	conversational agents alexa google assistant constantly need increase language understanding capabilities adding new domains massive amount labeled data required training new domain domain adaptation approaches alleviate annotation cost prior approaches suffer increased training time suboptimal concept alignments tackle introduce novel zero-shot adaptive transfer method slot tagging utilizes slot description transferring reusable concepts across domains enjoys efficient training without explicit concept alignments extensive experimentation dataset 10 domains relevant commercial personal digital assistant shows model outperforms previous state-of-the-art systems large margin achieves even higher improvement low data regime	positive
higher-order multi-layer community detection	paper define new problem multi-layer network community detection namely higher-order multi-layer community detection multi-layer motif m-motif approach proposed discovers communities good intralayer higher-order community quality preserving interlayer higher-order community consistency experimental results confirmed superiority proposed method	positive
learning document embeddings with crossword prediction	paper propose document embedding network den learn document embeddings unsupervised manner model uses encoder-decoder architecture backbone tries reconstruct input document encoded document embedding unlike standard decoder text reconstruction randomly block words input document use incomplete context information encoded document embedding predict blocked words document inspired crossword game thus decoder keep balance known unknown information consider global partial information decoding missing words evaluate learned document embeddings two tasks document classification document retrieval experimental results show model substantially outperforms compared methods.1	positive
densely supervised grasp detector (dsgd)	paper presents densely supervised grasp detector dsgd deep learning framework combines cnn structures layer-wise feature fusion produces grasps confidence scores different levels image hierarchy i.e. global- region- pixel-levels specifically global-level dsgd uses entire image information predict grasp region-level dsgd uses region proposal network identify salient regions image uses grasp prediction network generate segmentations corresponding grasp poses salient regions pixel-level dsgd uses fully convolutional network predicts grasp confidence every pixel inference dsgd selects confident grasp output selection hierarchically generated grasp candidates overcomes limitations individual models dsgd outperforms state-of-the-art methods cornell grasp dataset terms grasp accuracy evaluation multi-object dataset real-world robotic grasping experiments show dsgd produces highly stable grasps set unseen objects new environments achieves 97 grasp detection accuracy 90 robotic grasping success rate real-time inference speed	positive
predicting the argumenthood of english prepositional phrases	distinguishing arguments adjuncts verb longstanding nontrivial problem natural language processing argumenthood information important tasks semantic role labeling srl prepositional phrase pp attachment disambiguation theoretical linguistics many diagnostic tests argumenthood exist often yield conflicting potentially gradient results especially case syntactically oblique items pps propose two pp argumenthood prediction tasks branching two motivations 1 binary argumentadjunct classification pps verbnet 2 gradient argumenthood prediction using human judgments gold standard report results prediction models use pretrained word embeddings linguistically informed features best results task 1 acc 0.955 f1 0.954 elmo+bilstm 2 pearson ’ r 0.624 word2vec+mlp furthermore demonstrate utility argumenthood prediction improving sentence representations via performance gains srl sentence encoder pretrained tasks	negative
multi-view anomaly detection: neighborhood in locality matters	identifying anomalies multi-view data difficult task due complicated data characteristics anomalies specifically two types anomalies multi-view data–anomalies inconsistent features across multiple views anomalies consistently anomalous view existing multi-view anomaly detection approaches issues e.g. assume multiple views normal instance share consistent normal clustering structures anomaly exhibits anomalous clustering characteristics across multiple views clusters data difficult existing approaches detect anomalies besides existing approaches construct profile normal instances identify instances conform normal profile anomalies objective formulated profile normal instances estimate set normal instances results sub-optimal detectors addition model trained profile normal instances uses entire dataset including anomalies however anomalies could undermine model i.e. model robust anomalies address issues propose nearest neighborbased multi-view anomaly detection muvad approach specifically first propose anomaly measurement criterion utilize criterion formulate objective muvad estimate set normal instances explicitly develop two concrete relaxations implementing muvad muvad-qpr muvad-fsr experimental results validate superiority proposed muvad approaches	negative
unsupervised cross-spectral stereo matching by learning to synthesize	unsupervised cross-spectral stereo matching aims recovering disparity given cross-spectral image pairs without depth disparity supervision estimated depth provides additional information complementary original images helpful vision tasks tracking recognition detection however large appearance variations images different spectral bands challenge cross-spectral stereo matching existing deep unsupervised stereo matching methods sensitive appearance variations perform well cross-spectral data propose novel unsupervised crossspectral stereo matching framework based image-to-image translation first style adaptation network transforms images across different spectral bands cycle consistency adversarial learning appearance variations minimized stereo matching network trained image pairs spectra using view reconstruction loss last estimated disparity utilized supervise spectral translation network end-to-end way moreover novel style adaptation network f-cyclegan proposed improve robustness spectral translation method tackle appearance variations enhance robustness unsupervised cross-spectral stereo matching experimental results show method achieves good performance without using depth supervision explicit semantic information	negative
robust multi-object detection based on data augmentation with realistic image synthesis for point-of-sale automation	alternative bar-code scanning developing real-time retail product detector point-of-sale automation major challenge associated image based object detection arise occlusion presence objects close proximity robust product detection conditions crucial train detector rich set images varying degrees occlusion proximity products fairly represents wide range customer tendencies placing products together however generating fairly large database images traditionally requires large amount human effort hand acquiring individual object images corresponding masks relatively easy task propose realistic image synthesis approach uses individual object images corresponding masks create training images desired properties occlusion congestion among products train product detector images thus generated achieve consistent performance improvement across different types test data proposed approach detector achieves improvement 46.2 0.67 0.98 40 0.60 0.84 precision recall respectively compared using basic training dataset containing one product per image	negative
towards non-saturating recurrent units for modelling long-term dependencies	modelling long-term dependencies challenge recurrent neural networks primarily due fact gradients vanish training sequence length increases gradients attenuated transition operators attenuated dropped activation functions canonical architectures like lstm alleviate issue skipping information memory mechanism propose new recurrent architecture non-saturating recurrent unit nru relies memory mechanism forgoes saturating activation functions saturating gates order alleviate vanishing gradients series synthetic real world tasks demonstrate proposed model model performs among top 2 models across tasks without long-term dependencies compared range architectures	negative
cycle-sum: cycle-consistent adversarial lstm networks for unsupervised video summarization	paper present novel unsupervised video summarization model requires manual annotation proposed model termed cycle-sum adopts new cycleconsistent adversarial lstm architecture effectively maximize information preserving compactness summary video consists frame selector cycle-consistent learning based evaluator selector bi-direction lstm network learns video representations embed long-range relationships among video frames evaluator defines learnable information preserving metric original video summary video “ supervises ” selector identify informative frames form summary video particular evaluator composed two generative adversarial networks gans forward gan learned reconstruct original video summary video backward gan learns invert processing consistency output cycle learning adopted information preserving metric video summarization demonstrate close relation mutual information maximization cycle learning procedure experiments two video summarization benchmark datasets validate state-of-theart performance superiority cycle-sum model previous baselines	positive
title-guided encoding for keyphrase generation	keyphrase generation kg aims generate set keyphrases given document fundamental task natural language processing nlp previous methods solve problem extractive manner recently several attempts made generative setting using deep neural networks however state-of-the-art generative methods simply treat document title document main body equally ignoring leading role title overall document solve problem introduce new model called title-guided network tg-net automatic keyphrase generation task based encoderdecoder architecture two new features title additionally employed query-like input ii titleguided encoder gathers relevant information title word document experiments range kg datasets demonstrate model outperforms state-of-the-art models large margin especially documents either low high title length ratios	negative
semantic sentence matching with densely-connected recurrent and co-attentive information	sentence matching widely used various natural language tasks natural language inference paraphrase identification question answering tasks understanding logical semantic relationship two sentences required yet challenging although attention mechanism useful capture semantic relationship properly align elements two sentences previous methods attention mechanism simply use summation operation retain original features enough inspired densenet densely connected convolutional network propose densely-connected co-attentive recurrent neural network layer uses concatenated information attentive features well hidden features preceding recurrent layers enables preserving original co-attentive feature information bottommost word embedding layer uppermost recurrent layer alleviate problem ever-increasing size feature vectors due dense concatenation operations also propose use autoencoder dense concatenation evaluate proposed architecture highly competitive benchmark datasets related sentence matching experimental results show architecture retains recurrent attentive features achieves state-of-the-art performances tasks	negative
one-class adversarial nets for fraud detection	many online applications online social networks knowledge bases often attacked malicious users commit different types actions vandalism wikipedia fraudulent reviews ebay currently fraud detection approaches require training dataset contains records benign malicious users however practice often records malicious users paper develop one-class adversarial nets ocan fraud detection benign users training data ocan first uses lstm-autoencoder learn representations benign users sequences online activities detects malicious users training discriminator complementary gan model different regular gan model experimental results show ocan outperforms state-of-the-art oneclass classification models achieves comparable performance latest multi-source lstm model requires benign malicious users training phase	negative
semantic relationships guided representation learning for facial action unit recognition	facial action unit au recognition crucial task facial expressions analysis attracted extensive attention field artificial intelligence computer vision existing works either focused designing learning complex regional feature representations delved various types au relationship modeling albeit varying degrees progress still arduous existing methods handle complex situations paper investigate integrate semantic relationship propagation aus deep neural network framework enhance feature representation facial regions propose au semantic relationship embedded representation learning srerl framework specifically analyzing symbiosis mutual exclusion aus various facial expressions organize facial aus form structured knowledge-graph integrate gated graph neural network ggnn multi-scale cnn framework propagate node information graph generating enhanced au representation learned feature involves appearance characteristics au relationship reasoning proposed model robust cope challenging cases e.g. illumination change partial occlusion extensive experiments two public benchmarks demonstrate method outperforms previous work achieves state art performance	negative
adapting translation models for transcript disfluency detection	transcript disfluency detection tdd important component real-time speech translation system arouses interests recent years paper presents study adapting neural machine translation nmt models tdd propose general training framework adapting nmt models tdd task rapidly framework main structure model implemented similar nmt model additionally several extended modules training techniques independent nmt model proposed improve performance constrained decoding denoising autoencoder initialization tdd-specific training object proposed training framework achieve significant improvement however slow decoding practical build feasible production-ready solution tdd propose fast non-autoregressive tdd model following non-autoregressive nmt model emerged recently even assume specific architecture nmt model build tdd model basis transformer state-of-the-art nmt model conduct extensive experiments publicly available set switchboard in-house chinese set experimental results show proposed model significantly outperforms previous state-ofthe-art models	negative
a neural multi-task learning framework to jointly model medical named entity recognition and normalization	state-of-the-art studies demonstrated superiority joint modeling pipeline implementation medical named entity recognition normalization due mutual benefits two processes exploit benefits sophisticated way propose novel deep neural multi-task learning framework explicit feedback strategies jointly model recognition normalization one hand method benefits general representations tasks provided multi-task learning hand method successfully converts hierarchical tasks parallel multi-task setting maintaining mutual supports tasks aspects improve model performance experimental results demonstrate method performs significantly better state-of-theart approaches two publicly available medical literature datasets	negative
supervae: superpixelwise variational autoencoder for salient object detection	image saliency detection recently witnessed rapid progress due deep neural networks however still exist many important problems existing deep learning based methods pixel-wise convolutional neural network cnn methods suffer blurry boundaries due convolutional pooling operations region-based deep learning methods lack spatial consistency since deal region independently paper propose novel salient object detection framework using superpixelwise variational autoencoder supervae network first use vae model image background separate salient objects background reconstruction residuals better capture semantic spatial contexts information also propose perceptual loss take advantage deep pre-trained cnns train supervae network without supervision mask-level annotated data method generates high quality saliency results better preserve object boundaries maintain spatial consistency extensive experiments five wildly-used benchmark datasets show proposed method achieves superior competitive performance compared algorithms including recent state-of-the-art supervised methods	negative
data-adaptive metric learning with scale alignment	central problem existing metric learning methods find suitable projection matrix differences pairs data points however single unified projection matrix hardly characterize data similarities accurately practical data usually complicated simply adopting one global projection matrix might ignore important local patterns hidden dataset address issue paper proposes novel method dubbed “ data-adaptive metric learning ” daml constructs data-adaptive projection matrix data pair selectively combining set learned candidate matrices result every data pair obtain specific projection matrix enabling proposed daml flexibly fit training data produce discriminative projection results model daml formulated optimization problem jointly learns candidate projection matrices sparse combination every data pair nevertheless over-fitting problem may occur due large amount parameters learned tackle issue adopt total variation tv regularizer align scales data embedding produced candidate projection matrices thus generated metrics learned candidates generally comparable furthermore extend basic linear daml model kernerlized version denoted “ kdaml ” handle non-linear cases iterative shrinkage-thresholding algorithm ista employed solve optimization model intensive experimental results various applications including retrieval classification verification clearly demonstrate superiority algorithm state-of-the-art metric learning methodologies	positive
modeling local dependence in natural language with multi-channel recurrent neural networks	recurrent neural networks rnns widely used processing natural language tasks achieve huge success traditional rnns usually treat token sentence uniformly equally however may miss rich semantic structure information sentence useful understanding natural languages since semantic structures word dependence patterns parameterized challenge capture leverage structure information paper propose improved variant rnn multi-channel rnn mc-rnn dynamically capture leverage local semantic structure information concretely mc-rnn contains multiple channels represents local dependence pattern time attention mechanism introduced combine patterns step according semantic information parameterize structure information adaptively selecting appropriate connection structures among channels way diverse local structures dependence patterns sentences well captured mc-rnn verify effectiveness mc-rnn conduct extensive experiments typical natural language processing tasks including neural machine translation abstractive summarization language modeling experimental results tasks show significant improvements mc-rnn current top systems	negative
popularity prediction on online articles with deep fusion of temporal process and content features	predicting popularity online article sheds light many applications recommendation advertising information retrieval however several technical challenges addressed developing best predictive capability 1 popularity fluctuates impacts external factors unpredictable hard capture 2 content meta-data features largely determining online content popularity usually multi-modal nontrivial model 3 besides also needs figure integrate temporal process content features modeling popularity prediction different lifecycle stages online articles paper propose deep fusion temporal process content features dftc method tackle modeling temporal popularity process adopt recurrent neural network convolutional neural network multi-modal content features exploit hierarchical attention network embedding technique finally temporal attention fusion employed dynamically integrating parts using datasets collected wechat show proposed model significantly outperforms state-of-the-art approaches popularity prediction	positive
deriving subgoals autonomously to accelerate learning in sparse reward domains	sparse reward games infamous montezuma ’ revenge pose significant challenge reinforcement learning rl agents hierarchical rl promotes efficient exploration via subgoals shown promise games however existing agents rely either human domain knowledge slow autonomous methods derive suitable subgoals work describe new autonomous approach deriving subgoals raw pixels efficient competing methods propose novel intrinsic reward scheme exploiting derived subgoals applying three atari games sparse rewards agent ’ performance comparable state-of-the-art methods demonstrating usefulness subgoals found	negative
free vqa models from knowledge inertia by pairwise inconformity learning	paper uncover issue knowledge inertia visual question answering vqa commonly exists vqa models forces models mainly rely question content “ guess ” answer without regard visual information issue impairs performance vqa models also greatly reduces credibility answer prediction end simply highlighting visual features model undoable since prediction built upon joint modeling two modalities largely influenced data distribution paper propose pairwise inconformity learning pil tackle issue knowledge inertia particular pil takes full advantage similar image pairs diverse answers identical question provided vqa2.0 dataset builds multi-modal embedding space project pos./neg feature pairs upon word vectors answers modeled anchors pil strengthens importance visual features prediction novel dynamic-margin based triplet loss efficiently increases semantic discrepancies pos./neg image pairs verify proposed pil plug baseline vqa model well set recent vqa models conduct extensive experiments two benchmark datasets i.e. vqa1.0 vqa2.0 experimental results show pil boost accuracy existing vqa models 1.56 -2.93 gain negligible increase parameters 0.85 -5.4 parameters qualitative results also reveal elimination knowledge inertia existing vqa models implementing pil	positive
deepchannel: salience estimation by contrastive learning for extractive document summarization	propose deepchannel robust data-efficient interpretable neural model extractive document summarization given document-summary pair estimate salience score modeled using attention-based deep neural network represent salience degree summary yielding document devise contrastive training strategy learn salience estimation network use learned salience score guide iteratively extract salient sentences document generated summary experiments model achieves state-of-the-art rouge scores cnn/daily mail dataset also shows strong robustness out-of-domain test duc2007 test set moreover model reaches rouge-1 f-1 score 39.41 cnn/daily mail test set merely 1/100 training set demonstrating tremendous data efficiency	positive
adversarial label learning	consider task training classifiers without labels propose weakly supervised method—adversarial label learning—that trains classifiers perform well adversary chooses labels training data weak supervision constrains labels adversary choose method therefore minimizes upper bound classifier ’ error rate using projected primal-dual subgradient descent minimizing bound protects bias dependencies weak supervision experiments real datasets show method train without labels outperforms approaches weakly supervised learning	positive
hsme: hypersphere manifold embedding for visible thermal person re-identification	person re-identification re-id great potential contribute video surveillance automatically searches identifies people across different cameras heterogeneous person re-identification thermal infrared visible images essentially cross-modality problem important night-time surveillance application current methods usually train model combining classification metric learning algorithms obtain discriminative robust feature representations however combined loss function ignored correlation classification subspace feature embedding subspace paper use sphere softmax learn hypersphere manifold embedding constrain intra-modality variations cross-modality variations hypersphere propose end-to-end dualstream hypersphere manifold embedding network hsmenet classification identification constraint meanwhile design two-stage training scheme acquire decorrelated features refer hsme decorrelation d-hsme conduct experiments two crossmodality person re-identification datasets experimental results demonstrate method outperforms state-of-the-art methods two datasets regdb dataset rank-1 accuracy improved 33.47 50.85 map improved 31.83 47.00	positive
implementation of boolean and and or logic gates with biologically reasonable time constants in spiking neural networks	latest developments field power-efficient neural interface circuits provide excellent platform applications power consumption primary concern developing neural networks achieve pattern recognition hardware remains daunting task owing substantial computational complexity propose demonstrate spiking neural network snn biologically reasonable time constants implement basic boolean logic gates network applied complex problem statements employ frequency spike encoding data representation model simplified computationally efficient model neuron exponential synapses spike timing dependent plasticity stdp	negative
biomedical image segmentation via representative annotation	deep learning applied successfully many biomedical image segmentation tasks however due diversity complexity biomedical image data manual annotation training common deep learning models timeconsuming labor-intensive especially normally biomedical experts annotate image data well human experts often involved long iterative process annotation active learning type annotation schemes paper propose representative annotation ra new deep learning framework reducing annotation effort biomedical image segmentation ra uses unsupervised networks feature extraction selects representative image patches annotation latent space learned feature descriptors implicitly characterizes underlying data minimizing redundancy fully convolutional network fcn trained using annotated selected image patches image segmentation ra scheme offers three compelling advantages 1 leverages ability deep neural networks learn better representations image data 2 performs one-shot selection manual annotation frees annotators iterative process common active learning based annotation schemes 3 deployed 3d images simple extensions evaluate ra approach using three datasets two 2d one 3d show framework yields competitive segmentation results comparing state-of-the-art methods	negative
mining entity synonyms with efficient neural set generation	mining entity synonym sets i.e. sets terms referring entity important task many entity-leveraging applications previous work either rank terms based similarity given query term treats problem two-phase task i.e. detecting synonymy pairs followed organizing pairs synonym sets however approaches fail model holistic semantics set suffer error propagation issue propose new framework named synsetmine efficiently generates entity synonym sets given vocabulary using example sets external knowledge bases distant supervision synsetmine consists two novel modules 1 set-instance classifier jointly learns represent permutation invariant synonym set whether include new instance i.e. term set 2 set generation algorithm enumerates vocabulary applies learned set-instance classifier detect entity synonym sets experiments three real datasets different domains demonstrate effectiveness efficiency synsetmine mining entity synonym sets	negative
multi3net: segmenting flooded buildings via fusion of multiresolution, multisensor, and multitemporal satellite imagery	propose novel approach rapid segmentation flooded buildings fusing multiresolution multisensor multitemporal satellite imagery convolutional neural network model significantly expedites generation satellite imagery-based flood maps crucial first responders local authorities early stages flood events incorporating multitemporal satellite imagery model allows rapid accurate post-disaster damage assessment used governments better coordinate medium- long-term financial assistance programs affected areas network consists multiple streams encoder-decoder architectures extract spatiotemporal information medium-resolution images spatial information high-resolution images fusing resulting representations single medium-resolution segmentation map flooded buildings compare model state-of-the-art methods building footprint segmentation well alternative fusion approaches segmentation flooded buildings find model performs best tasks also demonstrate model produces highly accurate segmentation maps flooded buildings using publicly available medium-resolution data instead significantly detailed sparsely available high-resolution data release first open-source dataset fully preprocessed labeled multiresolution multispectral multitemporal satellite images disaster sites along source code	positive
graph based translation memory for neural machine translation	translation memory tm proved helpful improve neural machine translation nmt existing approaches either pursue decoding efficiency merely accessing local information tm encode global information tm yet sacrificing efficiency due redundancy propose efficient approach making use global information tm key idea pack redundant tm compact graph perform additional attention mechanisms packed graph integrating tm representation decoding network implement model extending state-of-the-art nmt transformer extensive experiments three language pairs show proposed approach efficient terms running time space occupation particularly outperforms multiple strong baselines terms bleu scores	negative
learning basis representation to refine 3d human pose estimations	estimating 3d human poses 2d joint positions illposed problem complicated fact estimated 2d joints usually errors 3d pose estimators sensitive work present approach refine inaccurate 3d pose estimations core idea approach learn number bases obtain tight approximations low-dimensional pose manifold 3d pose represented convex combination bases representation requires globally refined poses close pose manifold thus avoiding generating illegitimate poses second designed bases also property guarantee distances among body joints pose within reasonable ranges experiments benchmark datasets show approach obtains legitimate poses baselines particular limb lengths closer ground truth	negative
human-like delicate region erasing strategy for weakly supervised detection	explore principle method address weakly supervised detection problem many deep learning methods solve weakly supervised detection mining various object proposal pooling strategies may cause redundancy generate coarse location overcome limitation propose novel human-like active searching strategy recurrently ignores background discovers class-specific objects erasing undesired pixels image proposed detector acts agent providing guidance erase unremarkable regions eventually concentrating attention foreground proposed agents composed deep q-network trained q-learning algorithm analyze contents image features infer localization action according learned policy best knowledge first attempt apply reinforcement learning address weakly supervised localization image-level labels consequently proposed method validated pascal voc 2007 pascal voc 2012 datasets experimental results show proposed method capable locating single object within 5 steps great significance research weakly supervised localization human-like mechanism	negative
tablesense: spreadsheet table detection with convolutional neural networks	spreadsheet table detection task detecting tables given sheet locating respective ranges automatic table detection key enabling technique initial step spreadsheet data intelligence however detection task challenged diversity table structures table layouts spreadsheet considering analogy cell matrix spreadsheet pixel matrix image encouraged successful application convolutional neural networks cnn computer vision developed tablesense novel end-to-end framework spreadsheet table detection first devise effective cell featurization scheme better leverage rich information cell second develop enhanced convolutional neural network model table detection meet domain-specific requirement precise table boundary detection third propose effective uncertainty metric guide active learning based smart sampling algorithm enables efficient build-up training dataset 22,176 tables 10,220 sheets broad coverage diverse table structures layouts evaluation shows tablesense highly effective 91.3 recall 86.5 precision eob-2 metric significant improvement current detection algorithm used commodity spreadsheet tools state-of-the-art convolutional neural networks computer vision	negative
improving image captioning with conditional generative adversarial nets	paper propose novel conditional-generativeadversarial-nets-based image captioning framework extension traditional reinforcement-learning rl -based encoder-decoder architecture deal inconsistent evaluation problem among different objective language metrics motivated design “ discriminator ” networks automatically progressively determine whether generated caption human described machine generated two kinds discriminator architectures cnn rnnbased structures introduced since advantages proposed algorithm generic enhance existing rl-based image captioning framework show conventional rl training method special case approach empirically show consistent improvements language evaluation metrics different state-of-the-art image captioning models addition well-trained discriminators also viewed objective image captioning evaluators	positive
classification with costly features using deep reinforcement learning	study classification problem feature acquired cost goal optimize trade-off expected classification error feature cost revisit former approach framed problem sequential decision-making problem solved q-learning linear approximation individual actions either requests feature values terminate episode providing classification decision set eight problems demonstrate replacing linear approximation neural networks approach becomes comparable state-of-the-art algorithms developed specifically problem approach flexible improved new reinforcement learning enhancement allows inclusion pre-trained high-performance classifier unlike prior art performance robust across evaluated datasets	negative
bayesian posterior approximation via greedy particle optimization	bayesian inference posterior distributions difficult obtain analytically complex models neural networks variational inference usually uses parametric distribution approximation easily draw samples recently discrete approximation particles attracted attention high expression ability example stein variational gradient descent svgd iteratively optimizes particles although svgd shown computationally efficient empirically theoretical properties clarified yet finite sample bound convergence rate known another example stein points sp method minimizes kernelized stein discrepancy directly althoughafinitesampleboundisassuredtheoretically sp computationally inefficient empirically especially high-dimensional problems paper propose novel method named maximum mean discrepancy minimization frank-wolfe algorithm mmd-fw minimizes mmd greedy way fw algorithm method computationally efficient empirically show finite sample convergence bound linear order finite dimensions	positive
predicting unsolvable deals in the birds of a feather solitaire game	paper analyze birds feather boaf solitaire game played 16 cards large majority deals solvable set unsolvable deals share certain characteristics determined adjacency matrix corresponding “ compatibility graph ” create binary decision tree based three variables predict whether given deal solvable predictive model tested 30,000 random deals correctly classifies 99.9 data	positive
colnet: embedding the semantics of web tables for column type prediction	automatically annotating column types knowledge base kb concepts critical task gain basic understanding web tables current methods rely either table metadata like column name entity correspondences cells kb may fail deal growing web tables incomplete meta information paper propose neural network based column type annotation framework named colnet able integrate kb reasoning lookup machine learning automatically train convolutional neural networks prediction prediction model considers contextual semantics within cell using word representation also embeds semantics column learning locality features multiple cells method evaluated dbpedia two different web table datasets t2dv2 general web limaye wikipedia pages achieves higher performance state-of-the-art approaches	negative
find a reasonable ending for stories: does logic relation help the story cloze test?	natural language understanding challenging problem covers wide range tasks previous methods generally train task separately consider combining cross-task features enhance task performance paper incorporate logic information help natural language inference nli task story cloze test sct previous work sct considered various semantic information sentiment topic lack logic information sentences essential element stories thus propose extract logic information course story improve understanding whole story logic information modeled help nli task experimental results prove strength logic information	negative
plan-and-write: towards better automatic storytelling	automatic storytelling challenging since requires generating long coherent natural language describes sensible sequence events despite considerable efforts automatic story generation past prior work either restricted plot planning generate stories narrow domain paper explore open-domain story generation writes stories given title topic input propose plan-and-write hierarchical generation framework first plans storyline generates story based storyline compare two planning strategies dynamic schema interweaves story planning surface realization text static schema plans entire storyline generating stories experiments show explicit storyline planning generated stories diverse coherent topic generated without creating full plan according automatic human evaluations	negative
consensus adversarial domain adaptation	propose novel domain adaptation framework namely consensus adversarial domain adaptation cada gives freedom target encoder source encoder embed data domains common domaininvariant feature space achieve consensus adversarial learning manner domain discrepancy minimized embedded space yielding generalizable representations framework also extended establish new few-shot domain adaptation scheme f-cada remarkably enhances ada performance efficiently propagating labeled data available target domain extensive experiments conducted task digit recognition across multiple benchmark datasets real-world problem involving wifi-enabled device-free gesture recognition spatial dynamics results show compelling performance cada versus state-of-the-art unsupervised domain adaptation uda supervised domain adaptation sda methods numerical experiments also demonstrate f-cada significantly improve adaptation performance even sparsely labeled data target domain	positive
fast incremental svdd learning algorithm with the gaussian kernel	support vector data description svdd machine learning technique used single-class classification outlier detection idea svdd find set support vectors defines boundary around data dealing online large data existing batch svdd methods rerun iteration propose incremental learning algorithm svdd uses gaussian kernel algorithm builds observation support vectors boundary distance center sphere higher-dimensional feature space mapped gaussian kernel function iteration involves existing support vectors new data point moreover algorithm based solely matrix manipulations support vectors corresponding lagrange multiplier αi ’ automatically selected determined iteration seen complexity algorithm iteration k2 k number support vectors experimental results real data sets indicate fisvdd demonstrates significant gains efficiency almost loss either outlier detection accuracy objective function value	negative
on strength adjustment for mcts-based programs	paper proposes approach strength adjustment mcts-based game-playing programs approach use softmax policy strength index z choose moves importantly filter low quality moves excluding lower simulation count pre-defined threshold ratio maximum simulation count perform theoretical analysis reaching result adjusted policy guaranteed choose moves exceeding lower bound strength using threshold ratio approach applied go program elf opengo experiment results show z highly correlated empirical strength namely given threshold ratio 0.1 z linearly related elo rating regression error 47.95 elo −2≤z ≤2 meanwhile covered strength range 800 elo ratings interval z −2,2 ease strength adjustment using z present two methods adjust strength predict opponents ’ strengths dynamically knowledge result state-of-the-art terms range strengths elo rating maintaining controllable relationship strength strength index	positive
efficient quantization for neural networks with binary weights and low bitwidth activations	quantization shown stunning efficiency deep neural network especially portable devices limited resources existing works uncritically extend weight quantization methods activations however take view best performance obtained applying different quantization methods weights activations respectively paper design new activation function dubbed crelu quantization perspective complement design appropriate initialization method training procedure moreover develop specific quantization strategy formulate forward backward approximation weights binary values quantize activations low bitwdth using linear logarithmic quantizer show first time final quantized model binary weights ultra low bitwidth activations outperforms previous best models large margins imagenet well achieving nearly 10.85× theoretical speedup resnet-18 furthermore ablation experiments theoretical analysis demonstrate effectiveness robustness crelu comparison activation functions	negative
bayesian deep collaborative matrix factorization	paper propose bayesian deep collaborative matrix factorization bdcmf algorithm collaborative filtering cf bdcmf novel bayesian deep generative model learns user item latent vectors users ’ social interactions contents items auxiliary information user-item rating feedback matrix alleviates problem matrix sparsity incorporating items ’ auxiliary users ’ social information model learn robust dense latent representations integrating deep learning bayesian probabilistic framework one deep generative models non-linearity bayesian nature additionally bdcmf derive efficient em-style point estimation algorithm parameter learning improve recommendation performance also derive full bayesian posterior estimation algorithm inference experiments conducted two sparse datasets show bdcmf significantly outperform state-of-the-art cf methods	positive
knowledge tracing machines: factorization machines for knowledge tracing	knowledge tracing sequence prediction problem goal predict outcomes students questions interacting learning platform tracking evolution knowledge student one optimize instruction existing methods either based temporal latent variable models factor analysis temporal features show factorization machines fms model regression classification encompasses several existing models educational literature special cases notably additive factor model performance factor model multidimensional item response theory show using several real datasets tens thousands users items fms estimate student knowledge accurately fast even student data sparsely observed handle side information multiple knowledge components number attempts item skill level approach allows fit student models higher dimension existing models provides testbed try new combinations features order improve existing models	negative
deepdpm: dynamic population mapping via deep neural network	dynamic high resolution data human population distribution great importance wide spectrum activities real-life applications difficult expensive obtain directly therefore generating fine-scaled population distributions coarse population data great significance however three major challenges 1 complexity spatial relations high low resolution population 2 dependence population distributions external information 3 difficulty retrieving temporal distribution patterns paper first propose idea generate dynamic population distributions full-time series design dynamic population mapping via deep neural network deepdpm model describes spatial temporal patterns using coarse data point interest information deepdpm utilize super-resolution convolutional neural network srcnn based model directly map coarse data higher resolution data timeembedded long short-term memory model effectively capture periodicity nature smooth finer-scaled results previous static srcnn model perform extensive experiments real-life mobile dataset collected shanghai results demonstrate deepdpm outperforms previous state-of-the-art methods suite frequent data-mining approaches moreover deepdpm breaks limitation previous works time dimension dynamic predictions all-day time slots obtained	negative
a2-net: molecular structure estimation from cryo-em density volumes	constructing molecular structural models cryoelectron microscopy cryo-em density volumes critical last step structure determination cryo-em technologies methods evolved manual construction structural biologists perform 6d translation-rotation searching extremely compute-intensive paper propose learning-based method formulate problem vision-inspired 3d detection pose estimation task develop deep learning framework amino acid determination 3d cryo-em density volume also design sequence-guided monte carlo tree search mcts thread candidate amino acids form molecular structure framework achieves 91 coverage newly proposed dataset takes minutes typical structure thousand amino acids method hundreds times faster several times accurate existing automated solutions without human intervention	positive
image aesthetic assessment assisted by attributes through adversarial learning	inherent connections among aesthetic attributes aesthetics crucial image aesthetic assessment thoroughly explored yet paper propose novel image aesthetic assessment assisted attributes representation-level label-level attributes used privileged information required training specifically first propose multitask deep convolutional rating network learn aesthetic score attributes simultaneously attributes explored construct better feature representations aesthetic assessment multi-task learning introduce discriminator distinguish predicted attributes aesthetics multi-task deep network ground truth label distribution embedded training data multi-task deep network wants output aesthetic score attributes close ground truth labels possible thus deep network discriminator compete adversarial learning attributes explored enforce distribution predicted attributes aesthetics converge ground truth label distribution experimental results two benchmark databases demonstrate superiority proposed method state art work	negative
non-local context encoder: robust biomedical image segmentation against adversarial attacks	recent progress biomedical image segmentation based deep convolutional neural networks cnns drawn much attention however vulnerability towards adversarial samples overlooked paper first one discovers cnn-based state-of-the-art biomedical image segmentation models sensitive adversarial perturbations limits deployment methods safety-critical biomedical fields paper discover global spatial dependencies global contextual information biomedical image exploited defend adversarial attacks end non-local context encoder nlce proposed model short- long-range spatial dependencies encode global contexts strengthening feature activations channel-wise attention nlce modules enhance robustness accuracy non-local context encoding network nlcen learns robust enhanced pyramid feature representations nlce modules integrates information across different levels experiments lung skin lesion segmentation datasets demonstrated nlcen outperforms state-of-the-art biomedical image segmentation methods adversarial attacks addition nlce modules applied improve robustness cnn-based biomedical image segmentation methods	negative
reinforcement learning under threats	several reinforcement learning rl scenarios mainly security settings may adversaries trying interfere reward generating process however non-stationary environments considered q-learning leads suboptimal results busoniu babuska de schutter 2010 previous game-theoretical approaches problem focused modeling whole multi-agent system game instead shall face problem prescribing decisions single agent supported decision maker dm potential threat model adversary augment mdp account threat introducing threatened markov decision processes tmdps furthermore propose level-k thinking scheme resulting new learning framework deal tmdps empirically test framework showing benefits opponent modeling	negative
data fine-tuning	real-world applications commercial off-the-shelf systems utilized performing automated facial analysis including face recognition emotion recognition attribute prediction however majority commercial systems act black boxes due inaccessibility model parameters makes challenging fine-tune models specific applications stimulated advances adversarial perturbations research proposes concept data fine-tuning improve classification accuracy given model without changing parameters model accomplished modeling data image perturbation problem small amount “ noise ” added input objective minimizing classification loss without affecting visual appearance experiments performed three publicly available datasets lfw celeba muct demonstrate effectiveness proposed concept	negative
training temporal word embeddings with a compass	temporal word embeddings proposed support analysis word meaning shifts time study evolution languages different approaches proposed generate vector representations words embed meaning specific time interval however training process used approaches complex may inefficient may require large text corpora consequence approaches may difficult apply resource-scarce domains scientists limited in-depth knowledge embedding models paper propose new heuristic train temporal word embeddings based word2vec model heuristic consists using atemporal vectors reference i.e. compass training representations specific given time interval use compass simplifies training process makes efficient experiments conducted using state-of-the-art datasets methodologies suggest approach outperforms equals comparable approaches robust terms required corpus size	negative
robust metric learning on grassmann manifolds with generalization guarantees	recent research metric learning methods attracted increasing interests machine learning community applied many applications however existing metric learning methods usually use fixed l2-norm measure distance pairwise data samples projection space provide effective mechanism automatically remove noise exist data samples address issue propose new robust formulation metric learning new model constructs projection higher dimensional grassmann manifold one relative low-dimensional discriminative capability errors sample points considered mle maximum likelihood estimation -like estimator efficient iteratively reweighted algorithm derived solve proposed metric learning model importantly establish generalization bounds proposed algorithm utilizing techniques u-statistics experiments six benchmark datasets clearly show proposed method achieves consistent improvements discrimination accuracy comparison state-of-the-art methods	negative
realtime generation of audible textures inspired by a video stream	showcase model generate soundscape camera stream real time approach relies training video associated meaningful audio track granular synthesizer generates novel sound randomly sampling mixing audio data video favoring timestamps whose frame similar current camera frame semantic similarity frames computed pretrained neural network demo interactive user points mobile phone different objects hears generated sound changes	negative
multi-level deep cascade trees for conversion rate prediction in recommendation system	developing effective efficient recommendation methods challenging modern e-commerce platforms generally speaking two essential modules named “ clickthrough rate prediction ” ctr “ conversion rate prediction ” cvr included cvr module crucial factor affects final purchasing volume directly however indeed challenging due sparseness nature paper tackle problem proposing multilevel deep cascade trees ldctree novel decision tree ensemble approach leverages deep cascade structures stacking gradient boosting decision trees gbdt effectively learn feature representation addition propose utilize cross-entropy tree preceding gbdt input feature representation next level gbdt clear explanation i.e. traversal root leaf nodes next level gbdt corresponds combination certain traversals preceding gbdt deep cascade structure combination rule enable proposed ldctree stronger distributed feature representation ability moreover inspired ensemble learning propose ensemble ldctree e-ldctree encourage model ’ diversity enhance representation ability finally propose improved feature learning method based eldctree f-eldctree taking adequate use weak strong correlation features identified pretrained gbdt models experimental results off-line data set online deployment demonstrate effectiveness proposed methods	negative
anytime recursive best-first search for bounding marginal map	marginal map difficult mixed inference task graphical models existing state-of-the-art solvers task based hybrid best-first depth-first search scheme allows compute upper lower bounds optimal solution value anytime fashion methods however memory intensive schemes via best-first component efficient memory management mechanism reason often less effective practice especially difficult problem instances large search spaces paper introduce new recursive best-first search based bounding scheme operates efficiently within limited memory computes anytime upper lower bounds improve time empirical evaluation demonstrates effectiveness proposed approach current solvers	negative
a framework to coordinate segmentation and recognition	novel coordination framework segmentation recognition proposed conduct two tasks collaboratively iteratively accomplish cooperation objects expressed two aspects shape appearance learned leveraged constraints segmentation object segmentation mask consistent object regions image knowledge shape bottom-top-bottom pathway built using encoder-decoder network capsule neurons encoder extracts features shape used recognition decoder generates reference shapes according features recognition result procedure capsule neurons parse existence object cope interference segmentation appearance knowledge utilized another pathway assist segmentation processing shape appearance information dependent recognition result thus allowing classifier convey object information segmenter experiments demonstrate effectiveness framework model collaboratively segmenting recognizing objects recognized using shapes/shape-patterns	negative
dirichlet multinomial mixture with variational manifold regularization: topic modeling over short texts	conventional topic models suffer severe sparsity problem facing extremely short texts social media posts family dirichlet multinomial mixture dmm handle sparsity problem however still sensitive ordinary noisy words resulting inaccurate topic representations document level paper alleviate problem preserving local neighborhood structure short texts enabling spread topical signals among neighboring documents correct inaccurate topic representations achieved using variational manifold regularization constraining close short texts similar variational topic representations upon idea propose novel laplacian dmm lapdmm topic model document graph construction use word mover ’ distance word embeddings measure document similarities semantic level evaluate lapdmm compare state-of-theart short text topic models several traditional tasks experimental results demonstrate lapdmm achieves significant performance gains baseline models e.g. achieving even 0.2 higher scores clustering classification tasks many cases	positive
cooperative multimodal approach to depression detection in twitter	advent social media presented promising new opportunity early detection depression effectively two challenges overcome first textual visual information must jointly considered make accurate inferences depression second challenge due variety content types posted users difficult extract many relevant indicator texts images work propose use novel cooperative multi-agent model address challenges historical posts users proposed method automatically select related indicator texts images experimental results demonstrate proposed method outperforms state-of-the-art methods large margin 30 error reduction several experiments examples also verify selected posts successfully indicate user depression model obtained robust performance realistic scenarios	negative
scaling-up split-merge mcmc with locality sensitive sampling (lss)	split-merge mcmc monte carlo markov chain one essential popular variants mcmc problems mcmc state consists unknown number components well known state-of-the-art methods split-merge mcmc scale well strategies rapid mixing requires smart informative proposals reduce rejection rate however known smart proposals involve expensive operations suggest informative transitions result cost iteration prohibitive massive scale datasets known uninformative computationally efficient proposals random split-merge leads extremely slow convergence tradeoff mixing time per update cost seems hard get around	negative
an open-world extension to knowledge graph completion models	present novel extension embedding-based knowledge graph completion models enables perform open-world link prediction i.e predict facts entities unseen training based textual description model combines regular link prediction model learned knowledge graph word embeddings learned textual corpus training independently learn transformation map embeddings entity ’ name description graph-based embedding space	positive
tapas: train-less accuracy predictor for architecture search	recent years increasing number researchers practitioners suggesting algorithms large-scale neural network architecture search genetic algorithms reinforcement learning learning curve extrapolation accuracy predictors none however demonstrated highperformance without training new experiments presence unseen datasets propose new deep neural network accuracy predictor estimates fractions second classification performance unseen input datasets without training contrast previously proposed approaches prediction calibrated topological network information also characterization dataset-difficulty allows us re-tune prediction without training predictor achieves performance exceeds 100 networks per second single gpu thus creating opportunity perform large-scale architecture search within minutes present results two searches performed 400 seconds single gpu best discovered networks reach 93.67 accuracy cifar-10 81.01 cifar-100 verified training networks performance competitive automatically discovered state-of-the-art networks however needed small fraction time solution computational resources	negative
selective refinement network for high performance face detection	high performance face detection remains challenging problem especially exists many tiny faces paper presents novel single-shot face detector named selective refinement network srn introduces novel twostep classification regression operations selectively anchor-based face detector reduce false positives improve location accuracy simultaneously particular srn consists two modules selective two-step classification stc module selective two-step regression str module stc aims filter simple negative anchors low level detection layers reduce search space subsequent classifier str designed coarsely adjust locations sizes anchors high level detection layers provide better initialization subsequent regressor moreover design receptive field enhancement rfe block provide diverse receptive field helps better capture faces extreme poses consequence proposed srn detector achieves state-of-the-art performance widely used face detection benchmarks including afw pascal face fddb wider face datasets codes released facilitate studies face detection problem	negative
deep video frame interpolation using cyclic frame generation	video frame interpolation algorithms predict intermediate frames produce videos higher frame rates smooth view transitions given two consecutive frames inputs propose synthesized frames reliable used reconstruct input frames high quality based idea introduce new loss term cycle consistency loss cycle consistency loss better utilize training data enhance interpolation results also maintain performance better less training data integrated frame interpolation network trained end-to-end manner addition cycle consistency loss propose two extensions motion linearity loss edge-guided training motion linearity loss approximates motion two input frames linear regularizes training applying edge-guided training improve results integrating edge information training qualitative quantitative experiments demonstrate model outperforms state-of-the-art methods source codes proposed method experimental results available https //github.com/alex04072000/cyclicgen	negative
ace: an actor ensemble algorithm for continuous control with tree search	paper propose actor ensemble algorithm named ace continuous control deterministic policy reinforcement learning ace use actor ensemble i.e. multiple actors search global maxima critic besides ensemble perspective also formulate ace option framework extending option-critic architecture deterministic intra-option policies revealing relationship ensemble options furthermore perform look-ahead tree search actors learned value prediction model resulting refined value estimation demonstrate significant performance boost ace ddpg variants challenging physical robot simulators	positive
less but better: generalization enhancement of ordinal embedding via distributional margin	absence prior knowledge ordinal embedding methods obtain new representation items low-dimensional euclidean space via set quadruple-wise comparisons ordinal comparisons often come human annotators sufficient comparisons induce success classical approaches however collecting large number labeled data known hard task existing work pay little attention generalization ability insufficient samples meanwhile recent progress large margin theory discloses rather maximizing minimum margin margin mean variance characterize margin distribution crucial overall generalization performance address issue insufficient training samples propose margin distribution learning paradigm ordinal embedding entitled distributional margin based ordinal embedding dmoe precisely first define margin ordinal embedding problem secondly formulate concise objective function avoids maximizing margin mean minimizing margin variance directly exhibits similar effect moreover augmented lagrange multiplier based algorithm customized seek optimal solution dmoe effectively experimental studies simulated realworld datasets provided show effectiveness proposed algorithm	negative
probabilistic logic programming with beta-distributed random variables	enable aproblog—a probabilistic logical programming approach—to reason presence uncertain probabilities represented beta-distributed random variables achieve performance state-of-the-art algorithms highly specified engineered domains simultaneously maintain flexibility offered aproblog handling complex relational domains motivation faithfully capturing distribution probabilities necessary compute expected utility effective decision making uncertainty unfortunately probability distributions highly uncertain due sparse data understand accurately manipulate probability distributions need well-defined theoretical framework provided beta distribution specifies distribution probabilities representing possible values probability exact value unknown	negative
two-stage label embedding via neural factorization machine for multi-label classification	label embedding widely used method exploit label dependency dimension reduction multilabel classification tasks however existing embedding methods intend extract label correlations directly thus might easily trapped complex label hierarchies tackle issue propose novel two-stage label embedding tsle paradigm involves neural factorization machine nfm jointly project features labels latent space encoding phase introduce twin encoding network ten digs pairwise feature label interactions first stage efficiently learn higherorder correlations deep neural networks dnns second stage codewords obtained set hidden layers applied recover output labels decoding phase moreover develop novel learning model leveraging max margin encoding loss label-correlation aware decoding loss adopt mini-batch adam optimize learning model lastly also provide kernel insight better understand proposed tsle extensive experiments various real-world datasets demonstrate proposed model significantly outperforms state-ofthe-art approaches	negative
interaction-aware factorization machines for recommender systems	factorization machine fm widely used supervised learning approach effectively modeling feature interactions despite successful application fm many deep learning variants treating every feature interaction fairly may degrade performance example interactions useless feature may introduce noises importance feature may also differ interacting different features work propose novel model named interaction-aware factorization machine ifm introducing interaction-aware mechanism iam comprises feature aspect field aspect learn flexible interactions two levels feature aspect learns feature interaction importance via attention network field aspect learns feature interaction effect parametric similarity feature interaction vector corresponding field interaction prototype ifm introduces structured control learns feature interaction importance stratified manner allows leverage tweaking interactions feature-wise field-wise levels besides give generalized architecture propose interaction-aware neural network inn deepifm capture higher-order interactions improve performance efficiency ifm sampling scheme developed select interactions based field aspect importance experimental results two well-known datasets show superiority proposed models state-of-the-art methods	negative
data augmentation for spoken language understanding via joint variational generation	data scarcity one main obstacles domain adaptation spoken language understanding slu due high cost creating manually tagged slu datasets recent works neural text generative models particularly latent variable models variational autoencoder vae shown promising results regards generating plausible natural sentences paper propose novel generative architecture leverages generative power latent variable models jointly synthesize fully annotated utterances experiments show existing slu models trained additional synthetic examples achieve performance gains approach helps alleviate data scarcity issue slu task many datasets also indiscriminately improves language understanding performances various slu models supported extensive experiments rigorous statistical testing	negative
using benson’s algorithm for regularization parameter tracking	regularized loss minimization statistical model obtained minimizing sum loss function weighted regularization terms still widespread use machine learning statistical performance resulting models depends choice weights regularization parameters typically tuned cross-validation finding best regularization parameters regularized minimization problem needs solved whole parameter domain practically feasible approach covering parameter domain approximate solutions loss minimization problem prescribed approximation accuracy problem computing covering known approximate solution gamut problem existing algorithms solution gamut problem suffer several problems instance require grid parameter domain whose spacing difficult determine practice generic sense rely problem specific plug-in functions show well-known algorithm vector optimization namely benson algorithm used directly computing approximate solution gamuts avoiding problems existing algorithms experiments elastic net real world data sets demonstrate effectiveness benson ’ algorithm regularization parameter tracking	negative
data augmentation based on adversarial autoencoder handling imbalance for learning to rank	data imbalance key limiting factor learning rank ltr models information retrieval resampling methods ensemble methods handle imbalance problem well since none incorporate informative data training procedure ltr models propose data generation model based adversarial autoencoder aae tackling data imbalance ltr via informative data augmentation model utilized handling two types data imbalance namely imbalance regarding relevance levels particular query imbalance regarding amount relevance judgements different queries proposed model relevance information disentangled latent representations aae-based model order reconstruct data specific relevance levels semantic information queries derived word embeddings incorporated adversarial training stage regularizing distribution latent representation two informative data augmentation strategies suitable ltr designed utilizing proposed data generation model experiments benchmark ltr datasets demonstrate proposed framework significantly improve performance ltr models	negative
improving gan with neighbors embedding and gradient matching	propose two new techniques training generative adversarial networks gans unsupervised setting objectives alleviate mode collapse gan improve quality generated samples first propose neighbor embedding manifold learning-based regularization explicitly retain local structures latent samples generated samples prevents generator producing nearly identical data samples different latent samples reduces mode collapse propose inverse t-sne regularizer achieve second propose new technique gradient matching align distributions generated samples real samples challenging work high-dimensional sample distributions propose align distributions scalar discriminator scores constrain difference discriminator scores real samples generated ones constrain difference gradients discriminator scores derive constraints taylor approximations discriminator function perform experiments demonstrate proposed techniques computationally simple easy incorporated existing systems gradient matching neighbour embedding applied together gn-gan achieves outstanding results 1d/2d synthetic cifar-10 stl-10 datasets e.g fid score 30.80 stl-10 dataset code available https //github.com/tntrung/gan	positive
fanda: a novel approach to perform follow-up query analysis	recent work natural language interfaces databases nlidb attracted considerable attention nlidb allow users search databases using natural language instead sql-like query languages saving users learn query languages multi-turn interaction nlidb usually involves multiple queries contextual information vital understand users ’ query intents paper address typical contextual understanding problem termed follow-up query analysis spite ubiquity follow-up query analysis well studied due two primary obstacles multifarious nature follow-up query scenarios lack high-quality datasets work summarizes typical follow-up query scenarios provides new followup dataset 1000 query triples 120 tables moreover propose novel approach fanda takes account structures queries employs ranking model weakly supervised max-margin learning experimental results followup demonstrate superiority fanda multiple baselines across multiple metrics	negative
sepne: bringing separability to network embedding	many successful methods proposed learning low dimensional representations large-scale networks almost existing methods designed inseparable processes learning embeddings entire networks even small proportion nodes interest leads great inconvenience especially super-large dynamic networks methods become almost impossible implement paper formalize problem separated matrix factorization based elaborate novel objective function preserves local global information propose sepne simple flexible network embedding algorithm independently learns representations different subsets nodes separated processes implementing separability algorithm reduces redundant efforts embed irrelevant nodes yielding scalability super-large networks automatic implementation distributed learning adaptations demonstrate effectiveness approach several real-world networks different scales subjects comparable accuracy approach significantly outperforms state-of-the-art baselines running times large networks	negative
latent multi-task architecture learning	multi-task learning mtl allows deep neural networks learn related tasks sharing parameters networks practice however mtl involves searching enormous space possible parameter sharing architectures find layers subspaces benefit sharing b appropriate amount sharing c appropriate relative weights different task losses recent work addressed problems isolation work present approach learns latent multi-task architecture jointly addresses – c present experiments synthetic data data ontonotes 5.0 including four different tasks seven different domains extension consistently outperforms previous approaches learning latent architectures multi-task problems achieves 15 average error reductions common approaches mtl	negative
unified embedding alignment with missing views inferring for incomplete multi-view clustering	multi-view clustering aims partition data collected diverse sources based assumption views complete however prior assumption hardly satisfied many real-world applications resulting incomplete multi-view learning problem existing attempts problem still following limitations 1 underlying semantic information missing views commonly ignored 2 local structure data well explored 3 importance different views effectively evaluated address issues paper proposes unified embedding alignment framework ueaf robust incomplete multi-view clustering particular locality-preserved reconstruction term introduced infer missing views views naturally aligned consensus graph adaptively learned embedded via reverse graph regularization guarantee common local structure multiple views turn align incomplete views inferred views moreover adaptive weighting strategy designed capture importance different views extensive experimental results show proposed method significantly improve clustering performance comparison state-of-the-art methods	negative
neural machine translation with adequacy-oriented learning	although neural machine translation nmt models advanced state-of-the-art performance machine translation face problems like inadequate translation attribute standard maximum likelihood estimation mle judge real translation quality due several limitations work propose adequacyoriented learning mechanism nmt casting translation stochastic policy reinforcement learning rl reward estimated explicitly measuring translation adequacy benefiting sequence-level training rl strategy accurate reward designed specifically translation model outperforms multiple strong baselines including 1 standard coverage-augmented attention models mle-based training 2 advanced reinforcement adversarial training strategies rewards based word-level bleu character-level chrf3 quantitative qualitative analyses different language pairs nmt architectures demonstrate effectiveness universality proposed approach	negative
transconv: relationship embedding in social networks	representation learning rl social networks facilitates real-world tasks visualization link prediction friend recommendation traditional knowledge graph embedding models learn continuous low-dimensional embedding entities relations however applied social networks existing approaches consider rich textual communications users contains valuable information describe social relationships paper propose transconv novel approach incorporates textual interactions pair users improve representation learning users relationships experiments real social network data show transconv learns better user relationship embeddings compared state-of-theart knowledge graph embedding models moreover results illustrate model robust sparse relationships fewer examples	negative
neural speech synthesis with transformer network	although end-to-end neural text-to-speech tts methods tacotron2 proposed achieve state-of-theart performance still suffer two problems 1 low efficiency training inference 2 hard model long dependency using current recurrent neural networks rnns inspired success transformer network neural machine translation nmt paper introduce adapt multi-head attention mechanism replace rnn structures also original attention mechanism tacotron2 help multi-head self-attention hidden states encoder decoder constructed parallel improves training efficiency meanwhile two inputs different times connected directly self-attention mechanism solves long range dependency problem effectively using phoneme sequences input transformer tts network generates mel spectrograms followed wavenet vocoder output final audio results experiments conducted test efficiency performance new network efficiency transformer tts network speed training 4.25 times faster compared tacotron2 performance rigorous human tests show proposed model achieves state-of-the-art performance outperforms tacotron2 gap 0.048 close human quality 4.39 vs 4.44 mos	negative
evolutionary manytasking optimization based on symbiosis in biocoenosis	evolutionary multitasking significant emerging search paradigm utilizes evolutionary algorithms concurrently optimize multiple tasks multi-factorial evolutionary algorithm renders effectual realization evolutionary multitasking two three tasks however remains room improvement performance capability evolutionary multitasking beyond three tasks paper proposes novel framework called symbiosis biocoenosis optimization sbo address evolutionary many-tasking optimization sbo leverages notion symbiosis biocoenosis transferring information knowledge among different tasks three major components 1 transferring information inter-task individual replacement 2 measuring symbiosis intertask paired evaluations 3 coordinating frequency quantity transfer based symbiosis biocoenosis inter-task individual replacement paired evaluations caters estimation symbiosis symbiosis biocoenosis provides good estimator transfer study examines effectiveness efficiency sbo suite many-tasking benchmark problems designed deal 30 tasks simultaneously experimental results show sbo leads better solutions faster convergence state-of-the-art evolutionary multitasking algorithms moreover results indicate sbo highly capable identifying similarity problems transferring information appropriately	negative
distantly supervised entity relation extraction with adapted manual annotations	investigate task distantly supervised joint entity relation extraction ’ known training distant supervision suffer noisy samples tackle problem propose adapt small manually labelled dataset large automatically generated dataset developing novel adaptation algorithm able transfer high quality heterogeneous entity relation annotations robust consistent way experiments benchmark nyt dataset show approach significantly outperforms state-ofthe-art methods	negative
automatic generation of chinese short product titles for mobile display	paper studies problem automatically extracting short title manually written longer description ecommerce products display mobile devices new extractive summarization problem short text inputs propose feature-enriched network model combining three different categories features parallel experimental results show framework significantly outperforms several baselines substantial gain 4.5 moreover produce extractive summarization dataset ecommerce short texts release research community	positive
learning vine copula models for synthetic data generation	vine copula model flexible high-dimensional dependence model uses bivariate building blocks however number possible configurations vine copula grows exponentially number variables increases making model selection major challenge development work formulate vine structure learning problem vector reinforcement learning representation use neural network find embeddings best possible vine model generate structure throughout experiments synthetic real-world datasets show proposed approach fits data better terms loglikelihood moreover demonstrate model able generate high-quality samples variety applications making good candidate synthetic data generation	negative
subtask gated networks for non-intrusive load monitoring	non-intrusive load monitoring nilm also known energy disaggregation blind source separation problem household ’ aggregate electricity consumption broken electricity usages individual appliances way cost trouble installing many measurement devices numerous household appliances avoided one device needs installed problem well-known since hart ’ seminal paper 1992 recently significant performance improvements achieved adopting deep networks work focus idea appliances on/off states develop deep network performance improvements specifically propose subtask gated network combines main regression network on/off classification subtask network unlike typical multitask learning algorithms multiple tasks simply share network parameters take advantage relevance among tasks subtask gated network multiply main network ’ regression output subtask ’ classification probability standby-power additionally learned proposed solution surpasses state-of-the-art performance benchmark cases subtask gated network effective problem inherently on/off states	negative
active generative adversarial network for image classification	sufficient supervised information crucial machine learning models boost performance however labeling data expensive sometimes difficult obtain active learning approach acquire annotations data human oracle selecting informative samples high probability enhance performance recent emerging studies generative adversarial network gan integrated active learning generate good candidates presented oracle paper propose novel model able obtain labels data cheaper manner without need query oracle model novel reward sample devised measure degree uncertainty obtained classifier trained existing labeled data reward used guide conditional gan generate informative samples higher probability certain label extensive evaluations confirmed effectiveness model showing generated samples capable improving classification performance popular image classification tasks	negative
deeply fusing reviews and contents for cold start users in cross-domain recommendation systems	one promising way solve challenging issues data sparsity cold start recommender systems crossdomain recommendation gained increasing research interest recently cross-domain recommendation aims improve recommendation performance means transferring explicit implicit feedback auxiliary domain target domain although side information review texts item contents proven useful recommendation existing works use one kind side information deeply fuse side information ratings paper propose review content based deep fusion model named rc-dfm crossdomain recommendation first extend stacked denoising autoencoders sdae effectively fuse review texts item contents rating matrix auxiliary target domains way learned latent factors users items domains preserve semantic information recommendation utilize multi-layer perceptron transfer user latent factors two domains address data sparsity cold start issues experimental results real datasets demonstrate superior performance rc-dfm compared state-of-the-art recommendation methods	negative
relation structure-aware heterogeneous information network embedding	heterogeneous information network hin embedding aims embed multiple types nodes low-dimensional space although existing hin embedding methods consider heterogeneous relations hins usually employ one single model relations without distinction inevitably restricts capability network embedding paper take structural characteristics heterogeneous relations consideration propose novel relation structure-aware heterogeneous information network embedding model rhine exploring real-world networks thorough mathematical analysis present two structure-related measures consistently distinguish heterogeneous relations two categories affiliation relations ars interaction relations irs respect distinctive characteristics relations rhine propose different models specifically tailored handle ars irs better capture structures semantics networks last combine optimize models unified elegant manner extensive experiments three real-world datasets demonstrate model significantly outperforms state-of-the-art methods various tasks including node clustering link prediction node classification	negative
a layer decomposition-recomposition framework for neuron pruning towards accurate lightweight networks	neuron pruning efficient method compress network slimmer one reducing computational cost storage overhead state-of-the-art results obtained layer-by-layer optimization mode discards unimportant input neurons uses survived ones reconstruct output neurons approaching original ones layer-by-layer manner however unnoticed problem arises information loss accumulated layer increases since survived neurons still encode entire information better alternative propagate entire useful information reconstruct pruned layer instead directly discarding less important neurons end propose novel layer decompositionrecomposition framework ldrf neuron pruning layer ’ output information recovered embedding space propagated reconstruct following pruned layers useful information preserved mainly conduct experiments ilsvrc-12 benchmark vgg-16 resnet-50 emphasized results end-to-end fine-tuning significantly superior owing information-preserving property proposed framework end-to-end fine-tuning achieve state-of-the-art results 5.13× 3× speed-up 0.5 0.65 top-5 accuracy drop respectively outperform existing neuron pruning methods	negative
joint extraction of entities and overlapping relations using position-attentive sequence labeling	joint entity relation extraction detect entity relation using single model paper present novel unified joint extraction model directly tags entity relation labels according query word position p i.e. detecting entity p identifying entities positions relationship former end first design tagging scheme generate n tag sequences n-word sentence position-attention mechanism introduced produce different sentence representations every query position model n tag sequences way method simultaneously extract entities type well overlapping relations experiment results show framework performances significantly better extracting overlapping relations well detecting long-range relation thus achieve state-of-the-art performance two public datasets	negative
virtual-taobao: virtualizing real-world online retail environment for reinforcement learning	applying reinforcement learning physical-world tasks extremely challenging commonly infeasible sample large number trials required current reinforcement learning methods physical environment paper reports project using reinforcement learning better commodity search taobao one largest online retail platforms meanwhile physical environment high sampling cost instead training reinforcement learning taobao directly present environment-building approach build virtual-taobao simulator learned historical customer behavior data train policies virtual-taobao physical sampling costs improve simulation precision propose gan-sd gan simulating distributions customer feature generation better matched distribution propose mail multiagent adversarial imitation learning generating better generalizable customer actions avoid overfitting imperfection simulator propose anc action norm constraint strategy regularize policy model experiments virtual-taobao trained hundreds millions real taobao customers ’ records compared real taobao virtual-taobao faithfully recovers important properties real environment show policies trained purely virtual-taobao zero physical sampling cost significantly superior real-world performance traditional supervised approaches online a/b tests hope work may shed light applying reinforcement learning complex physical environments	negative
biologically motivated algorithms for propagating local target representations	finding biologically plausible alternatives back-propagation errors fundamentally important challenge artificial neural network research paper propose learning algorithm called error-driven local representation alignment lra-e strong connections predictive coding theory offers mechanistic way describing neurocomputational machinery addition propose improved variant difference target propagation another procedure comes family algorithms lra-e. compare procedures several biologicallymotivated algorithms including two feedback alignment algorithms equilibrium propagation two benchmarks find proposed algorithms yield stable performance strong generalization compared competing back-propagation alternatives training deeper highly nonlinear networks lra-e performing best overall	negative
variational autoencoder with implicit optimal priors	variational autoencoder vae powerful generative model estimate probability data point using latent variables vae posterior latent variable given data point regularized prior latent variable using kullback leibler kl divergence although standard gaussian distribution usually used prior simple prior incurs over-regularization sophisticated prior aggregated posterior introduced expectation posterior data distribution prior optimal vae terms maximizing training objective function however kl divergence aggregated posterior calculated closed form prevents us using optimal prior proposed method introduce density ratio trick estimate kl divergence without modeling aggregated posterior explicitly since density ratio trick work well high dimensions rewrite kl divergence contains high-dimensional density ratio sum analytically calculable term lowdimensional density ratio term density ratio trick applied experiments various datasets show vae implicit optimal prior achieves high density estimation performance	negative
generating distractors for reading comprehension questions from real examinations	investigate task distractor generation multiple choice reading comprehension questions examinations contrast previous works aim preparing words short phrases distractors instead endeavor generate longer semantic-rich distractors closer distractors real reading comprehension examinations taking reading comprehension article pair question correct option input goal generate several distractors somehow related answer consistent semantic context question trace article propose hierarchical encoderdecoder framework static dynamic attention mechanisms tackle task specifically dynamic attention combine sentence-level word-level attention varying recurrent time step generate readable sequence static attention modulate dynamic attention focus question irrelevant sentences sentences contribute correct option proposed framework outperforms several strong baselines first prepared distractor generation dataset real reading comprehension questions human evaluation compared distractors generated baselines generated distractors functional confuse annotators	negative
learning resource allocation and pricing for cloud profit maximization	cloud computing widely adopted support various computation services fundamental problem faced cloud providers efficiently allocate resources upon user requests price resource usage order maximize resource efficiency hence provider profit existing studies establish detailed performance models cloud resource usage propose offline online algorithms decide allocation pricing differently adopt blackbox approach leverage model-free deep reinforcement learning drl capture dynamics cloud users better characterize inherent connections optimal allocation/pricing policy states dynamic cloud system goal learn policy maximizes net profit cloud provider trial error better decisions made explicit performance models combine long short-term memory lstm units fully-connected neural networks drl deal online user arrivals adjust output update methods basic drl algorithms address resource allocation pricing evaluation based real-world datasets shows drl approach outperforms basic drl algorithms state-of-theart white-box online cloud resource allocation/pricing algorithms significantly terms profit number accepted users	positive
tile2vec: unsupervised representation learning for spatially distributed data	geospatial analysis lacks methods like word vector representations pre-trained networks significantly boost performance across wide range natural language computer vision tasks fill gap introduce tile2vec unsupervised representation learning algorithm extends distributional hypothesis natural language — words appearing similar contexts tend similar meanings — spatially distributed data demonstrate empirically tile2vec learns semantically meaningful representations image non-image datasets learned representations significantly improve performance downstream classification tasks similarly word vectors allow visual analogies obtained via simple arithmetic latent space	positive
cousin network guided sketch recognition via latent attribute warehouse	study problem sketch image recognition problem plagued two major challenges 1 sketch images often scarce contrast abundance natural images rendering training task difficult 2 significant domain gap sketch image natural image counterpart makes task bridging two domains challenging order overcome challenges paper propose transfer knowledge network learned natural images sketch network new deep net architecture term cousin network network guides sketch-recognition network extract relevant features close natural images via adversarial training moreover enhance transfer ability classification model sketch-to-image attribute warehouse constructed approximate transformation sketch domain real image domain extensive experiments conducted tu-berlin dataset show proposed model able efficiently distill knowledge natural images achieves superior performance current state art	negative
robust multi-agent reinforcement learning via minimax deep deterministic policy gradient	despite recent advances deep reinforcement learning drl agents trained drl tend brittle sensitive training environment especially multi-agent scenarios multi-agent setting drl agent ’ policy easily get stuck poor local optima w.r.t training partners – learned policy may locally optimal agents ’ current policies paper focus problem training robust drl agents continuous actions multi-agent learning setting trained agents still generalize opponents ’ policies alter tackle problem proposed new algorithm minimax multi-agent deep deterministic policy gradient m3ddpg following contributions 1 introduce minimax extension popular multi-agent deep deterministic policy gradient algorithm maddpg robust policy learning 2 since continuous action space leads computational intractability minimax learning objective propose multi-agent adversarial learning maal efficiently solve proposed formulation empirically evaluate m3ddpg algorithm four mixed cooperative competitive multi-agent environments agents trained method significantly outperforms existing baselines	negative
cross-domain visual representations via unsupervised graph alignment	unsupervised domain adaptation distributions visual representations mismatched across domains leads performance drop source model target domain therefore distribution alignment methods proposed explore cross-domain visual representations however alignment methods considered difference distribution structures across domains adaptation would subject insufficient aligned cross-domain representations avoid misclassification/misidentification due difference distribution structures paper proposes novel unsupervised graph alignment method aligns data representations distribution structures across source target domains adversarial network developed unsupervised graph alignment maps source target data feature space data distributed unified structure criteria experimental results show graph-aligned visual representations achieve good performance crossdataset recognition cross-modal re-identification	negative
energy confused adversarial metric learning for zero-shot image retrieval and clustering	deep metric learning widely applied many computer vision tasks recently attractive zeroshot image retrieval clustering zsrc good embedding requested unseen classes distinguished well existing works deem ’ good ’ embedding discriminative one thus race devise powerful metric objectives hard-sample mining strategies leaning discriminative embedding however paper first emphasize generalization ability core ingredient ’ good ’ embedding well largely affects metric performance zero-shot settings matter fact propose energy confused adversarial metric learning ecaml framework explicitly optimize robust metric mainly achieved introducing interesting energy confusion regularization term daringly breaks away traditional metric learning idea discriminative objective devising seeks ’ confuse ’ learned model encourage generalization ability reducing overfitting seen classes train confusion term together conventional metric objective adversarial manner although seems weird ’ confuse ’ network show ecaml indeed serves efficient regularization technique metric learning applicable various conventional metric methods paper empirically experimentally demonstrates importance learning embedding good generalization achieving state-of-theart performances popular cub cars stanford online products in-shop datasets zsrc tasks code available http //www.bhchen.cn/	negative
uncovering specific-shape graph anomalies in attributed graphs	networks ubiquitous modern era point anomalies changed graph anomalies terms anomaly shapes however specific-shape priors anomalous subgraphs interest seldom considered traditional approaches detecting subgraphs attributed graphs e.g. computer networks bitcoin networks etc. paper proposes nonlinear approach specific-shape graph anomaly detection nonlinear approach focuses optimizing broad class nonlinear cost functions via specific-shape constraints attributed graphs approach used many different graph anomaly settings traditional approaches support linear cost functions e.g. aggregation function summation node weights however approach employ powerful nonlinear cost functions enjoys rigorous theoretical guarantee near-optimal solution geometrical convergence rate	negative
spatial mixture models with learnable deep priors for perceptual grouping	humans perceive seemingly chaotic world structured compositional way prerequisite able segregate conceptual entities complex visual scenes mechanism grouping basic visual elements scenes conceptual entities termed perceptual grouping work propose new type spatial mixture models learnable priors perceptual grouping different existing methods proposed method disentangles representation object “ shape ” “ appearance ” modeled separately mixture weights conditional probability distributions specifically object visual scene modeled one mixture component whose mixture weights parameter conditional probability distribution generated two neural networks respectively mixture weights focus modeling spatial dependencies i.e. shape conditional probability distributions deal intra-object variations i.e. appearance addition background separately modeled special component complementary foreground objects extensive empirical tests two perceptual grouping datasets demonstrate proposed method outperforms stateof-the-art methods experimental configurations learned conceptual entities generalizable novel visual scenes insensitive diversity objects	negative
partial multi-label learning by low-rank and sparse decomposition	multi-label learning mll aims learn training data example represented single instance associated set candidate labels existing mll methods typically designed handle problem missing labels however many real-world scenarios labeling information multi-label data always redundant solved classical mll methods thus novel partial multi-label learning pml framework proposed cope problem i.e removing noisy labels multi-label sets paper order improve denoising capability pml framework utilize low-rank sparse decomposition scheme propose novel partial multi-label learning low-rank sparse decomposition pml-lrs approach specifically first reformulate observed label set label matrix decompose groundtruth label matrix irrelevant label matrix former constrained low rank latter assumed sparse next utilize feature mapping matrix explore label correlations meanwhile constrain feature mapping matrix low rank prevent proposed method overfitting finally obtain ground-truth labels via minimizing label loss augmented lagrange multiplier alm algorithm incorporated solve optimization problem enormous experimental results demonstrate pml-lrs achieve superior competitive performance state-of-the-art methods	negative
mind your language: abuse and offense detection for code-switched languages	multilingual societies like indian subcontinent use code-switched languages much popular convenient users paper study offense abuse detection code-switched pair hindi english i.e hinglish pair spoken task made difficult due non-fixed grammar vocabulary semantics spellings hinglish language apply transfer learning make lstm based model hate speech classification model surpasses performance shown current best models establish state-of-the-art unexplored domain hinglish offensive text classification also release model embeddings trained research purposes	negative
learning disentangled representation with pairwise independence	unsupervised disentangled representation learning one foundational methods learn interpretable factors data existing learning methods based assumption disentangled factors mutually independent incorporate assumption evidence lower bound however experiment reveals factors real-world data tend pairwise independent accordingly propose new method based pairwise independence assumption learn disentangled representation evidence lower bound implicitly encourages mutual independence latent codes strong assumption therefore introduce another lower bound method extensive experiments show proposed method gives competitive performances compared state-of-the-art methods	negative
scientific article search system based on discourse facet representation	present browser-based scientific article search system graphical visualization system based triples distributed representations articles triple representing scientific discourse facet objective method result using text citation information facet article encoded separate vector similarity articles measured considering articles entirety also facet-by-facet basis system provides three search options similarity ranking search citation graph facet-labeled edges scatter plot visualization facets axes	positive
mr-net: exploiting mutual relation for visual relationship detection	inferring interactions objects a.k.a visual relationship detection crucial point vision understanding captures definite concepts object detection previous work treats interaction pair objects one way fail exploit mutual relation objects essential modern visual application work propose mutual relation net dubbed mr-net explore mutual relation paired objects visual relationship detection specifically construct mutual relation space model mutual interaction paired objects employ linear constraint optimize mutual interaction called mutual relation learning mutual relation learning introduce parameters adapt improve performance methods addition devise semantic ranking loss discriminatively penalize predicates semantic similarity ignored traditional loss function e.g. cross entropy softmax mr-net optimizes mutual relation learning together semantic ranking loss siamese network experimental results two commonly used datasets vg vrd demonstrate superior performance proposed approach	negative
a layer-based sequential framework for scene generation with gans	visual world sense interpret interact everyday complex composition interleaved physical entities therefore challenging task generate vivid scenes similar complexity using computers work present scene generation framework based generative adversarial networks gans sequentially compose scene breaking underlying problem smaller ones different existing approaches framework offers explicit control elements scene separate background foreground generators starting initially generated background foreground objects populate scene one-by-one sequential manner via quantitative qualitative experiments subset ms-coco dataset show proposed framework produces diverse images also copes better affine transformations occlusion artifacts foreground objects counterparts	negative
gated residual recurrent graph neural networks for traffic prediction	traffic prediction great importance traffic management public safety challenging affected many complex factors spatial dependency complicated road networks temporal dynamics many factors make traffic prediction challenging task due uncertainty complexity traffic states literature many research works applied deep learning methods traffic prediction problems combining convolutional neural networks cnns recurrent neural networks rnns cnns utilized spatial dependency rnns temporal dynamics however combinations capture connectivity globality traffic networks paper first propose adopt residual recurrent graph neural networks res-rgnn capture graph-based spatial dependencies temporal dynamics jointly due gradient vanishing rnns hard capture periodic temporal correlations hence propose novel hop scheme res-rgnn utilize periodic temporal dependencies based res-rgnn hop res-rgnn finally propose novel end-to-end multiple res-rgnns framework referred “ mres-rgnn ” traffic prediction experimental results two traffic datasets demonstrated proposed mres-rgnn outperforms state-of-the-art methods significantly	negative
geometry-aware face completion and editing	face completion challenging generation task requires generating visually pleasing new pixels semantically consistent unmasked face region paper proposes geometry-aware face completion editing network fcenet systematically studying facial geometry unmasked region firstly facial geometry estimator learned estimate facial landmark heatmaps parsing maps unmasked face image encoder-decoder structure generator serves complete face image disentangle mask areas conditioned masked face image estimated facial geometry images besides since low-rank property exists manually labeled masks low-rank regularization term imposed disentangled masks enforcing completion network manage occlusion area various shape size furthermore network generate diverse results masked input modifying estimated facial geometry provides flexible mean edit completed face appearance extensive experimental results qualitatively quantitatively demonstrate network able generate visually pleasing face completion results edit face attributes well	negative
transductive bounds for the multi-class majority vote classifier	paper propose transductive bound risk majority vote classifier learned partially labeled data multi-class classification bound obtained considering class confusion matrix error indicator involves margin distribution classifier class bound risk associated gibbs classifier latter bound tight errors majority vote classifier per class concentrated low margin zone prove bound bayes classifier ’ risk tight application extend self-learning algorithm multi-class case algorithm iteratively assigns pseudo-labels subset unlabeled training examples associated class margin threshold obtained proposed transductive bound empirical results different data sets show effectiveness approach compared algorithm threshold fixed manually extension tsvm multi-class classification graph-based semi-supervised algorithm	positive
i know the relationships: zero-shot action recognition via two-stream graph convolutional networks and knowledge graphs	recently ever-growing action categories zero-shot action recognition zsar achieved automatically mining underlying concepts e.g. actions attributes videos however existing methods exploit visual cues concepts ignore external knowledge information modeling explicit relationships fact humans remarkable ability transfer knowledge learned familiar classes recognize unfamiliar classes narrow knowledge gap existing methods humans propose end-to-end zsar framework based structured knowledge graph jointly model relationships action-attribute action-action attribute-attribute effectively leverage knowledge graph design novel two-stream graph convolutional network ts-gcn consisting classifier branch instance branch specifically classifier branch takes semantic-embedding vectors concepts input generates classifiers action categories instance branch maps attribute embeddings scores video instance attribute-feature space finally generated classifiers evaluated attribute features video classification loss adopted optimizing whole network addition self-attention module utilized model temporal information videos extensive experimental results three realistic action benchmarks olympic sports hmdb51 ucf101 demonstrate favorable performance proposed framework	negative
learning uniform semantic features for natural language and programming language globally, locally and sequentially	semantic feature learning natural language programming language preliminary step addressing many software mining tasks many existing methods leverage information lexicon syntax learn features textual data however information inadequate represent entire semantics either text sentence code snippet motivates us propose new approach learn semantic features languages extracting three levels information namely global local sequential information textual data tasks involving modalities project data types uniform feature space complementary knowledge utilized representation paper build novel general-purpose feature learning framework called uniembed uniformly learn comprehensive semantic representation natural language programming language experimental results three real-world software mining tasks show uniembed outperforms state-of-the-art models feature learning prove capacity effectiveness model	negative
on completing sparse knowledge base with transitive relation embedding	multi-relation embedding popular approach knowledge base completion learns embedding representations entities relations compute plausibility missing triplet effectiveness embedding approach depends sparsity kb falls infrequent entities appeared times paper addresses issue proposing new model exploiting entity-independent transitive relation patterns namely transitive relation embedding tre tre model alleviates sparsity problem predicting infrequent entities enjoys generalisation power embedding experiments three public datasets seven baselines showed merits tre terms knowledge base completion accuracy well computational complexity	negative
learning incremental triplet margin for person re-identification	person re-identification reid aims match people across multiple non-overlapping video cameras deployed different locations address challenging problem many metric learning approaches proposed among triplet loss one state-of-the-arts work explore margin positive negative pairs triplets prove large margin beneficial particular propose novel multi-stage training strategy learns incremental triplet margin improves triplet loss effectively multiple levels feature maps exploited make learned features discriminative besides introduce global hard identity searching method sample hard identities generating training batch extensive experiments market-1501 cuhk03 dukemtmcreid show approach yields performance boost outperforms existing state-of-the-art methods	positive
multi-perspective relevance matching with hierarchical convnets for social media search	despite substantial interest applications neural networks information retrieval neural ranking models mostly applied “ standard ” ad hoc retrieval tasks web pages newswire articles paper proposes mp-hcnn multi-perspective hierarchical convolutional neural network novel neural ranking model specifically designed ranking short social media posts identify document length informal language heterogeneous relevance signals features distinguish documents domain present model specifically designed characteristics mind model uses hierarchical convolutional layers learn latent semantic soft-match relevance signals character word phrase levels poolingbased similarity measurement layer integrates evidence multiple types matches query social media post well urls contained post extensive experiments using twitter data trec microblog tracks 2011–2014 show model significantly outperforms prior feature-based well existing neural ranking models best knowledge paper presents first substantial work tackling search social media posts using neural ranking models code data publicly available.1	negative
“bilingual expert” can find translation errors	performances machine translation mt systems usually evaluated metric bleu golden references provided however case model inference production deployment golden references usually expensively available human annotation bilingual expertise order address issue translation quality estimation qe without reference propose general framework automatic evaluation translation output qe task conference statistical machine translation wmt first build conditional target language model novel bidirectional transformer named neural bilingual expert model pre-trained large parallel corpora feature extraction qe inference bilingual expert model simultaneously produce joint latent representation source translation real-valued measurements possible erroneous tokens based prior knowledge learned parallel data subsequently features fed simple bi-lstm predictive model quality estimation experimental results show approach achieves state-of-the-art performance public available datasets wmt 2017/2018 qe task	positive
learning to localize objects with noisy labeled instances	paper addresses weakly supervised object localization wsol image-level supervision model missing object locations latent variables contribute novel self-directed optimization strategy infer strategy developed self-directed localization network sd-locnet able localize object instance whose initial location noisy self-directed inference hinges adaptive sampling method identify reliable object instance via measuring localization stability score way resulted model robust noisy initialized object locations find important wsol furthermore introduce reliability induced prior propagation strategy transfer object priors reliable instances unreliable ones promoting feature similarity effectively refines unreliable object instances better localization proposed sd-locnet achieves 70.9 cor-loc 51.3 map pascal voc 2007 surpassing state-of-the-arts large margin	positive
a powerful global test statistic for functional statistical inference	consider problem performing association test functional data scalar variables varying coefficient model setting propose functional projection regression model associated global test statistic aggregate relatively weak signals across domain functional data reducing dimension optimal functional projection direction selected maximize signal-to-noise ratio ridge penalty theoretically systematically study asymptotic distribution global test statistic provide strategy adaptively select optimal tuning parameter use simulations show proposed test outperforms existing state-of-the-art methods functional statistical inference finally apply proposed testing method genome-wide association analysis imaging genetic data uk biobank dataset	positive
stochastic submodular maximization with performance-dependent item costs	formulate new stochastic submodular maximization problem introducing performance-dependent costs items problem consider selecting items case performance item i.e. much item contributes objective function decided randomly cost item depends performance goal problem maximize objective function subject budget constraint costs selected items present adaptive algorithm problem theoretical guaran-√ tee expected objective value least 1−1/ 4 e /2 times maximum value attained adaptive algorithms verify performance algorithm numerical experiments	negative
tensor decomposition for multilayer networks clustering	clustering multilayer networks shown promising approach enhance accuracy various multilayer networks clustering algorithms assume networks derive latent clustering structure jointly learn compatible complementary information different networks excavate one shared underlying structure however assumption conflict many emerging real-life applications due existence noisy/irrelevant networks address issue propose centroid-based multilayer network clustering cmnc novel approach divide irrelevant relationships different network groups uncover cluster structure group simultaneously multilayer networks represented within unified tensor framework simultaneously capturing multiple types relationships set entities imposing rank- lr lr,1 block term decomposition nonnegativity able well interpretations multiple clustering results based graph cut theory numerically transform tensor decomposition problem unconstrained optimization thus solve efficiently nonlinear least squares nls framework extensive experimental results synthetic real-world datasets show effectiveness robustness method noise irrelevant data	positive
direct training for spiking neural networks: faster, larger, better	spiking neural networks snns enables energy efficient implementation emerging neuromorphic hardware gaining attention yet snns shown competitive performance compared artificial neural networks anns due lack effective learning algorithms efficient programming frameworks address issue two aspects 1 propose neuron normalization technique adjust neural selectivity develop direct learning algorithm deep snns 2 via narrowing rate coding window converting leaky integrate-and-fire lif model explicitly iterative version present pytorch-based implementation method towards training large-scale snns way able train deep snns tens times speedup result achieve significantly better accuracy reported works neuromorphic datasets n-mnist dvscifar10 comparable accuracy existing anns pre-trained snns non-spiking datasets cifar10 best knowledge first work demonstrates direct training deep snns high performance cifar10 efficient implementation provides new way explore potential snns	negative
a recursive algorithm for projected model counting	present recursive algorithm projected model counting i.e. problem consisting determining number models k∃x.σk propositional formula σ eliminating given set x variables based ” standard ” model counter algorithm projmc takes advantage disjunctive decomposition scheme ∃x.ς computing k∃x.σk also looks disjoint components input improving computation experiments show many cases projmc significantly efficient previous algorithms projected model counting literature	positive
robust negative sampling for network embedding	many recent network embedding algorithms use negative sampling ns approximate variant computationally expensive skip-gram neural network architecture sga objective paper provide theoretical arguments reveal ns fail properly estimate sga objective suitable candidate network embedding problem distinct objective show ns learn undesirable embeddings result “ popular neighbor problem. ” use theory develop new method “ r-ns ” alleviates problems ns using intelligent negative sampling scheme careful penalization embeddings r-ns scalable large-scale networks empirically demonstrate superiority r-ns ns multi-label classification variety real-world networks including social networks language networks	negative
distributional semantics meets multi-label learning	present label embedding based approach large-scale multi-label learning drawing inspiration ideas rooted distributional semantics specifically skip gram negative sampling sgns approach widely used learn word embeddings besides leading highly scalable model multi-label learning approach highlights interesting connections label embedding methods commonly used multi-label learning paragraph embedding methods commonly used learning representations text data framework easily extends incorporating auxiliary information label-label correlations crucial especially many training instances partially annotated facilitate end-to-end learning develop joint learning algorithm learn embeddings well regression model predicts embeddings new input annotated via efficient gradient based methods demonstrate effectiveness approach extensive set experiments variety benchmark datasets show proposed models perform favorably compared state-of-the-art methods large-scale multi-label learning	positive
successor features based multi-agent rl for event-based decentralized mdps	decentralized mdps dec-mdps provide rigorous framework collaborative multi-agent sequential decisionmaking uncertainty however computational complexity limits practical impact address focus class dec-mdps consisting independent collaborating agents tied together global reward function depends upon entire histories states actions accomplish joint tasks overcome scalability barrier main contributions propose new actor-critic based reinforcement learning rl approach event-based dec-mdps using successor features sf value function representation decouples dynamics environment rewards b present dec-esr decentralized event based successor representation generalizes learning event-based dec-mdps using sf within end-to-end deep rl framework c also show dec-esr allows useful transfer information related different tasks hence bootstraps learning faster convergence new tasks validation purposes test approach large multi-agent coverage problem models schedule coordination agents real urban subway network achieves better quality solutions previous best approaches	negative
learning heterogeneous spatial-temporal representation for bike-sharing demand prediction	bike-sharing systems aiming meeting public ’ need ” last mile ” transportation becoming popular recent years accurate demand prediction model shared bikes though limited amount effectively utilized whenever wherever travel demands despite deep learning methods especially long shortterm memory neural networks lstms improve performance traditional demand prediction methods based temporal representation improvement limited due lack mining complex spatial-temporal relations address issue proposed novel model named stg2vec learn representation heterogeneous spatial-temporal graph specifically developed event-flow serializing method encode evolution dynamic heterogeneous graph special language pattern word sequence corpus furthermore dynamic attention-based graph embedding model introduced obtain importance-awareness vectorized representation event flow additionally together multi-source information geographical position historical transition patterns weather e.g. representation learned stg2vec fed lstms temporal modeling experimental results citi-bike electronic usage records dataset new york city illustrated proposed model achieve competitive prediction performance compared variants baseline models	positive
weisfeiler and leman go neural: higher-order graph neural networks	recent years graph neural networks gnns emerged powerful neural architecture learn vector representations nodes graphs supervised end-to-end fashion gnns evaluated empirically—showing promising results following work investigates gnns theoretical point view relates 1-dimensional weisfeiler-leman graph isomorphism heuristic 1-wl show gnns expressiveness 1-wl terms distinguishing non-isomorphic sub- graphs hence algorithms also shortcomings based propose generalization gnns so-called k-dimensional gnns k-gnns take higher-order graph structures multiple scales account higher-order structures play essential role characterization social networks molecule graphs experimental evaluation confirms theoretical findings well confirms higher-order information useful task graph classification regression	negative
attentive temporal pyramid network for dynamic scene classification	dynamic scene classification important yet challenging problem especially presence defected irrelevant frames due unconstrained imaging conditions illumination camera motion irrelevant background paper propose attentive temporal pyramid network atp-net establish effective representations dynamic scenes extracting aggregating informative discriminative features proposed atp-net detects informative features frames contain relevant information scenes temporal pyramid structure incorporated attention mechanism frame features effectively fused newly designed kernel aggregation layer based kernel approximation discriminative holistic representations dynamic scenes proposed atp-net leverages strength attention mechanism select relevant frame features ability kernels achieve optimal feature fusion discriminative representations dynamic scenes extensive experiments comparisons conducted three benchmark datasets results show superiority state-of-the-art methods three benchmark datasets	negative
hierarchical photo-scene encoder for album storytelling	paper propose novel model hierarchical photo-scene encoder reconstructor task album storytelling photo-scene encoder contains two subencoders namely photo scene encoders stacked together behave hierarchically fully exploit structure information photos within album specifically photo encoder generates semantic representation photo exploiting temporal relationships among scene encoder relying obtained photo representations responsible detecting scene changes generating scene representations subsequently decoder dynamically attentively summarizes encoded photo scene representations generate sequence album representations based story consisting multiple coherent sentences generated order fully extract useful semantic information album reconstructor employed reproduce summarized album representations based hidden states decoder proposed model trained end-to-end manner results improved performance state-of-the-arts public visual storytelling vist dataset ablation studies demonstrate effectiveness proposed hierarchical photo-scene encoder reconstructor	positive
self-supervised mixture-of-experts by uncertainty estimation	learning related tasks various domains transferring exploited knowledge new situations significant challenge reinforcement learning rl however rl algorithms data inefficient fail generalize complex environments limiting adaptability applicability multi-task scenarios paper propose selfsupervised mixture-of-experts sum effective algorithm driven predictive uncertainty estimation multitask rl sum utilizes multi-head agent shared parameters experts learn series related tasks simultaneously deep deterministic policy gradient ddpg expert extended predictive uncertainty estimation known unknown states enhance q-value evaluation capacity overfitting overall generalization ability enable agent capture diffuse common knowledge across different tasks improving sample efficiency task effectiveness expert scheduling across multiple tasks instead task-specific design common moes self-supervised gating network adopted determine potential expert handle interaction unseen environments calibrated completely uncertainty feedback experts without explicit supervision alleviate imbalanced expert utilization crux moe optimization accomplished via decayedmasked experience replay encourages diversification specialization experts different periods demonstrate approach learns faster achieves better performance efficient transfer robust generalization outperforming several related methods extended openai gym ’ mujoco multi-task environments	negative
non-autoregressive neural machine translation with enhanced decoder input	non-autoregressive translation nat models remove dependence previous target tokens inputs decoder achieve significantly inference speedup cost inferior accuracy compared autoregressive translation models previous work shows quality inputs decoder important largely impacts model accuracy paper propose two methods enhance decoder inputs improve nat models first one directly leverages phrase table generated conventional smt approaches translate source tokens target tokens fed decoder inputs second one transforms source-side word embeddings target-side word embeddings sentence-level alignment word-level adversary learning feeds transformed word embeddings decoder inputs experimental results show method largely outperforms nat baseline gu et al 2017 5.11 bleu scores wmt14 english-german task 4.72 bleu scores wmt16 english-romanian task	negative
multi-agent coordination under uncertain communication	multi-agent coordination simple problem significant research gone computing plans efficiently managing competing preferences execution multiagent plans still fail even plan space small agent goals universally aligned reason difficulty order guarantee successful execution plan effective multi-agent coordination requires communication ensure actors accurate beliefs state world thesis focus problem characterizing modeling providing efficient algorithms addressing planning execution agents maintain perfect communication	negative
metastyle: three-way trade-off among speed, flexibility, and quality in neural style transfer	unprecedented booming witnessed research area artistic style transfer ever since gatys et al introduced neural method one remaining challenges balance trade-off among three critical aspects—speed flexibility quality vanilla optimization-based algorithm produces impressive results arbitrary styles unsatisfyingly slow due iterative nature ii fast approximation methods based feed-forward neural networks generate satisfactory artistic effects bound limited number styles iii feature-matching methods like adain achieve arbitrary style transfer real-time manner cost compromised quality find considerably difficult balance trade-off well merely using single feed-forward step ask instead whether exists algorithm could adapt quickly style adapted model maintains high efficiency good image quality motivated idea propose novel method coined metastyle formulates neural style transfer bilevel optimization problem combines learning post-processing update steps adapt fast approximation model satisfying artistic effects comparable optimization-based methods arbitrary style qualitative quantitative analysis experiments demonstrates proposed approach achieves high-quality arbitrary artistic style transfer effectively good trade-off among speed flexibility quality	negative
deepstn+: context-aware spatial-temporal neural network for crowd flow prediction in metropolis	crowd flow prediction great importance wide range applications urban planning traffic control public safety aims predict inflow traffic crowds entering region given time interval outflow traffic crowds leaving region places region city knowing historical flow data paper propose deepstn+ deep learning-based convolutional model predict crowd flows metropolis first deepstn+ employs convplus structure model longrange spatial dependence among crowd flows different regions poi distributions time factor combined express effect location attributes introduce prior knowledge crowd movements finally propose effective fusion mechanism stabilize training process improves performance extensive experimental results based two real-life datasets demonstrate superiority model i.e. deepstn+ reduces error crowd flow prediction approximately 8 ∼13 compared state-of-the-art baselines	positive
residual attribute attention network for face image super-resolution	facial prior knowledge based methods recently achieved great success task face image super-resolution sr combination different type facial knowledge could leveraged better super-resolving face images e.g. facial attribute information texture shape information paper present novel deep end-to-end network face super resolution named residual attribute attention network raan realizes efficient feature fusion various types facial information specifically construct multi-block cascaded structure network dense connection block three branches texture prediction network tpn shape generation network sgn attribute analysis network aan divide task face image reconstruction three steps extracting pixel level representation information input low resolution lr image via tpn sgn extracting semantic level representation information aan input finally combining pixel level semantic level information recover high resolution hr image experiments benchmark database illustrate raan significantly outperforms state-of-the-arts low-resolution face sr problem quantitatively qualitatively	negative
structured two-stream attention network for video question answering	date visual question answering vqa i.e. image qa video qa still holy grail vision language understanding especially video qa compared image qa focuses primarily understanding associations image region-level details corresponding questions video qa requires model jointly reason across spatial long-range temporal structures video well text provide accurate answer paper specifically tackle problem video qa proposing structured two-stream attention network namely sta answer free-form open-ended natural language question content given video first infer rich longrange temporal structures videos using structured segment component encode text features structured two-stream attention component simultaneously localizes important visual instance reduces influence background video focuses relevant text finally structured two-stream fusion component incorporates different segments query video aware context representation infers answers experiments large-scale video qa dataset tgif-qa show proposed method significantly surpasses best counterpart i.e. one representation video input 13.0 13.5 11.0 0.3 action trans. trameqa count tasks also outperforms best competitor i.e. two representations action trans. trameqa tasks 4.1 4.7 5.1	negative
improving neural question generation using answer separation	neural question generation nqg task generating question given passage deep neural networks previous nqg models suffer problem significant proportion generated questions include words question target resulting generation unintended questions paper propose answer-separated seq2seq better utilizes information passage target answer replacing target answer original passage special token model learns identify interrogative word used also propose new module termed keyword-net helps model better capture key information target answer generate appropriate question experimental results demonstrate answer separation method significantly reduces number improper questions include answers consequently model significantly outperforms previous state-of-the-art nqg models	negative
feature isolation for hypothesis testing in retinal imaging: an ischemic stroke prediction case study	ischemic stroke leading cause death long-term disability difficult predict reliably retinal fundus photography proposed stroke risk assessment due non-invasiveness similarity retinal cerebral microcirculations past studies claiming correlation venular caliber stroke risk however may retinal features appropriate paper extensive experiments deep learning six retinal datasets described feature isolation involving segmented vascular tree images applied establish effectiveness vessel caliber shape alone stroke classification dataset ablation applied investigate model generalizability unseen sources results suggest vessel caliber shape could indicative ischemic stroke sourcespecific features could influence model performance	negative
unsupervised neural machine translation with smt as posterior regularization	without real bilingual corpus available unsupervised neural machine translation nmt typically requires pseudo parallel data generated back-translation method model training however due weak supervision pseudo data inevitably contain noises errors accumulated reinforced subsequent training process leading bad translation performance address issue introduce phrase based statistic machine translation smt models robust noisy data posterior regularizations guide training unsupervised nmt models iterative back-translation process method starts smt models built pre-trained language models word-level translation tables inferred cross-lingual embeddings smt nmt models optimized jointly boost incrementally unified em framework way 1 negative effect caused errors iterative back-translation process alleviated timely smt filtering noises phrase tables meanwhile 2 nmt compensate deficiency fluency inherent smt experiments conducted en-fr en-de translation tasks show method outperforms strong baseline achieves new state-of-the-art unsupervised machine translation performance	negative
efficient neutrino oscillation parameter inference with gaussian process	many experiments set-up measure parameters governing neutrino oscillation probabilities accurately implications fundamental structure universe often involves inferences tiny samples data complicated dependencies multiple oscillation parameters simultaneously typically carried using unified approach feldman cousins computationally expensive order tens millions cpu hours work propose iterative method using gaussian process efficiently find confidence contour oscillation parameters show produces results fraction computation cost	negative
object detection based on region decomposition and assembly	region-based object detection infers object regions one categories image due recent advances deep learning region proposal methods object detectors based convolutional neural networks cnns flourishing provided promising detection results however detection accuracy degraded often low discriminability object cnn features caused occlusions inaccurate region proposals paper therefore propose region decomposition assembly detector r-dad accurate object detection	negative
deep bayesian trust: a dominant and fair incentive mechanism for crowd	important class game-theoretic incentive mechanisms eliciting effort crowd peer based mechanisms workers paid matching answers one another classic mechanism workers solve gold standard tasks pay according accuracy gold tasks mechanism ensures stronger incentive compatibility peer based mechanisms assigning gold tasks workers becomes inefficient large scale propose novel mechanism assigns gold tasks workers exploits transitivity derive accuracy rest workers peers ’ accuracy show resulting mechanism ensures dominant notion incentive compatibility fairness	negative
deep bayesian optimization on attributed graphs	attributed graphs contain rich contextual features beyond network structure ubiquitous observed benefit various network analytics applications graph structure optimization aiming find optimal graphs terms specific measures become effective computational tool complex network analysis however traditional model-free methods suffer expensive computational cost evaluating graphs existing vectorial bayesian optimization methods directly applied attributed graphs scalability issue due use gaussian processes gps bridge gap paper propose novel scalable deep graph bayesian optimization dgbo method attributed graphs proposed dgbo prevents cubical complexity gps adopting deep graph neural network surrogate black-box functions scale linearly number observations intensive experiments conducted artificial real-world problems including molecular discovery urban road network design demonstrate effectiveness dgbo compared state-of-the-art	negative
similarity learning via kernel preserving embedding	data similarity key concept many data-driven applications many algorithms sensitive similarity measures tackle fundamental problem automatically learning similarity information data via self-expression developed successfully applied various models low-rank representation sparse subspace learning semisupervised learning however tries reconstruct original data valuable information e.g. manifold structure largely ignored paper argue beneficial preserve overall relations extract similarity information specifically propose novel similarity learning framework minimizing reconstruction error kernel matrices rather reconstruction error original data adopted existing work taking clustering task example evaluate method observe considerable improvements compared state-ofthe-art methods importantly proposed framework general provides novel fundamental building block many similarity-based tasks besides proposed kernel preserving opens large number possibilities embed high-dimensional data low-dimensional space	negative
asynchronous proximal stochastic gradient algorithm for composition optimization problems	machine learning research many emerging applications formulated composition optimization problem nonsmooth regularization penalty solve problem traditional stochastic gradient descent sgd algorithm variants either low convergence rate computationally expensive recently several stochastic composition gradient algorithms proposed however methods still inefficient scalable large-scale composition optimization problem instances address challenges propose asynchronous parallel algorithm named async-proxscvr effectively combines asynchronous parallel implementation variance reduction method prove algorithm admits fastest convergence rate strongly convex general nonconvex cases furthermore analyze query complexity proposed algorithm prove linear speedup accessible increase number processors finally evaluate algorithm async-proxscvr two representative composition optimization problems including value function evaluation reinforcement learning sparse mean-variance optimization problem experimental results show algorithm achieves significant speedups much faster existing compared methods	negative
adaptive sparse confidence-weighted learning for online feature selection	paper propose new online feature selection algorithm streaming data aim focus following two problems remain unaddressed literature first existing online feature selection algorithms merely utilize first-order information data streams regardless fact second-order information explores correlations features significantly improves performance second online feature selection algorithms based balanced data presumption true many real-world applications example fraud detection number positive examples much less negative examples cases fraud balanced assumption make selected features biased towards majority class fail detect fraud cases propose adaptive sparse confidence-weighted ascw algorithm solve aforementioned two problems first introduce 0-norm constraint second-order confidence-weighted cw learning feature selection original loss substituted cost-sensitive loss function address imbalanced data issue furthermore algorithm maintains multiple sparse cw learner corresponding cost vector dynamically select optimal cost theoretically enhance theory sparse cw learning analyze performance behavior f-measure empirical studies show superior performance stateof-the-art online learning methods online-batch setting	positive
robust deep co-saliency detection with group semantic	high-level semantic knowledge addition low-level visual cues essentially crucial co-saliency detection paper proposes novel end-to-end deep learning approach robust co-saliency detection simultaneously learning highlevel group-wise semantic representation well deep visual features given image group inter-image interaction semantic-level well complementarity group semantics visual features exploited boost inferring co-salient regions specifically proposed approach consists co-category learning branch co-saliency detection branch former proposed learn group-wise semantic vector using co-category association image group supervision latter infer precise co-salient maps based ensemble group semantic knowledge deep visual cues group semantic vector broadcasted spatial location multi-scale visual feature maps used top-down semantic guidance boosting bottom-up inferring co-saliency co-category learning co-saliency detection branches jointly optimized multi-task learning manner improving robustness approach moreover construct new large-scale co-saliency dataset coco-seg facilitate research co-saliency detection extensive experimental results coco-seg widely used benchmark cosal2015 demonstrated superiority proposed approach compared state-of-the-art methods	negative
efficient temporal planning using metastates	performing temporal planning forward state-space search effective state memoisation challenging whereas classical planning two states equal facts variable values temporal planning case plans led two states subject temporal constraints one might extendable temporally valid plan might paper present approach reducing state space explosion arises due keep many copies ‘ classically ’ equal state – states classically equal aggregated metastates separated lazily case temporal inconsistency evaluation shows approach implemented optic compared existing state-of-the-art memoisation techniques improves performance across range temporal domains	positive
livebot: generating live video comments based on visual and textual contexts	introduce task automatic live commenting live commenting also called “ video barrage ” emerging feature online video sites allows real-time comments viewers fly across screen like bullets roll right side screen live comments mixture opinions video chit chats comments automatic live commenting requires ai agents comprehend videos interact human viewers also make comments good testbed ai agent ’ ability deal dynamic vision language work construct large-scale live comment dataset 2,361 videos 895,929 live comments introduce two neural models generate live comments based visual textual contexts achieve better performance previous neural baselines sequence-to-sequence model finally provide retrieval-based evaluation protocol automatic live commenting model asked sort set candidate comments based log-likelihood score evaluated metrics mean-reciprocal-rank putting together demonstrate first “ livebot ” datasets codes found https //github.com/lancopku/livebot	negative
ea-cg: an approximate second-order method for training fully-connected neural networks	training fully-connected neural networks fcnns propose practical approximate second-order method including 1 approximation hessian matrix 2 conjugate gradient cg based method proposed approximate hessian matrix memory-efficient applied fcnns activation criterion functions twice differentiable devise cg-based method incorporating one-rank approximation derive newton directions training fcnns significantly reduces space time complexity cg-based method employed solve linear equation coefficient matrix kroneckerfactored symmetric positive definite empirical studies show efficacy efficiency proposed method	negative
on the time complexity of algorithm selection hyper-heuristics for multimodal optimisation	selection hyper-heuristics automated algorithm selection methodologies choose different heuristics optimisation process recently selection hyperheuristics choosing collection elitist randomised local search heuristics different neighbourhood sizes shown optimise standard unimodal benchmark function evolutionary computation optimal expected runtime achievable available low-level heuristics paper extend understanding domain multimodal optimisation considering hyper-heuristic literature switch elitist nonelitist heuristics run first identify range parameters allow hyper-heuristic hillclimb efficiently prove optimise standard hillclimbing benchmark function best expected asymptotic time achievable unbiased mutation-based randomised search heuristics afterwards use standard multimodal benchmark functions highlight function characteristics hyper-heuristic efficient swiftly escaping local optima ones function class called cliffd new gradient increasing fitness identified escaping local optima hyper-heuristic extremely efficient wide range established elitist non-elitist algorithms including well-studied metropolis algorithm complete picture analysis another standard benchmark function called jumpd example highlight problem characteristics hyper-heuristic inefficient yet still outperforms wellestablished non-elitist metropolis algorithm	negative
sentence-wise smooth regularization for sequence to sequence learning	maximum-likelihood estimation mle widely used sequence sequence tasks model training uniformly treats generation/prediction target token multiclass classification yields non-smooth prediction probabilities target sequence tokens predicted small probabilities tokens large probabilities according empirical study find non-smoothness probabilities results low quality generated sequences paper propose sentence-wise regularization method aims output smooth prediction probabilities tokens target sequence proposed method automatically adjust weights gradients token one sentence ensure predictions sequence uniformly well experiments three neural machine translation tasks one text summarization task show method outperforms conventional mle loss tasks achieves promising bleu scores wmt14 english-german wmt17 chinese-english translation task	negative
transferable interactive memory network for domain adaptation in fine-grained opinion extraction	fine-grained opinion mining aspect opinion terms extraction become fundamental task provides key information user-generated texts despite importance lack annotated resources many domains impede ability train precise model attempts applied unsupervised domain adaptation methods transfer fine-grained knowledge word level labeled source domain unlabeled target domain existing methods depend construction “ pivot ” knowledge e.g. common opinion terms syntactic relations aspect opinion words work propose interactive memory network consists local global memory units model could exploit local global memory interactions capture intra-correlations among aspect words opinion words well interconnections aspect opinion words source space target space aligned domaininvariant interactions incorporating auxiliary task domain adversarial networks proposed model require external resources demonstrates promising results 3 benchmark datasets	negative
who blames whom in a crisis? detecting blame ties from news articles using neural networks	blame games tend follow major disruptions financial crises natural disasters terrorist attacks study blame game evolves shapes dominant crisis narratives great significance sense-making processes affect regulatory outcomes social hierarchies cultural norms however takes tremendous time efforts social scientists manually examine relevant news article extract blame ties blames b study define new task blame tie extraction construct new dataset related united states financial crisis 20072010 new york times wall street journal usa today build bi-directional long short-term memory bilstm network contexts entities appear learns automatically extract blame ties document level leveraging large unsupervised model glove elmo best model achieves f1 score 70 test set blame tie extraction making useful tool social scientists extract blame ties efficiently	positive
a dynamic bayesian network based merge mechanism for autonomous vehicles	work explores design central collaborative driving strategy connected cars objective improving road safety case highway on-ramp merging scenario based suitable method predicting vehicle motion behavior central collaborative strategy dynamic bayesian network method predicts intention drivers highway on-ramp proposed method validated using real data detailed vehicle trajectories segment interstate 80 emeryville california	negative
frame revisited: an interpretation view based on particle evolution	frame filters random fields maximum entropy energy-based descriptive model synthesizes visual realism capturing mutual patterns structural input signals maximum likelihood estimation mle applied default yet conventionally causes unstable training energy wrecks generated structures remains unexplained paper provide new theoretical insight analyze frame perspective particle physics ascribing weird phenomenon kl-vanishing issue order stabilize energy dissipation propose alternative wasserstein distance discrete time based conclusion jordan-kinderlehrer-otto jko discrete flow approximates kl discrete flow time step size tends 0. besides metric still maintain model ’ statistical consistency quantitative qualitative experiments respectively conducted several widely used datasets empirical studies evidenced effectiveness superiority method	negative
k3s: knowledge-driven solution support system	volume scientific papers grows rapidly size knowledge management scientific publications greatly needed information extraction knowledge fusion techniques proposed obtain information scholarly publications build knowledge repositories however retrieving knowledge problem/solution academic papers support users solving specific research problems rarely seen state art therefore remedy gap knowledge-driven solution support system k3s proposed paper extract information research problems proposed solutions academic papers integrate knowledge maps bibliometric information papers k3s capable providing recommended solutions extracted problems subject intrusion detection chosen demonstration required information extracted high accuracy knowledge map constructed properly solutions address intrusion problems recommended	negative
dynamic layer aggregation for neural machine translation with routing-by-agreement	promising progress deep neural networks layer aggregation used fuse information across layers various fields computer vision machine translation however previous methods combine layers static fashion aggregation strategy independent specific hidden states inspired recent progress capsule networks paper propose use routing-by-agreement strategies aggregate layers dynamically specifically algorithm learns probability part individual layer representations assigned whole aggregated representations iterative way combines parts accordingly implement algorithm top state-of-the-art neural machine translation model transformer conduct experiments widely-used wmt14 sh⇒german wmt17 chinese⇒english translation datasets experimental results across language pairs show proposed approach consistently outperforms strong baseline model representative static aggregation model	positive
manifold distance-based over-sampling technique for class imbalance learning	over-sampling technology handling class imbalanced problem generates minority samples balance dataset size different classes however sampling original data space ineffective data different classes overlapped disjunct based new minority sample presented terms manifold distance rather euclidean distance overlapped majority minority samples apt distribute fully disjunct subspaces view manifold learning moreover avoid generating samples minority data locating far away manifold space experiments 23 uci datasets show proposed method better classification accuracy	negative
a unified model for opinion target extraction and target sentiment prediction	target-based sentiment analysis involves opinion target extraction target sentiment classification however existing works usually studied one two sub-tasks alone hinders practical use paper aims solve complete task target-based sentiment analysis end-to-end fashion presents novel unified model applies unified tagging scheme framework involves two stacked recurrent neural networks upper one predicts unified tags produce final output results primary target-based sentiment analysis lower one performs auxiliary target boundary prediction aiming guiding upper network improve performance primary task explore inter-task dependency propose explicitly model constrained transitions target boundaries target sentiment polarities also propose maintain sentiment consistency within opinion target via gate mechanism models relation features current word previous word conduct extensive experiments three benchmark datasets framework achieves consistently superior results	negative
exploiting coarse-to-fine task transfer for aspect-level sentiment classification	aspect-level sentiment classification asc aims identifying sentiment polarities towards aspects sentence aspect behave general aspect category ac specific aspect term however due especially expensive labor-intensive labeling existing public corpora at-level relatively small meanwhile previous methods rely complicated structures given scarce data largely limits efficacy neural models paper exploit new direction named coarse-to-fine task transfer aims leverage knowledge learned rich-resource source domain coarse-grained ac task easily accessible improve learning low-resource target domain fine-grained task resolve aspect granularity inconsistency feature mismatch domains propose multi-granularity alignment network mgan mgan novel coarse2fine attention guided auxiliary task help ac task modeling finegrained level task alleviate feature false alignment contrastive feature alignment method adopted align aspect-specific feature representations semantically addition large-scale multi-domain dataset ac task provided empirically extensive experiments demonstrate effectiveness mgan	negative
learning personalized attribute preference via multi-task auc optimization	traditionally existing attribute learning methods trained based consensus annotations aggregated limited number annotators however consensus might fail settings especially wide spectrum annotators different interests comprehension attribute words involved paper develop novel multi-task method understand predict personalized attribute annotations regarding attribute preference learning annotator specific task first propose multi-level task parameter decomposition capture evolution highly popular opinion mass highly personalized choices special person meanwhile personalized learning methods ranking prediction much important accurate classification motivates us employ area roc curve auc based loss function improve model top auc-based loss propose efficient method evaluate loss gradients theoretically propose novel closed-form solution one non-convex subproblem leads provable convergence behaviors furthermore also provide generalization bound guarantee reasonable performance finally empirical analysis consistently speaks efficacy proposed method	negative
crash to not crash: learn to identify dangerous vehicles using a simulator	developing computer vision-based algorithm identifying dangerous vehicles requires large amount labeled accident data difficult collect real world tackle challenge first develop synthetic data generator built top driving simulator observe synthetic labels generated based simulation results noisy resulting poor classification performance order improve quality synthetic labels propose new label adaptation technique first extracts internal states vehicles underlying driving simulator refines labels predicting future paths vehicles based well-studied motion model via real-data experiments show dangerous vehicle classifier reduce missed detection rate least 18.5 compared trained real data time-to-collision 1.6s 1.8s	negative
multistream classification with relative density ratio estimation	supervised learning availability sufficient labeled data prime importance unfortunately sparingly available many real-world applications particularly performing classification non-stationary data stream unavailability sufficient labeled data undermines classifier ’ long-term performance limiting adaptability changes data distribution time recently studies settings appealed transfer learning techniques data stream detecting drifts data distribution time data stream represented two independent non-stationary streams one containing labeled data instances called source stream biased distribution compared unlabeled data instances called target stream task label prediction representation called multistream classification instances two streams occur independently studies addressed various challenges multistream setting still suffers large computational overhead mainly due frequent bias correction drift adaptation methods employed paper focus utilizing alternative bias correction technique called relative density-ratio estimation known computationally faster importantly propose novel mechanism automatically learn appropriate mixture relative density adapts changes multistream setting time theoretically study properties empirically demonstrate superior performance within multistream framework called mscrdr benchmark datasets comparing competing methods	negative
a memetic approach for sequential security games on a plane with moving targets	paper introduces new type security games sg played plane targets moving along predefined straight line trajectories respective mixed integer linear programming milp formulation three approaches solving game proposed experimentally evaluated application milp solver finding exact solutions small-size games milp-based extension recently published zero-sum sg approach case generalsum games finding approximate solutions medium-size games use memetic algorithm mediumsize large-size game instances beyond milp ’ scalability utilization best knowledge new idea field sg novelty proposed solution lies specifically efficient chromosome-based game encoding dedicated local improvement heuristics vast majority test cases known equilibrium profiles method leads optimal solutions high stability approximately linear time scalability another advantage iteration-based construction system makes approach essentially anytime method property paramount importance case restrictive time limits could hinder possibility calculating exact solution general note believe ma-based methods may offer viable alternative milp solvers complex games require application approximate solving methods	positive
deep reinforcement learning for green security games with real-time information	green security games gsgs proposed applied optimize patrols conducted law enforcement agencies green security domains combating poaching illegal logging overfishing however real-time information footprints agents ’ subsequent actions upon receiving information e.g. rangers following footprints chase poacher neglected previous work fill gap first propose new game model gsg-i augments gsgs sequential movement vital element real-time information second design novel deep reinforcement learning-based algorithm dedol compute patrolling strategy adapts real-time information best-responding attacker dedol built upon double oracle framework policy-space response oracle solving restricted game iteratively adding best response strategies training deep q-networks exploring game structure dedol uses domain-specific heuristic strategies initial strategies constructs several local modes efficient parallelized training knowledge first attempt use deep q-learning security games	positive
understanding pictograph with facial features: end-to-end sentence-level lip reading of chinese	breakthrough deep learning lip reading technologies extraordinarily rapid progress well-known chinese widely spoken language world unlike alphabetic languages involves 1,000 pronunciations pinyin nearly 90,000 pictographic characters hanzi makes lip reading chinese challenging paper implement visual-only chinese lip reading unconstrained sentences two-step end-to-end architecture lipch-net two deep neural network models employed perform recognition pictureto-pinyin mouth motion pictures pronunciations recognition pinyin-to-hanzi pronunciations texts respectively jointly optimization improve overall performance addition two modules pinyin-to-hanzi model pre-trained separately large auxiliary data advance sequence-to-sequence training make best long sequence matches avoiding ambiguity collect 6-month daily news broadcasts china central television cctv website semi-automatically label 20.95 gb dataset 20,495 natural chinese sentences trained cctv dataset lipch-net model outperforms performance stateof-the-art lip reading frameworks according results scheme accelerates training reduces overfitting also overcomes syntactic ambiguity chinese provides baseline future relevant work	positive
randomized strategies for robust combinatorial optimization	paper study following robust optimization problem given independence system candidate objective functions choose independent set adversary chooses one objective function knowing choice goal find randomized strategy i.e. probability distribution independent sets maximizes expected objective value worst case problem fundamental wide areas artificial intelligence machine learning game theory optimization solve problem propose two types schemes designing approximation algorithms one scheme case objective functions linear first finds approximately optimal aggregated strategy retrieves desired solution little loss objective value approximation ratio depends relaxation independence system polytope applications provide approximation algorithms knapsack constraint matroid intersection developing appropriate relaxations retrievals scheme based multiplicative weights update mwu method direct application mwu method yield strict multiplicative approximation algorithm yield one additional additive error term key technique overcome issue introduce new concept called η γ -reductions objective functions parameters η γ. show scheme outputs nearly α-approximate solution exists α-approximation algorithm subproblem defined η γ -reductions improves approximation ratios previous results using result provide approximation algorithms objective functions submodular correspond cardinality robustness knapsack problem	positive
f-similarity preservation loss for soft labels: a demonstration on cross-corpus speech emotion recognition	paper propose deep metric learning dml approach supports soft labels dml seeks learn representations encode similarity examples deep neural networks dml generally presupposes data divided discrete classes using hard labels however tasks exemplary domain speech emotion recognition ser work inherently subjective data data may possible identify single hard label propose family loss functions fsimilarity preservation loss f-spl based dual form f-divergence dml soft labels show minimizer f-spl preserves pairwise label similarities learned feature embeddings demonstrate efficacy proposed loss function task cross-corpus ser soft labels approach combines f-spl classification loss significantly outperforms baseline ser system structure trained classification loss experiments show presented techniques robust over-training learn embedding space similarity examples meaningful	positive
guided dropout	dropout often used deep neural networks prevent over-fitting conventionally dropout training invokes random drop nodes hidden layers neural network hypothesis guided selection nodes intelligent dropout lead better generalization compared traditional dropout research propose “ guided dropout ” training deep neural network drop nodes measuring strength node also demonstrate conventional dropout specific case proposed guided dropout experimental evaluation multiple datasets including mnist cifar10 cifar100 svhn tiny imagenet demonstrate efficacy proposed guided dropout	negative
data-to-text generation with content selection and planning	recent advances data-to-text generation led use large-scale datasets neural network models trained end-to-end without explicitly modeling say order work present neural network architecture incorporates content selection planning without sacrificing end-to-end training decompose generation task two stages given corpus data records paired descriptive documents first generate content plan highlighting information mentioned order generate document taking content plan account automatic human-based evaluation experiments show model1 outperforms strong baselines improving state-of-the-art recently released rotowire dataset	negative
unsupervised meta-learning of figure-ground segmentation via imitating visual effects	paper presents “ learning learn ” approach figureground image segmentation exploring webly-abundant images specific visual effects method effectively learn visual-effect internal representations unsupervised manner uses knowledge differentiate figure ground image specifically formulate meta-learning process compositional image editing task learns imitate certain visual effect derive corresponding internal representation generative process help instantiate underlying figure-ground notion enables system accomplish intended image segmentation whereas existing generative methods mostly tailored image synthesis style transfer approach offers flexible learning mechanism model general concept figure-ground segmentation unorganized images explicit pixel-level annotations validate approach via extensive experiments six datasets demonstrate proposed model end-to-end trained without ground-truth pixel labeling yet outperforms existing methods unsupervised segmentation tasks	positive
learning logistic circuits	paper proposes new classification model called logistic circuits mnist fashion datasets learning algorithm outperforms neural networks order magnitude parameters yet logistic circuits distinct origin symbolic ai forming discriminative counterpart probabilistic-logical circuits acs spns psdds show parameter learning logistic circuits convex optimization simple local search algorithm induce strong model structures data	positive
a multi-task learning framework for abstractive text summarization	propose multi-task learning approach abstractive text summarization mats motivated fact humans difficulty performing task capabilities multiple domains specifically mats consists three components text categorization model learns rich category-specific text representations using bi-lstm encoder ii syntax labeling model learns improve syntax-aware lstm decoder iii abstractive text summarization model shares encoder decoder text categorization syntax labeling tasks respectively particular abstractive text summarization model enjoys significant benefit additional text categorization syntax knowledge experimental results show mats outperforms competitors.1	positive
dynamic compositionality in recursive neural networks with structure-aware tag representations	existing recursive neural network rvnn architectures utilize structure parse trees ignoring syntactic tags provided by-products parsing present novel rvnn architecture provide dynamic compositionality considering comprehensive syntactic information derived structure linguistic tags specifically introduce structure-aware tag representation constructed separate tag-level tree-lstm control composition function existing wordlevel tree-lstm augmenting representation supplementary input gate functions tree-lstm extensive experiments show models built upon proposed architecture obtain superior competitive performance several sentence-level tasks sentiment analysis natural language inference compared previous tree-structured models sophisticated neural models	negative
learning attribute-specific representations for visual tracking	recent years convolutional neural networks cnns achieved great success visual tracking existing methods train fine-tune binary classifier distinguish target background however may suffer performance degradation due insufficient training data paper show attribute information e.g. illumination changes occlusion motion context facilitates training effective classifier visual tracking particular design attribute-based cnn multiple branches branch responsible classifying target specific attribute design reduces appearance diversity target attribute thus requires less data train model combine attributespecific features via ensemble layers obtain discriminative representations final target/background classification proposed method achieves favorable performance otb100 dataset compared state-of-the-art tracking methods trained vot datasets proposed network also shows good generalization ability uav-traffic dataset significantly different attributes target appearances vot datasets	negative
generating character descriptions for automatic summarization of fiction	summaries fictional stories allow readers quickly decide whether story catches interest major challenge automatic summarization fiction lack standardized evaluation methodology high-quality datasets experimentation work take bottomup approach problem assuming story authors uniquely qualified inform decisions collect dataset one million fiction stories accompanying author-written summaries wattpad online story sharing platform identify commonly occurring summary components description main characters frequent elicit descriptions main characters directly authors sample stories propose two approaches generate character descriptions one based ranking attributes found story text based classifying list pre-defined attributes find classification-based approach performs best predicting character descriptions	negative
fuzzy-classification assisted solution preselection in evolutionary optimization	evolutionary optimization preselection efficient operator improve search efficiency aims filter unpromising candidate solutions fitness evaluation existing preselection operators rely fitness values surrogate models classification models basically classification based preselection regards preselection classification procedure i.e. differentiating promising unpromising candidate solutions however difference promising unpromising classes becomes fuzzy running process goes left solutions likely promising ones facing challenge paper proposes fuzzy classification based preselection fcps scheme utilizes membership function measure quality candidate solutions proposed fcps scheme applied two state-of-the-art evolutionary algorithms test suite experimental results show potential fcps improving algorithm performance	negative
play as you like: timbre-enhanced multi-modal music style transfer	style transfer polyphonic music recordings challenging task considering modeling diverse imaginative reasonable music pieces style different original one achieve learning stable multi-modal representations domain-variant i.e. style domaininvariant i.e. content information music unsupervised manner critical paper propose unsupervised music style transfer method without need parallel data besides characterize multi-modal distribution music pieces employ multi-modal unsupervised image-to-image translation munit framework proposed system allows one generate diverse outputs learned latent distributions representing contents styles moreover better capture granularity sound perceptual dimensions timbre nuance instrument-specific performance cognitively plausible features including mel-frequency cepstral coefficients mfcc spectral difference spectral envelope combined widely-used mel-spectrogram timbreenhanced multi-channel input representation relativistic average generative adversarial networks ragan also utilized achieve fast convergence high stability conduct experiments bilateral style transfer tasks among three different genres namely piano solo guitar solo string quartet results demonstrate advantages proposed method music style transfer improved sound quality allowing users manipulate output	negative
derpn: taking a further step toward more general object detection	current detection methods adopted anchor boxes regression references however detection performance sensitive setting anchor boxes proper setting anchor boxes may vary significantly across different datasets severely limits universality detectors improve adaptivity detectors paper present novel dimension-decomposition region proposal network derpn perfectly displace traditional region proposal network rpn derpn utilizes anchor string mechanism independently match object widths heights conducive treating variant object shapes addition novel scale-sensitive loss designed address imbalanced loss computations different scaled objects avoid small objects overwhelmed larger ones comprehensive experiments conducted general object detection datasets pascal voc 2007 2012 ms coco scene text detection datasets icdar 2013 coco-text prove derpn significantly outperform rpn worth mentioning proposed derpn employed directly different models tasks datasets without modifications hyperparameters specialized optimization demonstrates adaptivity code released https //github.com/hciilab/derpn	positive
refining coarse-grained spatial data using auxiliary spatial data sets with various granularities	propose probabilistic model refining coarse-grained spatial data utilizing auxiliary spatial data sets existing methods require spatial granularities auxiliary data sets desired granularity target data proposed model effectively make use auxiliary data sets various granularities hierarchically incorporating gaussian processes proposed model distribution auxiliary data set continuous space modeled using gaussian process representation uncertainty considers levels granularity finegrained target data modeled another gaussian process considers spatial correlation auxiliary data sets uncertainty integrate gaussian process spatial aggregation process transforms fine-grained target data coarse-grained target data infer fine-grained target gaussian process coarse-grained data model designed inference model parameters based exact marginal likelihood possible variables finegrained target auxiliary data analytically integrated experiments real-world spatial data sets demonstrate effectiveness proposed model	positive
solving integer quadratic programming via explicit and structural restrictions	study parameterized complexity integer quadratic programming two kinds restrictions explicit restrictions domain coefficients structural restrictions variable interactions argue kinds restrictions necessary achieve tractability integer quadratic programming obtain four new algorithms problem tuned possible explicit restrictions instances may wish solve presented algorithms exact deterministic complemented appropriate lower bounds	negative
learning plackett-luce mixtures from partial preferences	propose em-based framework learning plackett-luce model mixtures partial orders core framework efficient sampling linear extensions partial orders plackett-luce model propose two markov chain monte carlo mcmc samplers gibbs sampler generalized repeated insertion method tuned mcmc grim-mcmc prove efficiency grim-mcmc large class preferences	positive
few-shot image and sentence matching via gated visual-semantic embedding	although image sentence matching widely studied intrinsic few-shot problem commonly ignored become bottleneck performance improvement work focus challenging problem few-shot image sentence matching propose gated visual-semantic embedding gvse model deal model consists three corporative modules terms uncommon vse common vse gated metric fusion uncommon vse exploits external auxiliary resources extract generic features representing uncommon instances words images sentences integrates modeling semantic relation obtain global representations association analysis better model common instances words rest content images sentences common vse learns discriminative representations directly scratch obtaining two similarity metrics two vse modules different advantages gated metric fusion module adaptively fuses automatically balancing relative importance based fused metric perform extensive experiments terms few-shot conventional image sentence matching demonstrate effectiveness proposed model achieving state-of-the-art results two public benchmark datasets	negative
enhancing lazy grounding with lazy normalization in answer-set programming	answer-set programming asp expressive rule-based knowledge-representation formalism lazy grounding solving technique avoids well-known grounding bottleneck traditional asp evaluation restricted normal rules severely limiting expressive power work introduce framework handle aggregates normalizing demand lazy grounding hence relieving restrictions lazy grounding significantly term approach lazy normalization demonstrate feasibility different types aggregates asymptotic behavior analyzed correctness presented lazy normalizations shown benchmark results indicate lazy normalization bring up-to exponential gains space time well enable asp used new application areas	negative
parameterized heuristics for incomplete weighted csps	key assumption weighted constraint satisfaction problems wcsps constraints specified priori assumption hold applications involve users preferences incomplete wcsps iwcsps extend wcsps allowing constraints partially specified unfortunately existing iwcsp approaches either guarantee return optimal solutions provide quality guarantees solutions found bridge two extremes propose number parameterized heuristics allow users find boundedly-suboptimal solutions error bound depends user-defined parameters heuristics thus allow users trade solution quality fewer elicited preferences faster computation times	negative
insufficient data can also rock! learning to converse using smaller data with augmentation	recent successes open-domain dialogue generation mainly rely advances deep neural networks effectiveness deep neural network models depends amount training data laboursome expensive acquire huge amount data scenarios effectively utilize existing data crux issue paper use data augmentation techniques improve performance neural dialogue models condition insufficient data specifically propose novel generative model augment existing data conditional variational autoencoder cvae employed generator output training data diversified expressions improve correlation augmented training pair design discriminator adversarial training supervise augmentation process moreover thoroughly investigate various data augmentation schemes neural dialogue system generative models gan cvae experimental results two open corpora weibo twitter demonstrate superiority proposed data augmentation model	negative
temporal anomaly detection: calibrating the surprise	propose hybrid approach temporal anomaly detection access data users databases — generally kind subject-object co-occurrence data consider high-dimensional setting also requires fast computation test time methodology identifies anomalies based single stationary model instead requiring full temporal one would prohibitive setting learn low-rank stationary model training data fit regression model predicting expected likelihood score normal access patterns future disparity predicted likelihood score observed one used assess “ surprise ” test time approach enables calibration anomaly score time-varying normal behavior patterns considered anomalous provide detailed description algorithm including convergence analysis report encouraging empirical results one data sets tested new public domain consists two months ’ worth database access records live system data set code publicly available https //github.com/eyalgut/tlr anomaly detection.git	positive
video inpainting by jointly learning temporal structure and spatial details	present new data-driven video inpainting method recovering missing regions video frames novel deep learning architecture proposed contains two subnetworks temporal structure inference network spatial detail recovering network temporal structure inference network built upon 3d fully convolutional architecture learns complete low-resolution video volume given expensive computational cost 3d convolution low resolution result provides temporal guidance spatial detail recovering network performs imagebased inpainting 2d fully convolutional network produce recovered video frames original resolution two-step network design ensures spatial quality frame temporal coherence across frames method jointly trains sub-networks end-to-end manner provide qualitative quantitative evaluation three datasets demonstrating method outperforms previous learning-based video inpainting methods	positive
lena: locality-expanded neural embedding for knowledge base completion	embedding based models knowledge base completion demonstrated great successes attracted significant research interest work observe existing embedding models loss functions decomposed atomic loss functions triple postulated edge knowledge graph approach essentially implies conditioned embeddings triple whether triple factual independent structure knowledge graph although arguably embeddings entities relation triple contain certain structural information knowledge base believe global information contained embeddings triple insufficient assumption overly optimistic heterogeneous knowledge bases motivated understanding work propose new embedding model discard assumption embeddings entities relation triple sufficient statistic triple ’ factual existence specifically proposed model assumes whether triple factual depends embedding triple also embeddings entities relations larger graph neighbourhood model attention mechanisms constructed select relevant information graph neighbourhood irrelevant signals neighbourhood suppressed termed locality-expanded neural embedding attention lena model tested four standard datasets compared several stateof-the-art models knowledge base completion extensive experiments suggest lena outperforms existing models virtually every metric	negative
induction of non-monotonic logic programs to explain boosted tree models using lime	present heuristic based algorithm induce nonmonotonic logic programs explain behavior xgboost trained classifiers use technique based lime approach locally select important features contributing classification decision order explain model ’ global behavior propose lime-fold algorithm —a heuristic-based inductive logic programming ilp algorithm capable learning nonmonotonic logic programs—that apply transformed dataset produced lime proposed approach agnostic choice ilp algorithm experiments uci standard benchmarks suggest significant improvement terms classification evaluation metrics meanwhile number induced rules dramatically decreases compared aleph state-of-the-art ilp system	positive
task-driven common representation learning via bridge neural network	paper introduces novel deep learning based method named bridge neural network bnn dig potential relationship two given data sources task task proposed approach employs two convolutional neural networks project two data sources feature space learn desired common representation required specific task training objective artificial negative samples introduced ability mini-batch training ’ asymptotically equivalent maximizing total correlation two data sources verified theoretical analysis experiments tasks including pair matching canonical correlation analysis transfer learning reconstruction demonstrate state-of-the-art performance bnn may provide new insights aspect common representation learning	positive
combining axiom injection and knowledge base completion for efficient natural language inference	logic-based approaches reasoning tasks recognizing textual entailment rte important system large amount knowledge data however tradeoff adding knowledge data improved rte performance maintaining efficient rte system big database problematic terms memory usage computational complexity work show processing time state-of-the-art logic-based rte system significantly reduced replacing search-based axiom injection abduction mechanism based knowledge base completion kbc integrate mechanism coq plugin provides proof automation tactic natural language inference additionally show empirically adding new knowledge data contributes better rte performance harming processing speed framework	negative
multi-view information-theoretic co-clustering for co-occurrence data	multi-view clustering received much attention recently existing multi-view clustering methods focus one-sided clustering co-occurring data elements involve counts sample-feature co-occurrences efficient conduct two-sided clustering along samples features simultaneously take advantage two-sided clustering co-occurrences scene multi-view clustering two-sided multi-view clustering method proposed i.e. multi-view information-theoretic co-clustering mv-itcc proposed method realizes two-sided clustering co-occurring multi-view data formulation information theory specifically exploits agreement disagreement among views sharing common clustering results along sample dimension keeping clustering results view specific along feature dimension addition mechanism maximum entropy also adopted control importance different views give right balance leveraging agreement disagreement extensive experiments conducted text image multiview datasets results clearly demonstrate superiority proposed method	negative
dynamic learning of sequential choice bandit problem under marketing fatigue	motivated observation overexposure unwanted marketing activities leads customer dissatisfaction consider setting platform offers sequence messages users penalized users abandon platform due marketing fatigue propose novel sequential choice model capture multiple interactions taking place platform user upon receiving message user decides one three actions accept message skip receive next message abandon platform based user feedback platform dynamically learns users ’ abandonment distribution valuations messages determine length sequence order messages maximizing cumulative payoff horizon length t. refer online learning task sequential choice bandit problem offline combinatorial optimization problem show polynomialtime algorithm online problem propose algorithm balances exploration exploitation characterize regret bound lastly demonstrate extend model user contexts incorporate personalization	negative
gradient harmonized single-stage detector	despite great success two-stage detectors single-stage detector still elegant efficient way yet suffers two well-known disharmonies training i.e huge difference quantity positive negative examples well easy hard examples work first point essential effect two disharmonies summarized term gradient propose novel gradient harmonizing mechanism ghm hedging disharmonies philosophy behind ghm easily embedded classification loss function like cross-entropy ce regression loss function like smooth-l1 sl1 loss end two novel loss functions called ghm-c ghm-r designed balancing gradient flow anchor classification bounding box refinement respectively ablation study ms coco demonstrates without laborious hyper-parameter tuning ghm-c ghm-r bring substantial improvement single-stage detector without whistles bells proposed model achieves 41.6 map coco testdev set surpass state-of-the-art method focal loss fl sl1 0.8. code1 released facilitate future research	negative
response generation by context-aware prototype editing	open domain response generation achieved remarkable progress recent years sometimes yields short uninformative responses propose new paradigm prototypethen-edit response generation first retrieves prototype response pre-defined index edits prototype response according differences prototype context current context motivation retrieved prototype provides good start-point generation grammatical informative post-editing process improves relevance coherence prototype practice design contextaware editing model built upon encoder-decoder framework augmented editing vector first generate edit vector considering lexical differences prototype context current context edit vector prototype response representation fed decoder generate new response experiment results large scale dataset demonstrate new paradigm significantly increases relevance diversity originality generation results compared traditional generative models furthermore model outperforms retrieval-based methods terms relevance originality	negative
towards better interpretability in deep q-networks	deep reinforcement learning techniques demonstrated superior performance wide variety environments improvements training algorithms continue brisk pace theoretical empirical studies understanding networks seem learn far behind paper propose interpretable neural network architecture q-learning provides global explanation model ’ behavior using key-value memories attention reconstructible embeddings directed exploration strategy model reach training rewards comparable state-of-the-art deep q-learning models however results suggest features extracted neural network extremely shallow subsequent testing using out-of-sample examples shows agent easily overfit trajectories seen training	negative
exploiting background knowledge in compact answer generation for why-questions	paper proposes novel method generating compact answers open-domain why-questions following answer “ deep learning technologies introduced ” question “ google ’ machine translation service improve drastically ” although many works dealt why-question answering focused retrieving answers relatively long text passages consist several sentences length passages appropriate read aloud spoken dialog systems smart speakers hence need create method generates compact answers developed novel neural summarizer compact answer generation task combines recurrent neural network-based encoderdecoder model stacked convolutional neural networks designed effectively exploit background knowledge case set causal relations e.g. “ microsoft ’ machine translation made great progress last years effect since started use deep learning cause ” extracted large web data archive 4 billion web pages experimental results show method achieved significantly better rouge f-scores existing encoder-decoder models variations augmented query-attention memory networks used exploit background knowledge	positive
deep interest evolution network for click-through rate prediction	click-through rate ctr prediction whose goal estimate probability user clicking item become one core tasks advertising system ctr prediction model necessary capture latent user interest behind user behavior data besides considering changing external environment internal cognition user interest evolves time dynamically several ctr prediction methods interest modeling regard representation behavior interest directly lack specially modeling latent interest behind concrete behavior moreover little work considers changing trend interest paper propose novel model named deep interest evolution network dien ctr prediction specifically design interest extractor layer capture temporal interests history behavior sequence layer introduce auxiliary loss supervise interest extracting step user interests diverse especially e-commerce system propose interest evolving layer capture interest evolving process relative target item interest evolving layer attention mechanism embedded sequential structure novelly effects relative interests strengthened interest evolution experiments public industrial datasets dien significantly outperforms state-of-the-art solutions notably dien deployed display advertisement system taobao obtained 20.7 improvement ctr	positive
embedding uncertain knowledge graphs	embedding models deterministic knowledge graphs kg extensively studied purpose capturing latent semantic relations entities incorporating structured knowledge contain machine learning however many kgs model uncertain knowledge typically model inherent uncertainty relations facts confidence score embedding uncertain knowledge represents unresolved challenge capturing uncertain knowledge benefit many knowledge-driven applications question answering semantic search providing natural characterization knowledge paper propose novel uncertain kg embedding model ukge aims preserve structural uncertainty information relation facts embedding space unlike previous models characterize relation facts binary classification techniques ukge learns embeddings according confidence scores uncertain relation facts enhance precision ukge also introduce probabilistic soft logic infer confidence scores unseen relation facts training propose evaluate two variants ukge based different confidence score modeling strategies experiments conducted three real-world uncertain kgs via three tasks i.e confidence prediction relation fact ranking relation fact classification ukge shows effectiveness capturing uncertain knowledge achieving promising results consistently outperforms baselines tasks	negative
unsupervised domain adaptation based on source-guided discrepancy	unsupervised domain adaptation problem setting data generating distributions source target domains different labels target domain unavailable important question unsupervised domain adaptation measure difference source target domains existing discrepancy measures unsupervised domain adaptation either require high computation costs theoretical guarantee mitigate problems paper proposes novel discrepancy measure called source-guided discrepancy s-disc exploits labels source domain unlike existing ones consequence s-disc computed efficiently finitesample convergence guarantee addition shown s-disc provide tighter generalization error bound one based existing discrepancy measure finally experimental results demonstrate advantages s-disc existing discrepancy measures	negative
svm-based deep stacking networks	deep network model majority built neural networks proved powerful framework represent complex data high performance machine learning recent years studies turn nonneural network approaches build diverse deep structures deep stacking network dsn model one approaches uses stacked easy-to-learn blocks build parameter-training-parallelizable deep network paper propose novel svm-based deep stacking network svm-dsn uses dsn architecture organize linear svm classifiers deep learning bp-like layer tuning scheme also proposed ensure holistic local optimizations stacked svms simultaneously good math properties svm convex optimization introduced dsn framework model global view svm-dsn iteratively extract data representations layer layer deep neural network parallelizability local view stacked svm converge optimal solution obtain support vectors compared neural networks could lead interesting improvements anti-saturation interpretability experimental results image text data sets demonstrate excellent performances svm-dsn compared competitive benchmark models	negative
comparelda: a topic model for document comparison	number real-world applications require comparison entities based textual representations work develop topic model supervised pairwise comparisons documents model seeks yield topics help differentiate entities along dimension interest may vary one application another previous supervised topic models consider document labels independent pointwise manner proposed comparative latent dirichlet allocation comparelda learns predictive topic distributions comply pairwise comparison observations fit model derive maximum likelihood estimation method via augmented variational approximation algorithm evaluation several public datasets underscores strengths comparelda modelling document comparisons	negative
mechanism design for multi-type housing markets with acceptable bundles	extend top-trading-cycles ttc mechanism select strict core allocations housing markets multiple types items agent may endowed allocated multiple items type advance state art mechanism design housing markets along two dimensions first setting general multi-type housing markets moulin 1995 sikdar adali xia 2017 setting fujita et al 2015 introduce housing markets acceptable bundles hmabs general setting agent may arbitrary sets acceptable bundles second extension ttc strict core selecting weaker restriction preferences cmi-trees introduce new domain restriction preferences generalizes commonly-studied languages previous works	negative
real-life: accelerating the discovery of individualized brain connectomes on gpus	diffusion imaging tractography enable mapping structural connections human brain in-vivo linear fascicle evaluation life state-of-the-art approach pruning spurious connections estimated structural connectome optimizing fit measured diffusion data yet life imposes heavy demands computing time precluding use analyses large connectome databases introduce gpu-based implementation life achieves 50-100x speedups conventional cpu-based implementations connectome sizes several million fibers briefly algorithm accelerates generalized matrix multiplications compressed tensor efficient gpu kernels ensuring favorable memory access patterns leveraging speedups advance life ’ algorithm imposing regularization constraint estimated fiber weights connectome pruning regularized accelerated life algorithm “ real-life ” estimates sparser connectomes also provide accurate fits underlying diffusion signal demonstrate utility approach classifying pathological signatures structural connectivity patients alzheimer ’ disease ad estimated million fiber whole-brain connectomes followed pruning real-life 90 individuals 45 ad patients 45 healthy controls linear classifiers based support vector machines achieved 80 accuracy classifying ad patients healthy controls based real-life pruned structural connectomes alone moreover classification based real-life pruned connectome outperformed unpruned connectome well life pruned connectome terms accuracy propose gpu-accelerated approach widely relevant tool non-negative least squares optimization across many domains	negative
hashtag recommendation for photo sharing services	hashtags greatly facilitate content navigation improve user engagement social media meaningful might recommending hashtags photo sharing services instagram pinterest remains daunting task due following two reasons endogenous side posts photo sharing services often contain images text likely correlated therefore crucial coherently model image text well interaction exogenous side hashtags generated users different users might come different tags similar posts due different preference and/or community effect therefore highly desirable characterize users ’ tagging habits paper propose integral effective hashtag recommendation approach photo sharing services particular proposed approach considers endogenous exogenous effects content modeling module habit modeling module respectively content modeling module adopt parallel co-attention mechanism coherently model image text well interaction habit modeling module introduce external memory unit characterize historical tagging habit user overall hashtag recommendations generated basis post features content modeling module habit influences habit modeling module evaluate proposed approach real instagram data experimental results demonstrate proposed approach significantly outperforms state-of-theart methods terms recommendation accuracy content modeling habit modeling contribute significantly overall recommendation accuracy	negative
learning flexible latent representations via encapsulated variational encoders	learning flexible latent representation observed data important precursor downstream ai applications end propose novel form variational encoder i.e. encapsulated variational encoders eve exert direct control encoded latent representations along learning algorithm i.e. eve compatible automatic variational differentiation inference algorithm armed property derived eve capable learning converged diverged latent representations using cifar-10 example show learning converged latent representations brings considerable improvement discriminative performance semi-supervised eve using mnist demonstration generative modelling performance eve induced variational auto-encoder evae largely enhanced help learned diverged latent representations	negative
qadiver: interactive framework for diagnosing qa models	question answering qa extracting answers text given question natural language actively studied existing models shown promise outperforming human performance trained evaluated squad dataset however performance may replicated actual setting need diagnose cause non-trivial due complexity model thus propose web-based ui provides model contributes qa performances integrating visualization analysis tools model explanation expect framework help qa model researchers refine improve models	negative
coupled cyclegan: unsupervised hashing network for cross-modal retrieval	recent years hashing attracted attention owing superior capacity low storage cost high query efficiency large-scale cross-modal retrieval benefiting deep leaning continuously compelling results cross-modal retrieval community achieved however existing deep cross-modal hashing methods either rely amounts labeled information ability learn accuracy correlation different modalities paper proposed unsupervised coupled cycle generative adversarial hashing networks uch cross-modal retrieval outer-cycle network used learn powerful common representation inner-cycle network explained generate reliable hash codes specifically proposed uch seamlessly couples two networks generative adversarial mechanism optimized simultaneously learn representation hash codes extensive experiments three popular benchmark datasets show proposed uch outperforms state-of-the-art unsupervised cross-modal hashing methods	negative
bihmp-gan: bidirectional 3d human motion prediction gan	human motion prediction model applications various fields computer vision without taking account inherent stochasticity prediction future pose dynamics methods often converges deterministic undesired mean multiple probable outcomes devoid propose novel probabilistic generative approach called bidirectional human motion prediction gan bihmp-gan able generate multiple probable human-pose sequences conditioned given starting sequence introduce random extrinsic factor r drawn predefined prior distribution furthermore enforce direct content loss predicted motion sequence also avoid mode-collapse novel bidirectional framework incorporated modifying usual discriminator architecture discriminator trained also regress extrinsic factor r used alongside intrinsic factor encoded starting pose sequence generate particular pose sequence regularize training introduce novel recursive prediction strategy spite probabilistic framework enhanced discriminator architecture allows predictions intermediate part pose sequence used conditioning prediction latter part sequence bidirectional setup also provides new direction evaluate prediction quality given test sequence fair assessment bihmp-gan report performance generated motion sequence using critic model trained discriminate real fake motion sequence ii action classifier trained real human motion dynamics outcomes qualitative quantitative evaluations probabilistic generations model demonstrate superiority bihmp-gan previously available methods	negative
optimal surveillance of covert networks by minimizing inverse geodesic length	inverse geodesic length igl well-known widely used measure network performance equals sum inverse distances pairs vertices network analysis igl network often used assess evaluate well heuristics perform strengthening weakening network consider edge-deletion problem minigled formally given graph g budget k target inverse geodesic length question whether exists subset edges x |x| ≤ ck inverse geodesic length g − x	negative
efficient online learning for mapping kernels on linguistic structures	kernel methods popular effective techniques learning structured data trees graphs one major drawbacks computational cost related making prediction example manifests classification phase batch kernel methods especially online learning algorithms paper analyze speed prediction kernel function instance mapping kernels general framework specifying kernels structured data extends popular convolution kernel framework theoretically study general model derive various optimization strategies show apply popular kernels structured data additionally derive reliable empirical evidence semantic role labeling task natural language classification task highly dependent syntactic trees results show faster approach clearly improve standard kernel-based svms run large datasets	negative
meshnet: mesh neural network for 3d shape representation	mesh important powerful type data 3d shapes widely studied field computer vision computer graphics regarding task 3d shape representation extensive research efforts concentrating represent 3d shapes well using volumetric grid multi-view point cloud however little effort using mesh data recent years due complexity irregularity mesh data paper propose mesh neural network named meshnet learn 3d shape representation mesh data method face-unit feature splitting introduced general architecture available effective blocks proposed way meshnet able solve complexity irregularity problem mesh conduct 3d shape representation well applied proposed meshnet method applications 3d shape classification retrieval experimental results comparisons state-of-the-art methods demonstrate proposed meshnet achieve satisfying 3d shape classification retrieval performance indicates effectiveness proposed method 3d shape representation	negative
a neural network approach to verb phrase ellipsis resolution	verb phrase ellipsis vpe linguistic phenomenon verb phrases syntactic constituents omitted typically referred auxiliary verb ubiquitous formal informal text news articles dialogues previous work vpe resolution mainly focused manually constructing features extracted auxiliary verbs syntactic trees etc however optimization feature representation effectiveness continuous features automatic composition features well addressed paper explore advantages neural models vpe resolution pipeline end-to-end processes comparing differences statistical neural models two neural models namely multi-layer perception transformer employed subtasks vpe detection resolution experimental results show neural models outperform state-of-the-art baselines subtasks end-to-end results	negative
jointly learning to label sentences and tokens	learning construct text representations end-to-end systems difficult natural languages highly compositional task-specific annotated datasets often limited size methods directly supervising language composition allow us guide models based existing knowledge regularizing towards robust interpretable representations paper investigate objectives different granularities used learn better language representations propose architecture jointly learning label sentences tokens predictions level combined together using attention mechanism token-level labels also acting explicit supervision composing sentence-level representations experiments show learning perform tasks jointly multiple levels model achieves substantial improvements sentence classification sequence labeling	negative
affinitynet: semi-supervised few-shot learning for disease type prediction	deep learning achieved great success computer vision many fields currently work well patient genomic data “ big p small n ” problem i.e. relatively small number samples highdimensional features order make deep learning work small amount training data design new models facilitate few-shot learning present affinity network model affinitynet data efficient deep learning model learn limited number training examples generalize well backbone affinitynet model consists stacked k-nearest-neighbor knn attention pooling layers knn attention pooling layer generalization graph attention model gam applied graphs also set objects regardless whether graph given new deep learning module knn attention pooling layers plugged neural network model like convolutional layers simple special case knn attention pooling layer feature attention layer directly select important features useful classification tasks experiments synthetic data cancer genomic data tcga projects show affinitynet model better generalization power conventional neural network models little training data	negative
combining fact extraction and verification with neural semantic matching networks	increasing concern misinformation stimulated research efforts automatic fact checking recentlyreleased fever dataset introduced benchmark factverification task system asked verify claim using evidential sentences wikipedia documents paper present connected system consisting three homogeneous neural semantic matching models conduct document retrieval sentence selection claim verification jointly fact extraction verification evidence retrieval document retrieval sentence selection unlike traditional vector space ir models queries sources matched pre-designed term vector space develop neural models perform deep semantic matching raw textual input assuming intermediate term representation access structured external knowledge bases also show pageview frequency also help improve performance evidence retrieval results later matched using neural semantic matching network claim verification unlike previous approaches simply feed upstream retrieved evidence claim natural language inference nli model enhance nli model providing internal semantic relatedness scores hence integrating evidence retrieval modules ontological wordnet features experiments fever dataset indicate 1 neural semantic matching method outperforms popular tf-idf encoder models significant margins evidence retrieval metrics 2 additional relatedness score wordnet features improve nli model via better semantic awareness 3 formalizing three subtasks similar semantic matching problem improving three stages complete model able achieve state-of-the-art results fever test set two times greater baseline results .1	negative
dba: dynamic multi-armed bandit algorithm	introduce dynamic bandit algorithm dba practical solution improve shortcoming pervasively employed reinforcement learning algorithm called multi-arm bandit aka bandit bandit makes real-time decisions based prior observations however bandit heavily biased priors quickly adapt trend interchanging result bandit quickly enough make profitable decisions trend changing unlike bandit dba focuses quickly adapting detect trends early enough furthermore dba remains almost light bandit terms computations therefore dba easily deployed production light process similar bandit demonstrate critical beneficial main focus dba i.e ability quickly finding profitable option real-time stateof-the-art competitors experiments augmented visualization mechanism explains profitability decisions made algorithm step animations finally observe dba substantially outperform original bandit close 3 times set key performance indicator kpi case 3 arms	negative
evolving solutions to community-structured satisfiability formulas	study ability simple mutation-only evolutionary algorithm solve propositional satisfiability formulas inherent community structure show community structure translates good fitness-distance correlation properties implies objective function provides strong signal search space evolutionary algorithms locate satisfying assignment efficiently prove formula clusters communities size ∈ ω logn ∩o nε/ 2ε+2 constant 0 ε 1 nonuniform distribution communities simple evolutionary algorithm called 1+1 ea finds satisfying assignment polynomial time 1−o 1 fraction formulas least constant constraint density significant improvement recent results uniform random formulas algorithm proven efficient uniform formulas least logarithmic density	negative
spectral clustering in heterogeneous information networks	heterogeneous information network hin one whose objects different types links objects could model different object relations study spectral clustering effectively applied hins particular focus meta-path relations used construct effective similarity matrix based spectral clustering done formulate similarity matrix construction optimization problem propose sclump algorithm solving problem conduct extensive experiments comparing sclump state-of-the-art clustering algorithms hins results show sclump outperforms competitors range datasets w.r.t different clustering quality measures	negative
automatic code review by learning the revision of source code	code review process manual inspection revision source code order find whether revised source code eventually meets revision requirements however manual code review time-consuming automating code review process alleviate burden code reviewers speed software maintenance process construct model automatic code review characteristics revisions source code i.e. difference two pieces source code properly captured modeled unfortunately existing techniques easily model overall correlation two pieces source code “ difference ” two pieces source code paper propose novel deep model named dace automatic code review model able learn revision features contrasting revised hunks original revised source code respect code context containing hunks experimental results six open source software projects indicate learning revision features dace outperform competing approaches automatic code review	positive
allocating planning effort when actions expire	making plans depend external events tricky example agent considering partial plan involves taking bus must recognize partial plan viable completed selected execution time agent arrive bus stop setting raises thorny problem allocating agent ’ planning effort across multiple open search nodes expiration time expected completion effort addition usual estimated plan cost paper formalizes metareasoning problem studies theoretical properties presents several algorithms solving theoretical results include surprising connection job scheduling well deliberation scheduling time-dependent planning empirical results indicate algorithms effective practice work advances understanding heuristic search planners might address realistic problem settings	negative
adding constraints to bayesian inverse problems	using observation data estimate unknown parameters computational models broadly important task often challenging solutions non-unique due complexity model limited observation data however parameters states model often known satisfy additional constraints beyond model thus propose approach improve parameter estimation inverse problems incorporating constraints bayesian inference framework constraints imposed constructing likelihood function based fitness solution constraints posterior distribution parameters conditioned 1 observed data 2 satisfaction constraints obtained estimate parameters given maximum posteriori estimation posterior mean equality inequality constraints considered framework strictness constraints controlled constraint uncertainty denoting confidence correctness furthermore extend framework approximate bayesian inference framework terms ensemble kalman filter method constraint imposed re-weighing ensemble members based likelihood function synthetic model presented demonstrate effectiveness proposed method exact bayesian inference ensemble kalman filter scenarios numerical simulations show imposing constraints using method presented improves identification true parameter solution among multiple local minima	negative
infovae: balancing learning and inference in variational autoencoders	key advance learning generative models use amortized inference distributions jointly trained models find existing training objectives variational autoencoders lead inaccurate amortized inference distributions cases improving objective provably degrades inference quality addition observed variational autoencoders tend ignore latent variables combined decoding distribution flexible identify cause existing training criteria propose new class objectives info-vae mitigate problems show model significantly improve quality variational posterior make effective use latent features regardless flexibility decoding distribution extensive qualitative quantitative analyses demonstrate models outperform competing approaches multiple performance metrics	negative
teaching machines to extract main content for machine reading comprehension	machine reading comprehension whose goal find answers candidate passages given question attracted lot research efforts recent years one key challenge machine reading comprehension identify main content large redundant overlapping set candidate sentences paper propose tackle challenge markov decision process main content identification formalized sequential decision making action corresponds selecting sentence policy gradient used learn model parameters experimental results based msmarco showed proposed model called mc-mdp select high quality main contents significantly improved performances answer span prediction	negative
stochastic goal recognition design	given environment set allowed modifications task goal recognition design grd select valid set modifications minimizes maximal number steps agent take goal revealed observer document presents extension grd stochastic domain stochastic goal recognition design s-grd grd framework aims consider 1 stochastic agent action outcomes 2 partial observability agent states actions 3 suboptimal agents abstract present progress made towards final objective well timeline projected conclusion	positive
amsterdam to dublin eventually delayed? lstm and transfer learning for predicting delays of low cost airlines	flight delays impact airlines airports passengers delay prediction crucial decision-making process players commercial aviation particular airlines meet on-time performance objectives although many machine learning approaches experimented fail predicting delays minutes low errors less 15 minutes ii applied small carriers i.e. low cost companies characterized small amount data work presents long short-term memory lstm approach predicting flight delay modeled sequence flights across multiple airports particular aircraft throughout day suggest transfer learning approach heterogeneous feature spaces train prediction model given smaller airline using data another larger airline approach demonstrated robust accurate low cost airlines europe	negative
asynchronous delay-aware accelerated proximal coordinate descent for nonconvex nonsmooth problems	nonconvex nonsmooth problems recently attracted considerable attention machine learning however developing efficient methods nonconvex nonsmooth optimization problems certain performance guarantee remains challenge proximal coordinate descent pcd widely used solving optimization problems knowledge pcd methods nonconvex setting limited hand asynchronous proximal coordinate descent apcd recently received much attention order solve large-scale problems however accelerated variants apcd algorithms rarely studied paper extend apcd method accelerated algorithm aapcd nonsmooth nonconvex problems satisfies sufficient descent property comparing function values proximal update linear extrapolated point using delay-aware momentum value best knowledge first provide stochastic deterministic accelerated extension apcd algorithms general nonconvex nonsmooth problems ensuring bounded delays unbounded delays every limit point critical point leveraging kurdyka-łojasiewicz property show linear sublinear convergence rates deterministic aapcd bounded delays numerical results demonstrate practical efficiency algorithm speed	negative
unsupervised feature selection by pareto optimization	dimensionality reduction often employed deal data huge number features generally divided two categories feature transformation feature selection due interpretability efficiency inference abundance unlabeled data unsupervised feature selection attracted much attention paper consider natural formulation column subset selection css minimize reconstruction error data matrix selecting subset features propose anytime randomized iterative approach pocss minimizes reconstruction error number selected features simultaneously approximation guarantee well bounded empirical results exhibit superior performance pocss state-of-the-art algorithms	negative
inter-class angular loss for convolutional neural networks	convolutional neural networks cnns shown great power various classification tasks achieved remarkable results practical applications however distinct learning difficulties discriminating different pairs classes largely ignored existing networks instance cifar-10 dataset distinguishing cats dogs usually harder distinguishing horses ships carefully studying behavior cnn models training process observe confusion level two classes strongly correlated angular separability feature space larger inter-class angle lower confusion based observation propose novel loss function dubbed “ inter-class angular loss ” ical explicitly models class correlation directly applied many existing deep networks minimizing proposed ical networks effectively discriminate examples similar classes enlarging angle corresponding class vectors thorough experimental results series vision nonvision datasets confirm ical critically improves discriminative ability various representative deep neural networks generates superior performance original networks conventional softmax loss	positive
multiple independent subspace clusterings	multiple clustering aims discovering diverse ways organizing data clusters despite progress made ’ still challenge users analyze understand distinctive structure output clustering ease process consider diverse clusterings embedded different subspaces analyze embedding subspaces shed light structure clustering end provide two-stage approach called misc multiple independent subspace clusterings first stage misc uses independent subspace analysis seek multiple statistical independent i.e non-redundant subspaces determines number subspaces via minimum description length principle second stage account intrinsic geometric structure samples embedded subspace misc performs graph regularized semi-nonnegative matrix factorization explore clusters additionally integrates kernel trick matrix factorization handle non-linearly separable clusters experimental results synthetic datasets show misc find different interesting clusterings sought independent subspaces also outperforms related competitive approaches real-world datasets	negative
where to go next: a spatio-temporal gated network for next poi recommendation	next point-of-interest poi recommendation great value location-based service providers users however state-of-the-art recurrent neural networks rnns rarely consider spatio-temporal intervals neighbor check-ins essential modeling user check-in behaviors next poi recommendation end paper propose new spatio-temporal gated network stgn enhancing long-short term memory network spatio-temporal gates introduced capture spatio-temporal relationships successive checkins specifically two pairs time gate distance gate designed control short-term interest longterm interest updates respectively moreover introduce coupled input forget gates reduce number parameters improve efficiency finally evaluate proposed model using four real-world datasets various location-based social networks experimental results show model significantly outperforms state-ofthe-art approaches next poi recommendation	negative
a deep reinforcement learning based multi-step coarse to fine question answering (mscqa) system	paper present multi-step coarse fine question answering mscqa system efficiently processes documents different lengths choosing appropriate actions system designed using actor-critic based deep reinforcement learning model achieve multistep question answering compared previous qa models targeting datasets mainly containing either short long documents multi-step coarse fine model takes merits multiple system modules handle short long documents system hence obtains much better accuracy faster trainings speed compared current state-of-the-art models test model four qa datasets wikereading wikireading long cnn squad demonstrate 1.3 -1.7 accuracy improvements 1.5x-3.4x training speed-ups comparison baselines using state-of-the-art models	positive
distribution-based semi-supervised learning for activity recognition	supervised learning methods widely applied activity recognition prevalent success existing methods however two crucial prerequisites proper feature extraction sufficient labeled training data former important differentiate activities latter crucial build precise learning model two prerequisites become bottlenecks make existing methods practical existing feature extraction methods highly depend domain knowledge labeled data requires intensive human annotation effort therefore paper propose novel method named distribution-based semi-supervised learning tackle aforementioned limitations proposed method capable automatically extracting powerful features domain knowledge required meanwhile alleviating heavy annotation effort semi-supervised learning specifically treat data stream sensor readings received period distribution map training distributions including labeled unlabeled reproducing kernel hilbert space rkhs using kernel mean embedding technique rkhs altered exploiting underlying geometry structure unlabeled distributions finally altered rkhs classifier trained labeled distributions conduct extensive experiments three public datasets verify effectiveness method compared state-of-the-art baselines	negative
universal approximation property and equivalence of stochastic computing-based neural networks and binary neural networks	large-scale deep neural networks memory computation-intensive thereby posing stringent requirements computing platforms hardware accelerations deep neural networks extensively investigated specific forms binary neural networks bnns stochastic computing-based neural networks scnns particularly appealing hardware implementations since implemented almost entirely binary operations	negative
a theoretically guaranteed deep optimization framework for robust compressive sensing mri	magnetic resonance imaging mri one dynamic safe imaging techniques available clinical applications however rather slow speed mri acquisitions limits patient throughput potential indications compressive sensing cs proven efficient technique accelerating mri acquisition widely used cs-mri model founded premise reconstructing image incompletely filled k-space leads ill-posed inverse problem past years lots efforts made efficiently optimize cs-mri model inspired deep learning techniques preliminary works tried incorporate deep architectures cs-mri process unfortunately convergence issues due experience-based networks robustness i.e. lack real-world noise modeling deeply trained optimization methods still missing work develop new paradigm integrate designed numerical solvers data-driven architectures cs-mri introducing optimal condition checking mechanism successfully prove convergence established deep cs-mri optimization scheme furthermore explicitly formulate rician noise distributions within framework obtain extended cs-mri network handle real-world nosies mri process extensive experimental results verify proposed paradigm outperforms existing state-of-theart techniques reconstruction accuracy efficiency well robustness noises real scene	negative
abstractive text summarization by incorporating reader comments	neural abstractive summarization field conventional sequence-to-sequence based models often suffer summarizing wrong aspect document respect main aspect tackle problem propose task reader-aware abstractive summary generation utilizes reader comments help model produce better summary main aspect unlike traditional abstractive summarization task reader-aware summarization confronts two main challenges 1 comments informal noisy 2 jointly modeling news document reader comments challenging tackle challenges design adversarial learning model named reader-aware summary generator rasg consists four components 1 sequence-to-sequence based summary generator 2 reader attention module capturing reader focused aspects 3 supervisor modeling semantic gap generated summary reader focused aspects 4 goal tracker producing goal generation step supervisor goal tacker used guide training framework adversarial manner extensive experiments conducted large-scale real-world text summarization dataset results show rasg achieves stateof-the-art performance terms automatic metrics human evaluations experimental results also demonstrate effectiveness module framework release large-scale dataset research1	negative
object reachability via swaps along a line	housing market problem widely studied resources allocation problem problem agent receive single object preferences objects starting initial endowment want reach certain assignment via sequence rational trades consider problem whether object reachable given agent social network trade two agents allowed neighbors network participant deficit trade assume preferences agents strict tie allowed problem polynomially solvable star-network npcomplete tree-network left challenging open problem whether problem polynomially solvable network path answer open problem positively giving polynomial-time algorithm furthermore show problem path become np-hard preferences agents weak ties allowed	negative
deepccfv: camera constraint-free multi-view convolutional neural network for 3d object retrieval	3d object retrieval compelling demand field computer vision rapid development 3d vision technology increasing applications 3d objects 3d objects described different ways voxel point cloud multi-view among multi-view based approaches proposed recent years show promising results require fixed predefined camera position setting provides complete uniform sampling views objects training stage however causes heavy over-fitting problems make models failed generalize well free camera setting applications particularly insufficient views provided experiments show performance drastically drops number views reduces hindering methods practical applications paper investigate over-fitting issue remove constraint camera setting first two basic feature augmentation strategies dropout dropview introduced solve over-fitting issue precise efficient method named dropmax proposed analyzing drawback basic ones reducing over-fitting issue camera constraint-free multi-view convolutional neural network named deepccfv constructed extensive experiments single-modal cross-modal cases demonstrate effectiveness proposed method free camera settings comparing existing state-of-theart 3d object retrieval methods	positive
learning anytime predictions in neural networks via adaptive loss balancing	work considers trade-off accuracy testtime computational cost deep neural networks dnns via anytime predictions auxiliary predictions specifically optimize auxiliary losses jointly adaptive weighted sum weights inversely proportional average loss intuitively balances losses scale demonstrate theoretical considerations motivate approach multiple viewpoints including connecting optimizing geometric mean expectation loss objective ignores scale losses experimentally adaptive weights induce competitive anytime predictions multiple recognition data-sets models non-adaptive approaches including weighing losses equally particular anytime neural networks anns achieve accuracy faster using adaptive weights small network using static constant weights large one problems high performance saturation also show sequence exponentially deepening anns achieve near-optimal anytime results budget cost const fraction extra computation	negative
online convex optimization for sequential decision processes and extensive-form games	regret minimization powerful tool solving large-scale extensive-form games state-of-the-art methods rely minimizing regret locally decision point work derive new framework regret minimization sequential decision problems extensive-form games general compact convex sets decision point general convex losses opposed prior work simplex decision points linear losses call framework laminar regret decomposition generalizes cfr algorithm general setting furthermore framework enables new proof cfr even known setting derived perspective decomposing polytope regret thereby leading arguably simpler interpretation algorithm generalization convex compact sets convex losses allows us develop new algorithms several problems regularized sequential decision making regularized nash equilibria zero-sum extensive-form games computing approximate extensive-form perfect equilibria generalization also leads first regret-minimization algorithm computing reduced-normal-form quantal response equilibria based minimizing local regrets experiments show framework leads algorithms scale rate comparable fastest variants counterfactual regret minimization computing nash equilibrium therefore approach leads first algorithm computing quantal response equilibria extremely large games algorithms quadratically regularized equilibrium finding orders magnitude faster fastest algorithms nash equilibrium finding suggests regret-minimization algorithms based decreasing regularization nash equilibrium finding future work finally show framework enables new kind scalable opponent exploitation approach	negative
network recasting: a universal method for network architecture transformation	paper proposes network recasting general method network architecture transformation primary goal method accelerate inference process transformation many practical applications method based block-wise recasting recasts source block pre-trained teacher network target block student network recasting target block trained output activation approximates source block block-by-block recasting sequential manner transforms network architecture preserving accuracy method used transform arbitrary teacher network type arbitrary student network type even generate mixed-architecture network consists two types block network recasting generate network fewer parameters and/or activations reduce inference time significantly naturally used network compression recasting trained network smaller network type experiments show outperforms previous compression approaches terms actual speedup gpu	positive
dependency or span, end-to-end uniform semantic role labeling	semantic role labeling srl aims discover predicateargument structure sentence end-to-end srl without syntactic input received great attention however focus either span-based dependency-based semantic representation form show specific model optimization respectively meanwhile handling two srl tasks uniformly less successful paper presents end-to-end model dependency span srl unified argument representation deal two different types argument annotations uniform fashion furthermore jointly predict predicates arguments especially including long-term ignored predicate identification subtask single model achieves new state-of-the-art results span conll 2005 2012 dependency conll 2008 2009 srl benchmarks	negative
on the persistence of clustering solutions and true number of clusters in a dataset	typically clustering algorithms provide clustering solutions prespecified number clusters lack priori knowledge true number underlying clusters dataset makes important metric compare clustering solutions different number clusters article quantifies notion persistence clustering solutions enables comparing solutions different number clusters persistence relates range dataresolution scales clustering solution persists quantified terms maximum two-norms associated cluster-covariance matrices thus associate persistence value element set clustering solutions different number clusters show datasets natural clusters priori known clustering solutions identify natural clusters persistent way notion used identify solutions true number clusters detailed experiments variety standard synthetic datasets demonstrate proposed persistence-based indicator outperforms existing approaches gap-statistic method x-means gmeans pg-means dip-means algorithms informationtheoretic method accurately identifying clustering solutions true number clusters interestingly method explained terms phase-transition phenomenon deterministic annealing algorithm number distinct cluster centers changes bifurcates respect annealing parameter	negative
context-tree recommendation vs matrix-factorization: algorithm selection and live users evaluation	describe selection implementation online evaluation two e-commerce recommender systems developed partner company prediggo first one based novel method bayesian variable-order markov modeling bvmm second ssagd novel variant matrix-factorization technique mf considered state-of-the-art recommender literature	negative
a generative model for dynamic networks with applications	networks observed real world like social networks collaboration networks etc. exhibit temporal dynamics i.e nodes edges appear and/or disappear time paper propose generative latent space based statistical model networks called dynamic networks consider case number nodes fixed presence edges vary time model allows number communities network different different time steps use neural network based methodology perform approximate inference proposed model simplified version experiments done synthetic real world networks task community detection link prediction demonstrate utility effectiveness model compared similar existing approaches	negative
joint dynamic pose image and space time reversal for human action recognition from videos	human action recognition aims classify given video according type action contains disturbance brought clutter background unrelated motions makes task challenging video frame-based methods solve problem paper takes advantage pose estimation enhance performances video frame features first present pose feature called dynamic pose image dpi describes human action aggregation sequence joint estimation maps different traditional pose features using sole joints dpi suffers less disturbance provides richer information human body shape movements second present attention-based dynamic texture images att-dtis pose-guided video frame feature specifically video treated space-time volume dtis obtained observing volume different views alleviate effect disturbance dtis accumulate joint estimation maps attention map extend dtis attention-based dtis att-dtis finally fuse dpi att-dtis multi-stream deep neural networks late fusion scheme action recognition experiments ntu rgb+d utd-mhad penn-action datasets show effectiveness dpi att-dtis well complementary property	negative
lattice cnns for matching based chinese question answering	short text matching often faces challenges great word mismatch expression diversity two texts would aggravated languages like chinese natural space segment words explicitly paper propose novel lattice based cnn model lcns utilize multi-granularity information inherent word lattice maintaining strong ability deal introduced noisy information matching based question answering chinese conduct extensive experiments document based question answering knowledge based question answering tasks experimental results show lcns models significantly outperform state-of-the-art matching models strong baselines taking advantages better ability distill rich discriminative information word lattice input	negative
tied transformers: neural machine translation with shared encoder and decoder	sharing source target side vocabularies word embeddings popular practice neural machine translation briefly nmt similar languages e.g. english french german translation success wordlevel sharing motivates us move one step consider model-level sharing tie whole parts encoder decoder nmt model share encoder decoder transformer vaswani et al 2017 state-of-the-art nmt model obtain compact model named tied transformer experimental results demonstrate simple method works well similar dissimilar language pairs empirically verify framework supervised nmt unsupervised nmt achieve 35.52 bleu score iwslt 2014 german english translation 28.98/29.89 bleu scores wmt 2014 english german translation without/with monolingual data 22.05 bleu score wmt 2016 unsupervised german english translation	negative
cogra: concept-drift-aware stochastic gradient descent for time-series forecasting	approach time-series forecasting problem presence concept drift automatic learning rate tuning stochastic gradient descent sgd sgd-based approach preferable concept drift algorithms applied model keep learning efficiently whilst predicting online among number sgd algorithms variance-based sgd vsgd successfully handle concept drift automatic learning rate tuning reduced adaptive mean estimation problem however performance still limited heuristic mean estimator paper present concept-drift-aware stochastic gradient descent cogra equipped theoretically-sound mean estimator called sequential mean tracker smt key contribution define goodness criterion mean estimators smt designed optimal according criterion result comprehensive experiments find smt estimate mean better vsgd ’ estimator presence concept drift ii terms predictive performance cogra reduces predictive loss 16–67 real-world datasets indicating smt improves prediction accuracy significantly	negative
temporal bilinear networks for video action recognition	temporal modeling videos fundamental yet challenging problem computer vision paper propose novel temporal bilinear tb model capture temporal pairwise feature interactions adjacent frames compared existing temporal methods limited linear transformations tb model considers explicit quadratic bilinear transformations temporal domain motion evolution sequential relation modeling leverage factorized bilinear model linear complexity bottleneck network design build tb blocks also constrains parameters computation cost consider two schemes terms incorporation tb blocks original 2d spatial convolutions namely wide deep temporal bilinear networks tbn finally perform experiments several widely adopted datasets including kinetics ucf101 hmdb51 effectiveness tbns validated comprehensive ablation analyses comparisons various state-of-the-art methods	negative
transfer learning for sequence labeling using source model and target data	paper propose approach transferring knowledge neural model sequence labeling learned source domain new model trained target domain new label categories appear transfer learning tl techniques enable adapt source model using target data new categories without accessing source data solution consists adding new neurons output layer target model transferring parameters source model fine-tuned target data additionally propose neural adapter learn difference source target label distribution provides additional important information target model experiments named entity recognition show learned knowledge source model effectively transferred target data contains new categories ii neural adapter improves transfer	positive
algorithms for estimating trends in global temperature volatility	trends terrestrial temperature variability perhaps relevant species viability trends mean temperature paper develop methodology estimating trends using multi-resolution climate data polar orbiting weather satellites derive two novel algorithms computation tailored dense gridded observations space time evaluate methods simulation mimics data ’ features large publicly available global temperature dataset eventual goal tracking trends cloud reflectance temperature variability	negative
temporal video analyzer (tvan): efficient temporal video analysis for robust video description and search	increasing popularity video content automatic video understanding becoming important streamlining video content consumption reuse work present tvan—temporal video analyzer—a system temporal video analysis aimed enabling efficient robust video description search main components include temporal video segmentation compact scene representation efficient visual recognition concise scene description generation provide technical overview system well demonstrate usefulness task video search navigation	negative
scene text detection with supervised pyramid context network	scene text detection methods based deep learning achieved remarkable results past years however due high diversity complexity natural scenes previous state-of-the-art text detection methods may still produce considerable amount false positives applied images captured real-world environments tackle issue mainly inspired mask r-cnn propose paper effective model scene text detection based feature pyramid network fpn instance segmentation propose supervised pyramid context network spcnet precisely locate text regions suppressing false positives	negative
angular triplet-center loss for multi-view 3d shape retrieval	obtain desirable representation 3d shape discriminative across categories polymerized within classes significant challenge 3d shape retrieval existing 3d shape retrieval methods focus capturing strong discriminative shape representation softmax loss classification task shape feature learning metric loss neglected 3d shape retrieval paper address problem based intuition cosine distance shape embeddings close enough within class far away across categories since 3d shape retrieval tasks use cosine distance shape features measuring shape similarity propose novel metric loss named angular triplet-center loss directly optimizes cosine distances features inherits triplet-center loss property achieve larger inter-class distance smaller intra-class distance simultaneously unlike previous metric loss utilized 3d shape retrieval methods euclidean distance adopted margin design difficult proposed method convenient train feature embeddings suitable 3d shape retrieval moreover angle margin adopted replace cosine margin order provide explicit discriminative constraints embedding space extensive experimental results two popular 3d object retrieval benchmarks modelnet40 shapenetcore 55 demonstrate effectiveness proposed loss method achieved state-ofthe-art results various 3d shape datasets	negative
discrete social recommendation	social recommendation aims improving performance traditional recommender systems considering social information attracted broad range interests one widely used methods matrix factorization typically uses continuous vectors represent user/item latent features however large volume user/item latent features results expensive storage computation cost particularly terminal user devices computation resource operate model limited thus taking extra social information account precisely extracting k relevant items given user massive candidates tends consume even time memory imposes formidable challenges efficient accurate recommendations promising way simply binarize latent features obtained training phase compute relevance score hamming distance however two-stage hashing based learning procedure capable preserving original data geometry real-value space may result severe quantization loss address issues work proposes novel discrete social recommendation dsr method learns binary codes unified framework users items considering social information put balanced uncorrelated constraints objective ensure learned binary codes informative yet compact finally develop efficient optimization algorithm estimate model parameters extensive experiments three real-world datasets demonstrate dsr runs nearly 5 times faster consumes 1/37 real-value competitor ’ memory usage cost almost loss accuracy	negative
detect or track: towards cost-effective video object detection/tracking	state-of-the-art object detectors trackers developing fast trackers general efficient detectors bear risk drifting question hence raised – improve accuracy video object detection/tracking utilizing existing detectors trackers within given time budget baseline frame skipping – detecting every n-th frames tracking frames baseline however suboptimal since detection frequency depend tracking quality end propose scheduler network determines detect track certain frame generalization siamese trackers although light-weight simple structure scheduler network effective frame skipping baselines flow-based approaches validated imagenet vid dataset video object detection/tracking	negative
syntax-aware neural semantic role labeling	semantic role labeling srl also known shallow semantic parsing important yet challenging task nlp motivated close correlation syntactic semantic structures traditional discrete-feature-based srl approaches make heavy use syntactic features contrast deep-neural-network-based approaches usually encode input sentence word sequence without considering syntactic structures work investigate several previous approaches encoding syntactic trees make thorough study whether extra syntax-aware representations beneficial neural srl models experiments benchmark conll-2005 dataset show syntax-aware srl approaches effectively improve performance strong baseline external word representations elmo extra syntax-aware representations approaches achieve new state-of-the-art 85.6 f1 single model 86.6 f1 ensemble test data outperforming corresponding strong baselines elmo 0.8 1.0 respectively detailed error analysis conducted gain insights investigated approaches	negative
hierarchical attention networks for sentence ordering	modeling discourse coherence important problem natural language generation understanding sentence ordering goal organize set sentences coherent text commonly used task learn evaluate model paper propose novel hierarchical attention network captures word clues dependencies sentences address problem model outperforms prior methods achieves state-of-the-art performance several datasets different domains furthermore experiments demonstrate model performs well even though adding noisy sentences set shows robustness effectiveness model visualization analysis case study show model captures structure pattern coherent texts simple word clues also consecution context	negative
hierarchical encoder with auxiliary supervision for neural table-to-text generation: learning better representation for tables	generating natural language descriptions structured tables consist multiple attribute-value tuples convenient way help people understand tables neural table-to-text models based encoder-decoder framework however hard vanilla encoder learn accurate semantic representation complex table challenges two-fold firstly table-to-text datasets often contain large number attributes across different domains thus hard encoder incorporate heterogeneous resources secondly single encoder also difficulties modeling complex attribute-value structure tables end first propose two-level hierarchical encoder coarse-to-fine attention handle attribute-value structure tables furthermore capture accurate semantic representations tables propose 3 joint tasks apart prime encoder-decoder learning namely auxiliary sequence labeling task text autoencoder multi-labeling classification auxiliary supervisions table encoder test models widely used dataset wikibio contains wikipedia infoboxes related descriptions dataset contains complex tables well large number attributes across different domains achieve state-of-the-art performance automatic human evaluation metrics	positive
dynamic spatial-temporal graph convolutional neural networks for traffic forecasting	graph convolutional neural networks gcnn become increasingly active field research models spatial dependencies nodes graph pre-defined laplacian matrix based node distances however many application scenarios spatial dependencies change time use fixed laplacian matrix capture change track spatial dependencies among traffic data propose dynamic spatio-temporal gcnn accurate traffic forecasting core deep learning framework finding change laplacian matrix dynamic laplacian matrix estimator enable timely learning low complexity creatively incorporate tensor decomposition deep learning framework real-time traffic data decomposed global component stable depends long-term temporal-spatial traffic relationship local component captures traffic fluctuations propose novel design estimate dynamic laplacian matrix graph two components based theoretical derivation introduce design basis forecasting performance evaluated two realtime traffic datasets experiment results demonstrate network achieve 25 accuracy improvement	positive
online learning from data streams with varying feature spaces	study problem online learning varying feature spaces problem challenging unlike traditional online learning problems varying feature spaces introduce new features stop features without following pattern existing methods online streaming feature selection wu et al 2013 online learning trapezoidal data streams zhang et al 2016 learning feature evolvable streams hou zhang zhou 2017 capable learn arbitrarily varying feature spaces make assumptions feature space dynamics paper propose novel online learning algorithm olvf learn data arbitrarily varying feature spaces olvf algorithm learns classify feature spaces instances feature spaces simultaneously classify instance algorithm dynamically projects instance classifier training instance onto shared feature subspace feature space classifier predicts projection confidences given feature space instance classifier updated following empirical risk minimization principle strength constraints scaled projection confidences afterwards feature sparsity method applied reduce model complexity experiments 10 datasets varying feature spaces conducted demonstrate performance proposed olvf algorithm moreover experiments trapezoidal data streams datasets conducted show olvf performs better state-of-the-art learning algorithm zhang et al 2016	negative
vidyutvanika: a reinforcement learning based broker agent for a power trading competition	smart grid efficient sustainable energy system integrates diverse generation entities distributed storage capacity smart appliances buildings smart grid brings new kinds participants energy market served whose effect grid determined high fidelity simulations power tac offers one simulation platform using real-world weather data complex state-of-the-art customer models power tac autonomous energy brokers compete make profits across tariff wholesale balancing markets maintaining stability grid paper design autonomous broker vidyutvanika runner-up 2018 power tac competition vidyutvanika relies reinforcement learning rl tariff market dynamic programming wholesale market solve modified versions known markov decision process mdp formulations respective markets novelty lies defining reward functions mdps solving mdps application solutions real actions market unlike previous participating agents vidyutvanika uses neural network predict energy consumption various customers using weather data use several heuristic ideas bridge gap restricted action spaces mdps much extensive action space available vidyutvanika heuristics allow vidyutvanika convert near-optimal fixed tariffs time-of-use tariffs aimed mitigating transmission capacity fees spread orders across several auctions wholesale market procure energy lower price accurately estimate parameters required implementing mdp solution wholesale market account wholesale procurement costs optimizing tariffs use power tac 2018 tournament data controlled experiments analyze performance vidyutvanika illustrate efficacy strategies	negative
exploiting time-series image-to-image translation to expand the range of wildlife habitat analysis	characterizing wildlife habitat one main topics animal ecology locational data obtained radio tracking field observation widely used habitat analysis however sampling methods costly laborious insufficient relocations often prevent scientists conducting large-range long-term research paper innovatively exploit image-to-image translation technology expand range wildlife habitat analysis proposed novel approach implementing time-series imageto-image translation via metric embedding siamese neural network used learn euclidean temporal embedding image space embedding produces temporal vectors bring time information adversarial network well-trained framework could effectively map probabilistic habitat models remote sensing imagery helping scientists get rid persistent dependence animal relocations illustrate approach real-world application mapping habitats bar-headed geese qinghai lake breeding ground compare model several baselines achieve promising results	positive
cross-view local structure preserved diversity and consensus learning for multi-view unsupervised feature selection	multi-view unsupervised feature selection mv-ufs aims select feature subset multi-view data without using labels samples however observe existing mv-ufs algorithms well consider local structure cross views diversity different views could adversely affect performance subsequent learning tasks paper propose cross-view local structure preserved diversity consensus semantic learning model mv-ufs termed crv-dcl briefly address issues specifically project view data common semantic label space composed consensus part diversity part aim capture common information distinguishing knowledge across different views inter-view similarity graph pairwise view intra-view similarity graph view respectively constructed preserve local structure data different views different samples view l2,1-norm constraint imposed feature projection matrix select discriminative features carefully design efficient algorithm convergence guarantee solve resultant optimization problem extensive experimental study conducted six publicly real multi-view datasets experimental results well demonstrate effectiveness crv-dcl	negative
lipper: speaker independent speech synthesis using multi-view lipreading	lipreading process understanding interpreting speech observing speaker ’ lip movements past work lipreading limited classifying silent videos fixed number text classes however limits applications lipreading since human language bound fixed set words languages aim work reconstruct intelligible acoustic speech signals silent videos various poses person lipper never seen lipper therefore vocabulary language agnostic speaker independent near real-time model deals variety poses speaker model leverages silent video feeds multiple cameras recording subject generate intelligent speech speaker uses deep learning based stcnn+bigru architecture achieve goal evaluate speech reconstruction speaker independent scenarios demonstrate speech output overlaying audios reconstructed lipper corresponding videos	negative
orderly subspace clustering	semi-supervised representation-based subspace clustering partition data underlying subspaces finding effective data representations partial supervisions essentially effective accurate representation able uncover preserve true data structure meanwhile reliable easy-to-obtain supervision desirable practical learning meet two objectives paper make first attempt towards utilizing orderly relationship data closer b c novel supervision propose orderly subspace clustering approach novel regularization term osc enforces learned representations simultaneously capture intrinsic subspace structure reveal orderly structure faithful true data relationship experimental results several benchmarks demonstrated aside accurate clustering state-of-the-arts osc interprets orderly data structure beyond current approaches offer	negative
preference-aware task assignment in spatial crowdsourcing	ubiquity smart devices spatial crowdsourcing sc emerged new transformative platform engages mobile users perform spatio-temporal tasks physically traveling specified locations thus various sc techniques studied performance optimization among one major challenges assign workers tasks really interested willing perform paper propose novel preference-aware spatial task assignment system based workers ’ temporal preferences consists two components history-based context-aware tensor decomposition hctd workers ’ temporal preferences modeling preference-aware task assignment model worker preferences three-dimension tensor worker-task-time supplementing missing entries tensor hctd assistant historical data two context matrices recover worker preferences different categories tasks different time slots several preference-aware task assignment algorithms devised aiming maximize total number task assignments every time instance give higher priorities workers interested tasks conduct extensive experiments using real dataset verifying practicability proposed methods	negative
smooth deep image generator from noises	generative adversarial networks gans demonstrated strong ability fit complex distributions since presented especially field generating natural images linear interpolation noise space produces continuously changing image space impressive property gans however special consideration property objective function gans derived models paper analyzes perturbation input generator influence generated images smooth generator developed investigating tolerable input perturbation integrate smooth generator gradient penalized discriminator design smooth gan generates stable high-quality images experiments real-world image datasets demonstrate necessity studying smooth generator effectiveness proposed algorithm	positive
hypergraph neural networks	paper present hypergraph neural networks hgnn framework data representation learning encode high-order data correlation hypergraph structure confronting challenges learning representation complex data real practice propose incorporate data structure hypergraph flexible data modeling especially dealing complex data method hyperedge convolution operation designed handle data correlation representation learning way traditional hypergraph learning procedure conducted using hyperedge convolution operations efficiently hgnn able learn hidden layer representation considering high-order data structure general framework considering complex data correlations conducted experiments citation network classification visual object recognition tasks compared hgnn graph convolutional networks traditional methods experimental results demonstrate proposed hgnn method outperforms recent state-of-theart methods also reveal results proposed hgnn superior dealing multi-modal data compared existing methods	positive
a hierarchical multi-task approach for learning embeddings from semantic tasks	much effort devoted evaluate whether multi-task learning leveraged learn rich representations used various natural language processing nlp down-stream applications however still lack understanding settings multi-task learning significant effect work introduce hierarchical model trained multi-task learning setup set carefully selected semantic tasks model trained hierarchical fashion introduce inductive bias supervising set low level tasks bottom layers model complex tasks top layers model model achieves state-of-the-art results number tasks namely named entity recognition entity mention detection relation extraction without hand-engineered features external nlp tools like syntactic parsers hierarchical training supervision induces set shared semantic representations lower layers model show move bottom top layers model hidden states layers tend represent complex semantic information	negative
attention based spatial-temporal graph convolutional networks for traffic flow forecasting	forecasting traffic flows critical issue researchers practitioners field transportation however challenging since traffic flows usually show high nonlinearities complex patterns existing traffic flow prediction methods lacking abilities modeling dynamic spatial-temporal correlations traffic data thus yield satisfactory prediction results paper propose novel attention based spatial-temporal graph convolutional network astgcn model solve traffic flow forecasting problem astgcn mainly consists three independent components respectively model three temporal properties traffic flows i.e. recent daily-periodic weekly-periodic dependencies specifically component contains two major parts 1 spatial-temporal attention mechanism effectively capture dynamic spatialtemporal correlations traffic data 2 spatial-temporal convolution simultaneously employs graph convolutions capture spatial patterns common standard convolutions describe temporal features output three components weighted fused generate final prediction results experiments two real-world datasets caltrans performance measurement system pems demonstrate proposed astgcn model outperforms state-of-the-art baselines	negative
approximate stream reasoning with metric temporal logic under uncertainty	stream reasoning defined incremental reasoning incrementally-available information formula progression procedure metric temporal logic mtl makes use syntactic formula rewritings incrementally evaluate formulas incrementally-available states progression however assumes complete state information problematic state information available observed qualitative spatial reasoning tasks robotics applications cases may uncertainty state set possible states represents ‘ true ’ state main contribution paper therefore extension progression procedure efficiently keeps track consistent hypotheses resulting procedure flexible allowing trade-off faster approximate slower precise progression uncertainty proposed approach empirically evaluated considering time space requirements well impact permitting varying degrees uncertainty	negative
neural relation extraction within and across sentence boundaries	past work relation extraction mostly focuses binary relation entity pairs within single sentence recently nlp community gained interest relation extraction entity pairs spanning multiple sentences paper propose novel architecture task inter-sentential dependency-based neural networks idepnn idepnn models shortest augmented dependency paths via recurrent recursive neural networks extract relationships within intra- across inter- sentence boundaries compared svm neural network baselines idepnn robust false positives relationships spanning sentences evaluate models four datasets newswire muc6 medical bionlp shared task domains achieve state-of-the-art performance show better balance precision recall inter-sentential relationships perform better 11 teams participating bionlp shared task 2016 achieve gain 5.2 0.587 vs 0.558 f1 winning team also release crosssentence annotations muc6	positive
nearest-neighbour-induced isolation similarity and its impact on density-based clustering	recent proposal data dependent similarity called isolation kernel/similarity enabled svm produce better classification accuracy identify shortcomings using tree method implement isolation similarity propose nearest neighbour method instead formally prove characteristic isolation similarity use proposed method impact isolation similarity densitybased clustering studied show first time clustering performance classic density-based clustering algorithm dbscan significantly uplifted surpass recent density-peak clustering algorithm dp achieved simply replacing distance measure proposed nearest-neighbour-induced isolation similarity dbscan leaving rest procedure unchanged new type clusters called mass-connected clusters formally defined show dbscan detects density-connected clusters becomes one detects mass-connected clusters distance measure replaced proposed similarity also provide condition mass-connected clusters detected density-connected clusters	negative
towards reliable learning for high stakes applications	paper focus delivering reliable learning results high stakes applications self-driving financial investment clinical diagnosis accuracy predictions considered crucial requirement giving predictions query samples adopt learning reject option framework learning model predict samples convince give correct answer however prevailing deep learning predictors confidence estimated model far reflecting real generalization performance model reliability prediction concisely propose exploratory solution called galve generative adversarial learning variance expansion adopts generative adversarial learning implicitly measure region model achieve good generalization performance applying galve measure reliability predictions achieved error rate less half straightforwardly measured confidence cifar10 svhn computer vision tasks	positive
learning deviation payoffs in simulation-based games	present novel approach identifying approximate role-symmetric nash equilibria large simulation-based games method uses neural networks learn mapping mixed-strategy profiles deviation payoffs—the expected values playing pure-strategy deviations profiles learning generalize data tiny fraction game ’ outcomes permitting tractable analysis exponentially large normal-form games give procedure iteratively refining learned model new data produced sampling neighborhood candidate nash equilibrium relative existing state art deviation payoff learning dramatically simplifies task computing equilibria effectively addresses player asymmetries demonstrate empirically deviation payoff learning identifies better approximate equilibria previous methods handle difficult settings including games many players strategies roles	positive
recognizing unseen attribute-object pair with generative model	paper studying problem recognizing attribute-object pairs appear training dataset called unseen attribute-object pair recognition existing methods mainly learn discriminative classifier compose multiple classifiers tackle problem exhibit poor performance unseen pairs key reasons failure 1 learned intrinsic attributeobject representation 2 attribute object processed either separately equally inner relation attribute object explored explore inner relation attribute object well intrinsic attribute-object representation propose generative model encoder-decoder mechanism bridges visual linguistic information unified end-to-end network encoder-decoder mechanism presents impressive potential find intrinsic attribute-object feature representation addition combining visual linguistic features unified model allows mine relation attribute object conducted extensive experiments compare method several state-of-the-art methods two challenging datasets results show method outperforms methods	positive
discriminative feature learning for unsupervised video summarization	paper address problem unsupervised video summarization automatically extracts key-shots input video specifically tackle two critical issues based empirical observations ineffective feature learning due flat distributions output importance scores frame ii training difficulty dealing longlength video inputs alleviate first problem propose simple yet effective regularization loss term called variance loss proposed variance loss allows network predict output scores frame high discrepancy enables effective feature learning significantly improves model performance second problem design novel two-stream network named chunk stride network csnet utilizes local chunk global stride temporal view video features csnet gives better summarization results long-length videos compared existing methods addition introduce attention mechanism handle dynamic information videos demonstrate effectiveness proposed methods conducting extensive ablation studies show final model achieves new state-of-the-art results two benchmark datasets	positive
sam-net: integrating event-level and chain-level attentions to predict what happens next	scripts represent knowledge event sequences help text understanding script event prediction requires measure relation existing chain subsequent event dominant approaches either focus effects individual events influence chain sequence however considering individual events lose much semantic relations within event chain considering sequence chain introduce much noise observations individual events event segments within chain facilitate prediction subsequent event paper develops self attention mechanism focus diverse event segments within chain event chain represented set event segments utilize event-level attention model relations subsequent events individual events propose chain-level attention model relations subsequent events event segments within chain finally integrate event-level chain-level attentions interact chain predict happens next comprehensive experiment results widely used new york times corpus demonstrate model achieves better results state-of-the-art baselines adopting evaluation multi-choice narrative cloze task	negative
regular boardgames	propose new general game playing ggp language called regular boardgames rbg based theory regular languages objective rbg join key properties expressiveness efficiency naturalness description one ggp formalism compensating certain drawbacks existing languages often makes rbg suitable various research practical developments ggp dedicated mostly describing board games rbg universal class finite deterministic turn-based games perfect information establish foundations rbg analyze theoretically experimentally focusing efficiency reasoning regular boardgames first ggp language allows efficient encoding playing games complex rules large branching factor e.g amazons arimaa large chess variants go international checkers paper soccer	positive
how many pairwise preferences do we need to rank a graph consistently?	consider problem optimal recovery true ranking n items randomly chosen subset pairwise preferences well known without assumption one requires sample size ω n2 purpose analyze problem additional structure relational graph g n e n items added assumption locality neighboring items similar rankings noting preferential nature data choose embed graph strong product capture pairwise node relationships furthermore unlike existing literature uses laplacian embedding graph based learning problems use richer class graph embeddings—orthonormal representations—that includes normalized laplacian special case proposed algorithm pref-rank predicts underlying ranking using svm based approach using chosen embedding product graph first provide statistical consistency two ranking losses kendall ’ tau spearman ’ footrule required sample complexity n2χ g¯ ⅔ pairs χ g¯ chromatic number complement graph g¯ clearly sample complexity smaller dense graphs χ g¯ characterizing degree node connectivity also intuitive due locality assumption e.g n4/3 union k-cliques n5/3 random power law graphs etc.—a quantity much smaller fundamental limit ω n2 large n. first time relates ranking complexity structural properties graph also report experimental evaluations different synthetic real-world datasets algorithm shown outperform state art methods	positive
on-line adaptative curriculum learning for gans	generative adversarial networks gans successfully approximate probability distribution produce realistic samples however open questions sufficient convergence conditions mode collapse still persist paper build existing work area proposing novel framework training generator ensemble discriminator networks seen one-student/multiple-teachers setting formalize problem within full-information adversarial bandit framework evaluate capability algorithm select mixtures discriminators providing generator feedback learning end propose reward function reflects progress made generator dynamically update mixture weights allocated discriminator also draw connections algorithm stochastic optimization methods show existing approaches using multiple discriminators literature recovered framework argue less expressive discriminators smoother general coarse grained view modes map enforces generator cover wide portion data distribution support hand highly expressive discriminators ensure samples quality finally experimental results show approach improves samples quality diversity existing baselines effectively learning curriculum results also support claim weaker discriminators higher entropy improving modes coverage	positive
fully convolutional network with multi-step reinforcement learning for image processing	paper tackles new problem setting reinforcement learning pixel-wise rewards pixelrl image processing introduction deep q-network deep rl achieving great success however applications deep rl image processing still limited therefore extend deep rl pixelrl various image processing applications pixelrl pixel agent agent changes pixel value taking action also propose effective learning method pixelrl significantly improves performance considering future states pixel also neighbor pixels proposed method applied image processing tasks require pixel-wise manipulations deep rl never applied	positive
localizing natural language in videos	paper consider task natural language video localization nlvl given untrimmed video natural language description goal localize segment video semantically corresponds given natural language description propose localizing network lnet working end-to-end fashion tackle nlvl task first match natural sentence video sequence cross-gated attended recurrent networks exploit fine-grained interactions generate sentence-aware video representation self interactor proposed perform crossframe matching dynamically encodes aggregates matching evidences finally boundary model proposed locate positions video segments corresponding natural sentence description predicting starting ending points segment extensive experiments conducted public tacos didemo datasets demonstrate proposed model performs effectively efficiently state-of-the-art approaches	positive
a non–convex optimization approach to correlation clustering	develop non-convex optimization approach correlation clustering using frank-wolfe fw framework show basic approach leads simple natural local search algorithm guaranteed convergence algorithm already beats alternative algorithms substantial margins running time quality clustering using ideas fw algorithms develop subsampling variance reduction paradigms approach yields practical improvement algorithm interesting directions investigate demonstrate performance synthetic real world data sets	negative
interleave variational optimization with monte carlo sampling: a tale of two approximate inference paradigms	computing partition function graphical model fundamental task probabilistic inference variational bounds monte carlo methods two important approximate paradigms task respective strengths solving different types problems often nontrivial decide one apply particular problem instance without significant prior knowledge high level expertise paper propose general framework interleaves optimization variational bounds via message passing monte carlo sampling adaptive interleaving policy automatically balance computational effort two schemes instance-dependent way provides framework strengths schemes leads tighter anytime bounds unbiased estimate partition function allows flexible tradeoffs memory time solution quality verify approach empirically real-world problems taken recent uai inference competitions	positive
learning representations in model-free hierarchical reinforcement learning	common approaches reinforcement learning rl seriously challenged large-scale applications involving huge state spaces sparse delayed reward feedback hierarchical reinforcement learning hrl methods attempt address scalability issue learning action selection policies multiple levels temporal abstraction abstraction identifying relatively small set states likely useful subgoals concert learning corresponding skill policies achieve subgoals many approaches subgoal discovery hrl depend analysis model environment need learn model introduces problems scale subgoals identified skills may learned intrinsic motivation introducing internal reward signal marking subgoal attainment present novel model-free method subgoal discovery using incremental unsupervised learning small memory recent experiences agent combined intrinsic motivation learning mechanism method learns subgoals skills together based experiences environment thus offer original approach hrl require acquisition model environment suitable large-scale applications demonstrate efficiency method variant rooms environment	positive
performance guarantees for homomorphisms beyond markov decision processes	real-world problems huge state and/or action spaces therefore naive application existing tabular solution methods tractable problems nonetheless solution methods quite useful agent access relatively small state-action space homomorphism true environment near-optimal performance guaranteed map plethora research focused case homomorphism markovian representation underlying process however show nearoptimal performance sometimes guaranteed even homomorphism non-markovian	negative
a radical-aware attention-based model for chinese text classification	recent years chinese text classification attracted research attention however existing techniques specifically aim english materials may lose effectiveness task due huge difference chinese english actually special kind hieroglyphics chinese characters radicals semantically useful still unexplored task text classification end paper first analyze motives using multiple granularity features represent chinese text inspecting characteristics radicals characters words better representing chinese text implementing chinese text classification propose novel radicalaware attention-based four-granularity rafg model take full advantages chinese characters words characterlevel radicals word-level radicals simultaneously specifically rafg applies serialized blstm structure context-aware able capture long-range information model character sharing property chinese sequence characteristics texts design attention mechanism enhance effects radicals thus model radical sharing property integrating granularities finally conduct extensive experiments experimental results show superiority model also validate effectiveness radicals task chinese text classification	negative
manifold-valued image generation with wasserstein generative adversarial nets	generative modeling natural images one fundamental machine learning problems however modern generative models including wasserstein generative adversarial nets wgans studied manifold-valued images frequently encountered real-world applications fill gap paper first formulates problem generating manifold-valued images exploits three typical instances hue-saturation-value hsv color image generation chromaticity-brightness cb color image generation diffusion-tensor dt image generation proposed generative modeling problem introduce theorem optimal transport derive new wasserstein distance data distributions complete manifolds enabling us achieve tractable objective wgan framework addition recommend three benchmark datasets cifar-10 hsv/cb color images imagenet hsv/cb color images ucl dt image datasets three datasets experimentally demonstrate proposed manifold-aware wgan model generate plausible manifold-valued images competitors	positive
transferable curriculum for weakly-supervised domain adaptation	domain adaptation improves target task knowledge transfer source domain rich annotations uncommon “ source-domain engineering ” becomes cumbersome process domain adaptation high-quality source domains highly related target domain hardly available thus weakly-supervised domain adaptation introduced address difficulty tolerate source domain noises labels features particular target task simply collect source domain coarse labeling corrupted data paper try address two entangled challenges weaklysupervised domain adaptation sample noises source domain distribution shift across domains disentangle challenges transferable curriculum learning tcl approach proposed train deep networks guided transferable curriculum informing source examples noiseless transferable approach enhances positive transfer clean source examples target mitigates negative transfer noisy source examples thorough evaluation shows approach significantly outperforms state-of-the-art weakly-supervised domain adaptation tasks	negative
message-dropout: an efficient training method for multi-agent deep reinforcement learning	paper propose new learning technique named message-dropout improve performance multi-agent deep reinforcement learning two application scenarios 1 classical multi-agent reinforcement learning direct message communication among agents 2 centralized training decentralized execution first application scenario multi-agent systems direct message communication among agents allowed messagedropout technique drops received messages agents block-wise manner certain probability training phase compensates effect multiplying weights dropped-out block units correction probability applied message-dropout technique effectively handles increased input dimension multi-agent reinforcement learning communication makes learning robust communication errors execution phase second application scenario centralized training decentralized execution particularly consider application proposed messagedropout multi-agent deep deterministic policy gradient maddpg uses centralized critic train decentralized actor agent evaluate proposed message-dropout technique several games numerical results show proposed message-dropout technique proper dropout rate improves reinforcement learning performance significantly terms training speed steady-state performance execution phase	positive
topiceq: a joint topic and mathematical equation model for scientific texts	scientific documents rely mathematics text communicate ideas inspired topical correspondence mathematical equations word contexts observed scientific texts propose novel topic model jointly generates mathematical equations surrounding text topiceq using extension correlated topic model context generated mixture latent topics equation generated rnn depends latent topic activations experiment model create corpus 400k equation-context pairs extracted range scientific articles arxiv fit model using variational autoencoder approach experimental results show joint model significantly outperforms existing topic models equation models scientific texts moreover qualitatively show model effectively captures relationship topics mathematics enabling novel applications topic-aware equation generation equation topic inference topic-aware alignment mathematical symbols words	negative
understanding vaes in fisher-shannon plane	information theory fisher information shannon information entropy respectively used quantify uncertainty associated distribution modeling uncertainty specifying outcome given variables two quantities complementary jointly applied information behavior analysis cases uncertainty property information asserts fundamental trade-off fisher information shannon information enlightens us relationship encoder decoder variational auto-encoders vaes paper investigate vaes fisher-shannon plane demonstrate representation learning log-likelihood estimation intrinsically related two information quantities extensive qualitative quantitative experiments provide better comprehension vaes tasks high-resolution reconstruction representation learning perspective fisher information shannon information propose variant vaes termed fisher auto-encoder fae practical needs balance fisher information shannon information experimental results demonstrated promise improving reconstruction accuracy avoiding noninformative latent code occurred previous works	negative
document informed neural autoregressive topic models with distributional prior	address two challenges topic models 1 context information around words helps determining actual meaning e.g. “ networks ” used contexts artificial neural networks vs. biological neuron networks generative topic models infer topic-word distributions taking little context account extend neural autoregressive topic model exploit full context information around words document language modeling fashion proposed model named idocnade 2 due small number word occurrences i.e. lack context short text data sparsity corpus documents application topic models challenging texts therefore propose simple efficient way incorporating external knowledge neural autoregressive topic models use embeddings distributional prior proposed variants named docnadee idocnadee present novel neural autoregressive topic model variants consistently outperform state-of-the-art generative topic models terms generalization interpretability topic coherence applicability retrieval classification 7 long-text 8 short-text datasets diverse domains	negative
drr-net: dynamic re-read network for sentence semantic matching	sentence semantic matching requires agent determine semantic relation two sentences widely used various natural language tasks natural language inference nli paraphrase identification pi among matching methods attention mechanism plays important role capturing semantic relations properly aligning elements two sentences previous methods utilized attention mechanism select important parts sentences one time however important parts sentence semantic matching dynamically changing degree sentence understanding selecting important parts one time may insufficient semantic understanding end propose dynamic re-read network drr-net approach sentence semantic matching able pay close attention small region sentences step re-read important words better sentence semantic understanding specific first employ attention stack-gru asg unit model original sentence repeatedly preserve information bottom-most word embedding input up-most recurrent output second utilize dynamic re-read drr unit pay close attention one important word one time consideration learned information re-read important words better sentence semantic understanding extensive experiments three sentence matching benchmark datasets demonstrate drr-net ability model sentence semantic precisely significantly improve performance sentence semantic matching addition interesting finding experiments consistent findings psychological research	negative
efficient concept induction for description logics	concept induction refers problem creating complex description logic class descriptions i.e. tbox axioms instance examples i.e. abox data paper look particularly case set positive set negative instances given complex class expressions sought positive negative examples fall concept induction found applications ontology engineering existing algorithms fundamental performance issues scenarios mainly high number invokations external description logic reasoner usually required paper present new algorithm problem drastically reduces number reasoner invokations needed comes expense limited traversal search space show approach improves execution times several orders magnitude output correctness measured amount correct coverage input instances remains reasonably high many cases approach thus provide strong alternative existing systems particular settings systems prohibitively slow	positive
an improved hierarchical datastructure for nearest neighbor search	nearest neighbor search fundamental computational tool wide applications past decades many datastructures developed speed operation paper propose novel hierarchical datastructure nearest neighbor search moderately high dimension proposed method maintains good run time guarantees outperforms several state-of-the-art methods practice	negative
automatic bayesian density analysis	making sense dataset automatic unsupervised fashion challenging problem statistics ai classical approaches exploratory data analysis usually flexible enough deal uncertainty inherent real-world data often restricted fixed latent interaction models homogeneous likelihoods sensitive missing corrupt anomalous data moreover expressiveness generally comes price intractable inference result supervision statisticians usually needed find right model data however since domain experts necessarily also experts statistics propose automatic bayesian density analysis abda make exploratory data analysis accessible large specifically abda allows automatic efficient missing value estimation statistical data type likelihood discovery anomaly detection dependency structure mining top providing accurate density estimation extensive empirical evidence shows abda suitable tool automatic exploratory analysis mixed continuous discrete tabular data	negative
detecting incongruity between news headline and body text via a deep hierarchical encoder	news headlines mislead readers overrated false information identifying advance better assist readers choosing proper news stories consume research introduces million-scale pairs news headline body text dataset incongruity label uniquely utilized detecting news stories misleading headlines dataset develop two neural networks hierarchical architectures model complex textual representation news articles measure incongruity headline body text also present data augmentation method dramatically reduces text input size model handles independently investigating paragraph news stories boosts performance experiments qualitative evaluations demonstrate proposed methods outperform existing approaches efficiently detect news stories misleading headlines real world	negative
a human-like semantic cognition network for aspect-level sentiment classification	paper propose novel human-like semantic cognition network hscn aspect-level sentiment classification motivated principles human beings ’ reading cognitive process pre-reading active reading post-reading first design word-level interactive perception module capture correlation context words given target words regarded pre-reading second mimic process active reading propose targetaware semantic distillation module produce targetspecific context representation aspect-level sentiment prediction third devise semantic deviation metric module measure semantic deviation targetspecific context representation given target evaluates degree understand target-specific context semantics measured semantic deviation used fine-tune active reading process feedback regulation way verify effectiveness approach conduct extensive experiments three widely used datasets experiments demonstrate hscn achieves impressive results compared strong competitors	positive
violence rating prediction from movie scripts	violent content movies influence viewers ’ perception society example frequent depictions certain demographics perpetrators victims abuse shape stereotyped attitudes work propose characterize aspects violent content movies solely language used scripts makes method applicable movie earlier stages content creation even produced complementary previous works rely audio video post production approach based broad range features designed capture lexical semantic sentiment abusive language characteristics use features learn vector representation 1 complete movie 2 act movie former representation used train movie-level classification model latter train deep-learning sequence classifiers make use context tested models dataset 732 hollywood scripts annotated experts violent content performance evaluation suggests linguistic features good indicator violent content furthermore ablation studies show semantic sentiment features important predictors violence data date first show language used movie scripts strong indicator violent content offers novel computational tools assist creating awareness storytelling	negative
bayesian graph convolutional neural networks for semi-supervised classification	recently techniques applying convolutional neural networks graph-structured data emerged graph convolutional neural networks gcnns used address node graph classification matrix completion although performance impressive current implementations limited capability incorporate uncertainty graph structure almost gcnns process graph though ground-truth depiction relationship nodes often graphs employed applications derived noisy data modelling assumptions spurious edges may included edges may missing nodes strong relationships paper adopt bayesian approach viewing observed graph realization parametric family random graphs target inference joint posterior random graph parameters node graph labels present bayesian gcnn framework develop iterative learning procedure case assortative mixed-membership stochastic block models present results experiments demonstrate bayesian formulation provide better performance labels available training process	negative
defsi: deep learning based epidemic forecasting with synthetic information	influenza-like illness ili among common diseases worldwide producing timely well-informed reliable forecasts ili crucial preparedness optimal interventions work focus short-term highresolution forecasting propose defsi deep learning based epidemic forecasting synthetic information epidemic forecasting framework integrates strengths artificial neural networks causal methods defsi build two-branch neural network structure take within-season observations between-season observations features model trained geographically highresolution synthetic data enables detailed forecasting high-resolution surveillance data available furthermore model provided better generalizability physical consistency method achieves comparable/better performance state-of-the-art methods short-term ili forecasting state level high-resolution forecasting county level defsi significantly outperforms methods	negative
multi-fidelity automatic hyper-parameter tuning via transfer series expansion	automatic machine learning automl aims automatically choosing best configuration machine learning tasks however configuration evaluation time consuming particularly learning tasks large datasets limitation usually restrains derivative-free optimization releasing full power fine configuration search using many evaluations alleviate limitation paper propose derivative-free optimization framework automl using multi-fidelity evaluations uses many lowfidelity evaluations small data subsets highfidelity evaluations full dataset however lowfidelity evaluations badly biased need corrected low cost thus propose transfer series expansion tse learns low-fidelity correction predictor efficiently linearly combining set base predictors base predictors obtained cheaply down-scaled experienced tasks experimental results real-world automl problems verify proposed framework accelerate derivative-free configuration search significantly making use multi-fidelity evaluations	negative
adaptive proximal average based variance reducing stochastic methods for optimization with composite regularization	focus empirical risk minimization composite regulariser widely applied various machine learning tasks introduce important structural information regarding problem data general challenging calculate proximal operator composite regulariser recently proximal average pa involves feasible proximal operator calculation proposed approximate composite regularisers augmented prevailing variance reducing vr stochastic methods e.g svrg saga pa based algorithms would achieve better performance however existing works require fixed stepsize needs rather small ensure pa approximation sufficiently accurate meantime smaller stepsize would incur many iterations convergence paper propose two fast pa based vr stochastic methods – apa-svrg apa-saga initializing stepsize much larger value adaptively decreasing proposed methods proved enjoy ô n log 1/ε mo 1/ε iteration complexity achieve accurate solutions m0 initial number inner iterations n number samples moreover experimental results demonstrate superiority proposed algorithms	negative
translating with bilingual topic knowledge for neural machine translation	dominant neural machine translation nmt models based encoder-decoder architecture recently achieved state-of-the-art performance traditionally nmt models depend representations learned training mapping source sentence target domain however learned representations often suffer implicit inadequately informed properties paper propose novel bilingual topic enhanced nmt bltnmt model improve translation performance incorporating bilingual topic knowledge nmt specifically bilingual topic knowledge included hidden states encoder decoder well attention mechanism new setting proposed blt-nmt access background knowledge implied bilingual topics beyond sequential context enables attention mechanism attend topic-level attentions generating accurate target words translation experimental results show proposed model consistently outperforms traditional rnnsearch previous topic-informed nmt chinese-english englishgerman translation tasks also introduce bilingual topic knowledge newly emerged transformer base model english-german translation achieve notable improvement	negative
exploiting synthetically generated data with semi-supervised learning for small and imbalanced datasets	data augmentation rapidly gaining attention machine learning synthetic data generated simple transformations data distribution latter case main challenge estimate label associated new synthetic patterns paper studies effect generating synthetic data convex combination patterns use unsupervised information semi-supervised learning framework support vector machines avoiding thus need label synthetic examples perform experiments total 53 binary classification datasets results show type data over-sampling supports well-known cluster assumption semi-supervised learning showing outstanding results small high-dimensional datasets imbalanced learning problems	negative
improving domain-independent planning via critical section macro-operators	macro-operators macros short well-known technique enhancing performance planning engines providing “ short-cuts ” state space existing macro learning systems usually generate macros frequent sequences actions training plans approach priorities frequently used sequences actions meaningful activities performed solving planning tasks	negative
video imprint segmentation for temporal action detection in untrimmed videos	propose temporal action detection spatial segmentation framework simultaneously categorize actions temporally localize action instances untrimmed videos core idea conversion temporal detection task spatial semantic segmentation task firstly video imprint representation employed capture spatial/temporal interdependences within/among frames represent spatial proximity feature space subsequently obtained imprint representation spatially segmented fully convolutional network segmentation labels projected back video space temporal action boundary localization per-frame spatial annotation obtained simultaneously proposed framework robust variable lengths untrimmed videos due underlying fixed-size imprint representations efficacy framework validated two public action detection datasets	positive
dtmt: a novel deep transition architecture for neural machine translation	past years witnessed rapid developments neural machine translation nmt recently advanced modeling training techniques rnn-based nmt rnmt shown potential strength even compared well-known transformer self-attentional model although rnmt model possess deep architectures stacking layers transition depth consecutive hidden states along sequential axis still shallow paper enhance rnn-based nmt increasing transition depth consecutive hidden states build novel deep transition rnn-based architecture neural machine translation named dtmt model enhances hidden-to-hidden transition multiple non-linear transformations well maintains linear transformation path throughout deep transition well-designed linear transformation mechanism alleviate gradient vanishing problem experiments show specially designed deep transition modules dtmt achieve remarkable improvements translation quality experimental results chinese⇒english translation task show dtmt outperform transformer model +2.09 bleu points achieve best results ever reported dataset wmt14 english⇒german english⇒french translation tasks dtmt shows superior quality state-of-the-art nmt systems including transformer rnmt+	positive
exploiting sentence embedding for medical question answering	despite great success word embedding sentence embedding remains not-well-solved problem paper present supervised learning framework exploit sentence embedding medical question answering task learning framework consists two main parts 1 sentence embedding producing module 2 scoring module former developed contextual self-attention multi-scale techniques encode sentence embedding tensor module shortly called contextual self-attention multi-scale sentence embedding camse latter employs two scoring strategies semantic matching scoring sms semantic association scoring sas sms measures similarity sas captures association sentence pairs medical question concatenated candidate choice piece corresponding supportive evidence proposed framework examined two medical question answering medicalqa datasets collected real-world applications medical exam clinical diagnosis based electronic medical records emr comparison results show proposed framework achieved significant improvements compared competitive baseline approaches additionally series controlled experiments also conducted illustrate multi-scale strategy contextual self-attention layer play important roles producing effective sentence embedding two kinds scoring strategies highly complementary question answering problems	negative
a new ensemble learning framework for 3d biomedical image segmentation	3d image segmentation plays important role biomedical image analysis many 2d 3d deep learning models achieved state-of-the-art segmentation performance 3d biomedical image datasets yet 2d 3d models strengths weaknesses unifying together one may able achieve accurate results paper propose new ensemble learning framework 3d biomedical image segmentation combines merits 2d 3d models first develop fully convolutional network based meta-learner learn improve results 2d 3d models base-learners minimize over-fitting sophisticated meta-learner devise new training method uses results baselearners multiple versions “ ground truths ” furthermore since new meta-learner training scheme depend manual annotation utilize abundant unlabeled 3d image data improve model extensive experiments two public datasets hvsmr 2016 challenge dataset mouse piriform cortex dataset show approach effective fully-supervised semisupervised transductive settings attains superior performance state-of-the-art image segmentation methods	positive
reasoning over streaming data in metric temporal datalog	study stream reasoning datalogmtl—an extension datalog metric temporal operators propose sound complete stream reasoning algorithm applicable fragment datalogmtlfp datalogmtl propagation derived information towards past time points precluded memory consumption algorithm depends properties rule set input data stream particular depends distances timestamps occurring data undesirable since distances small case algorithm may require large amounts memory address issue propose second algorithm size required memory becomes independent timestamps data expense disallowing punctual intervals rule set finally provide tight bounds data complexity standard query answering datalogmtlfp without punctual intervals rules yield new pspace lower bound data complexity full datalogmtl	negative
cash-out user detection based on attributed heterogeneous information network with a hierarchical attention mechanism	one major frauds financial services cash-out fraud users pursue cash gains illegal insincere means conventional solutions cash-out user detection perform subtle feature engineering user apply classifier gdbt neural network however users financial services rich interaction relations seldom fully exploited conventional solutions paper real datasets ant credit pay ant financial services group first study cashout user detection problem propose novel hierarchical attention mechanism based cash-out user detection model called hacud specifically model different types objects rich attributes interaction relations scenario credit payment service attributed heterogeneous information network ahin hacud model enhances feature representation objects meta-path based neighbors exploiting different aspects structure information ahin furthermore hierarchical attention mechanism elaborately designed model user ’ preferences towards attributes meta-paths experimental results two real datasets show hacud outperforms state-of-the-art methods	negative
has-qa: hierarchical answer spans model for open-domain question answering	paper concerned open-domain question answering i.e. openqa recently works viewed problem reading comprehension rc task directly applied successful rc models however performances models good rc task opinion perspective rc ignores three characteristics openqa task 1 many paragraphs without answer span included data collection 2 multiple answer spans may exist within one given paragraph 3 end position answer span dependent start position paper first propose new probabilistic formulation openqa based three-level hierarchical structure i.e. question level paragraph level answer span level hierarchical answer spans model hasqa designed capture probability has-qa ability tackle three problems experiments public openqa datasets show significantly outperforms traditional rc baselines recent openqa baselines	positive
generating chinese ci with designated metrical structure	ci lyric poetry form follows highly restrictive metrical structures makes challenging computer compose ci subject specified metrical requirement work adapt cvae framework automated ci generation metrical constraints specifically present first neural model explicitly encodes designated metrical structure ci generation proposed model shown experimentally generate ci nearly perfect metrical structures	positive
deep transformation method for discriminant analysis of multi-channel resting state fmri	analysis resting state functional magnetic resonance imaging rs-fmri data challenging problem due high homogeneity large intra-class variability limited samples difference acquisition technologies/techniques issues predominant case attention deficit hyperactivity disorder adhd paper propose new deep transformation method dtm extracts discriminant latent feature space rsfmri projects subsequent layer classification rs-fmri data hidden transformation layer dtm projects original rs-fmri data new space using learning policy extracts spatio-temporal correlations functional activities latent feature space subsequent convolution decision layers transform latent feature space high-level features provide accurate classification performance dtm evaluated using adhd200 rs-fmri benchmark data crossvalidation results show proposed dtm achieves mean classification accuracy 70.36 improvement 8.25 state art methodologies observed improvement due concurrent analysis spatio-temporal correlations different regions brain easily extended study cognitive disorders using rs-fmri brain network analysis studied identify difference functional activities corresponding regions behind cognitive symptoms adhd	negative
zero shot learning for code education: rubric sampling with deep learning inference	modern computer science education massive open online courses moocs log thousands hours data students solve coding challenges rich data platforms garnered interest machine learning community many new algorithms attempting autonomously provide feedback help future students learn first hundred thousand students educational contexts i.e classrooms assignments enough historical data supervised learning paper introduce human-in-the-loop “ rubric sampling ” approach tackle “ zero shot ” feedback challenge able provide autonomous feedback first students working introductory programming assignment accuracy substantially outperforms data-hungry algorithms approaches human level fidelity rubric sampling requires minimal teacher effort associate feedback specific parts student ’ solution articulate student ’ misconceptions language instructor deep learning inference enables rubric sampling improve assignment specific student data acquired demonstrate results novel dataset code.org world ’ largest programming education platform	negative
a bottom-up clustering approach to unsupervised person re-identification	person re-identification re-id approaches based supervised learning requires intensive manual annotation training data however resourceintensive acquire identity annotation also impractical label large-scale real-world data relieve problem propose bottom-up clustering buc approach jointly optimize convolutional neural network cnn relationship among individual samples algorithm considers two fundamental facts re-id task i.e. diversity across different identities similarity within identity specifically algorithm starts regarding individual sample different identity maximizes diversity identity gradually groups similar samples one identity increases similarity within identity utilizes diversity regularization term bottom-up clustering procedure balance data volume cluster finally model achieves effective trade-off diversity similarity conduct extensive experiments large-scale image video re-id datasets including market-1501 dukemtmcreid mars dukemtmc-videoreid experimental results demonstrate algorithm superior state-of-the-art unsupervised re-id approaches also performs favorably competing transfer learning semi-supervised learning methods	positive
deepeta: a spatial-temporal sequential neural network model for estimating time of arrival in package delivery system	100 million packages delivered every day china due fast development e-commerce precisely estimating time packages ’ arrival eta significantly important improving customers ’ experience raising efficiency package dispatching existing methods mainly focus predicting time origin destination however package delivery problem one trip contains multiple destinations delivery time destinations predicted time furthermore eta affected many factors especially sequence latest route regularity delivery pattern sequence packages delivered difficult learn traditional models paper proposed novel spatial-temporal sequential neural network model deepeta take fully advantages factors deepeta end-to-end network mainly consists three parts first spatial encoding recurrent cells proposed capture spatial-temporal sequential features latest delivery route two attention-based layers designed indicate possible eta historical frequent relative delivery routes based similarity latest route future destinations finally fully connected layer utilized jointly learn delivery time experiments real logistics dataset demonstrate proposed approach outperforming results	negative
label embedding with partial heterogeneous contexts	label embedding plays important role many real-world applications enhance label relatedness captured embeddings multiple contexts adopted however contexts heterogeneous often partially observed practical tasks imposing significant challenges capture overall relatedness among labels paper propose general partial heterogeneous context label embedding phcle framework address challenges categorizing heterogeneous contexts two groups relational context descriptive context design tailor-made matrix factorization formula effectively exploit label relatedness context shared embedding principle across heterogeneous contexts label relatedness selectively aligned shared space due elegant formulation phcle overcomes partial context problem nicely incorporate contexts tackled existing multi-context label embedding methods effective alternative optimization algorithm derived solve sparse matrix factorization problem experimental results demonstrate label embeddings obtained phcle achieve superb performance image classification task exhibit good interpretability downstream label similarity analysis image understanding task	negative
an imperfect algorithm for coalition structure generation	optimal coalition structure generation csg significant research problem remains difficult solve given n agents odp-ip algorithm michalak et al 2016 achieves current lowest worst-case time complexity 3n devise imperfect dynamic programming imdp algorithm csg runtime n2n imperfect algorithm means contrived inputs algorithm fails give optimal result experimental results confirmed imdp algorithm performance better several data distribution improves dramatically odp-ip example given 27 agents imdp agentbased uniform distribution time gain 91 i.e 49 minutes	negative
geometric hawkes processes with graph convolutional recurrent neural networks	hawkes processes popular modeling correlated temporal sequences exhibit mutual-excitation properties existing approaches feature-enriched processes variations multivariate hawkes processes either fail describe exact mutual influence sequences become computational inhibitive real-world applications involving large dimensions incorporating additional geometric structure form graphs hawkes processes effective efficient way improving model prediction accuracy paper propose geometric hawkes process ghp model better correlate individual processes integrating hawkes processes graph convolutional recurrent neural network deep network structure computational efficient since requires constant parameters independent graph size experiment results real-world data show framework outperforms recent state-of-art methods	negative
complex unitary recurrent neural networks using scaled cayley transform	recurrent neural networks rnns successfully used wide range sequential data problems well known difficulty using rnns vanishing exploding gradient problem recently several different rnn architectures try mitigate issue maintaining orthogonal unitary recurrent weight matrix one architecture scaled cayley orthogonal recurrent neural network scornn parameterizes orthogonal recurrent weight matrix scaled cayley transform parametrization contains diagonal scaling matrix consisting positive negative one entries optimized gradient descent thus scaling matrix fixed training hyperparameter introduced tune matrix particular task paper develop unitary rnn architecture based complex scaled cayley transform unlike real orthogonal case transformation uses diagonal scaling matrix consisting entries complex unit circle optimized using gradient descent longer requires tuning hyperparameter also provide analysis potential issue modrelu activiation function used work several unitary rnns experiments conducted scaled cayley unitary recurrent neural network scurnn achieves comparable better results scornn unitary rnns without fixing scaling matrix	negative
large scale personalized categorization of financial transactions	major part financial accounting involves tracking organizing business transactions month hence automation task significant value users accounting software paper present large-scale recommendation system successfully recommends company specific categories several million small businesses us uk australia canada india france handles billions financial transactions year system uses machine learning combine fragments information millions users manner allows us accurately recommend user-specific chart accounts categories accounts handled even named using abbreviations foreign language transactions handled even given user never categorized transaction like development system testing scale billions transactions first financial industry	negative
towards to reasonable decision basis in automatic bone x-ray image classification: a weakly-supervised approach	weakly-supervised framework proposed make class inference also provides reasonable decision basis bone x-ray images implement three stages progressively 1 design classification network use positive class activation map pcam attention location 2 generate masks attention maps lead model make classification prediction activation areas 3 label lesions images guide model learn simultaneously test proposed method bone x-ray dataset results show achieves significant improvements lesion location	negative
differential networks for visual question answering	task visual question answering vqa emerged recent years potential applications address vqa task model fuse feature elements images questions efficiently existing models fuse image feature element vi question feature element qi directly element product viqi solutions largely ignore following two key points 1 whether vi qi space 2 reduce observation noises vi qi argue two differences two feature elements like vi − vj qi −qj probably space difference operation would beneficial reduce observation noise achieve first propose differential networks dn novel plug-and-play module enables differences pair-wise feature elements tool dn propose dn based fusion df novel model vqa task achieve state-of-the-art results four publicly available datasets ablation studies also show effectiveness difference operations df model	negative
weakly-supervised simultaneous evidence identification and segmentation for automated glaucoma diagnosis	evidence identification optic disc segmentation automated glaucoma diagnosis clinically significant tasks clinicians assess fundus images however delivering three tasks simultaneously extremely challenging due high variability fundus structure lack datasets complete annotations paper propose innovative weakly-supervised multi-task learning method wsmtl accurate evidence identification optic disc segmentation automated glaucoma diagnosis wsmtl method uses weak-label data binary diagnostic labels normal/glaucoma training obtains pixel-level segmentation mask diagnosis testing wsmtl constituted skip densely connected cnn capture multi-scale discriminative representation fundus structure well-designed pyramid integration structure generate high-resolution evidence map evidence identification pixels higher value represent higher confidence highlight abnormalities constrained clustering branch optic disc segmentation fully-connected discriminator automated glaucoma diagnosis experimental results show proposed wsmtl effectively simultaneously delivers evidence identification optic disc segmentation 89.6 tp dice accurate glaucoma diagnosis 92.4 auc endows wsmtl great potential effective clinical assessment glaucoma	negative
trust region evolution strategies	evolution strategies es class black-box optimization algorithms recently demonstrated viable alternative popular mdp-based rl techniques qlearning policy gradients es achieves fairly good performance challenging reinforcement learning problems easier scale distributed setting however standard es algorithms perform one gradient update per data sample efficient paper purpose efficient using sampled data propose novel iterative procedure optimizes surrogate objective function enabling reuse data sample multiple epochs updates prove monotonic improvement guarantee procedure making several approximations theoretically-justified procedure develop practical algorithm called trust region evolution strategies tres experiments demonstrate effectiveness tres range popular mujoco locomotion tasks openai gym achieving better performance es algorithm	negative
globaltrait: personality alignment of multilingual word embeddings	propose multilingual model recognize big five personality traits text data four different languages english spanish dutch italian analysis shows words similar semantic meaning different languages necessarily correspond personality traits therefore propose personality alignment method globaltrait mapping trait source language target language english words correlate positively trait close together multilingual vector space using aligned embeddings training transfer personality related training features high-resource languages english low-resource languages get better multilingual results compared using simple monolingual unaligned multilingual embeddings achieve average f-score increase across three languages except english 65 73.4 +8.4 comparing monolingual model multilingual using cnn personality aligned embeddings also show relatively good performance regression tasks better classification results evaluating model separate chinese dataset	positive
human motion prediction via learning local structure representations and temporal dependencies	human motion prediction motion capture data classical problem computer vision conventional methods take holistic human body input methods ignore fact various human activities different body components limbs torso distinctive characteristics terms moving pattern paper argue local representations different body components learned separately based idea propose network skeleton network skelnet long-term human motion prediction specifically time-step local structure representations input human body obtained via skelnet ’ branches component-specific layers shared layer uses local spatial representations predict future human pose skelnet first use local structure representations predicting human motion short-term human motion prediction propose second network named skeleton temporal network skel-tnet skel-tnet consists three components skelnet recurrent neural network advantages learning spatial temporal dependencies predicting human motion respectively feed-forward network outputs final estimation methods achieve promising results human3.6m dataset cmu motion capture dataset code publicly available 1	negative
bézier simplex fitting: describing pareto fronts of simplicial problems with small samples in multi-objective optimization	multi-objective optimization problems require simultaneously optimizing two objective functions many studies reported solution set m-objective optimization problem often forms − 1 -dimensional topological simplex curved line 2 curved triangle 3 curved tetrahedron 4 etc. since dimensionality solution set increases number objectives grows exponentially large sample size needed cover solution set reduce required sample size paper proposes bézier simplex model fitting algorithm techniques exploit simplex structure solution set decompose high-dimensional surface fitting task sequence low-dimensional ones approximation theorem bézier simplices proven numerical experiments synthetic real-world optimization problems demonstrate proposed method achieves accurate approximation high-dimensional solution sets small samples practice approximation conducted postoptimization process enable better trade-off analysis	negative
counting complexity for reasoning in abstract argumentation	paper consider counting projected model counting extensions abstract argumentation various semantics asking projected counts interested counting number extensions given argumentation framework multiple extensions identical restricted projected arguments count one projected extension establish classical complexity results parameterized complexity results problems parameterized treewidth undirected argumentation graph obtain upper bounds counting projected extensions introduce novel algorithms exploit small treewidth undirected argumentation graph input instance dynamic programming dp algorithms run time double triple exponential treewidth depending considered semantics finally take exponential time hypothesis eth account establish lower bounds bounded treewidth algorithms counting extensions projected extension	positive
what if we simply swap the two text fragments? a straightforward yet effective way to test the robustness of methods to confounding signals in nature language inference tasks	nature language inference nli task predictive task determining inference relationship pair natural language sentences increasing popularity nli many state-of-the-art predictive models proposed impressive performances however several works noticed statistical irregularities collected nli data set may result over-estimated performance models proposed remedies paper investigate statistical irregularities refer confounding factors nli data sets belief nli labels preserve swapping operations propose simple yet effective way swapping two text fragments evaluating nli predictive models naturally mitigate observed problems continue train predictive models swapping manner propose use deviation model ’ evaluation performances different percentages training text fragments swapped describe robustness predictive model evaluation metrics leads interesting understandings recent published nli methods finally also apply swapping operation nli models see effectiveness straightforward method mitigating confounding factor problems training generic sentence embeddings nlp transfer tasks	negative
novelty detection for multispectral images with application to planetary exploration	work present system based convolutional autoencoders detecting novel features multispectral images introduce sammie selections based autoencoder modeling multispectral image expectations previous work using autoencoders employed scalar reconstruction error classify new images novel typical show spatial-spectral error map enable accurate classification novelty multispectral images well human-comprehensible explanations detection apply methodology detection novel geologic features multispectral images martian surface collected mastcam imaging system mars science laboratory curiosity rover	negative
nevae: a deep generative model for molecular graphs	deep generative models praised ability learn smooth latent representation images text audio used generate new plausible data however current generative models unable work molecular graphs due unique characteristics—their underlying structure euclidean grid-like remain isomorphic permutation nodes labels come different number nodes edges paper propose nevae novel variational autoencoder molecular graphs whose encoder decoder specially designed account properties means several technical innovations addition using masking decoder able guarantee set valid properties generated molecules experiments reveal model discover plausible diverse novel molecules effectively several state art methods moreover utilizing bayesian optimization continuous latent representation molecules model finds also find molecules maximize certain desirable properties effectively alternatives	negative
learning a key-value memory co-attention matching network for person re-identification	person re-identification re-id typically cast problem semantic representation alignment requires precisely discovering modeling inherent spatial structure information person images motivated observation propose key-value memory matching network kvm-mn model consists key-value memory representation key-value co-attention matching proposed kvm-mn model capable building effective local-position-aware person representation encodes spatial feature information form multi-head key-value memory furthermore proposed kvm-mn model makes use multi-head co-attention automatically learn number cross-person-matching patterns resulting robust interpretable matching results finally build setwise learning mechanism implements generalized query-to-gallery-image-set learning procedure experimental results demonstrate effectiveness proposed model state-of-the-art	positive
what is one grain of sand in the desert? analyzing individual neurons in deep nlp models	despite remarkable evolution deep neural networks natural language processing nlp interpretability remains challenge previous work largely focused models learn representation level break analysis study individual dimensions neurons vector representation learned end-to-end neural models nlp tasks propose two methods linguistic correlation analysis based supervised method extract relevant neurons respect extrinsic task cross-model correlation analysis unsupervised method extract salient neurons w.r.t model evaluate effectiveness techniques ablating identified neurons reevaluating network ’ performance two tasks neural machine translation nmt neural language modeling nlm present comprehensive analysis neurons aim address following questions localized distributed different linguistic properties models ii certain neurons exclusive properties others iii information less distributed nmt vs. nlm iv important neurons identified linguistic correlation method overall task code publicly available part neurox toolkit dalvi et al 2019a paper non-archived version paper published aaai dalvi et al 2019b	negative
goal-oriented dialogue policy learning from failures	reinforcement learning methods used learning dialogue policies however learning effective dialogue policy frequently requires prohibitively many conversations partly sparse rewards dialogues successful dialogues early learning phase hindsight experience replay enables learning failures vanilla inapplicable dialogue learning due implicit goals work develop two complex methods providing different tradeoffs complexity performance first time enabled her-based dialogue policy learning experiments using realistic user simulator show methods perform better existing experience replay methods applied deep q-networks learning rate	negative
network structure and transfer behaviors embedding via deep prediction model	network-structured data becoming increasingly popular many applications however data present great challenges feature engineering due high non-linearity sparsity issue transfer link-connected nodes huge network feature representations critical basic properties real-world networks local global structure reflected dynamical transfer behaviors node node work propose deep embedding framework preserve transfer possibilities among network nodes first suggest degree-weight biased random walk model capture transfer behaviors network deep embedding framework introduced preserve transfer possibilities among nodes network structure embedding layer added conventional long short-term memory network utilize sequence prediction ability keep local network neighborhood perform laplacian supervised space optimization embedding feature representations experimental studies conducted various real-world datasets including social networks citation networks results show learned representations effectively used features variety tasks clustering visualization classification achieve promising performance compared state-of-the-art models	negative
amalgamating knowledge towards comprehensive classification	rapid development deep learning unprecedentedly large number trained deep network models available online reusing trained models significantly reduce cost training new models scratch infeasible annotations used training original networks often unavailable public propose paper study new model-reusing task term knowledge amalgamation given multiple trained teacher networks specializes different classification problem goal knowledge amalgamation learn lightweight student model capable handling comprehensive classification assume annotations except outputs teacher models available thus focus extracting amalgamating knowledge multiple teachers end propose pilot two-step strategy tackle knowledge amalgamation task learning first compact feature representations teachers network parameters layer-wise manner build student model apply approach four public datasets obtain encouraging results even without human annotation obtained student model competent handle comprehensive classification task cases outperforms teachers individual sub-tasks	negative
residual invertible spatio-temporal network for video super-resolution	video super-resolution challenging task attracted great attention research industry communities paper propose novel end-to-end architecture called residual invertible spatio-temporal network ristn video super-resolution ristn sufficiently exploit spatial information low-resolution high-resolution effectively models temporal consistency consecutive video frames compared existing recurrent convolutional network based approaches ristn much deeper efficient consists three major components spatial component lightweight residual invertible block designed reduce information loss feature transformation provide robust feature representations temporal component novel recurrent convolutional model residual dense connections proposed construct deeper network avoid feature degradation reconstruction component new fusion method based sparse strategy proposed integrate spatial temporal features experiments public benchmark datasets demonstrate ristn outperforms state-ofthe-art methods	negative
image saliency prediction in transformed domain: a deep complex neural network method	transformed domain fearures images show effectiveness distinguishing salient non-salient regions paper propose novel deep complex neural network named saldcnn predict image saliency learning features pixel transformed domains proposing sal-dcnn analyze saliency cues encoded discrete fourier transform dft domain consequently following findings 1 phase spectrum encodes saliency cues 2 certain pattern amplitude spectrum important saliency prediction 3 transformed domain spectrum robust noise down-sampling saliency prediction according findings develop structure saldcnn including two main stages complex dense encoder three-stream multi-domain decoder given new saldcnn structure saliency maps predicted supervision ground-truth fixation maps pixel transformed domains finally experimental results show sal-dcnn method outperforms 8 state-of-theart methods image saliency prediction 3 databases	negative
incorporating structured commonsense knowledge in story completion	ability select appropriate story ending first step towards perfect narrative comprehension story ending prediction requires explicit clues within context also implicit knowledge commonsense construct reasonable consistent story however previous approaches explicitly use background commonsense knowledge present neural story ending selection model integrates three types information narrative sequence sentiment evolution commonsense knowledge experiments show model outperforms state-ofthe-art approaches public dataset rocstory cloze task mostafazadeh et al 2017 performance gain adding additional commonsense knowledge significant	negative
recursively learning causal structures using regression-based conditional independence test	paper addresses two important issues causality inference one reduce redundant conditional independence ci tests heavily impact efficiency accuracy existing constraint-based methods another construct true causal graph set markov equivalence classes returned methods	positive
a bridge between liquid and social welfare in combinatorial auctions with submodular bidders	study incentive compatible mechanisms combinatorial auctions bidders submodular xos valuations budget-constrained objective maximize liquid welfare notion efficiency budgetconstrained bidders introduced dobzinski paes leme 2014 show known truthful mechanisms best-approximate social welfare combinatorial auctions submodular bidders demand query oracles adapted retain truthfulness achieve asymptotically approximation guarantees liquid welfare specifically problem optimizing liquid welfare combinatorial auctions submodular bidders obtain universally truthful randomized log -approximate mechanism number items adapting mechanism krysta vöcking 2012	negative
interpreting deep models for text analysis via optimization and regularization methods	interpreting deep neural networks great importance understand verify deep models natural language processing nlp tasks however existing approaches focus improving performance models ignore interpretability work propose approach investigate meaning hidden neurons convolutional neural network cnn models first employ saliency map optimization techniques approximate detected information hidden neurons input sentences develop regularization terms explore words vocabulary interpret detected information experimental results demonstrate approach identify meaningful reasonable interpretations hidden spatial locations additionally show approach describe decision procedure deep nlp models	negative
snr: sub-network routing for flexible parameter sharing in multi-task learning	machine learning applications object detection content recommendation often require training single model predict multiple targets time multi-task learning neural networks became popular recently helps improve accuracy many prediction tasks related also saves computation cost sharing model architectures low-level representations latter critical real-time large-scale machine learning systems	negative
distributionally robust semi-supervised learning for people-centric sensing	semi-supervised learning crucial alleviating labelling burdens people-centric sensing however humangenerated data inherently suffer distribution shift semi-supervised learning due diverse biological conditions behavior patterns humans address problem propose generic distributionally robust model semi-supervised learning distributionally shifted data considering discrepancy consistency labeled data unlabeled data learn latent features reduce person-specific discrepancy preserve task-specific consistency evaluate model variety people-centric recognition tasks real-world datasets including intention recognition activity recognition muscular movement recognition gesture recognition experiment results demonstrate proposed model outperforms state-of-the-art methods	negative
identifying bottlenecks in practical sat-based model finding for first-order logic ontologies with datasets	satisfiability first-order logic fol ontologies typically verified translation propositional satisfiability sat problems tackled sat solver unfortunately sat solvers often experience scalability issues reasoning fol ontologies even moderately sized datasets sat solvers found capably handle complex axiomatizations finding models datasets gets considerably complex time-intensive number clause exponentially increases increase individuals axiomatic complexity identify fol definitions specific bottleneck demonstrate via experiments presence many defined terms highest arity significantly slows model finding also show removing optional definitions substituting terms definiens leads reduction number clauses makes sat-based model finding practical 100 individuals fol theory	negative
multi-agent path finding for large agents	multi-agent path finding mapf widely studied ai community example conflict-based search cbs state-of-the-art mapf algorithm based twolevel tree-search however previous mapf algorithms assume agent occupies single location given time e.g. single cell grid limits applicability many real-world domains geometric agents lieu point agents geometric agents referred “ large ” agents occupy multiple points time paper formalize study lamapf i.e. mapf large agents first show cbs adapted solve la-mapf present generalized version cbs called multi-constraint cbs mccbs adds multiple constraints instead one constraint agent generates high-level search node introduce three different approaches choose constraints well approach compute admissible heuristics high-level search experimental results show mc-cbs variants outperform cbs three orders magnitude terms runtime best variant also outperforms epea* state-of-the-art a*-based mapf solver cases mdd-sat state-of-the-art reduction-based mapf solver cases	negative
incorporating semantic similarity with geographic correlation for query-poi relevance learning	point-of-interest poi retrieval searches relevant destination locations plays significant role on-demand ridehailing services existing solutions poi retrieval mainly retrieve rank pois based semantic similarity scores although intuitive quantifying relevance query-poi pair single-field semantic similarity subject inherent limitations paper propose novel query-poi relevance model effective poi retrieval ondemand ride-hailing services different existing relevance models capture represent multi-field local global semantic features query-poi pair measure semantic similarity besides observe hidden correlation origin-destination locations ride-hailing scenarios propose two location embeddings characterize specific correlation incorporating geographic correlation semantic similarity model achieves better performance poi ranking experimental results two real-world click-through datasets demonstrate improvements model state-of-the-art methods	positive
bayesian functional optimisation with shape prior	real world experiments expensive thus important reach target minimum number experiments experimental processes often involve control variables change time problems formulated functional optimisation problem develop novel bayesian optimisation framework functional optimisation expensive black-box processes represent control function using bernstein polynomial basis optimise coefficient space derive theory practice required dynamically adjust order polynomial degree show prior information shape integrated demonstrate effectiveness approach short polymer fibre design optimising learning rate schedules deep networks	negative
guiding the one-to-one mapping in cyclegan via optimal transport	cyclegan capable learning one-to-one mapping two data distributions without paired examples achieving task unsupervised data translation however theoretical guarantee property learned one-to-one mapping cyclegan paper experimentally find circumstances one-to-one mapping learned cyclegan random one within large feasible solution space based observation explore add extra constraints one-to-one mapping controllable satisfies properties related specific tasks propose solve optimal transport mapping restrained task-specific cost function reflects desired properties use barycenters optimal transport mapping serve references cyclegan experiments indicate proposed algorithm capable learning one-to-one mapping desired properties	positive
outlier aware network embedding for attributed networks	attributed network embedding received much interest research community networks come content node also known node attributes existing attributed network approaches work well network consistent structure attributes nodes behave expected real world networks often anomalous nodes typically outliers relatively unexplainable affect embeddings nodes network thus downstream network mining tasks fail miserably presence outliers hence integrated approach detect anomalies reduce overall effect network embedding required	negative
cseye: a proposed solution for accurate and accessible one-to-many face verification	facial verification core problem studied researchers computer vision recently published one-to-one comparison models successfully achieved accuracy results surpass abilities humans natural extension one-to-one facial verification problem one-to-many classification abstract present exploration different methods performing one-to-many facial verification using low-resolution images cseye model introduces direct comparison features extracted candidate images suspect performing classification task initial experiments using 10-to-1 comparisons faces labelled faces wild dataset yield promising results	negative
fair and efficient memory sharing: confronting free riders	cache memory unit needs shared among n strategic agents agent different preferences files brought memory goal design mechanism elicits preferences truthful manner outputs fair efficient memory allocation trivially truthful fair solution would isolate agent 1/n fraction memory however could inefficient agents similar preferences thus room cooperation hand agents isolated unless mechanism carefully designed incentives misreport preferences free ride files others bring memory paper explore power limitations truthful mechanisms setting demonstrate mechanisms blocking agents accessing parts memory achieve improved efficiency guarantees despite inherent inefficiencies blocking	positive
computing the yolk in spatial voting games without computing median lines	yolk important concept spatial voting games yolk center generalises equilibrium yolk radius bounds uncovered set present near-linear time algorithms computing yolk plane best knowledge algorithm first precompute median lines hence able break best known upper bound n4/3 number limiting median lines avoid requirement carefully applying megiddo ’ parametric search technique powerful framework could lead faster algorithms spatial voting problems	negative
global remote operation of intelligent space robot assistants	intelligent robotic coworkers considered valuable addition many application areas applies terrestrial domains also exploration solar system humankind moves toward ever increasing presence space infrastructure constructed maintained distant planets mars ai-enabled robots play major role scenario space agencies envisage robotic co-workers deployed set-up habitats energy return vessels future human scientists leveraging ai planning methods vision already become one step closer reality meteron supvis justin experiment intelligent robotic coworker rollin ’ justin controlled astronauts aboard international space station iss order maintain martian mock-up solar panel farm located earth demonstrate technology readiness developed methods work system demonstrated aaai 2019 controlling rollin ’ justin located munich germany honolulu hawaii	negative
efficient solving of birds of a feather puzzles	article describe lessons learned creating efficient solver solitaire game birds feather introduce new variant depth-first search call best-n depth-first search achieved 99.56 reduction search time 100,000 puzzle seeds evaluate number potential node-ordering search features pruning tests perform analysis solvability prediction search features consider possible future research directions suggested computationally expensive puzzle seeds encountered testing	negative
perceptual pyramid adversarial networks for text-to-image synthesis	generating photo-realistic images conditioned semantic text descriptions challenging task computer vision field due nature hierarchical representations learned cnn intuitive utilize richer convolutional features improve text-to-image synthesis paper propose perceptual pyramid adversarial network ppan directly synthesize multi-scale images conditioned texts adversarial way specifically design one pyramid generator three independent discriminators synthesize regularize multi-scale photo-realistic images one feed-forward process pyramid level method takes coarse-resolution features input synthesizes highresolution images uses convolutions up-sampling finer level furthermore generator adopts perceptual loss enforce semantic similarity synthesized image ground truth multi-purpose discriminator encourages semantic consistency image fidelity class invariance experimental results show ppan sets new records text-to-image synthesis two benchmark datasets cub i.e. 4.38 inception score .290 visual-semantic similarity oxford-102 i.e. 3.52 inception score .297 visual-semantic similarity	positive
dual-view ranking with hardness assessment for zero-shot learning	zero-shot learning zsl build recognition models previously unseen target classes labeled data training transferring knowledge related auxiliary source classes abundant labeled samples target ones class attributes bridge key learn similarity based ranking function samples class labels using labeled source classes proper unseen class label test sample identified function order learn function single-view ranking based loss widely used aims rank true label prior labels training sample however argue ranking performed view aims place images belonging label images classes motivated propose novel dual-view ranking dark loss zeroshot learning simultaneously ranking labels image point-to-point metric ranking images label pointto-set metric capable better modeling relationship images classes addition also notice previous zsl approaches mostly fail well exploit hardness training samples either using hard ones using samples indiscriminately work also introduce sample hardness assessment method zsl assigns different weights training samples based hardness leads accurate robust zsl model experiments benchmarks demonstrate dark outperforms state-of-the-arts generalized zsl	negative
bi-kronecker functional decision diagrams: a novel canonical representation of boolean functions	paper present novel data structure compact representation effective manipulations boolean functions called bi-kronecker functional decision diagrams bkfdds bkfdds integrate classical expansions shannon davio expansions bi-versions thus bkfdds generalizations existing decision diagrams bdds fdds kfdds bbdds interestingly certain conditions sufficient consider expansions classical expansions bi-versions imposing reduction ordering rules bkfdds compact canonical forms boolean functions experimental results demonstrate bkfdds outperform existing decision diagrams terms sizes	positive
exploiting the ground-truth: an adversarial imitation based knowledge distillation approach for event detection	ambiguity language expressions poses great challenge event detection disambiguate event types current approaches rely external nlp toolkits build knowledge representations unfortunately approaches work pipeline paradigm suffer error propagation problem paper propose adversarial imitation based knowledge distillation approach first time tackle challenge acquiring knowledge rawsentences event detection approach teacher module first devised learn knowledge representations ground-truth annotations set student module takes raw-sentences input student module taught imitate behavior teacher guidance adversarial discriminator way process knowledge distillation rawsentence implicitly integrated feature encoding stage student module end enhanced student used event detection processes raw texts requires extra toolkits naturally eliminating error propagation problem faced pipeline approaches conduct extensive experiments ace 2005 datasets experimental results justify effectiveness approach	negative
joint semi-supervised feature selection and classification through bayesian approach	increasing data dimensionality feature selection become fundamental task deal high-dimensional data semi-supervised feature selection focuses problem learn relevant feature subset case abundant unlabeled data labeled data recent years many semi-supervised feature selection algorithms proposed however algorithms implemented separating processes feature selection classifier training simultaneously select features learn classifier selected features moreover ignore difference reliability inside unlabeled samples directly use training stage might cause performance degradation paper propose joint semi-supervised feature selection classification algorithm jsfs adopts bayesian approach automatically select relevant features simultaneously learn classifier instead using unlabeled samples indiscriminately jsfs associates unlabeled sample self-adjusting weight distinguish difference effectively eliminate irrelevant unlabeled samples via introducing left-truncated gaussian prior experiments various datasets demonstrate effectiveness superiority jsfs	negative
bidirectional transition-based dependency parsing	transition-based dependency parsing fast effective approach dependency parsing traditionally transitionbased dependency parser processes input sentence predicts sequence parsing actions left-to-right manner process early prediction error may negatively impact prediction subsequent actions paper propose simple framework bidirectional transitionbased parsing training learn left-to-right parser right-to-left parser separately parse sentence perform joint decoding two parsers propose three joint decoding algorithms based joint scoring dual decomposition dynamic oracle respectively empirical results show methods lead competitive parsing accuracy method based dynamic oracle consistently achieves best performance	negative
skeleton-based gesture recognition using several fully connected layers with path signature features and temporal transformer module	skeleton based gesture recognition gaining popularity due wide possible applications key issues extract discriminative features design classification model paper first leverage robust feature descriptor path signature ps propose three ps features explicitly represent spatial temporal motion characteristics i.e. spatial ps ps temporal ps ps temporal spatial ps ps considering significance fine hand movements gesture propose ” attention hand ” aoh principle define joint pairs ps select single joint ps addition dyadic method employed extract ps ps features encode global local temporal dynamics motion secondly without recurrent strategy classification model still faces challenges temporal variation among different sequences propose new temporal transformer module ttm match sequence key frames learning temporal shifting parameter input learning-based module included standard neural network architecture finally design multi-stream fully connected layer based network treat spatial temporal features separately fused together final result tested method three benchmark gesture datasets i.e. chalearn 2016 chalearn 2013 msrc-12 experimental results demonstrate achieve state-of-the-art performance skeleton-based gesture recognition high computational efficiency	negative
which factorization machine modeling is better: a theoretical answer with optimal guarantee	factorization machine fm popular machine learning model capture second order feature interactions optimal learning guarantee fm generalized version yet developed rank k generalized fm dimensional input previous best known sampling complexity k3d · polylog kd gaussian distribution bound sub-optimal comparing information theoretical lower bound kd work aim tighten bound towards optimal generalize analysis sub-gaussian distribution prove input data satisfies so-called τ-moment invertible property sampling complexity generalized fm improved k2d · polylog kd /τ2 second order self-interaction terms excluded generalized fm bound improved optimal kd · polylog kd logarithmic factors analysis also suggests positive semi-definite constraint conventional fm redundant improve sampling complexity making model difficult optimize evaluate improved fm model real-time high precision gps signal calibration task validate superiority	negative
an improved generic bet-and-run strategy with performance prediction for stochastic local search	commonly used strategy improving optimization algorithms restart algorithm believed trapped inferior part search space building recent success bet-and-run approaches restarted local search solvers introduce generic version makes use performance prediction goal obtain best possible results within given time budget using given black-box optimization algorithm prior knowledge problem features algorithm behavior available question use time budget efficiently arises first start k ≥ 1 independent runs algorithm initialization budget t1 pause runs apply decision maker choose 1 ≤ k runs consuming t2 ≥ 0 time units continue runs remaining t3 t−t1−t2 time units previous bet-and-run strategies decision maker currentbest would simply select run best-so-far results negligible time propose using advanced methods discriminate “ good ” “ bad ” sample runs goal increasing correlation chosen run a-posteriori best one 157 million experiments test different approaches predict run may yield best results granted remaining budget show 1 currentbest method indeed reliable robust baseline approach 2 approach yield better results previous methods	positive
distributed representation of words in cause and effect spaces	paper focuses building distributed representation words cause effect spaces task-specific word embedding technique causality causal embedding model trained large set cause-effect phrase pairs extracted raw text corpus via set high-precision causal patterns three strategies proposed transfer positive negative labels level phrase pairs level word pairs leading three causal embedding models pairwise-matching max-matching attentivematching correspondingly experimental results shown max-matching attentive-matching models significantly outperform several state-of-the-art competitors large margin english chinese corpora	positive
scalable recollections for continual lifelong learning	given recent success deep learning applied variety single tasks natural consider human-realistic settings perhaps difficult settings continual lifelong learning model must learn online continuous stream non-stationary data successful continual lifelong learning system must three key capabilities must learn adapt time must forget learned must efficient training time memory recent techniques focused efforts primarily first two capabilities questions efficiency remain largely unexplored paper consider problem efficient effective storage experiences large time-frames particular consider case typical experiences n bits memories limited k bits k n. present novel scalable architecture training algorithm challenging domain provide extensive evaluation performance results show achieve considerable gains top state-of-the-art methods gem 1	positive
modelling of bi-directional spatio-temporal dependence and users’ dynamic preferences for missing poi check-in identification	human mobility data accumulated point-of-interest poi check-ins provides great opportunity user behavior understanding however data quality issues e.g. geolocation information missing unreal check-ins data sparsity real-life mobility data limit effectiveness existing poioriented studies e.g. poi recommendation location prediction applied real applications end paper develop model named bi-stddp integrate bi-directional spatio-temporal dependence users ’ dynamic preferences identify missing poi check-in user visited specific time specifically first utilize bi-directional global spatial local temporal information pois capture complex dependence relationships target temporal pattern combination user poi information fed multi-layer network capture users ’ dynamic preferences moreover dynamic preferences transformed space dependence relationships form final model finally proposed model evaluated three large-scale real-world datasets results demonstrate significant improvements model compared state-of-the-art methods also worth noting proposed model naturally extended address poi recommendation location prediction tasks competitive performances	negative
structured bayesian networks: from inference to learning with routes	structured bayesian networks sbns recently proposed class probabilistic graphical models integrate background knowledge two forms conditional independence constraints boolean domain constraints paper propose first exact inference algorithm sbns based compiling given sbn probabilistic sentential decision diagram psdd identify tractable subclass sbns psdds polynomial size sbns yield tractable model route distributions whose structure learned gps data using simple algorithm propose empirically demonstrate utility inference algorithm showing order-ofmagnitude efficient traditional approaches exact inference demonstrate utility learning algorithm showing learn accurate models classifiers gps data	negative
parabank: monolingual bitext generation and sentential paraphrasing via lexically-constrained neural machine translation	present parabank large-scale english paraphrase dataset surpasses prior work quantity quality following approach paranmt wieting gimpel 2018 train czech-english neural machine translation nmt system generate novel paraphrases english reference sentences adding lexical constraints nmt decoding procedure however able produce multiple high-quality sentential paraphrases per source sentence yielding english paraphrase resource 4 billion generated tokens exhibiting greater lexical diversity using human judgments also demonstrate parabank ’ paraphrases improve paranmt semantic similarity fluency finally use parabank train monolingual nmt model support lexically-constrained decoding sentence rewriting tasks	positive
parallel restarted sgd with faster convergence and less communication: demystifying why model averaging works for deep learning	distributed training deep neural networks parallel minibatch sgd widely used speed training process using multiple workers uses multiple workers sample local stochastic gradients parallel aggregates gradients single server obtain average updates worker ’ local model using sgd update averaged gradient ideally parallel mini-batch sgd achieve linear speed-up training time respect number workers compared sgd single worker however linear scalability practice significantly limited growing demand gradient communication workers involved model averaging periodically averages individual models trained parallel workers another common practice used distributed training deep neural networks since zinkevich et al 2010 mcdonald hall mann 2010 compared parallel mini-batch sgd communication overhead model averaging significantly reduced impressively tremendous experimental works verified model averaging still achieve good speed-up training time long averaging interval carefully controlled however remains mystery theory simple heuristic works well paper provides thorough rigorous theoretical study model averaging work well parallel mini-batch sgd significantly less communication overhead	negative
generalized planning via abstraction: arbitrary numbers of objects	consider class generalized planning problems based idea quantifying sets similar objects show adapt fully observable nondeterministic planning techniques produce generalized solutions easy instantiate particular problem instances also describe reformulate classical planning problem quantified one reformulation allows us solve original planning task without grounding every action respect objects problem single solution applied possibly infinite set related classical planning tasks report experimental results show approach practical promising technique solving interesting class problems	positive
mono3d++: monocular 3d vehicle detection with two-scale 3d hypotheses and task priors	present method infer 3d pose shape vehicles single image tackle ill-posed problem optimize two-scale projection consistency generated 3d hypotheses 2d pseudo-measurements specifically use morphable wireframe model generate fine-scaled representation vehicle shape pose reduce sensitivity 2d landmarks jointly model 3d bounding box coarse representation improves robustness also integrate three task priors including unsupervised monocular depth ground plane constraint well vehicle shape priors forward projection errors overall energy function	positive
mlvcnn: multi-loop-view convolutional neural network for 3d shape retrieval	3d shape retrieval attracted much attention become hot topic computer vision field recently.with development deep learning 3d shape retrieval also made great progress many view-based methods introduced recent years however represent 3d shapes better still challenging problem time intrinsic hierarchical associations among views still well utilized order tackle problems paper propose multi-loop-view convolutional neural network mlvcnn framework 3d shape retrieval method multiple groups views extracted different loop directions first given multiple loop views proposed mlvcnn framework introduces hierarchical view-loop-shape architecture i.e. view level loop level shape level conduct 3d shape representation different scales view-level convolutional neural network first trained extract view features proposed loop normalization lstm utilized loop view generate loop-level features considering intrinsic associations different views loop finally loop-level descriptors combined shape-level descriptor 3d shape representation used 3d shape retrieval proposed method evaluated public 3d shape benchmark i.e. modelnet40 experiments comparisons state-of-the-art methods show proposed mlvcnn method achieve significant performance improvement 3d shape retrieval tasks mlvcnn outperforms state-of-the-art methods map 4.84 3d shape retrieval task also evaluated performance proposed method 3d shape classification task mlvcnn also achieves superior performance compared recent methods	positive
deep learning for cost-optimal planning: task-dependent planner selection	classical planning known computationally hard single planner expected work well across many planning domains one solution problem use online portfolio planners select planner given task portfolios perform classification task well-known wellresearched task field machine learning classification usually performed using representation planning tasks collection hand-crafted statistical features recent techniques machine learning based automatic extraction features employed yet due lack suitable representations planning tasks	positive
number sequence prediction problems for evaluating computational powers of neural networks	inspired number series tests measure human intelligence suggest number sequence prediction tasks assess neural network models ’ computational powers solving algorithmic problems define complexity difficulty number sequence prediction task structure smallest automaton generate sequence suggest two types number sequence prediction problems number-level digit-level problems number-level problems format sequences 2-dimensional grids digits digit-level problems provide single digit input per time step complexity number-level sequence prediction defined depth equivalent combinatorial logic complexity digit-level sequence prediction defined equivalent state automaton generation rule experiments number-level sequences suggest cnn models capable learning compound operations sequence generation rules depths compound operations limited digitlevel problems simple gru lstm models solve problems complexity finite state automata memory augmented models stack-rnn attention neural turing machines solve reverse-order task complexity simple pushdown automaton however solve general fibonacci arithmetic geometric sequence generation problems represent complexity queue automata turing machines results show number sequence prediction problems effectively evaluate machine learning models ’ computational capabilities	negative
hierarchically structured reinforcement learning for topically coherent visual story generation	propose hierarchically structured reinforcement learning approach address challenges planning generating coherent multi-sentence stories visual storytelling task within framework task generating story given sequence images divided across two-level hierarchical decoder high-level decoder constructs plan generating semantic concept i.e. topic image sequence low-level decoder generates sentence image using semantic compositional network effectively grounds sentence generation conditioned topic two decoders jointly trained end-to-end using reinforcement learning evaluate model visual storytelling vist dataset empirical results automatic human evaluations demonstrate proposed hierarchically structured reinforced training achieves significantly better performance compared strong flat deep reinforcement learning baseline	positive
evolutionarily learning multi-aspect interactions and influences from network structure and node content	formation complex network highly driven multi-aspect node influences interactions reflected network structures content embodied network nodes limited work jointly modeled aspects typically focuses topological structures overlooks heterogeneous interactions behind node linkage contributions node content interactive heterogeneities propose multi-aspect interaction influence-unified evolutionary coupled system mai-ecs network representation involving node content linkage-based network structure mai-ecs jointly iteratively learns two systems multi-aspect interaction learning system capture heterogeneous hidden interactions nodes influence propagation system capture multiaspect node influences propagation nodes mai-ecs couples unifies optimizes two systems toward effective representation explicit node content network structure implicit node interactions influences mai-ecs shows superior performance node classification link prediction comparison stateof-the-art methods two real-world datasets demonstrate semantic interpretability results generated mai-ecs	negative
activitynet-qa: a dataset for understanding complex web videos via question answering	recent developments modeling language vision successfully applied image question answering crucial natural extend research direction video domain video question answering videoqa compared image domain large scale fully annotated benchmark datasets exists videoqa datasets limited small scale automatically generated etc limitations restrict applicability practice introduce activitynet-qa fully annotated large scale videoqa dataset dataset consists 58,000 qa pairs 5,800 complex web videos derived popular activitynet dataset present statistical analysis activitynet-qa dataset conduct extensive experiments comparing existing videoqa baselines moreover explore various video representation strategies improve videoqa performance especially long videos	negative
mvpnet: multi-view point regression networks for 3d object reconstruction from a single image	paper address problem reconstructing object ’ surface single image using generative networks first represent 3d surface aggregation dense point clouds multiple views point cloud embedded regular 2d grid aligned image plane viewpoint making point cloud convolution-favored ordered fit deep network architectures point clouds easily triangulated exploiting connectivities 2d grids form mesh-based surfaces second propose encoder-decoder network generates kind multiple view-dependent point clouds single image regressing 3d coordinates visibilities also introduce novel geometric loss able interpret discrepancy 3d surfaces opposed 2d projective planes resorting surface discretization constructed meshes demonstrate multi-view point regression network outperforms state-of-the-art methods significant improvement challenging datasets	positive
towards personalized review summarization via user-aware sequence network	address personalized review summarization generates condensed summary user ’ review accounting preference different aspects writing style propose novel personalized review summarization model named user-aware sequence network usn consider aforementioned users ’ characteristics generating summaries contains user-aware encoder useraware decoder specifically user-aware encoder adopts user-based selective mechanism select important information review user-aware decoder incorporates user characteristic user-specific word-using habits word prediction process generate personalized summaries validate model collected new dataset trip comprising 536,255 reviews 19,400 users quantitative human evaluation show usn achieves state-ofthe-art performance personalized review summarization	negative
attacking data transforming learners at training time	machine learning systems known vulnerable data-manipulation attacks training deployment time little known adapt attacks defender transforms data prior model estimation consider setting defender bob first transforms data learns model result alice attacker perturbs bob ’ input data prior transforming develop general-purpose “ plug play ” framework gradient-based attacks based matrix differentials focusing ordinary least-squares linear regression allows learning algorithms data transformations paired composed arbitrarily attacks adapted use chain rule—analogous backpropagation neural network parameters—to compositional learning maps bestresponse attacks computed matrix multiplications library attack matrices transformations learners treatment linear regression extends state-ofthe-art attacks training time permitting attacker affect features targets optimally simultaneously explore several transformations broadly used across machine learning driving motivation work autogressive modeling bob transforms univariate time series matrix observations vector target values fed standard learners learning reduction perturbation alice single value time series affects features several data points along target values	negative
online pandora’s boxes and bandits	consider online variations pandora ’ box problem weitzman 1979 standard model understanding issues related cost acquiring information decision-making problem generalizes classic pandora ’ box problem prophet inequality framework boxes presented online random value cost drawn jointly known distribution pandora chooses online whether open box given cost chooses irrevocably whether keep revealed prize pass aim approximation algorithms adversaries choose largest prize opened box use optimal offline policies decide boxes open without knowledge value inside 1. consider variations pandora collect multiple prizes subject feasibility constraints cardinality matroid knapsack constraints also consider variations related classic multi-armed bandit problems reinforcement learning results use reduction-based framework separate issues cost acquiring information online decision process prizes keep work shows many scenarios pandora achieve good approximation best possible performance	positive
gaussian-induced convolution for graphs	learning representation graph plays crucial role numerous tasks pattern recognition different gridshaped images/videos local convolution kernels lattices however graphs fully coordinate-free vertices edges work propose gaussianinduced convolution gic framework conduct local convolution filtering irregular graphs specifically edgeinduced gaussian mixture model designed encode variations subgraph region integrating edge information weighted gaussian models implicitly characterizes one component subgraph variations order coarsen graph derive vertex-induced gaussian mixture model cluster vertices dynamically according connection edges approximately equivalent weighted graph cut conduct multi-layer graph convolution network several public datasets graph classification extensive experiments demonstrate gic effective achieve state-of-the-art results	negative
cross-relation cross-bag attention for distantly-supervised relation extraction	distant supervision leverages knowledge bases automatically label instances thus allowing us train relation extractor without human annotations however generated training data typically contain massive noise may result poor performances vanilla supervised learning paper propose conduct multi-instance learning novel cross-relation cross-bag selective attention c2sa leads noise-robust training distant supervised relation extractor specifically employ sentence-level selective attention reduce effect noisy mismatched sentences correlation among relations captured improve quality attention weights moreover instead treating entity-pairs equally try pay attention entity-pairs higher quality similarly adopt selective attention mechanism achieve goal experiments two types relation extractor demonstrate superiority proposed approach state-of-the-art ablation studies verify intuitions demonstrate effectiveness proposed two techniques	negative
model-free irl using maximum likelihood estimation	problem learning expert ’ unknown reward function using limited number demonstrations recorded expert ’ behavior investigated area inverse reinforcement learning irl gain traction challenging underconstrained problem irl methods predominantly represent reward function expert linear combination known features existing irl algorithms either assume availability transition function provide complex inefficient approach learn paper present model-free approach irl casts irl maximum likelihood framework present modifications model-free q-learning replace maximization allow computing gradient q-function use gradient ascent update feature weights maximize likelihood expert ’ trajectories demonstrate two problem domains approach improves likelihood compared previous methods	positive
cnn-cert: an efficient framework for certifying robustness of convolutional neural networks	verifying robustness neural network classifiers attracted great interests attention due success deep neural networks unexpected vulnerability adversarial perturbations although finding minimum adversarial distortion neural networks relu activations shown np-complete problem obtaining non-trivial lower bound minimum distortion provable robustness guarantee possible however previous works focused simple fully-connected layers multilayer perceptrons limited relu activations motivates us propose general efficient framework cnn-cert capable certifying robustness general convolutional neural networks framework general – handle various architectures including convolutional layers max-pooling layers batch normalization layer residual blocks well general activation functions approach efficient – exploiting special structure convolutional layers achieve 17 11 times speed-up compared state-of-the-art certification algorithms e.g fast-lin crown 366 times speed-up compared dual-lp approach algorithm obtains similar even better verification bounds addition cnn-cert generalizes state-of-the-art algorithms e.g fast-lin crown demonstrate extensive experiments method outperforms state-of-the-art lowerbound-based certification algorithms terms bound quality speed	negative
bias reduction via end-to-end shift learning: application to citizen science	citizen science projects successful gathering rich datasets various applications however data collected citizen scientists often biased — particular aligned citizens ’ preferences scientific objectives propose shift compensation network scn end-to-end learning scheme learns shift scientific objectives biased data compensating shift re-weighting training data applied bird observational data citizen science project ebird demonstrate scn quantifies data distribution shift outperforms supervised learning models address data bias compared competing models context covariate shift demonstrate advantage scn effectiveness capability handling massive high-dimensional data	positive
commnets: communicating neural network architectures for resource constrained systems	applications require heterogeneous sensor deployments continue face practical challenges owing resource constraints within operating environments i.e energy efficiency computational power reliability motivated need effective ways selecting sensing strategy maximizes detection accuracy events interest using available resources data-driven approaches inspired limitations ask fundamental question whether state-of-the-art recurrent neural networks observe different series data communicate hidden states collectively solve objective distributed fashion realize answer conducting series systematic analyses communicating recurrent neural network architecture varying time-steps objective functions number nodes experimental setup employ models tasks synonymous wireless sensor networks contributions show recurrent neural networks communicate hidden states achieve promising results	negative
a deep neural network for unsupervised anomaly detection and diagnosis in multivariate time series data	nowadays multivariate time series data increasingly collected various real world systems e.g. power plants wearable devices etc anomaly detection diagnosis multivariate time series refer identifying abnormal status certain time steps pinpointing root causes building system however challenging since requires capture temporal dependency time series also need encode inter-correlations different pairs time series addition system robust noise provide operators different levels anomaly scores based upon severity different incidents despite fact number unsupervised anomaly detection algorithms developed jointly address challenges paper propose multi-scale convolutional recurrent encoder-decoder mscred perform anomaly detection diagnosis multivariate time series data specifically mscred first constructs multi-scale resolution signature matrices characterize multiple levels system statuses different time steps subsequently given signature matrices convolutional encoder employed encode inter-sensor time series correlations attention based convolutional long-short term memory convlstm network developed capture temporal patterns finally based upon feature maps encode inter-sensor correlations temporal information convolutional decoder used reconstruct input signature matrices residual signature matrices utilized detect diagnose anomalies extensive empirical studies based synthetic dataset real power plant dataset demonstrate mscred outperform state-ofthe-art baseline methods	negative
motiontransformer: transferring neural inertial tracking between domains	inertial information processing plays pivotal role egomotion awareness mobile agents inertial measurements entirely egocentric environment dependent however affected greatly changes sensor placement/orientation motion dynamics infeasible collect labelled data every domain overcome challenges domain adaptation long sensory sequences propose motiontransformer novel framework extracts domain-invariant features raw sequences arbitrary domains transforms new domains without paired data experiments demonstrate able efficiently effectively convert raw sequence new unlabelled target domain accurate inertial trajectory benefiting motion knowledge transferred labelled source domain also conduct real-world experiments show framework reconstruct physically meaningful trajectories raw imu measurements obtained standard mobile phone various attachments	negative
structured and sparse annotations for image emotion distribution learning	label distribution learning methods effectively address label ambiguity problem achieved great success image emotion analysis however methods ignore structured sparse information naturally contained annotations emotions example emotions grouped ordered due polarities degrees meanwhile emotions character intensity reflected different levels sparse annotations motivated observations present convolutional neural network based framework called structured sparse annotations image emotion distribution learning ssdl tackle two challenges order utilize structured annotations earth mover ’ distance employed calculate minimal cost required transform one distribution another ordered emotions emotion groups combined kullback-leibler divergence design loss penalize mispredictions according dissimilarities emotions different emotions simultaneously moreover order handle sparse annotations sparse regularization based emotional intensity adopted combined loss sparse regularization ssdl could effectively leverage structured sparse annotations predicting emotion distribution experiment results demonstrate proposed ssdl significantly outperforms state-of-the-art methods	negative
multi-precision quantized neural networks via encoding decomposition of {-1,+1}	training deep neural networks dnns requires intensive resources computation storage performance thus dnns efficiently applied mobile phones embedded devices seriously limits applicability industry applications address issue propose novel encoding scheme using −1 +1 decompose quantized neural networks qnns multibranch binary networks efficiently implemented bitwise operations xnor bitcount achieve model compression computational acceleration resource saving based method users easily achieve different encoding precisions arbitrarily according requirements hardware resources proposed mechanism suitable use fpga asic terms data storage computation provides feasible idea smart chips validate effectiveness method large-scale image classification tasks e.g. imagenet object detection tasks particular method lowbit encoding still achieve almost performance full-precision counterparts	negative
circconv: a structured convolution with low complexity	deep neural networks dnns especially deep convolutional neural networks cnns emerged powerful technique various machine learning applications however large model sizes dnns yield high demands computation resource weight storage thereby limiting practical deployment dnns overcome limitations paper proposes impose circulant structure construction convolutional layers hence leads circulant convolutional layers circconvs circulant cnns circulant structure models either trained scratch re-trained pre-trained non-circulant model thereby making flexible different training environments extensive experiments strong structureimposing approach proved able substantially reduce number parameters convolutional layers enable significant saving computational cost using fast multiplication circulant tensor	negative
interpretable preference learning: a game theoretic framework for large margin on-line feature and rule learning	large body research currently investigating connection machine learning game theory work game theory notions injected preference learning framework specifically preference learning problem seen two-players zero-sum game algorithm proposed incrementally include new useful features hypothesis particularly important dealing large number potential features like instance relational learning rule extraction game theoretical analysis used demonstrate convergence algorithm furthermore leveraging natural analogy features rules resulting models easily interpreted humans extensive set experiments classification tasks shows effectiveness proposed method terms interpretability feature selection quality accuracy state-of-the-art	negative
building trust in deep learning system towards automated disease detection	though deep learning systems achieved high accuracy detecting diseases medical images systems deployed highly automated disease screening settings due lack trust well systems generalize out-of-datasets propose use uncertainty estimates deep learning system ’ prediction know accept disregard prediction evaluate effectiveness using estimates real-life application screening diabetic retinopathy also generate visual explanation deep learning system convey pixels image influences decision together reveal deep learning system ’ competency limits human turn human know trust deep learning system	negative
beyond speech: generalizing d-vectors for biometric verification	deep learning based automatic feature extraction methods radically transformed speaker identification facial recognition current approaches typically specialized individual domains deep vectors d-vectors speaker identification provide two distinct contributions generalized framework biometric verification inspired d-vectors novel models outperform current stateof-the-art approaches approach supports substitution various feature extraction models improves robustness verification tests across domains demonstrate framework models two different behavioral biometric verification problems keystroke mobile gait present comprehensive empirical analysis comparing framework state-of-the-art domains models perform verification higher accuracy using orders magnitude less data state-of-the-art approaches domains believe combination high accuracy practical data requirements enable application behavioral biometric models outside laboratory support much-needed improvements cyber security	negative
identifying semantics in clinical reports using neural machine translation	clinical documents vital resources radiologists consult refer studying similar cases large healthcare facilities millions reports generated searching relevant documents quite challenging abundant interchangeable words clinical domain understanding semantics words clinical documents vital improve search results paper details end end semantic search application address large scale information retrieval problem clinical reports paper specifically focuses challenge identifying semantics clinical reports facilitate search semantic level semantic search works mapping documents concept space search performed concept space unique approach framing concept mapping problem language translation problem proposed paper concept mapper modelled using neural machine translation model nmt based encoder-decoder attention architecture regular expression based concept mapper takes approximately 3 seconds extract umls concepts single document trained nmt approximately 30 milliseconds nmt based model enables incorporation negation detection identify whether concept negated facilitating search negated queries	positive
balanced sparsity for efficient dnn inference on gpu	trained deep neural networks unstructured pruning reduce redundant weights lower storage cost however requires customization hardwares speed practical inference another trend accelerates sparse model inference general-purpose hardwares adopting coarse-grained sparsity prune regularize consecutive weights efficient computation method often sacrifices model accuracy paper propose novel fine-grained sparsity approach balanced sparsity achieve high model accuracy commercial hardwares efficiently approach adapts high parallelism property gpu showing incredible potential sparsity widely deployment deep learning services experiment results show balanced sparsity achieves 3.1x practical speedup model inference gpu retains high model accuracy finegrained sparsity	negative
tackling sparse rewards in real-time games with statistical forward planning methods	one issues general ai game players required deal different reward systems variety games expected able play high level games may present plentiful rewards agents use guide search best solution whereas others feature sparse reward landscapes provide little information agents work presented paper focuses latter case agents struggle thus modifications proposed two algorithms monte carlo tree search rolling horizon evolutionary algorithms aiming improving performance type games maintaining overall win rate across rewards plentiful results show longer rollouts individual lengths either fixed responsive changes fitness landscape features lead boost performance games testing without detrimental non-sparse reward scenarios	negative
unsupervised transfer learning for spoken language understanding in intelligent agents	user interaction voice-powered agents generates large amounts unlabeled utterances paper explore techniques efficiently transfer knowledge unlabeled utterances improve model performance spoken language understanding slu tasks use embeddings language model elmo take advantage unlabeled data learning contextualized word representations additionally propose elmo-light elmol faster simpler unsupervised pre-training method slu findings suggest unsupervised pre-training large corpora unlabeled utterances leads significantly better slu performance compared training scratch even outperform conventional supervised transfer additionally show gains unsupervised transfer techniques improved supervised transfer improvements pronounced low resource settings using 1000 labeled in-domain samples techniques match performance training scratch 10-15x labeled in-domain data	negative
adaptive optimization framework for control of multi-agent systems	main focus work optimization-based framework control multi-agent systems synthesizes actions steering given system towards specified state primary motivation research presented fascination birds save energy long-distance flights via forming v-shape ask following question v-formations result solving optimization problem concept utilized multi-agent systems particularly drones swarms increase safety resilience demonstrate framework applied system modeled controllable markov decision process cost reward function key feature procedure propose automatic adaptation performance optimization towards given global objective combining model-predictive control ideas sequential monte-carlo methods introduce performance-based adaptive horizon implicitly build lyapunov function guarantees convergence use statistical model-checking verify algorithm assess reliability	negative
bias-variance trade-off in hierarchical probabilistic models using higher-order feature interactions	hierarchical probabilistic models able use large number parameters create model high representation power however well known increasing number parameters also increases complexity model leads bias-variance trade-off although classical problem bias-variance trade-off hiddenlayers higher-order interactions well studied study propose efficient inference algorithm log-linear formulation higher-order boltzmann machine using combination gibbs sampling annealed importance sampling perform bias-variance decomposition study differences hidden layers higher-order interactions results shown using hidden layers higher-order interactions comparable error similar order magnitude using higherorder interactions produce less variance smaller sample size	negative
general robustness evaluation of incentive mechanism against bounded rationality using continuum-armed bandits	incentive mechanisms assume agents fully rational may fail due bounded rationality agents practice thus crucial evaluate extent mechanisms resist agents ’ bounded rationality termed robustness paper propose general empirical framework robustness evaluation one novelty framework develop robustness formulation generally applicable different types incentive mechanisms bounded rationality models formulation considers incentives agents also performance mechanisms novelty lies converting empirical robustness computation continuum-armed bandit problem developing efficient solver theoretically guaranteed error rate upper bound also conduct extensive experiments using various mechanisms verify advantages practicability robustness evaluation framework	negative
heterogeneous attributed network embedding with graph convolutional networks	network embedding assigns nodes networks lowdimensional representations received increasing attention recent years however existing approaches especially spectral-based methods consider attributes homogeneous networks weak heterogeneous attributed networks involve different node types well rich node attributes common real-world scenarios paper propose hane novel network embedding method based graph convolutional networks leverages heterogeneity node attributes generate high-quality embeddings experiments real-world dataset show effectiveness method	negative
precision-recall versus accuracy and the role of large data sets	practitioners data mining machine learning long observed imbalance classes data set negatively impacts quality classifiers trained data numerous techniques coping imbalances proposed nearly lack theoretical grounding contrast standard theoretical analysis machine learning admits dependence imbalance classes basic theorems statistical learning establish number examples needed estimate accuracy classifier function complexity vc-dimension confidence desired class imbalance enter formulas anywhere work consider measures classifier performance terms precision recall measure widely suggested appropriate classification imbalanced data observe whenever precision moderately large worse precision recall within small constant factor accuracy weighted class imbalance corollary observation larger number examples necessary sufficient address class imbalance finding also illustrate empirically	positive
robust facial landmark localization based on two-stage cascaded pose regression	paper propose two-stage cascaded pose regression facial landmark localization occlusion first stage global cascaded pose regression robust initialization performed get localization results original face mirror image localization difference original image mirror image used determine whether localization landmark reliable unreliable localization large difference adjusted second stage global results divided four parts refined local regressions finally four refined local results integrated adjusted get final output	positive
improving search with supervised learning in trick-based card games	trick-taking card games two-step process state sampling evaluation widely used approximate move values evaluation component vital accuracy move value estimates also fundamentally linked well sampling distribution corresponds true distribution despite recent work trick-taking card game ai mainly focused improving evaluation algorithms limited work improving sampling paper focus effect sampling strength player propose novel method sampling realistic states given move history particular use predictions locations individual cards made deep neural network — trained data human gameplay — order sample likely worlds evaluation technique used conjunction perfect information monte carlo pimc search provides substantial increase cardplay strength popular trick-taking card game skat	negative
depth prediction without the sensors: leveraging structure for unsupervised learning from monocular videos	learning predict scene depth rgb inputs challenging task indoor outdoor robot navigation work address unsupervised learning scene depth robot ego-motion supervision provided monocular videos cameras cheapest least restrictive ubiquitous sensor robotics	negative
reinforcement learning for improved low resource dialogue generation	thesis focus language independent methods improving utterance understanding response generation attempt tackle issues surrounding current systems aim create unified approach dialogue generation inspired developments goal oriented open ended dialogue systems main contributions thesis 1 introducing hybrid approaches dialogue generation using retrieval encoder-decoder architectures produce fluent precise utterances dialogues 2 proposing supervised semi-supervised reinforcement learning methods domain adaptation goal oriented dialogue 3 introducing models adapt cross lingually	negative
an efficient approach to informative feature extraction from multimodal data	one primary focus multimodal feature extraction find representations individual modalities maximally correlated well-known measure dependence hirschfeld-gebelein-rényi hgr maximal correlation be-´ comes appealing objective operational meaning desirable properties however strict whitening constraints formalized hgr maximal correlation limit application address problem paper proposes soft-hgr novel framework extract informative features multiple data modalities specifically framework prevents “ hard ” whitening constraints simultaneously preserving feature geometry hgr maximal correlation objective soft-hgr straightforward involving two inner products guarantees efficiency stability optimization generalize framework handle two modalities missing modalities labels partially available enhance discriminative power feature representations making semi-supervised adaptation empirical evaluation implies approach learns informative feature mappings efficient optimize	negative
partial multi-label learning via credible label elicitation	partial multi-label learning pml training example associated multiple candidate labels partially valid task pml naturally arises learning scenarios inaccurate supervision goal induce multi-label predictor assign set proper labels unseen instance learn pml training examples training procedure prone misled false positive labels concealed candidate label set light major difficulty novel two-stage pml approach proposed works eliciting credible labels candidate label set model induction way false positive labels expected excluded training procedure specifically first stage labeling confidence candidate label pml training example estimated via iterative label propagation second stage utilizing credible labels high labeling confidence multi-label predictor induced via pairwise label ranking virtual label splitting maximum posteriori map reasoning extensive experiments synthetic well real-world data sets clearly validate effectiveness credible label elicitation learning pml examples	positive
leveraging web semantic knowledge in word representation learning	much recent work focuses leveraging semantic lexicons like wordnet enhance word representation learning wrl achieves promising performance many nlp tasks however existing methods might limitations require high-quality manually created semantic lexicons linguistic structures paper propose leverage semantic knowledge automatically mined web structured data enhance wrl first construct semantic similarity graph referred semantic knowledge based large collection semantic lists extracted web using several pre-defined html tag patterns introduce efficient joint word representation learning model capture semantics semantic knowledge text corpora compared recent work improving wrl semantic resources approach general easily scaled additional effort extensive experimental results show approach outperforms state-of-the-art methods word similarity word sense disambiguation text classification textual similarity tasks	negative
a pspace subclass of dependency quantified boolean formulas and its effective solving	dependency quantified boolean formulas dqbfs powerful formalism subsumes quantified boolean formulas qbfs allows explicit specification dependencies existential variables universal variables enables succinct encoding decision problems nexptime complexity class solving general dqbfs nexptime complete contrast pspace completeness qbf solving characterizing dqbf subclasses lower computational complexity allows effective solving practical importance	negative
approximation and hardness of shift-bribery	shift-bribery problem given election preferred candidate costs shifting preferred candidate voters ’ preference orders goal find set shifts ensures preferred candidate wins election give first polynomial-time approximation scheme case positional scoring rules copeland rule show strong inapproximability results	negative
an efficient compressive convolutional network for unified object detection and image compression	paper addresses challenge designing efficient framework real-time object detection image compression proposed compressive convolutional network ccn basically compressive-sensing-enabled convolutional neural network instead designing different components compressive sensing object detection ccn optimizes reuses convolution operation recoverable data embedding image compression technically incoherence condition sufficient condition recoverable data embedding incorporated first convolutional layer ccn model regularization therefore ccn convolution kernels learned training voc coco image set used data embedding image compression reusing convolution operation extra computational overhead required image compression result ccn 3.1 5.0 fold efficient conventional approaches experiments ccn achieved 78.1 map object detection 3.0 db 5.2 db higher psnr image compression examined compressive sensing approaches	positive
estimating the causal effect from partially observed time series	many real-world systems involve interacting time series ability detect causal dependencies system components observed time series outputs essential understanding system behavior quantification causal influences time series based definition causality measure partial canonical correlation analysis partial cca extensions examples methods used robustly estimating causal relationships two multidimensional time series even time series short methods assume input data complete missing values however real-world data often contain missing values therefore crucial estimate causality measure robustly even input time series incomplete treating problem semi-supervised learning problem propose novel semi-supervised extension probabilistic partial cca called semi-bayesian partial cca method exploits information samples missing values prevent overfitting parameter estimation even complete samples experiments based synthesized real data demonstrate ability proposed method estimate causal relationships correctly existing methods data contain missing values dimensionality large number samples small	negative
a deep reinforcement learning framework for rebalancing dockless bike sharing systems	bike sharing provides environment-friendly way traveling booming world yet due high similarity user travel patterns bike imbalance problem constantly occurs especially dockless bike sharing systems causing significant impact service quality company revenue thus become critical task bike sharing operators resolve imbalance efficiently paper propose novel deep reinforcement learning framework incentivizing users rebalance systems model problem markov decision process take spatial temporal features consideration develop novel deep reinforcement learning algorithm called hierarchical reinforcement pricing hrp builds upon deep deterministic policy gradient algorithm different existing methods often ignore spatial information rely heavily accurate prediction hrp captures spatial temporal dependencies using divide-and-conquer structure embedded localized module conduct extensive experiments evaluate hrp based dataset mobike major chinese dockless bike sharing company results show hrp performs close 24-timeslot look-ahead optimization outperforms state-of-the-art methods service level bike distribution also transfers well applied unseen areas	positive
safe: a neural survival analysis model for fraud early detection	many online platforms deployed anti-fraud systems detect prevent fraudulent activities however usually gap time user commits fraudulent action time user suspended platform detect fraudsters time challenging problem existing approaches adopt classifiers predict fraudsters given activity sequences along time main drawback classification models prediction results consecutive timestamps often inconsistent paper propose survival analysis based fraud early detection model safe maps dynamic user activities survival probabilities guaranteed monotonically decreasing along time safe adopts recurrent neural network rnn handle user activity sequences directly outputs hazard values timestamp survival probability derived hazard values deployed achieve consistent predictions observe user suspended time instead fraudulent activity time training data revise loss function regular survival model achieve fraud early detection experimental results two real world datasets demonstrate safe outperforms survival analysis model recurrent neural network model alone well state-of-theart fraud early detection approaches	negative
story ending generation with incremental encoding and commonsense knowledge	generating reasonable ending given story context i.e. story ending generation strong indication story comprehension task requires understand context clues play important role planning plot also handle implicit knowledge make reasonable coherent story paper devise novel model story ending generation model adopts incremental encoding scheme represent context clues spanning story context addition commonsense knowledge applied multi-source attention facilitate story comprehension thus help generate coherent reasonable endings building context clues using implicit knowledge model able produce reasonable story endings automatic manual evaluation shows model generate reasonable story endings state-of-the-art baselines1	positive
operator mutexes and symmetries for simplifying planning tasks	simplifying classical planning tasks removing operators preserving least one optimal solution significantly enhance performance planners paper introduce notion operator mutex set operators part strongly optimal plan propose four different methods inference operator mutexes experimentally verify found sizable number planning tasks show operator mutexes used combination structural symmetries safely remove operators planning task	negative
logic attention based neighborhood aggregation for inductive knowledge graph embedding	knowledge graph embedding aims modeling entities relations low-dimensional vectors previous methods require entities seen training unpractical real-world knowledge graphs new entities emerging daily basis recent efforts issue suggest training neighborhood aggregator conjunction conventional entity relation embeddings may help embed new entities inductively via existing neighbors however neighborhood aggregators neglect unordered unequal natures entity ’ neighbors end summarize desired properties may lead effective neighborhood aggregators also introduce novel aggregator namely logic attention network lan addresses properties aggregating neighbors rules- network-based attention weights comparing conventional aggregators two knowledge graph completion tasks experimentally validate lan ’ superiority terms desired properties	negative
utilizing class information for deep network representation shaping	statistical characteristics deep network representations sparsity correlation known relevant performance interpretability deep learning statistical characteristic desired often adequate regularizer designed applied training phase typically regularizer aims manipulate statistical characteristic classes together classification tasks however might advantageous enforce desired characteristic per class different classes better distinguished motivated idea design two class-wise regularizers explicitly utilize class information class-wise covariance regularizer cw-cr classwise variance regularizer cw-vr cw-cr targets reduce covariance representations calculated class samples encouraging feature independence cw-vr similar variance instead covariance targeted improve feature compactness sake completeness counterparts without using class information covariance regularizer cr variance regularizer vr considered together four regularizers conceptually simple computationally efficient visualization shows regularizers indeed perform distinct representation shaping terms classification performance significant improvements baseline l1/l2 weight regularization methods found 21 22 tasks popular benchmark datasets particular cw-vr achieved best performance 13 tasks including resnet-32/110	negative
title	abstract	negative
lifted proximal operator machines	propose new optimization method training feedforward neural networks rewriting activation function equivalent proximal operator approximate feedforward neural network adding proximal operators objective function penalties hence call lifted proximal operator machine lpom lpom block multiconvex layer-wise weights activations allows us use block coordinate descent update layer-wise weights activations notably use mapping activation function rather derivative thus avoiding gradient vanishing blow-up issues gradient based training methods method applicable various non-decreasing lipschitz continuous activation functions saturating non-differentiable lpom require auxiliary variables layer-wise activations thus using roughly amount memory stochastic gradient descent sgd parameter tuning also much simpler prove convergence updating layer-wise weights activations point optimization could made parallel asynchronous update experiments mnist cifar-10 datasets testify advantages lpom	positive
super sparse convolutional neural networks	construct small mobile networks without performance loss address over-fitting issues caused less abundant training datasets paper proposes novel super sparse convolutional ssc kernel corresponding network called ssc-net ssc kernel every spatial kernel one non-zero parameter non-zero spatial positions different ssc kernel effectively select pixels feature maps according non-zero positions perform therefore ssc preserve general characteristics geometric channels ’ differences resulting preserving quality retrieved features meeting general accuracy requirements furthermore ssc entirely implemented “ shift ” “ group point-wise ” convolutional operations without spatial kernels e.g. “ 3×3 ” therefore ssc first method remove parameters ’ redundancy spatial extent channel extent leading largely decreasing parameters flops well reducing img2col col2img operations implemented low leveled libraries meanwhile ssc-net improve sparsity overcome over-fitting effectively mobile networks comparative experiments performed less abundant cifar low resolution imagenet datasets results showed ssc-nets significantly decrease parameters computational flops without performance losses additionally also improve ability addressing over-fitting problem challenging less abundant datasets	positive
a monte carlo tree search player for birds of a feather solitaire	artificial intelligence games serves excellent platform facilitating collaborative research undergraduates paper explores several aspects research challenge proposed newly-developed variant solitaire game present multiple classes game states identified solvable unsolvable present heuristic quickly finding goal states game state search tree finally introduce monte carlo tree search-based player solitaire variant win almost solvable starting deal efficiently	negative
communication-efficient stochastic gradient mcmc for neural networks	learning probability distributions weights neural networks recently proven beneficial many applications bayesian methods stochastic gradient markov chain monte carlo sg-mcmc offer elegant framework reason model uncertainty neural networks however advantages usually come high computational cost propose accelerating sg-mcmc masterworker framework workers asynchronously parallel share responsibility gradient computations master collects final samples reduce communication overhead two protocols downpour elastic developed allow periodic interaction master workers provide theoretical analysis finite-time estimation consistency posterior expectations establish connections sample thinning experiments various neural networks demonstrate proposed algorithms greatly reduce training time achieving comparable better test accuracy/log-likelihood levels relative traditional sg-mcmc applied reinforcement learning naturally provides exploration asynchronous policy optimization encouraging performance improvement	negative
horizontal pyramid matching for person re-identification	despite remarkable progress person re-identification re-id approaches still suffer failure cases discriminative body parts missing mitigate type failure propose simple yet effective horizontal pyramid matching hpm approach fully exploit various partial information given person correct person candidates identified even key parts missing hpm make following contributions produce robust feature representations re-id task 1 learn classify using partial feature representations different horizontal pyramid scales successfully enhance discriminative capabilities various person parts 2 exploit average max pooling strategies account person-specific discriminative information global-local manner validate effectiveness proposed hpm method extensive experiments conducted three popular datasets including market-1501 dukemtmcreid cuhk03 respectively achieve map scores 83.1 74.5 59.7 challenging benchmarks new state-of-the-arts	negative
pathological evidence exploration in deep retinal image diagnosis	though deep learning shown successful performance classifying label severity stage certain disease give evidence make prediction propose exploit interpretability deep learning application medical diagnosis inspired koch ’ postulates well-known strategy medical research identify property pathogen define pathological descriptor extracted activated neurons diabetic retinopathy detector visualize symptom feature encoded descriptor propose gan based method synthesize pathological retinal image given descriptor binary vessel segmentation besides descriptor arbitrarily manipulate position quantity lesions verified panel 5 licensed ophthalmologists synthesized images carry symptoms directly related diabetic retinopathy diagnosis panel survey also shows generated images qualitatively quantitatively superior existing methods	negative
learning to write stories with thematic consistency and wording novelty	automatic story generation challenging task involves automatically comprising sequence sentences words consistent topic novel wordings although many attention paid task prompting progress made still exists noticeable gap generated stories created humans especially terms thematic consistency wording novelty fill gap propose cache-augmented conditional variational autoencoder story generation cache module allows improve thematic consistency conditional variational autoencoder part used generating stories less common words using continuous latent variable combing cache module autoencoder part introduce effective gate mechanism experimental results rocstories writingprompts indicate proposed model generate stories consistency wording novelty outperforms existing models automatic metrics human evaluations	negative
multi-view multi-instance multi-label learning based on collaborative matrix factorization	multi-view multi-instance multi-label learning m3l deals complex objects encompassing diverse instances represented different feature views annotated multiple labels existing m3l solutions partially explore inter intra relations objects bags instances labels convey important contextual information m3l may compromised performance.\	negative
multi-attribute transfer via disentangled representation	recent studies show significant progress image-to-image translation task especially facilitated generative adversarial networks synthesize highly realistic images alter attribute labels images however works employ attribute vectors specify target domain diminishes image-level attribute diversity paper propose novel model formulating disentangled representations projecting images latent units grouped feature channels convolutional neural network disassemble information different attributes thanks disentangled representation transfer attributes according attribute labels moreover retain diversity beyond labels namely styles inside image achieved specifying attributes swapping corresponding latent units “ swap ” attributes appearance applying channel-wise interpolation blend different attributes verify motivation proposed model train evaluate model face dataset celeba furthermore evaluation another facial expression dataset rafd demonstrates generalizability proposed model	negative
memory bounded open-loop planning in large pomdps using thompson sampling	state-of-the-art approaches partially observable planning like pomcp based stochastic tree search approaches computationally efficient may still construct search trees considerable size could limit performance due restricted memory resources paper propose partially observable stacked thompson sampling posts memory bounded approach openloop planning large pomdps optimizes fixed size stack thompson sampling bandits empirically evaluate posts four large benchmark problems compare performance different tree-based approaches show posts achieves competitive performance compared tree-based open-loop planning offers performancememory tradeoff making suitable partially observable planning highly restricted computational memory resources	negative
fridays: a financial risk information detecting and analyzing system	present fridays financial risk information detecting analyzing system enables financial professionals efficiently comprehend financial reports terms risk domain-specific sentiment cues system designed integrate multiple nlp models trained financial reports different levels i.e. word multi-word sentence levels illustrate prediction results generated models system available online https //cfda.csie.org/fridays/	positive
predicting concrete and abstract entities in modern poetry	one dimension modernist poetry introducing entities surprising contexts wheelbarrow bob dylan ’ feel like falling love first woman meet/ putting wheelbarrow paper considers problem teaching neural language model select poetic entities based local context windows fine-tuning evaluating language models poetry american modernists seen unseen poets across range experimental designs also compare performance poetic language model human professional poets main finding perhaps surprisingly modernist poetry differs ordinary language entities concrete like wheelbarrow fine-tuning strategy successfully adapts poetic language general outperforming professional poets biggest error reduction observed concrete entities	negative
real-time planning as decision-making under uncertainty	real-time planning agent must select next action take within fixed time bound many popular real-time heuristic search methods approach expanding nodes using time-limited a* selecting action leading toward frontier node lowest f value paper reconsider real-time planning problem decision-making uncertainty propose treating heuristic values uncertain evidence explore several backup methods aggregating evidence propose novel lookahead strategy expands nodes minimize risk expected regret case non-optimal action chosen evaluate methods simple synthetic benchmark sliding tile puzzle find outperform previous methods work illustrates uncertainty arise even solving deterministic planning problems due inherent ignorance time-limited search algorithms portions state space computed agent benefit explicitly metareasoning uncertainty	negative
sat-based explicit ltlf satisfiability checking	present sat-based framework ltlf linear temporal logic finite traces satisfiability checking use propositional sat-solving techniques construct transition system input ltlf formula satisfiability checking reduced path-search problem transition system furthermore introduce cdlsc conflict-driven ltlf satisfiability checking novel algorithm leverages information produced propositional sat solvers satisfiability unsatisfiability results experimental evaluations show cdlsc outperforms existing approaches ltlf satisfiability checking demonstrating approximate four-fold speed-up compared second-best solver	positive
switch-lstms for multi-criteria chinese word segmentation	multi-criteria chinese word segmentation promising challenging task exploits several different segmentation criteria mines common underlying knowledge paper propose flexible multi-criteria learning chinese word segmentation usually segmentation criterion could decomposed multiple sub-criteria shareable segmentation criteria process word segmentation routing among sub-criteria perspective present switch-lstms segment words consist several long short-term memory neural networks lstm switcher automatically switch routing among lstms auto-switched lstms model provides flexible solution multi-criteria cws also easy transfer learned knowledge new criteria experiments show model obtains significant improvements eight corpora heterogeneous segmentation criteria compared previous method single-criterion learning	negative
what’s most broken? a tool to assist data-driven iterative improvement of an intelligent tutoring system	intelligent tutoring systems great potential change educational landscape bringing scientifically tested one-to-one tutoring remote under-served areas however effective itss complex perfect instead practical guiding principle development improvement fix ’ broken paper present spot statistical probe tutoring tool mines data logged intelligent tutoring system identify ‘ hot spots ’ detrimental efficiency effectiveness terms software reliability usability task difficulty student engagement criteria spot uses heuristics machine learning discover characterize prioritize hot spots order focus refinement matters applied spot data logged robotutor teaches children basic reading writing arithmetic	negative
ugsd: user generated sentiment dictionaries from online customer reviews	customer reviews platforms tripadvisor amazon provide rich information ways people convey sentiment certain domains given kinds user reviews paper proposes ugsd representation learning framework constructing domain-specific sentiment dictionaries online customer reviews leverage relationship user-generated reviews ratings reviews associate reviewer sentiment certain entities proposed framework following three main advantages first additional annotations words external dictionaries needed proposed framework resources needed review texts entity ratings second framework applicable across variety user-generated content different domains construct domain-specific sentiment dictionaries finally word constructed dictionary associated low-dimensional dense representation degree relatedness certain rating enable us obtain fine-grained dictionaries enhance application scalability constructed dictionaries word representations adopted various tasks applications entity ranking dictionary expansion experimental results three real-world datasets show framework effective constructing high-quality domain-specific sentiment dictionaries customer reviews	positive
3d face synthesis driven by personality impression	synthesizing 3d faces give certain personality impressions commonly needed computer games animations virtual world applications producing realistic virtual characters paper propose novel approach synthesize 3d faces based personality impression creating virtual characters approach consists two major steps first step train classifiers using deep convolutional neural networks dataset images personality impression annotations capable predicting personality impression face second step given 3d face desired personality impression type user inputs approach optimizes facial details trained classifiers synthesize face gives desired personality impression demonstrate approach synthesizing 3d faces giving desired personality impressions variety 3d face models perceptual studies show perceived personality impressions synthesized faces agree target personality impressions specified synthesizing faces	negative
temporal deformable convolutional encoder-decoder networks for video captioning	well believed video captioning fundamental challenging task computer vision artificial intelligence fields prevalent approach map input video variable-length output sentence sequence sequence manner via recurrent neural network rnn nevertheless training rnn still suffers degree vanishing/exploding gradient problem making optimization difficult moreover inherently recurrent dependency rnn prevents parallelization within sequence training therefore limits computations paper present novel design — temporal deformable convolutional encoder-decoder networks dubbed tdconved fully employ convolutions encoder decoder networks video captioning technically exploit convolutional block structures compute intermediate states fixed number inputs stack several blocks capture long-term relationships structure encoder equipped temporal deformable convolution enable free-form deformation temporal sampling model also capitalizes temporal attention mechanism sentence generation extensive experiments conducted msvd msr-vtt video captioning datasets superior results reported comparing conventional rnn-based encoder-decoder techniques remarkably tdconved increases cider-d performance 58.8 67.2 msvd	negative
switch-based active deep dyna-q: efficient adaptive planning for task-completion dialogue policy learning	training task-completion dialogue agents reinforcement learning usually requires large number real user experiences dyna-q algorithm extends q-learning integrating world model thus effectively boost training efficiency using simulated experiences generated world model effectiveness dyna-q however depends quality world model implicitly pre-specified ratio real vs. simulated experiences used q-learning end extend recently proposed deep dyna-q ddq framework integrating switcher automatically determines whether use real simulated experience q-learning furthermore explore use active learning improving sample efficiency encouraging world model generate simulated experiences stateaction space agent fully explored results show combining switcher active learning new framework named switch-based active deep dyna-q switch-ddq leads significant improvement ddq q-learning baselines simulation human evaluations.1	negative
the goldilocks zone: towards better understanding of neural network loss landscapes	explore loss landscape fully-connected convolutional neural networks using random low-dimensional hyperplanes hyperspheres evaluating hessian h loss function hypersurfaces observe 1 unusual excess number positive eigenvalues h 2 large value tr h /||h|| well defined range configuration space radii corresponding thick hollow spherical shell refer goldilocks zone observe effect fully-connected neural networks range network widths depths mnist cifar-10 datasets relu tanh non-linearities similar effect convolutional networks using observations demonstrate close connection goldilocks zone measures local convexity/prevalence positive curvature suitability network initialization show high stable accuracy reached optimizing random low-dimensional hypersurfaces directly related overlap hypersurface goldilocks zone corollary demonstrate notion intrinsic dimension initialization-dependent note common initialization techniques initialize neural networks particular region unusually high convexity/prevalence positive curvature offer geometric intuition success furthermore demonstrate initializing neural network number points selecting high measures local convexity tr h /||h|| number positive eigenvalues h low initial loss leads statistically significantly faster training mnist based observations hypothesize goldilocks zone contains unusually high density suitable initialization configurations	negative
inferring concept prerequisite relations from online educational resources	internet rich rapidly increasing sources high quality educational content inferring prerequisite relations educational concepts required modern large-scale online educational technology applications personalized recommendations automatic curriculum creation present prereq new supervised learning method inferring concept prerequisite relations prereq designed using latent representations concepts obtained pairwise latent dirichlet allocation model neural network based siamese network architecture prereq learn unknown concept prerequisites course prerequisites labeled concept prerequisite data outperforms state-of-the-art approaches benchmark datasets effectively learn less training data prereq also use unlabeled video playlists steadily growing source training data learn concept prerequisites thus obviating need manual annotation course prerequisites	negative
learning options with interest functions	learning temporal abstractions partial solutions task could reused solving tasks ingredient help agents plan learn efficiently work tackle problem options framework aim autonomously learn options specialized different state space regions proposing notion interest functions generalizes initiation sets options framework function approximation build option-critic framework derive policy gradient theorems interest functions leading new interest-option-critic architecture	negative
tallyqa: answering complex counting questions	counting questions visual question answering vqa datasets simple require object detection study algorithms complex counting questions involve relationships objects attribute identification reasoning created tallyqa world ’ largest dataset open-ended counting propose new algorithm counting uses relation networks region proposals method lets relation networks efficiently used high-resolution imagery yields stateof-the-art results compared baseline recent systems tallyqa howmany-qa benchmark	positive
transnfcm: translation-based neural fashion compatibility modeling	identifying mix-and-match relationships fashion items urgent task fashion e-commerce recommender system significantly enhance user experience satisfaction however due challenges inferring rich yet complicated set compatibility patterns large e-commerce corpus fashion items task still underexplored inspired recent advances multirelational knowledge representation learning deep neural networks paper proposes novel translation-based neural fashion compatibility modeling transnfcm framework jointly optimizes fashion item embeddings category-specific complementary relations unified space via end-to-end learning manner transnfcm places items unified embedding space category-specific relation category-comp-category modeled vector translation operating embeddings compatible items corresponding categories way capture specific notion compatibility conditioned specific pair complementary categories also preserve global notion compatibility also design deep fashion item encoder exploits complementary characteristic visual textual features represent fashion products best knowledge first work uses category-specific complementary relations model category-aware compatibility items translation-based embedding space extensive experiments demonstrate effectiveness transnfcm state-of-the-arts two real-world datasets	negative
revisiting spatial-temporal similarity: a deep learning framework for traffic prediction	traffic prediction drawn increasing attention ai research field due increasing availability large-scale traffic data importance real world example accurate taxi demand prediction assist taxi companies pre-allocating taxis key challenge traffic prediction lies model complex spatial dependencies temporal dynamics although factors considered modeling existing works make strong assumptions spatial dependence temporal dynamics i.e. spatial dependence stationary time temporal dynamics strictly periodical however practice spatial dependence could dynamic i.e. changing time time temporal dynamics could perturbation one period another period paper make two important observations 1 spatial dependencies locations dynamic 2 temporal dependency follows daily weekly pattern strictly periodic dynamic temporal shifting address two issues propose novel spatial-temporal dynamic network stdn flow gating mechanism introduced learn dynamic similarity locations periodically shifted attention mechanism designed handle long-term periodic temporal shifting best knowledge first work tackle issues unified framework experimental results real-world traffic datasets verify effectiveness proposed method	negative
collaboration based multi-label learning	well-known exploiting label correlations crucially important multi-label learning existing approaches take label correlations prior knowledge may correctly characterize real relationships among labels besides label correlations normally used regularize hypothesis space final predictions explicitly correlated paper suggest individual label final prediction involves collaboration prediction predictions labels based assumption first propose novel method learn label correlations via sparse reconstruction label space seamlessly integrating learned label correlations model training propose novel multi-label learning approach aims explicitly account correlated predictions labels training desired model simultaneously extensive experimental results show approach outperforms state-of-the-art counterparts	negative
zero-shot object detection with textual descriptions	object detection important real-world applications existing methods mainly focus object detection sufficient labelled training data zero-shot object detection concept names paper address challenging problem zero-shot object detection natural language description aims simultaneously detect recognize novel concept instances textual descriptions propose novel deep learning framework jointly learn visual units visual-unit attention word-level attention combined achieve word-proposal affinity element-wise multiplication best knowledge first work zero-shot object detection textual descriptions since directly related work literature investigate plausible solutions based existing zero-shot object detection fair comparison conduct extensive experiments three challenging benchmark datasets extensive experimental results confirm superiority proposed model	negative
regularized evolution for image classifier architecture search	effort devoted hand-crafting neural network image classifiers motivated use architecture search discover automatically although evolutionary algorithms repeatedly applied neural network topologies image classifiers thus discovered remained inferior human-crafted ones evolve image classifier— amoebanet-a—that surpasses hand-designs first time modify tournament selection evolutionary algorithm introducing age property favor younger genotypes matching size amoebanet-a comparable accuracy current state-of-the-art imagenet models discovered complex architecture-search methods scaled larger size amoebanet-a sets new state-of-theart 83.9 top-1 96.6 top-5 imagenet accuracy controlled comparison well known reinforcement learning algorithm give evidence evolution obtain results faster hardware especially earlier stages search relevant fewer compute resources available evolution thus simple method effectively discover high-quality architectures	negative
computational intractability and solvability for the birds of a feather game	paper analyze birds feather boaf perfectinformation one-player card game subject 2019 eaai undergraduate research challenge prove generalized n × n boaf game np-complete explore one million deals 4×4 boaf testbed present several graph-theoretic algorithms prove 1880 million deals unsolvable conclude paper two search algorithms efficiently show remaining 998,120 deals fact solvable	positive
attention guided imitation learning and reinforcement learning	propose framework uses learned human visual attention model guide learning process imitation learning reinforcement learning agent collected high-quality human action eye-tracking data playing atari games carefully controlled experimental setting shown incorporating learned human gaze model deep imitation learning yields promising results	positive
a sat+cas approach to finding good matrices: new examples and counterexamples	enumerate circulant good matrices odd orders divisible 3 order 70. consequence find previously overlooked set good matrices order 27 new set good matrices order 57. also find circulant good matrices exist orders 51 63 69 thereby finding three new counterexamples conjecture matrices exist odd orders additionally prove new relationship entries good matrices exploit relationship enumeration algorithm method applies sat+cas paradigm combining computer algebra functionality modern sat solvers efficiently search large spaces specified algebraic logical constraints	negative
non-autoregressive machine translation with auxiliary regularization	new neural machine translation approach nonautoregressive machine translation nat attracted attention recently due high efficiency inference however high efficiency come cost capturing sequential dependency target side translation causes nat suffer two kinds translation errors 1 repeated translations due indistinguishable adjacent decoder hidden states 2 incomplete translations due incomplete transfer source side information via decoder hidden states paper propose address two problems improving quality decoder hidden representations via two auxiliary regularization terms training process nat model first make hidden states distinguishable regularize similarity consecutive hidden states based corresponding target tokens second force hidden states contain information source sentence leverage dual nature translation tasks e.g. english german german english minimize backward reconstruction error ensure hidden states nat decoder able recover source side sentence extensive experiments conducted several benchmark datasets show regularization strategies effective alleviate issues repeated translations incomplete translations nat models accuracy nat models therefore improved significantly state-of-the-art nat models even better efficiency inference	negative
pareto optimization for subset selection with dynamic cost constraints	paper consider subset selection problem function f constraint bound b changes time point adaptive variants greedy approaches commonly used area submodular optimization able maintain approximation quality investigating recently introduced pomc pareto optimization approach show algorithm efficiently computes φ αf/2 1− α1f -approximation αf sube modularity ratio f possible constraint bound b ≤ b. furthermore show pomc able adapt set solutions quickly case b increases experimental investigations influence maximization social networks show advantage pomc generalized greedy algorithms	positive
learning semantic representations for novel words: leveraging both form and context	word embeddings key component high-performing natural language processing nlp systems remains challenge learn good representations novel words fly i.e. words occur training data general problem setting word embeddings induced unlabeled training corpus model trained embeds novel words induced embedding space currently two approaches learning embeddings novel words exist learning embedding novel word ’ surface-form e.g. subword n-grams ii learning embedding context occurs paper propose architecture leverages sources information – surface-form context – show results large increases embedding quality architecture obtains state-of-the-art results definitional nonce contextual rare words datasets input require embedding set unlabeled corpus training architecture produce embeddings appropriate induced embedding space thus model easily integrated existing nlp system enhance capability handle novel words	negative
learning to solve np-complete problems: a graph neural network for decision tsp	graph neural networks gnn promising technique bridging differential programming combinatorial domains gnns employ trainable modules assembled different configurations reflect relational structure problem instance paper show gnns learn solve little supervision decision variant traveling salesperson problem tsp highly relevant np-complete problem model trained function effective message-passing algorithm edges embedded weights communicate vertices number iterations model asked decide whether route cost c exists show network trained sets dual examples given optimal tour cost c∗ produce one decision instance target cost x smaller one target cost x larger c∗ able obtain 80 accuracy training −2 +2 deviations trained model generalize relaxed deviations increasing performance also show model capable generalizing larger problem sizes finally provide method predicting optimal route cost within 2 deviation ground truth summary work shows graph neural networks powerful enough solve np-complete problems combine symbolic numeric data	positive
solving imperfect-information games via discounted regret minimization	counterfactual regret minimization cfr family iterative algorithms popular practice fastest approach approximately solving large imperfectinformation games paper introduce novel cfr variants 1 discount regrets earlier iterations various ways cases differently positive negative regrets 2 reweight iterations various ways obtain output strategies 3 use non-standard regret minimizer and/or 4 leverage “ optimistic regret matching ” lead dramatically improved performance many settings one introduce variant outperforms cfr+ prior state-of-the-art algorithm every game tested including large-scale realistic settings cfr+ formidable benchmark algorithm able outperform finally show unlike cfr+ many important new variants compatible modern imperfect-informationgame pruning techniques one also compatible sampling game tree	positive
pvrnet: point-view relation neural network for 3d shape recognition	three-dimensional 3d shape recognition drawn much research attention field computer vision advances deep learning encourage various deep models 3d feature representation point cloud multi-view data two popular 3d data modalities different models proposed remarkable performance however relation point cloud views rarely investigated paper introduce point-view relation network pvrnet effective network designed well fuse view features point cloud feature proposed relation score module specifically based relation score module point-single-view fusion feature first extracted fusing point cloud feature single view feature point-singe-view relation pointmulti- view fusion feature extracted fusing point cloud feature features different number views point-multi-view relation finally point-single-view fusion feature point-multi-view fusion feature combined together achieve unified representation 3d shape proposed pvrnet evaluated modelnet40 dataset 3d shape classification retrieval experimental results indicate model achieve significant performance improvement compared state-of-the-art models	negative
large scale learning of agent rationality in two-player zero-sum games	recent advances solving large zero-sum extensive form games growing interest inverse problem inferring underlying game parameters given access agent actions although recent work provides powerful differentiable end-to-end learning frameworks embed game solver within deep-learning framework allowing unknown game parameters learned via backpropagation framework faces significant limitations applied boundedly rational human agents large scale problems leading poor practicality paper address limitations propose framework applicable practical settings first seeking learn rationality human agents complex two-player zero-sum games draw upon well-known ideas decision theory obtain concise interpretable agent behavior model derive solvers gradients end-to-end learning second scale large real-world scenarios propose efficient first-order primal-dual method exploits structure extensive-form games yielding significantly faster computation game solving gradient computation tested randomly generated games report speedups orders magnitude previous approaches also demonstrate effectiveness model real-world one-player settings synthetic data	negative
ethically aligned mobilization of community effort to reposition shared bikes	consider problem mobilizing community effort reposition indiscriminantly parked shared bikes urban environments crowdsourcing propose ethically aligned incentive optimization approach wsls maximizes rate success bike repositioning minimizing cost prioritizing users ’ wellbeing realistic simulations based dataset singapore demonstrate wsls significantly outperforms existing approaches	positive
a robust and efficient algorithm for the pnl problem using algebraic distance to approximate the reprojection distance	paper proposes novel algorithm solve pose estimation problem 2d/3d line correspondences known perspective-n-line pnl problem widely known minimizing geometric distance generally results accurate results minimizing algebraic distance however rational form reprojection distance line yields complicated cost function makes solving first-order optimality conditions infeasible furthermore iterative algorithms based reprojection distance time-consuming large-scale problem contrast previous works minimize cost function based algebraic distance may approximate reprojection distance line design two simple algebraic distances gradually approximate reprojection distance speeds computation maintains robustness geometric distance two algebraic distances result two polynomial cost functions efficiently solved directly solve first-order optimality conditions first problem novel hidden variable method algorithm makes use specific structure resulting polynomial system therefore stable general gröbner basis polynomial solver minimize second polynomial cost function damped newton iteration starting solution first cost function experimental results show first step algorithm already superior state-of-the-art algorithms terms accuracy applicability faster algorithms based gröbner basis polynomial solver second step yields comparable results results minimizing reprojection distance much efficient speed algorithm applicable real-time applications	positive
to find where you talk: temporal sentence localization in video with attention based location regression	witnessed tremendous growth videos internet videos typically paired abundant sentence descriptions video titles captions comments therefore increasingly crucial associate specific video segments corresponding informative text descriptions deeper understanding video content motivates us explore overlooked problem research community — temporal sentence localization video aims automatically determine start end points given sentence within paired video solving problem face three critical challenges 1 preserving intrinsic temporal structure global context video locate accurate positions entire video sequence 2 fully exploring sentence semantics give clear guidance localization 3 ensuring efficiency localization method adapt long videos address issues propose novel attention based location regression ablr approach localize sentence descriptions videos efficient end-to-end manner specifically preserve context information ablr first encodes video sentence via bi-directional lstm networks multi-modal co-attention mechanism presented generate video sentence attentions former reflects global video structure latter highlights sentence details temporal localization finally novel attention based location prediction network designed regress temporal coordinates sentence previous attentions evaluate proposed ablr approach two public datasets activitynet captions tacos experimental results show ablr significantly outperforms existing approaches effectiveness efficiency	negative
multi-task deep reinforcement learning with popart	reinforcement learning rl community made great strides designing algorithms capable exceeding human performance specific tasks algorithms mostly trained one task time new task requiring train brand new agent instance means learning algorithm general solution agent solve one task trained work study problem learning master one multiple sequentialdecision tasks general issue multi-task learning balance must found needs multiple tasks competing limited resources single learning system many learning algorithms get distracted certain tasks set tasks solve tasks appear salient learning process instance density magnitude in-task rewards causes algorithm focus salient tasks expense generality propose automatically adapt contribution task agent ’ updates tasks similar impact learning dynamics resulted state art performance learning play games set 57 diverse atari games excitingly method learned single trained policy single set weights exceeds median human performance knowledge first time single agent surpassed human-level performance multi-task domain approach also demonstrated state art performance set 30 tasks 3d reinforcement learning platform deepmind lab	negative
understanding learned models by identifying important features at the right resolution	many application domains important characterize complex learned models make decisions across distribution instances one way identify features interactions among contribute model ’ predictive accuracy present model-agnostic approach task makes following specific contributions approach tests feature groups addition base features tries determine level resolution important features determined ii uses hypothesis testing rigorously assess effect feature model ’ loss iii employs hierarchical approach control false discovery rate testing feature groups individual base features importance iv uses hypothesis testing identify important interactions among features feature groups evaluate approach analyzing random forest lstm neural network models learned two challenging biomedical applications	negative
a hierarchical framework for relation extraction with reinforcement learning	existing methods determine relation types entities recognized thus interaction relation types entity mentions fully modeled paper presents novel paradigm deal relation extraction regarding related entities arguments relation apply hierarchical reinforcement learning hrl framework paradigm enhance interaction entity mentions relation types whole extraction process decomposed hierarchy two-level rl policies relation detection entity extraction respectively feasible natural deal overlapping relations model evaluated public datasets collected via distant supervision results show gains better performance existing methods powerful extracting overlapping relations1	negative
ai-sketcher : a deep generative model for producing high-quality sketches	sketch drawings play important role assisting humans communication creative design since ancient period situation motivated development artificial intelligence ai techniques automatically generating sketches based user input sketch-rnn sequence-to-sequence variational autoencoder vae model developed purpose known state-of-the-art technique however suffers limitations including generation lowquality results incapability support multi-class generations address issues introduced ai-sketcher deep generative model generating high-quality multiclass sketches model improves drawing quality employing cnn-based autoencoder capture positional information stroke pixel level also introduces influence layer precisely guide generation stroke directly referring training data support multi-class sketch generation provided conditional vector help differentiate sketches various classes proposed technique evaluated based two large-scale sketch datasets results demonstrated power generating high-quality sketches	negative
g2c: a generator-to-classifier framework integrating multi-stained visual cues for pathological glomerulus classification	pathological glomerulus classification plays key role diagnosis nephropathy difference different subcategories subtle doctors often refer slides different staining methods make decisions however creating correspondence across various stains labor-intensive bringing major difficulties collecting data training vision-based algorithm assist nephropathy diagnosis	negative
rethinking the discount factor in reinforcement learning: a decision theoretic approach	reinforcement learning rl agents traditionally tasked maximizing value function markov decision process mdp either continuous settings fixed discount factor γ 1 episodic settings γ 1. proven effective specific tasks welldefined objectives e.g. games never established fixed discounting suitable general purpose use e.g. model human preferences paper characterizes rationality sequential decision making using set seven axioms arrives form discounting generalizes traditional fixed discounting particular framework admits state-action dependent “ discount ” factor constrained less 1 long eventual long run discounting although broadens range possible preference structures continuous settings show exists unique “ optimizing mdp ” fixed γ 1 whose optimal value function matches true utility optimal policy quantify difference value utility suboptimal policies work seen providing normative justification slight generalization martha white ’ rl task formalism 2017 recent departures traditional rl relevant task specification rl inverse rl preference-based rl	negative
a novel framework for robustness analysis of visual qa models	deep neural networks playing essential role many computer vision tasks including visual question answering vqa recently study accuracy main focus research trend toward assessing robustness models adversarial attacks evaluating tolerance varying noise levels vqa adversarial attacks target image and/or proposed main question yet lack proper analysis later work propose flexible framework focuses language part vqa uses semantically relevant questions dubbed basic questions acting controllable noise evaluate robustness vqa models hypothesize level noise negatively correlated similarity basic question main question hence apply noise given main question rank pool basic questions based similarity casting ranking task lasso optimization problem propose novel robustness measure rscore two largescale basic question datasets bqds order standardize robustness analysis vqa models	negative
revenue enhancement via asymmetric signaling in interdependent-value auctions	consider problem designing information environment revenue maximization sealed-bid second price auction two bidders much prior literature focused signal design settings bidders symmetrically informed design optimal mechanisms fixed information structures study commonand interdependent-value settings mechanism fixed second-price auction auctioneer controls signal structure bidders show standard common-value auction setting benefit auctioneer terms expected revenue sharing information bidders although effects distribution revenues interdependent-value model mixed private- common-value components however show asymmetric information-revealing signals increase revenue	positive
dimension-free error bounds from random projections	learning high dimensional data challenging general – however often data truly high dimensional sense may hidden low complexity geometry give new user-friendly pac-bounds able take advantage benign geometry reduce dimensional-dependence error-guarantees settings dependence known essential general achieved employing random projection analytic tool exploiting structure-preserving compression ability introduce auxiliary function class operates reduced dimensional inputs new complexity term distortion loss random projections latter hypothesis-dependent data-complexity whose analytic estimates turn recover various regularisation schemes parametric models notion intrinsic dimension quantified gaussian width input support case nearest neighbour rule benign geometry present bounds become tighter otherwise recover original dimension-dependent bounds	negative
embedding-based complex feature value coupling learning for detecting outliers in non-iid categorical data	non-iid categorical data ubiquitous common realworld applications learning various kinds couplings proved reliable measure detecting outliers non-iid data however critical yet challenging problem model represent utilise high-order complex value couplings existing outlier detection methods normally focus pairwise primary value couplings fail uncover real relations hide complex couplings resulting suboptimal unstable performance paper introduces novel unsupervised embedding-based complex value coupling learning framework emac instance scan address issues scan first models primary value couplings coupling bias defined capture complex value couplings different granularities highlight essence outliers embedding method performed value network constructed via biased value couplings learns high-order complex value couplings embeds couplings value representation matrix bidirectional selective value coupling learning proposed show estimate value object outlierness value couplings substantial experiments show scan significantly outperforms five state-of-the-art outlier detection methods thirteen real-world datasets ii much better resilience noise competitors	negative
ontology-mediated query answering over log-linear probabilistic data	large-scale knowledge bases heart modern information systems knowledge inherently uncertain hence often materialized probabilistic databases however probabilistic database management systems typically lack capability incorporate implicit background knowledge consequently fail capture intuitive query answers ontology-mediated query answering popular paradigm encoding commonsense knowledge provide complete answers user queries propose new data model integrates paradigm ontology-mediated query answering probabilistic databases employing log-linear probability model compare approach existing proposals provide supporting computational results	negative
leveraging textual specifications for grammar-based fuzzing of network protocols	grammar-based fuzzing technique used find software vulnerabilities injecting well-formed inputs generated following rules encode application semantics grammar-based fuzzers network protocols rely human experts manually specify rules work study automated learning protocol rules textual specifications i.e rfcs evaluate automatically extracted protocol rules applying state-of-the-art fuzzer transport protocols show leads smaller number test cases finding attacks system uses manually specified rules	positive
differentially private empirical risk minimization with smooth non-convex loss functions: a non-stationary view	paper study differentially private empirical risk minimization dp-erm problem non-convex loss functions give several upper bounds utility different settings first consider problem low-dimensional space dp-erm non-smooth regularizer generalize existing work measuring utility using ℓ2 norm projected gradient also extend error bound measurement first time empirical risk population risk using expected ℓ2 norm gradient investigate problem high dimensional space show measuring utility frank-wolfe gap possible bound utility gaussian width constraint set instead dimensionality p underlying space demonstrate advantages result achieved measure ℓ2 norm projected gradient somewhat surprising discovery although two kinds measurements quite different induced utility upper bounds asymptotically assumptions also show utility special non-convex loss functions reduced level i.e. depending log p similar convex loss functions finally test proposed algorithms synthetic real world datasets experimental results confirm theoretical analysis	positive
answer identification from product reviews for user questions by multi-task attentive networks	online shopping become part daily routine still offer intuitive experience store shopping nowadays e-commerce websites offer question answering qa system allows users consult users purchased product however users still need wait patiently others ’ replies paper investigate provide quick response asker plausible answer identification product reviews analyzing similarity discrepancy explicit answers reviews answers novel multi-task deep learning method carefully designed attention mechanisms developed method well exploit large amounts user generated qa data manually labeled review data address problem experiments data collected amazon demonstrate effectiveness superiority competitive baselines	negative
improving natural language inference using external knowledge in the science questions domain	natural language inference nli fundamental many natural language processing nlp applications including semantic search question answering nli problem gained significant attention due release large scale challenging datasets present approaches problem largely focus learning-based methods use textual information order classify whether given premise entails contradicts neutral respect given hypothesis surprisingly use methods based structured knowledge – central topic artificial intelligence – received much attention vis-a-vis nli problem many open knowledge bases contain various types reasoning information use nli well explored address present combination techniques harness external knowledge improve performance nli problem science questions domain present results applying techniques text graph text-and-graph based models discuss implications using external knowledge solve nli problem model achieves close state-of-the-art performance nli scitail science questions dataset	negative
bootstrapping conversational agents with weak supervision	many conversational agents market today follow standard bot development framework requires training intent classifiers recognize user input need create proper set training examples often bottleneck development process many occasions agent developers access historical chat logs provide good quantity well coverage training examples however cost labeling tens hundreds intents often prohibits taking full advantage chat logs paper present framework called search label propagate slp bootstrapping intents existing chat logs using weak supervision framework reduces hours days labeling effort minutes work using search engine find examples relies data programming approach automatically expand labels report user study shows positive user feedback new approach build conversational agents demonstrates effectiveness using data programming autolabeling system developed training conversational agents framework broader application significantly reducing labeling effort training text classifiers	negative
girnet: interleaved multi-task recurrent state sequence models	several natural language tasks labeled sequences available separate domains say languages goal label sequences mixed domain code-switched text may available models labeling whole passages say sentiments would like exploit toward better position-specific label inference say target-dependent sentiment annotation key characteristic shared across tasks different positions primary instance benefit different ‘ experts ’ trained auxiliary data labeled primary instances scarce labeling best expert position entails unacceptable cognitive burden propose girnet unified position-sensitive multi-task recurrent neural network rnn architecture applications auxiliary primary tasks need share training instances auxiliary rnns trained auxiliary instances primary instance also submitted auxiliary rnn state sequences gated merged novel composite state sequence tailored primary inference task approach sharp contrast recent multi-task networks like crossstitch sluice networks control state transfer fine granularity demonstrate superiority girnet using three applications sentiment classification code-switched passages part-of-speech tagging codeswitched text target position-sensitive annotation sentiment monolingual passages cases establish new state-of-the-art performance beyond recent competitive baselines	negative
character n-gram embeddings to improve rnn language models	paper proposes novel recurrent neural network rnn language model takes advantage character information focus character n-grams based research field word embedding construction wieting et al 2016 proposed method constructs word embeddings character ngram embeddings combines ordinary word embeddings demonstrate proposed method achieves best perplexities language modeling datasets penn treebank wikitext-2 wikitext-103 moreover conduct experiments application tasks machine translation headline generation experimental results indicate proposed method also positively affects tasks	positive
learning optimal classification trees using a binary linear program formulation	provide new formulation problem learning optimal classification tree given depth binary linear program limitation previously proposed mathematical optimization formulations create constraints variables every row training data result running time existing integer linear programming ilp formulations increases dramatically size data new binary formulation aim circumvent problem making formulation size largely independent training data size show experimentally formulation achieves better performance existing formulations small large problem instances within shorter running time	positive
artificial counselor system for stock investment	paper proposes novel trading system plays role artificial counselor stock investment paper stock future prices technical features predicted using support vector regression thereafter predicted prices used recommend portions budget investor invest different existing stocks optimum expected profit considering level risk tolerance two different methods used suggesting best portions markowitz portfolio theory fuzzy investment counselor first approach optimization-based method considers merely technical features second approach based fuzzy logic taking account technical fundamental features stock market experimental results new york stock exchange nyse show effectiveness proposed system	positive
identifying android malware using network-based approaches	proliferation android apps resulted many malicious apps entering market causing significant damage robust techniques determine app malicious greatly needed propose use network-based approach effectively separate malicious benign apps based small labeled dataset apps dataset come google play store scanned malicious behavior using virus total produce ground truth dataset labels malicous benign apps resulting dataset represented using binary feature vectors features represent permissions intent actions discriminative apis obfuscation signatures native code signatures used feature vectors corresponding apps build weighted network captures “ closeness ” apps propagate labels labeled apps unlabeled apps evaluate effectiveness proposed approach using f1-measure conducted experiments compare three variants label propagation approaches datasets include increasingly larger amounts labeled data results shown variant proposed study gives best results overall	positive
determinantal reinforcement learning	study reinforcement learning controlling multiple agents collaborative manner tasks insufficient individual agents take relevant actions actions also diversity propose approach using determinant positive semidefinite matrix approximate action-value function reinforcement learning learn matrix way represents relevance diversity actions experimental results show proposed approach allows agents learn nearly optimal policy approximately ten times faster baseline approaches benchmark tasks multi-agent reinforcement learning proposed approach also shown achieve performance achieved conventional approaches partially observable environment exponentially large action space	negative
tensor ring decomposition with rank minimization on latent space: an efficient approach for tensor completion	tensor completion tasks traditional low-rank tensor decomposition models suffer laborious model selection problem due high model sensitivity particular tensor ring tr decomposition number model possibilities grows exponentially tensor order makes rather challenging find optimal tr decomposition paper exploiting low-rank structure tr latent space propose novel tensor completion method robust model selection contrast imposing low-rank constraint data space introduce nuclear norm regularization latent tr factors resulting optimization step using singular value decomposition svd performed much smaller scale leveraging alternating direction method multipliers admm scheme latent tr factors optimal rank recovered tensor obtained simultaneously proposed algorithm shown effectively alleviate burden tr-rank selection thereby greatly reducing computational cost extensive experimental results synthetic real-world data demonstrate superior performance efficiency proposed approach state-of-the-art algorithms	negative
joint domain alignment and discriminative feature learning for unsupervised deep domain adaptation	recently considerable effort devoted deep domain adaptation computer vision machine learning communities however existing work concentrates learning shared feature representation minimizing distribution discrepancy across different domains due fact domain alignment approaches reduce remove domain shift target domain samples distributed near edge clusters far corresponding class centers easily misclassified hyperplane learned source domain alleviate issue propose joint domain alignment discriminative feature learning could benefit domain alignment final classification specifically instance-based discriminative feature learning method center-based discriminative feature learning method proposed guarantee domain invariant features better intra-class compactness inter-class separability extensive experiments show learning discriminative features shared feature space significantly boost performance deep domain adaptation methods	negative
combined reinforcement learning via abstract representations	quest efficient robust reinforcement learning methods model-free model-based approaches offer advantages paper propose new way explicitly bridging approaches via shared low-dimensional learned encoding environment meant capture summarizing abstractions show modularity brought approach leads good generalization computationally efficient planning happening smaller latent state space addition approach recovers sufficient low-dimensional representation environment opens new strategies interpretable ai exploration transfer learning	negative
temporal planning with temporal metric trajectory constraints	several industrial applications planning complex temporal metric trajectory constraints needed adequately model problem hand example production plants items must processed following “ recipe ” steps subject precise timing constraints modeling domains challenging existing action-based languages due lack sufficiently expressive trajectory constraints	negative
a domain generalization perspective on listwise context modeling	one popular techniques solving ranking problem information retrieval learning-to-rank letor received lot attention academia industry due importance wide variety data mining applications however existing letor approaches choose learn single global ranking function handle queries ignore substantial differences exist queries paper propose domain generalization strategy tackle problem propose queryinvariant listwise context modeling qilcm novel neural architecture eliminates detrimental influence inter-query variability learning query-invariant latent representations ranking system could generalize better unseen queries evaluate techniques benchmark datasets demonstrating qilcm outperforms previous state-of-the-art approaches substantial margin	negative
context-aware self-attention networks	self-attention model shown flexibility parallel computation effectiveness modeling long- short-term dependencies however calculates dependencies representations without considering contextual information proven useful modeling dependencies among neural representations various natural language tasks work focus improving self-attention networks capturing richness context maintain simplicity flexibility self-attention networks propose contextualize transformations query key layers used calculate relevance elements specifically leverage internal representations embed global deep contexts thus avoid relying external resources experimental results wmt14 english⇒german wmt17 chinese⇒english translation tasks demonstrate effectiveness universality proposed methods furthermore conducted extensive analyses quantify context vectors participate self-attention model	negative
separator-based pruned dynamic programming for steiner tree	steiner tree classical np-hard problem extensively studied theoretically empirically theory fastest approach inputs small number terminals uses dynamic programming practice stateof-the-art solvers based branch-and-cut method paper present novel separator-based pruning technique speeding theoretically fast dp algorithm empirical evaluation shows pruned dp algorithm quite effective real-world instances admitting small separators scales hundred terminals competitive branch-and-cut solver	negative
explainable reasoning over knowledge graphs for recommendation	incorporating knowledge graph recommender systems attracted increasing attention recent years exploring interlinks within knowledge graph connectivity users items discovered paths provide rich complementary information user-item interactions connectivity reveals semantics entities relations also helps comprehend user ’ interest however existing efforts fully explored connectivity infer user preferences especially terms modeling sequential dependencies within holistic semantics path	negative
safe partial diagnosis from normal observations	model-based diagnosis mbd difficult use practice requires model diagnosed system often hard obtain explore theoretically observing system normal state provide information system sufficient learn partial system model allows automated diagnosis analyze number observations needed learn model capable finding faulty components cases explore knowing system topology help us learn useful model normal observations settings many internal system variables observed unlike data-driven methods learned model safe sense subsystems identified faulty guaranteed truly faulty	negative
querying nosql with deep learning to answer natural language questions	almost today ’ knowledge stored databases thus accessed help domain specific query languages strongly limiting number people access data work demonstrate end-to-end trainable question answering qa system allows user query external nosql database using natural language major challenge system non-differentiability database operations overcome applying policy-based reinforcement learning evaluate approach facebook ’ babi movie dialog dataset achieve competitive score 84.2 compared several benchmark models conclude approach excels regard real-world scenarios knowledge resides external databases intermediate labels costly gather non-end-to-end trainable qa systems	negative
robust anomaly detection in videos using multilevel representations	detecting anomalies surveillance videos long important unsolved problem particular many existing solutions overly sensitive often ephemeral visual artifacts raw video data resulting false positives fragmented detection regions overcome sensitivity capture true anomalies semantic significance one natural idea seek validation abstract representations videos paper introduces framework robust anomaly detection using multilevel representations intensity motion data framework consists three main components 1 representation learning using denoising autoencoders 2 level-wise representation generation using conditional generative adversarial networks 3 consolidating anomalous regions detected representation level proposed multilevel detector shows significant improvement pixel-level equal error rate namely 11.35 12.32 4.31 improvement ucsd ped 1 ucsd ped 2 avenue datasets respectively addition model allowed us detect mislabeled anomalies ucds ped 1	negative
gamenet: graph augmented memory networks for recommending medication combination	recent progress deep learning revolutionizing healthcare domain including providing solutions medication recommendations especially recommending medication combination patients complex health conditions existing approaches either customize based patient health history ignore existing knowledge drug-drug interactions ddi might lead adverse outcomes fill gap propose graph augmented memory networks gamenet integrates drug-drug interactions knowledge graph memory module implemented graph convolutional networks models longitudinal patient records query trained end-to-end provide safe personalized recommendation medication combination demonstrate effectiveness safety gamenet comparing several state-of-the-art methods real ehr data gamenet outperformed baselines effectiveness measures also achieved 3.60 ddi rate reduction existing ehr data	negative
improving distantly supervised relation extraction with neural noise converter and conditional optimal selector	distant supervised relation extraction successfully applied large corpus thousands relations however inevitable wrong labeling problem distant supervision hurt performance relation extraction paper propose method neural noise converter alleviate impact noisy data conditional optimal selector make proper prediction noise converter learns structured transition matrix logit level captures property distant supervised relation extraction dataset conditional optimal selector hand helps make proper prediction decision entity pair even group sentences overwhelmed no-relation sentences conduct experiments widely used dataset results show significant improvement competitive baseline methods	negative
model learning for look-ahead exploration in continuous control	propose exploration method incorporates lookahead search basic learnt skills dynamics use reinforcement learning rl manipulation policies skills multi-goal policies learned isolation simpler environments using existing multigoal rl formulations analogous options macroactions coarse skill dynamics i.e. state transition caused complete skill execution learnt unrolled forward lookahead search policy search benefits temporal abstraction exploration though operates low-level primitive actions thus resulting policies suffer suboptimality inflexibility caused coarse skill chaining show proposed exploration strategy results effective learning complex manipulation policies faster current state-of-the-art rl methods converges better policies methods use options parametrized skills building blocks policy opposed guiding exploration show proposed exploration strategy results effective learning complex manipulation policies faster current state-of-the-art rl methods converges better policies methods use options parameterized skills building blocks policy opposed guiding exploration	positive
sublinear time numerical linear algebra for structured matrices	show solve number problems numerical linear algebra least squares regression lp-regression p ≥ 1 low rank approximation kernel regression time poly log nd given input matrix ∈ rn×d time needed compute · arbitrary vector ∈ rd since ≤ nnz nnz denotes number non-zero entries time worse polylogarithmic factors recent advances problems run input-sparsity time however many applications much smaller nnz yielding significantly sublinear time algorithms example overconstrained 1+ε -approximate polynomial interpolation problem vandermonde matrix n log n case running time n · poly log n poly d/ε recover results avron sindhwani woodruff 2013 special case overconstrained autoregression common problem arising dynamical systems n log n immediately obtain n· poly log n poly d/ε time kernel autoregression significantly improve running time prior algorithms general kernels important case autoregression polynomial kernel arbitrary target vector b ∈ rn obtain even faster algorithms algorithms show perhaps surprisingly optimization problems require much time polylogarithmic number matrix-vector multiplications	negative
pgans: personalized generative adversarial networks for ecg synthesis to improve patient-specific deep ecg classification	electrocardiogram ecg performed routinely medical personnel identify structural functional electrical cardiac events many attempts made automate task using machine learning algorithms including classic supervised learning algorithms deep neural networks reaching state-of-the-art performance ecg signal conveys specific electrical cardiac activity subject thus extreme variations observed patients variations challenging deep learning algorithms impede generalization work propose semisupervised approach patient-specific ecg classification propose generative model learns synthesize patient-specific ecg signals used additional training data improve patient-specific classifier performance empirical results prove generated signals significantly improve ecg classification patient-specific setting	negative
labelforest: non-parametric semi-supervised learning for activity recognition	activity recognition central many motion analysis applications ranging health assessment gaming however need obtaining sufficiently large amounts labeled data limited development personalized activity recognition models semi-supervised learning traditionally promising approach many application domains alleviate reliance large amounts labeled data learning label information small set seed labels nonetheless existing approaches perform poorly highly dynamic settings wearable systems algorithms rely predefined hyper-parameters distribution models needs tuned user context address challenges introduce labelforest 1 novel non-parametric semi-supervised learning framework activity recognition labelforest two algorithms core 1 spanning forest algorithm sample selection label inference 2 silhouette-based filtering method finalize label augmentation machine learning model training thorough analysis three human activity datasets demonstrate labelforest achieves labeling accuracy 90.1 presence skewed label distribution seed data compared self-training sequential learning algorithms labelforest achieves 56.9 175.3 improvement accuracy balanced unbalanced seed data respectively	negative
understanding persuasion cascades in online product rating systems	online product rating systems become indispensable component numerous web services amazon ebay google play store tripadvisor one functionality systems uncover product quality via product ratings reviews contributed consumers however well-known psychological phenomenon called “ messagebased persuasion ” lead “ biased ” product ratings cascading manner call persuasion cascade paper investigates 1 persuasion cascade influence product quality estimation accuracy 2 given real-world product rating dataset infer persuasion cascade analyze draw practical insights first develop mathematical model capture key factors persuasion cascade formulate high-order markov chain characterize opinion dynamics persuasion cascade prove convergence opinions bound product quality estimation error class rating aggregation rules including averaging scoring rule via matrix perturbation theory chernoff bound also design maximum likelihood algorithm infer parameters persuasion cascade conduct experiments data amazon tripadvisor show persuasion cascades notably exist average scoring rule small product quality estimation error practical scenarios	negative
variance reduction in monte carlo counterfactual regret minimization (vr-mccfr) for extensive form games using baselines	learning strategies imperfect information games samples interaction challenging problem common method setting monte carlo counterfactual regret minimization mccfr slow long-term convergence rates due high variance paper introduce variance reduction technique vr-mccfr applies sampling variant mccfr using technique periteration estimated values updates reformulated function sampled values state-action baselines similar use policy gradient reinforcement learning new formulation allows estimates bootstrapped estimates within episode propagating benefits baselines along sampled trajectory estimates remain unbiased even bootstrapping estimates finally show given perfect baseline variance value estimates reduced zero experimental evaluation shows vr-mccfr brings order magnitude speedup empirical variance decreases three orders magnitude decreased variance allows first time cfr+ used sampling increasing speedup two orders magnitude	negative
devil in the details: towards accurate single and multiple human parsing	human parsing received considerable interest due wide application potentials nevertheless still unclear develop accurate human parsing system efficient elegant way paper identify several useful properties including feature resolution global context information edge details perform rigorous analyses reveal leverage benefit human parsing task advantages useful properties finally result simple yet effective context embedding edge perceiving ce2p framework single human parsing ce2p end-to-end trainable easily adopted conducting multiple human parsing benefiting superiority ce2p 1st places three human parsing tracks 2nd look person lip challenge without bells whistles achieved 56.50 miou 45.31 mean apr 33.34 app0.5 track 1 track 2 track 5 outperform state-of-the-arts 2.06 3.81 1.87 respectively hope ce2p serve solid baseline help ease future research single/multiple human parsing code made available https //github.com/liutinglt/ce2p	negative
efficient identification of approximate best configuration of training in large datasets	configuration training refers combinations feature engineering learner associated hyperparameters given set configurations large dataset randomly split training testing set study efficiently identify best configuration approximately highest testing accuracy trained training set guarantee small accuracy loss develop solution using confidence interval ci -based progressive sampling pruning strategy compared using full data find exact best configuration solution achieves two orders magnitude speedup returned top configuration identical close test accuracy	positive
supervised user ranking in signed social networks	task user ranking signed networks aiming predict potential friends enemies user attracted increasing attention numerous applications existing approaches mainly extended heuristics traditional models unsigned networks suffer two limitations 1 mainly focus global rankings thus provide effective personalized ranking results 2 relatively unrealistic assumption user treats neighbors ’ social strengths indifferently address two issues propose supervised method based random walk learn social strengths user neighbors random walk likely visits “ potential friends ” less likely visits “ potential enemies ” learn personalized social strengths optimizing particularly designed loss function oriented ranking present fast ranking method based local structure among seed node certain set candidates much simplifies proposed ranking model meanwhile maintains performance experimental results demonstrate superiority approach state-of-the-art approaches	negative
differentiated distribution recovery for neural text generation	neural language models based recurrent neural networks rnnlm significantly improved performance text generation yet quality generated text represented turing test pass rate still far satisfying researchers propose use adversarial training reinforcement learning promote quality however methods usually introduce great challenges training parameter tuning processes analysis find problem rnnlm comes usage maximum likelihood estimation mle objective function requires generated distribution precisely recover true distribution requirement favors high generation diversity restricted generation quality suitable overall quality low since high generation diversity usually indicates lot errors rather diverse good samples paper propose achieve differentiated distribution recovery ddr short key idea make optimal generation probability proportional β-th power true probability β 1. way generation quality greatly improved sacrificing diversity noises rare patterns experiments synthetic data two public text datasets show ddr method achieves flexible quality-diversity trade-off higher turing test pass rate compared baseline methods including rnnlm seqgan leakgan	positive
aligning domain-specific distribution and classifier for cross-domain classification from multiple sources	unsupervised domain adaptation uda algorithms i.e. labeled data source domains actively studied recent years algorithms theoretical results focus single-source unsupervised domain adaptation suda however practical scenario labeled data typically collected multiple diverse sources might different target domain also thus domain adapters multiple sources modeled way recent deep learning based multi-source unsupervised domain adaptation muda algorithms focus extracting common domain-invariant representations domains aligning distribution pairs source target domains common feature space however often hard extract domain-invariant representations domains muda addition methods match distributions without considering domain-specific decision boundaries classes solve problems propose new framework two alignment stages muda respectively aligns distributions pair source target domains multiple specific feature spaces also aligns outputs classifiers utilizing domainspecific decision boundaries extensive experiments demonstrate method achieve remarkable results popular benchmark datasets image classification	negative
academic reader: an interactive question answering system on academic literatures	present academic reader system read academic literatures answer relevant questions researchers academic reader leverages machine reading comprehension technique successfully applied many fields involved academic literature reading interactive platform established demonstrate functions academic reader pieces academic literature relevant questions input system outputs answers system also gather users ’ revised answers perform active learning continuously improve performance case study provided presenting performance system papers accepted kdd 2018 demonstrates system facilitates massive academic literature reading	positive
exploiting class learnability in noisy data	many domains collecting sufficient labeled training data supervised machine learning requires easily accessible noisy sources crowdsourcing services tagged web data noisy labels occur frequently data sets harvested via means sometimes resulting entire classes data learned classifiers generalize poorly real world applications argue beneficial avoid training classes entirely work aim explore classes given data set guide supervised training spend time class proportional learnability focusing training process aim improve model generalization classes strong signal end develop online algorithm works conjunction classifier training algorithm iteratively selecting training data classifier based well appears generalize class testing approach variety data sets show algorithm learns focus classes model low generalization error relative strong baselines yielding classifier good performance learnable classes	negative
dynamically identifying deep multimodal features for image privacy prediction	millions images shared online privacy concerns rise paper propose approach image privacy prediction dynamically identifying powerful features corresponding objects scene context image tags derived convolutional neural networks test image specifically approach identifies set “ competent ” features fly according test image whose privacy predicted experimental results thousands flickr images show approach predicts sensitive private content accurately models trained individual feature set object scene tags alone combination	negative
faster gradient-free proximal stochastic methods for nonconvex nonsmooth optimization	proximal gradient method playing important role solve many machine learning tasks especially nonsmooth problems however machine learning problems bandit model black-box learning problem proximal gradient method could fail explicit gradients problems difficult infeasible obtain gradient-free zeroth-order method address problems objective function values required optimization recently first zeroth-order proximal stochastic algorithm proposed solve nonconvex nonsmooth problems however convergence rate 1/√t nonconvex problems significantly slower best convergence rate t1 zerothorder stochastic algorithm iteration number fill gap paper propose class faster zeroth-order proximal stochastic methods variance reduction techniques svrg saga denoted zo-proxsvrg zo-proxsaga respectively theoretical analysis address main challenge unbiased estimate true gradient hold zerothorder case required previous theoretical analysis svrg saga moreover prove zo-proxsvrg zo-proxsaga algorithms t1 convergence rates finally experimental results verify algorithms faster convergence rate existing zeroth-order proximal stochastic algorithm	positive
election with bribed voter uncertainty: hardness and approximation algorithm	bribery election computational social choice general important problem received considerable amount attention classic bribery problem briber attacker bribes voters attempting make briber ’ designated candidate win election paper introduce novel variant bribery problem “ election bribed voter uncertainty ” bvu short accommodating uncertainty vote bribed voter may may counted uncertainty occurs either bribed voter may cast vote fear caught bribed voter indeed caught therefore vote discarded first step towards ultimately understanding addressing important problem show admit multiplicative 1 -approximation algorithm modulo standard complexity assumptions show approximation algorithm returns solution additive-ε error fpt time fixed ε	positive
lifted hinge-loss markov random fields	statistical relational learning models powerful tools combine ideas first-order logic probabilistic graphical models represent complex dependencies despite success encoding large problems compact set weighted rules performing inference models often challenging paper show effectively combine two powerful ideas scaling inference large graphical models first idea lifted inference wellstudied approach speeding inference graphical models exploiting symmetries underlying problem second idea frame maximum posteriori map inference convex optimization problem use alternating direction method multipliers admm solve problem parallel well-studied relaxation combinatorial optimization problem defined logical markov random fields gives rise hinge-loss markov random field hlmrf map inference convex optimization problem show formalism introduced coloring weighted bipartite graphs using color refinement algorithm integrated admm optimization technique take advantage sparse dependency structures hlmrfs proposed approach lifted hinge-loss markov random fields lhl-mrfs preserves structure original problem lifting solves lifted inference distributed convex optimization admm empirical evaluation real-world problems observe three times speed inference hl-mrfs	negative
comparative document summarisation via classification	thispaperconsidersextractivesummarisationinacomparative setting given two document groups e.g. separated publication time goal select small number documents representative group also maximally distinguishable groups formulate set new objective functions problem connect recent literature document summarisation interpretable machine learning data subset selection particular casting problem binary classification amongst different groups derive objectives based notion maximum mean discrepancy well simple yet effective gradient-based optimisation strategy new formulation allows scalable evaluations comparative summarisation classification task automatically via crowd-sourcing end evaluate comparative summarisation methods newly curated collection controversial news topics 13months.weobserve thatgradient-based optimisationoutperforms discrete baseline approaches 15 24 different automatic evaluation settings crowd-sourced evaluations summaries gradient optimisation elicit 7 accurate classification human workers discrete optimisation result contrasts recent literature submodular data subset selection favours discrete optimisation posit formulation comparative summarisation prove useful diverse range use cases comparing content sources authors related topics distinct view points	negative
counting and sampling from markov equivalent dags using clique trees	directed acyclic graph dag common graphical model representing causal relationships among set variables restricted using observational data structure ground truth dag identifiable markov equivalence based conditional independence relations among variables therefore number dags equivalent ground truth dag indicator causal complexity underlying structure–roughly speaking shows many interventions much additional information needed recover underlying dag paper propose new technique counting number dags markov equivalence class approach based clique tree representation chordal graphs show case bounded degree graphs proposed algorithm polynomial time demonstrate technique utilized uniform sampling markov equivalence class provides stochastic way enumerate dags equivalence class may needed finding best dag causal inference given equivalence class input also extend counting sampling method case prior knowledge underlying dag available present applications extension causal experiment design estimating causal effect joint interventions	negative
multi-matching network for multiple choice reading comprehension	multiple-choice machine reading comprehension important challenging task machine required select correct answer set candidate answers given passage question existing approaches either match extracted evidence candidate answers shallowly model passage question candidate answers single paradigm matching paper propose multi-matching network mmn models semantic relationship among passage question candidate answers multiple different paradigms matching mmn model paradigm inspired human think designed unified compose-match framework demonstrate effectiveness model evaluate mmn large-scale multiple choice machine reading comprehension dataset i.e race empirical results show proposed model achieves significant improvement compared strong baselines obtains state-of-the-art results	negative
learning how to ground a plan – partial grounding in classical planning	current classical planners successful finding nonoptimal plans even large planning instances planners rely preprocessing stage computes grounded representation task whenever grounded task big generated i.e. whenever preprocess fails instance even tackled actual planner address issue introduce partial grounding approach grounds projection task complete grounding feasible propose guiding mechanism given domain identifies parts task relevant find plan using off-the-shelf machine learning methods empirical evaluation attests approach capable solving planning instances big fully grounded	positive
hotels-50k: a global hotel recognition dataset	recognizing hotel image hotel room important human trafficking investigations images directly link victims places help verify victims trafficked traffickers might move others future recognizing hotel images challenging low image quality uncommon camera perspectives large occlusions often victim similarity objects e.g. furniture art bedding across different hotel rooms support efforts towards hotel recognition task curated dataset 1 million annotated hotel room images 50,000 hotels images include professionally captured photographs travel websites crowd-sourced images mobile application similar types images analyzed real-world investigations present baseline approach based standard network architecture collection data-augmentation approaches tuned problem domain	negative
computer generation of birds of a feather puzzles	article describe computer-aided design process generating high-quality birds feather solitaire card puzzles iteration generate puzzles via combinatorial optimization objective function solving subjectively rating puzzles compute objective puzzle features regress ratings onto features provide insight objective function improvements iterative improvement process demonstrate importance halfway solvability ratio quality puzzle design relate observations recent work tension puzzle design suggest next steps efficient puzzle generation	negative
tensorial change analysis using probabilistic tensor regression	paper proposes new method change detection analysis using tensor regression change detection setting detect changes relationship input tensor output scalar change analysis compute responsibility score individual tensor modes dimensions change detected develop new probabilistic tensor regression method viewed probabilistic generalization alternating least squares algorithm thanks probabilistic formulation derived change scores clear information-theoretic interpretation apply method semiconductor manufacturing demonstrate utility best knowledge first work change analysis based probabilistic tensor regression	positive
optimal dynamic auctions are virtual welfare maximizers	interested setting seller sells sequentially arriving items one per period via dynamic auction beginning period buyer draws private valuation item sold period valuation independent across buyers periods auction dynamic sense auction period conditional bids period previous periods subject certain appropriately defined incentive compatible individually rational conditions perhaps surprisingly revenue optimal dynamic auctions computationally hard find existing literatures aim approximate optimal auctions based solving complex dynamic programs remains largely open structural interpretability optimal dynamic auctions	negative
hierarchical macro strategy model for moba game ai	next challenge game ai lies real time strategy rts games rts games provide partially observable gaming environments agents interact one another action space much larger go mastering rts games requires strong macro strategies delicate micro level execution recently great progress made micro level execution complete solutions macro strategies still lacking paper propose novel learning-based hierarchical macro strategy model mastering moba games sub-genre rts games trained hierarchical macro strategy model agents explicitly make macro strategy decisions guide micro level execution moreover agents makes independent strategy decisions simultaneously communicating allies leveraging novel imitated crossagent communication mechanism perform comprehensive evaluations popular 5v5 multiplayer online battle arena moba game 5-ai team achieves 48 winning rate human player teams ranked top 1 player ranking system	negative
non-ergodic convergence analysis of heavy-ball algorithms	paper revisit convergence heavy-ball method present improved convergence complexity results convex setting provide first non-ergodic 1/k rate result heavy-ball algorithm constant step size coercive objective functions objective functions satisfying relaxed strongly convex condition linear convergence established weaker assumptions step size inertial parameter made existing literature extend results multi-block version algorithm cyclic stochastic update rules addition results also extended decentralized optimization ergodic analysis applicable	positive
from zero-shot learning to cold-start recommendation	zero-shot learning zsl cold-start recommendation csr two challenging problems computer vision recommender system respectively general independently investigated different communities paper however reveals zsl csr two extensions intension instance attempt predict unseen classes involve two spaces one direct feature representation supplementary description yet existing approach addresses csr zsl perspective work first time formulates csr zsl problem tailor-made zsl method proposed handle csr specifically propose lowrank linear auto-encoder llae challenges three cruxes i.e. domain shift spurious correlations computing efficiency paper llae consists two parts low-rank encoder maps user behavior user attributes symmetric decoder reconstructs user behavior user attributes extensive experiments zsl csr tasks verify proposed method win-win formulation i.e. csr handled zsl models significant performance improvement compared several conventional state-of-the-art methods consideration csr benefit zsl well	negative
multi-labeled relation extraction with attentive capsule network	disclose overlapped multiple relations sentence still keeps challenging current works terms neural models inconveniently assuming sentence explicitly mapped relation label handle multiple relations properly overlapped features relations either ignored difficult identify tackle new issue propose novel approach multi-labeled relation extraction capsule network acts considerably better current convolutional recurrent net identifying highly overlapped relations within individual sentence better cluster features precisely extract relations devise attention-based routing algorithm sliding-margin loss function embed capsule network experimental results show proposed approach indeed extract highly overlapped features achieve significant performance improvement relation extraction comparing state-of-the-art works	negative
binary classifier inspired by quantum theory	machine learning ml helps us recognize patterns raw data ml used numerous domains i.e biomedical agricultural food technology etc despite recent technological advancements still room substantial improvement prediction current ml models based classical theories probability statistics replaced quantum theory qt aim improving effectiveness ml paper propose binary classifier inspired quantum theory bciqt model outperforms state art classification terms recall every category	negative
separating wheat from chaff: joining biomedical knowledge and patient data for repurposing medications	present system jointly harnesses large-scale electronic health records data concept graph mined medical literature guide drug repurposing—the process applying known drugs new ways treat diseases study unique methods scope per scale concept graph quantity data harness 10 years nation-wide medical records 1.5 million people extract medical knowledge pubmed world ’ largest corpus online biomedical literature employ links concept graph provide causal signals prioritize candidate influences medications target diseases show results system studies drug repurposing hypertension diabetes cases present drug families identified algorithm previously unknown verify results via clinical expert opinion prospective clinical trials hypertension	positive
high dimensional clustering with r-nets	clustering fundamental task data science machine learning groups set objects way objects cluster closer clusters paper consider well-known structure so-called r-nets rigorously captures properties clustering devise algorithms improve runtime approximating r-nets high-dimensional spaces with1 2 metrics algorithms also used improve framework provides approximate solutions high dimensional distance problems using framework several important related problems also solved efficiently e.g. pproximate kth-nearest neighbor distance-approximate min-max clustering -approximate k-center clustering addition build algorithm that-approximates greedy permutations time o˜ dn+n2−α ·logφ φ spread input algorithm used -approximate k-center time complexity	positive
abstractive summarization: a survey of the state of the art	focus automatic text summarization research exhibited gradual shift extractive methods abstractive methods recent years owing part advances neural methods originally developed machine translation neural methods provide viable framework obtaining abstract representation meaning input text generating informative fluent human-like summaries paper surveys existing approaches abstractive summarization focusing recently developed neural approaches	negative
low-rank semidefinite programming for the max2sat problem	paper proposes new algorithm solving max2sat problems based combining search methods semidefinite programming approaches semidefinite programming techniques well-known theoretical tool approximating maximum satisfiability problems application traditionally limited speed randomized nature approach overcomes difficult using recent approach low-rank semidefinite programming specialized work incremental fashion suitable use exact search algorithm method used within complete incomplete solver demonstrate variety problems recent competitions experiments show approach faster sometimes orders magnitude existing state-of-the-art complete incomplete solvers representing substantial advance search methods specialized max2sat problems	positive
weakly supervised scene parsing with point-based distance metric learning	semantic scene parsing suffering fact pixellevel annotations hard collected tackle issue propose point-based distance metric learning pdml paper pdml require dense annotated masks leverages several labeled points much easier obtain guide training process concretely leverage semantic relationship among annotated points encouraging feature representations intra- intercategory points keep consistent i.e points within category similar feature representations compared different categories formulate characteristic simple distance metric loss collaborates point-wise cross-entropy loss optimize deep neural networks furthermore fully exploit limited annotations distance metric learning conducted across different training images instead simply adopting image-dependent manner conduct extensive experiments two challenging scene parsing benchmarks pascalcontext ade 20k validate effectiveness pdml competitive miou scores achieved	negative
exploring knowledge graphs in an interpretable composite approach for text entailment	recognizing textual entailment key task many semantic applications question answering text summarization information extraction among others entailment scenarios range simple syntactic variation complex semantic relationships pieces text approaches try one-size-fits-all solution usually favors scenario detriment another propose composite approach recognizing text entailment analyzes entailment pair decide whether must resolved syntactically semantically also make answer interpretable whenever entailment solved semantically explore knowledge base composed structured lexical definitions generate natural language humanlike justifications explaining semantic relationship holding pieces text besides outperforming wellestablished entailment algorithms composite approach gives important step towards explainable ai using world knowledge make semantic reasoning process explicit understandable	negative
a fuzzy set based approach for rating bias	recommender systems user uncertain preference results unexpected ratings paper makes initial attempt integrating influence user uncertain degree matrix factorization framework specifically fuzzy set like user defined membership function utilized measure degree item belonging fuzzy set furthermore enhance computational effect sparse matrix uncertain preference formulated side-information fusion experimental results three real-world datasets show proposed approach produces stable improvements compared others	negative
learning from web data using adversarial discriminative neural networks for fine-grained classification	fine-grained classification absorbed recognizing subordinate categories one field need large number labeled images expensive label images utilizing web data attractive option meet demands training data convolutional neural networks cnns especially well-labeled data enough however directly training easily obtained images often leads unsatisfactory performance due factors noisy labels conventionally addressed reducing noise level web data paper take fundamentally different view propose adversarial discriminative loss advocate representation coherence standard web data encapsulated simple scalable end-to-end trainable multi-task learning framework experiment three public datasets using large-scale web data evaluate effectiveness generalizability proposed approach extensive experiments demonstrate approach performs favorably state-of-the-art methods	negative
analyzing compositionality-sensitivity of nli models	success natural language inference nli require model understand lexical compositional semantics however adversarial evaluation find several state-of-the-art models diverse architectures over-relying former fail use latter compositionality unawareness reflected via standard evaluation current datasets show removing rnns existing models shuffling input words training induce large performance loss despite explicit removal compositional information therefore propose compositionality-sensitivity testing setup analyzes models natural examples existing datasets solved via lexical features alone i.e. bag-of-words model gives high probability one wrong label hence revealing models ’ actual compositionality awareness show setup highlights limited compositional ability current nli models also differentiates model performance based design e.g. separating shallow bag-of-words models deeper linguistically-grounded tree-based models evaluation setup important analysis tool complementing currently existing adversarial linguistically driven diagnostic evaluations exposing opportunities future work evaluating models ’ compositional understanding	negative
kvqa: knowledge-aware visual question answering	visual question answering vqa emerged important problem spanning computer vision natural language processing artificial intelligence ai conventional vqa one may ask questions image answered purely based content example given image people typical vqa question may inquire number people image recently growing interest answering questions require commonsense knowledge involving common nouns e.g. cats dogs microphones present image spite progress important problem answering questions requiring world knowledge named entities e.g. barack obama white house united nations image addressed prior research address gap paper introduce kvqa – first dataset task world knowledge-aware vqa kvqa consists 183k question-answer pairs involving 18k named entities 24k images questions dataset require multi-entity multi-relation multi-hop reasoning large knowledge graphs kg arrive answer best knowledge kvqa largest dataset exploring vqa kg also provide baseline performances using state-of-the-art methods kvqa	negative
anomaly detection using autoencoders in high performance computing systems	anomaly detection supercomputers difficult problem due big scale systems high number components current state art automated anomaly detection employs machine learning methods statistical regression models supervised fashion meaning detection tool trained distinguish among fixed set behaviour classes healthy unhealthy states	negative
ensemble machine learning for estimating fetal weight at varying gestational age	obstetric ultrasound examination physiological parameters mainly used estimate fetal weight pregnancy baby weight labour monitor fetal growth reduce prenatal morbidity mortality however problem ultrasound estimation fetal weight subject populations ’ difference strict operating requirements sonographers poor access ultrasound low-resource areas inaccurate estimations may lead negative perinatal outcomes consider machine learning provide accurate estimation obstetricians alongside traditional clinical practices well efficient effective support tool pregnant women self-monitoring present robust methodology using data set comprising 4,212 intrapartum recordings cubic spline function used fit curves several key characteristics extracted ultrasound reports number simple powerful machine learning algorithms trained performance evaluated real test data also propose novel evaluation performance index called intersectionover-union lou study results encouraging using ensemble model consisting random forest xgboost lightgbm algorithms experimental results show lou 0.64 predicted range fetal weight gestational age ensemble model ultrasound comparing ultrasound method estimation accuracy improved 12 mean relative error reduced 3	negative
searching with consistent prioritization for multi-agent path finding	study prioritized planning multi-agent path finding mapf existing prioritized mapf algorithms depend rule-of-thumb heuristics random assignment determine fixed total priority ordering agents priori instead explore space possible partial priority orderings part novel systematic conflict-driven combinatorial search framework variety empirical comparisons demonstrate state-of-the-art solution qualities success rates often similar runtimes existing algorithms also develop new theoretical results explore limitations prioritized planning terms completeness optimality first time	negative
knowledge distillation with adversarial samples supporting decision boundary	many recent works knowledge distillation provided ways transfer knowledge trained network improving learning process new one finding good technique knowledge distillation still open problem paper provide new perspective based decision boundary one important component classifier generalization performance classifier closely related adequacy decision boundary good classifier bears good decision boundary therefore transferring information closely related decision boundary good attempt knowledge distillation realize goal utilize adversarial attack discover samples supporting decision boundary based idea transfer accurate information decision boundary proposed algorithm trains student classifier based adversarial samples supporting decision boundary experiments show proposed method indeed improves knowledge distillation achieves state-of-the-arts performance	negative
stepping stones to inductive synthesis of low-level looping programs	inductive program synthesis input/output examples provide opportunity automatically create programs scratch without presupposing algorithmic form solution induction general programs loops opposed loop-free programs synthesis domain-specific languages state art level introductory programming assignments problems require algorithmic subtlety fast sorting remained reach without benefit significant problem-specific background knowledge key challenge identify cues available guide search towards correct looping programs present makespeare simple delayed-acceptance hillclimbing method synthesizes low-level looping programs input/output examples search delayed acceptance bypasses small gains identify significantly-improved stepping stone programs tend generalize enable progress method performs well set established benchmarks succeeds previously unsolved “ collatz numbers ” program synthesis problem additional benchmarks include problem rapidly sorting integer arrays observe emergence comb sort shell sort variant empirically fast makespeare also synthesized record-setting program one puzzles tis100 assembly language programming game	negative
coala: a neural coverage-based approach for long answer selection with small data	current neural network based community question answering cqa systems fall short 1 properly handling long answers common cqa 2 performing small data conditions large amount training data unavailable—i.e. domains english even huge number datasets languages 3 benefiting syntactic information model—e.g. differentiate identical lexemes different syntactic roles paper propose coala answer selection approach selects appropriate long answers due effective comparison question-answer aspects b ability generalize small number training examples c makes use information syntactic roles words show approach outperforms existing answer selection models large margin six cqa datasets different domains furthermore report best results passage retrieval benchmark wikipassageqa	positive
automatic construction of parallel portfolios via explicit instance grouping	exploiting parallelism becoming important designing efficient solvers computationally hard problems however manually building parallel solvers typically requires considerable domain knowledge plenty human effort alternative automatic construction parallel portfolios acpp aims automatically building effective parallel portfolios based given problem instance set given rich configuration space one promising way solve acpp problem explicitly group instances different subsets promote component solver handle paper investigates solving acpp perspective especially studies obtain good instance grouping experimental results two widely studied problem domains boolean satisfiability problems sat traveling salesman problems tsp showed parallel portfolios constructed proposed method could achieve consistently superior performances ones constructed state-of-the-art acpp methods could even rival sophisticated hand-designed parallel solvers	negative
off-policy deep reinforcement learning by bootstrapping the covariate shift	paper revisit method off-policy corrections reinforcement learning cop-td pioneered hallak et al 2017 method online updates value function reweighted avoid divergence issues typical off-policy learning hallak et al. ’ solution appealing easily transferred nonlinear function approximation first requires projection step onto probability simplex second even though operator describing expected behavior off-policy learning algorithm convergent known contraction mapping hence may unstable practice address two issues introducing discount factor cop-td analyze behavior discounted cop-td find better behaved theoretical perspective also propose alternative soft normalization penalty minimized online obviates need explicit projection step complement analysis empirical evaluation two techniques off-policy setting game pong atari domain find discounted cop-td better behaved practice soft normalization penalty finally perform extensive evaluation discounted cop-td 5 games atari domain find performance gains approach	positive
atomic: an atlas of machine commonsense for if-then reasoning	present atomic atlas everyday commonsense reasoning organized 877k textual descriptions inferential knowledge compared existing resources center around taxonomic knowledge atomic focuses inferential knowledge organized typed if-then relations variables e.g. “ x pays compliment likely return compliment ” propose nine if-then relation types distinguish causes vs. effects agents vs. themes voluntary vs. involuntary events actions vs. mental states generatively training rich inferential knowledge described atomic show neural models acquire simple commonsense capabilities reason previously unseen events experimental results demonstrate multitask models incorporate hierarchical structure if-then relation types lead accurate inference compared models trained isolation measured automatic human evaluation	positive
improving full-body pose estimation from a small sensor set using artificial neural networks and a kalman filter	previous research shown estimating full-body poses minimal sensor set using trained ann without explicitly enforcing time coherence resulted output pose sequences occasionally show undesired jitter mitigate effect propose improve ann output combining state prediction using kalman filter preliminary results promising jitter effects diminished however overall error decrease substantially	positive
interactive semantic parsing for if-then recipes via hierarchical reinforcement learning	given text description existing semantic parsers synthesize program one shot however quite challenging produce correct program solely based description reality often ambiguous incomplete paper investigate interactive semantic parsing agent ask user clarification questions resolve ambiguities via multi-turn dialogue important type programs called “ if-then recipes. ” develop hierarchical reinforcement learning hrl based agent significantly improves parsing performance minimal questions user results simulation human evaluation show agent substantially outperforms non-interactive semantic parsers rule-based agents.1	positive
clustergan: latent space clustering in generative adversarial networks	generative adversarial networks gans obtained remarkable success many unsupervised learning tasks unarguably clustering important unsupervised learning problem one potentially exploit latent-space back-projection gans cluster demonstrate cluster structure retained gan latent space paper propose clustergan new mechanism clustering using gans sampling latent variables mixture one-hot encoded variables continuous latent variables coupled inverse network projects data latent space trained jointly clustering specific loss able achieve clustering latent space results show remarkable phenomenon gans preserve latent space interpolation across categories even though discriminator never exposed vectors compare results various clustering baselines demonstrate superior performance synthetic real datasets	positive
weighted oblique decision trees	decision trees attracted much attention past decades previous decision trees include axis-parallel oblique decision trees try find best splits via exhaustive search heuristic algorithms iteration oblique decision trees generally simplify tree structure take better performance always accompanied higher computation well initialization best axis-parallel splits work presents weighted oblique decision tree wodt based continuous optimization random initialization consider different weights instance child nodes internal nodes obtain split optimizing continuous differentiable objective function weighted information entropy extensive experiments show effectiveness proposed algorithm	negative
forecasting intra-hour imbalances in electric power systems	keeping electricity production balance actual demand becoming difficult expensive task spite involvement experienced human operators due increasing complexity electric power grid system intermittent renewable production one contributors beforehand information occurring imbalance help transmission system operator adjust production plans thus ensure high security supply reducing use costly balancing reserves consequently reduce undesirable fluctuations 50 hz power system frequency paper introduce relatively new problem intra-hour imbalance forecasting transmission system operator tso focus use case norwegian tso statnett present complementary imbalance forecasting tool able support tso determining trend future imbalances show potential proactively alleviate imbalances higher accuracy compared contemporary solution	negative
the curse of concentration in robust learning: evasion and poisoning attacks from concentration of measure	many modern machine learning classifiers shown vulnerable adversarial perturbations instances despite massive amount work focusing making classifiers robust task seems quite challenging work theoretical study investigate adversarial risk robustness classifiers draw connection well-known phenomenon “ concentration measure ” metric measure spaces show metric probability space test instance concentrated classifier initial constant error inherently vulnerable adversarial perturbations	negative
probabilistic-logic bots for efficient evaluation of business rules using conversational interfaces	present approach designing conversational interfaces chatbots users interact determine whether business rule applies context possessing uncertainty point view chatbot value input facts approach relies bayesian network models bring together business rule ’ logical deterministic aspects probabilistic components common framework probabilistic-logic bots pl-bots evaluate business rules iteratively prompting users provide values unknown facts order facts solicited dynamic depends known facts chosen using mutual information heuristic minimize number interactions user created web-based content creation editing tool quickly enables subject matter experts create validate pl-bots minimal training without requiring deep understanding logic probability date domain experts well-known insurance company successfully created deployed 80 plbots help insurance agents determine customer eligibility policy discounts endorsements	positive
enriching non-parametric bidirectional search algorithms	nbs non-parametric bidirectional search algorithm proven expand twice number node expansions required verify optimality solution introduce new variants nbs aimed finding optimal solutions introduce algorithmic framework includes nbs special case finally introduce dvcbs new algorithm framework aims reduce number expansions unlike nbs dvcbs worst-case bound guarantees practice outperforms nbs verifying optimality solutions	negative
community focusing: yet another query-dependent community detection	major kind query-dependent community detection community search finds densely connected subgraph containing set query nodes density major consideration community search methods community search often find dense subgraph many vertices far query nodes related query nodes motivated new problem called community focusing cf studied finds community members close densely connected query nodes distance-sensitive dense subgraph structure called β-attention-core proposed remove vertices loosely connected far query nodes combinational density designed guarantee density subgraph cf formalized finding subgraph largest combinational density among β-attention-core subgraphs containing query nodes largest β. thereafter effective methods devised cf furthermore speed-up strategy developed make methods scalable large networks extensive experimental results real synthetic networks demonstrate performance methods	negative
learning diffusions without timestamps	learn underlying parent-child influence relationships nodes diffusion network existing approaches require timestamps pinpoint exact time node infections occur historical diffusion processes many real-world diffusion processes like spread epidemics monitoring infection temporal information often expensive difficult work study carry diffusion network inference without infection timestamps using final infection statuses nodes historical diffusion process readily accessible practice main result probabilistic model find node appropriate number probable parent nodes likely generated historical infection results node extensive experiments synthetic real-world networks conducted results verify effectiveness efficiency approach	negative
distant supervision for relation extraction with linear attenuation simulation and non-iid relevance embedding	distant supervision relation extraction efficient method reduce labor costs widely used seek novel relational facts large corpora identified multi-instance multi-label problem however existing distant supervision methods suffer selecting important words sentence extracting valid sentences bag towards end propose novel approach address problems paper firstly propose linear attenuation simulation reflect importance words sentence respect distances entities words secondly propose non-independent identically distributed non-iid relevance embedding capture relevance sentences bag method capture complex information words hidden relations also express mutual information instances bag extensive experiments benchmark dataset well-validated effectiveness proposed method	negative
a generalized idiom usage recognition model based on semantic compatibility	many idiomatic expressions used figuratively literally depending context particular challenge automatic idiom usage recognition idioms nature idiosyncratic usages therefore previous work idiom usage recognition mainly adopted “ per idiom ” classifier approach i.e. classifier needs trained separately idiomatic expression interest often aid annotated training examples paper presents transferred learning approach developing generalized model recognize whether idiom used figuratively literally work based observation idioms taken literally would somehow semantically odds context therefore quantified notion semantic compatibility may help discern intended usage arbitrary idiom propose novel semantic compatibility model adapting training continuous bag-of-words cbow model purpose idiom usage recognition need annotate idiom usage examples training perform evaluative experiments two corpora results show proposed generalized model achieves competitive results compared state of-the-art per-idiom models	negative
a probabilistic derivation of lasso and l12-norm feature selections	lasso ℓ2,1-norm based feature selection achieved success many application areas paper first derive lasso ℓ1,2-norm feature selection probabilistic framework provides independent point view usual sparse coding point view propose feature selection approach based probability-derived ℓ1,2-norm point inflexibility standard feature selection feature selected different classes enforced exactly using widely used ℓ2,1-norm enforces joint sparsity across data instances using probabilityderived ℓ1,2-norm feature selection allowing certain flexibility selected features exactly classes resulting features lead better classification six benchmark datasets	negative
walrasian dynamics in multi-unit markets	multi-unit market seller brings multiple units good tries sell set buyers monetary endowments walrasian equilibrium always exist model natural relaxations concept retain desirable fairness properties exist study dynamics walrasian envy-free pricing mechanisms environment showing pricing mechanism best response dynamic starting truth-telling converges pure nash equilibrium small loss revenue welfare moreover generalize bounds capture reasonable nash equilibria large class monotone pricing mechanisms also identify natural mechanism selects minimum walrasian envy-free price n=2 buyers best response dynamic converges starting profile conjecture convergence mechanism number buyers provide simulation results support conjecture	negative
deepfuzz: automatic generation of syntax valid c programs for fuzz testing	compilers among fundamental programming tools building software however production compilers remain buggy fuzz testing often leveraged newlygenerated mutated inputs order find new bugs security vulnerabilities paper propose grammarbased fuzzing tool called deepfuzz based generative sequence-to-sequence model deepfuzz automatically continuously generates well-formed c programs use set new c programs fuzz off-the-shelf c compilers e.g. gcc clang/llvm present detailed case study analyze success rate coverage improvement generated c programs fuzz testing analyze performance deepfuzz three types sampling methods well three types generation strategies consequently deepfuzz improved testing efficacy regards line function branch coverage preliminary study found reported 8 bugs gcc actively addressed developers	positive
tet-gan: text effects transfer via stylization and destylization	text effects transfer technology automatically makes text dramatically impressive however previous style transfer methods either study model general style handle highly-structured text effects along glyph require manual design subtle matching criteria text effects paper focus use powerful representation abilities deep neural features text effects transfer purpose propose novel texture effects transfer gan tet-gan consists stylization subnetwork destylization subnetwork key idea train network accomplish objective style transfer style removal learn disentangle recombine content style features text effects images support training network propose new text effects dataset much 64 professionally designed styles 837 characters show disentangled feature representations enable us transfer remove styles arbitrary glyphs using one network furthermore flexible network design empowers tet-gan efficiently extend new text style via oneshot learning one example required demonstrate superiority proposed method generating high-quality stylized text state-of-the-art methods	negative
proppy: a system to unmask propaganda in online news	present proppy first publicly available real-world real-time propaganda detection system online news aims raising awareness thus potentially limiting impact propaganda helping fight disinformation system constantly monitors number news sources deduplicates clusters news events organizes articles event basis likelihood contain propagandistic content system trained known propaganda sources using variety stylistic features evaluation results standard dataset show stateof-the-art results propaganda detection	positive
session-based recommendation with graph neural networks	problem session-based recommendation aims predict user actions based anonymous sessions previous methods model session sequence estimate user representations besides item representations make recommendations though achieved promising results insufficient obtain accurate user vectors sessions neglect complex transitions items obtain accurate item embedding take complex transitions items account propose novel method i.e session-based recommendation graph neural networks sr-gnn brevity proposed method session sequences modeled graphstructured data based session graph gnn capture complex transitions items difficult revealed previous conventional sequential methods session represented composition global preference current interest session using attention network extensive experiments conducted two real datasets show sr-gnn evidently outperforms state-of-the-art session-based recommendation methods consistently	positive
expressive real-time intersection scheduling	traffic congestion widespread annoyance throughout global metropolitan areas causes increases travel time increases emissions inefficient usage gasoline driver frustration inefficient signal patterns traffic lights one major cause congestion intersection scheduling strategies make real-time decisions extend end green signal based real-time traffic data offer one opportunity reduce congestion negative impacts research proposes expressive real-time intersection scheduling eris eris decentralized schedule-driven control method makes decision every second based current traffic conditions reduce congestion	negative
sensitivity analysis of deep neural networks	deep neural networks dnns achieved superior performance various prediction tasks vulnerable adversarial examples perturbations therefore crucial measure sensitivity dnns various forms perturbations real applications introduce novel perturbation manifold associated influence measure quantify effects various perturbations dnn classifiers perturbations include various external internal perturbations input samples network parameters proposed measure motivated information geometry provides desirable invariance properties demonstrate influence measure useful four model building tasks detecting potential ‘ outliers ’ analyzing sensitivity model architectures comparing network sensitivity training test sets locating vulnerable areas experiments show reasonably good performance proposed measure popular dnn models resnet50 densenet121 cifar10 mnist datasets	negative
on structured argumentation with conditional preferences	study defeasible knowledge bases conditional preferences dkb dkb consists set undisputed facts rule-based system contains different types rules strict defeasible preference major challenge defining semantics dkb lies determining conditional preferences interact attack relations represented rebuts undercuts arguments	negative
better fine-tuning via instance weighting for text classification	transfer learning deep neural networks achieved great success many text classification applications simple yet effective transfer learning method fine-tune pretrained model parameters previous fine-tuning works mainly focus pre-training stage investigate pretrain set parameters help target task paper propose instance weighting based finetuning iw-fit method revises fine-tuning stage improve final performance target domain iw-fit adjusts instance weights fine-tuning epoch dynamically accomplish two goals 1 identify learn specific knowledge target domain effectively 2 well preserve shared knowledge source target domains designed instance weighting metrics used iw-fit model-agnostic easy implement general dnn-based classifiers experimental results show iw-fit consistently improve classification accuracy target domain	negative
an adaptive framework for conversational question answering	conversational question answering coqa humans propose series questions satisfy information needs based preliminary analysis two major types questions namely verification questions knowledgeseeking questions first one verify existing facts latter obtain new knowledge specific object two types questions differ significantly answering ways however existing methods usually treat uniformly may easily biased dominant type questions obtain inferior overall performance work propose adaptive framework handle two types questions different ways based characteristics conduct experiments recently released coqa benchmark dataset results demonstrate method outperforms state-of-the-art baseline methods	positive
unsupervised learning helps supervised neural word segmentation	exploiting unlabeled data performance improvement chinese word segmentation work makes first attempt exploring adding unsupervised segmentation information neural supervised segmenter survey various effective strategies including extending character embedding augmenting word score applying multi-task learning leveraging unsupervised information derived abundant unlabeled data experiments standard data sets show explored strategies indeed improve recall rate out-of-vocabulary words thus boost segmentation accuracy moreover model enhanced proposed methods outperforms state-of-theart models closed test shows promising improvement trend adopting three different strategies help large unlabeled data set thorough empirical study eventually verifies proposed approach outperforms widelyused pre-training approach terms effectively making use freely abundant unlabeled data	negative
erevise: using natural language processing to provide formative feedback on text evidence usage in student writing	writing good essay typically involves students revising initial paper draft receiving feedback present erevise web-based writing revising environment uses natural language processing features generated rubricbased essay scoring trigger formative feedback messages regarding students ’ use evidence response-to-text writing helping students understand criteria using text evidence writing erevise empowers students better revise paper drafts pilot deployment erevise 7 classrooms spanning grades 5 6 quality text evidence usage writing improved students received formative feedback engaged paper revision	negative
forming probably stable communities with limited interactions	community needs partitioned disjoint groups community member underlying preference groups would want member interested finding stable community structure one subset members wants deviate current structure model setting hedonic game players connected underlying interaction network consider joining groups connected subgraphs underlying graph analyze relation network structure one ’ capability infer statistically stable also known pac stable player partitions data show interaction network forest one efficiently infer pac stable coalition structures furthermore underlying interaction graph forest efficient pac stabilizability longer achievable thus results completely characterize one leverage underlying graph structure order compute pac stable outcomes hedonic games finally given unknown underlying interaction network show np-hard decide whether exists forest consistent data samples network	positive
deep robust unsupervised multi-modal network	real-world applications data often multiple modalities many multi-modal learning approaches proposed integrating information different sources previous multi-modal methods utilize modal consistency reduce complexity learning problem therefore modal completeness needs guaranteed however due data collection failures self-deficiencies various reasons multi-modal instances often incomplete real applications inconsistent anomalies even complete instances jointly result inconsistent problem degenerate multi-modal feature learning performance finally affect generalization abilities different tasks paper propose novel deep robust unsupervised multi-modal network structure drumn solving real problem within unified framework proposed drumn utilize extrinsic heterogeneous information unlabeled data insufficiency caused incompleteness hand inconsistent anomaly issue solved adaptive weighted estimation rather adjusting complex thresholds drumn extract discriminative feature representations modality experiments real-world multimodal datasets successfully validate effectiveness proposed method	negative
deliberate attention networks for image captioning	daily life deliberation common behavior human improve refine work e.g. writing reading drawing date encoder-decoder framework attention mechanisms achieved great progress image captioning however framework essential one-pass forward process encoding hidden states attending visual features lacks deliberation action learned hidden states visual attention directly used predict final captions without polishing paper present novel deliberate residual attention network namely da image captioning first-pass residual-based attention layer prepares hidden states visual attention generating preliminary version captions second-pass deliberate residual-based attention layer refines since second-pass based rough global features captured hidden layer visual attention first-pass da potential generate better sentences equip da discriminative loss reinforcement learning disambiguate image/caption pairs reduce exposure bias model improves state-of-the-arts mscoco dataset reaches 37.5 belu-4 28.5 meteor 125.6 cider also outperforms the-state-ofthe-arts 25.1 bleu-4 20.4 meteor 53.1 cider 29.4 bleu-4 23.0 meteor 66.6 flickr30k dataset	negative
model-based diagnosis for cyber-physical production systems based on machine learning and residual-based diagnosis models	paper introduces novel approach model-based diagnosis mbd hybrid technical systems unlike existing approaches normally rely qualitative diagnosis models expressed logic approach applies learned quantitative model used derive residuals based residuals diagnosis model generated used root cause identification new solution several advantages easy integration new machine learning algorithms mbd seamless integration qualitative models significant speed-up diagnosis runtime paper hand formally defines new approach outlines advantages drawbacks presents evaluation real-world use cases	positive
private model compression via knowledge distillation	soaring demand intelligent mobile applications calls deploying powerful deep neural networks dnns mobile devices however outstanding performance dnns notoriously relies increasingly complex models turn associated increase computational expense far surpassing mobile devices ’ capacity worse app service providers need collect utilize large volume users ’ data contain sensitive information build sophisticated dnn models directly deploying models public mobile devices presents prohibitive privacy risk benefit on-device deep learning without capacity privacy concerns design private model compression framework rona following knowledge distillation paradigm jointly use hint learning distillation learning self learning train compact fast neural network knowledge distilled cumbersome model adaptively bounded carefully perturbed enforce differential privacy propose elegant query sample selection method reduce number queries control privacy loss series empirical evaluations well implementation android mobile device show rona compress cumbersome models efficiently also provide strong privacy guarantee example svhn meaningful 9.83,10−6 -differential privacy guaranteed compact model trained rona obtain 20× compression ratio 19× speed-up merely 0.97 accuracy loss	negative
a two-stream mutual attention network for semi-supervised biomedical segmentation with noisy labels	learning-based methods suffer deficiency clean annotations especially biomedical segmentation although many semi-supervised methods proposed provide extra training data automatically generated labels usually noisy retrain models effectively paper propose two-stream mutual attention network tsman weakens influence back-propagated gradients caused incorrect labels thereby rendering network robust unclean data proposed tsman consists two sub-networks connected three types attention models different layers target attention model indicate potentially incorrect gradients certain layer sub-networks analyzing inferred features using input order achieve purpose attention models designed based propagation analysis noisy gradients different layers allows attention models effectively discover incorrect labels weaken influence parameter updating process exchanging multi-level features within two-stream architecture effects noisy labels sub-network reduced decreasing noisy gradients furthermore hierarchical distillation developed provide reliable pseudo labels unlabelded data boosts performance tsman experiments using hvsmr 2016 brats 2015 benchmarks demonstrate semi-supervised learning framework surpasses state-of-the-art fully-supervised results	negative
learning resolution-invariant deep representations for person re-identification	person re-identification re-id solves task matching images across cameras among research topics vision community since query images real-world scenarios might suffer resolution loss solve resolution mismatch problem person re-id becomes practical problem instead applying separate image super-resolution models propose novel network architecture resolution adaptation re-identification network rain solve cross-resolution person re-id advancing strategy adversarial learning aim extracting resolution-invariant representations re-id proposed model learned end-to-end training fashion experiments confirm use model recognize low-resolution query images even resolution seen training moreover extension model semi-supervised re-id confirms scalability proposed method real-world scenarios applications	positive
learning phenotypes and dynamic patient representations via rnn regularized collective non-negative tensor factorization	non-negative tensor factorization ntf shown effective discover clinically relevant interpretable phenotypes electronic health records ehr existing ntf based computational phenotyping models aggregate data observation window resulting learned phenotypes mixtures disease states appearing different times argue separating clinical events happening different times input tensor temporal dynamics disease progression within observation window could modeled learned phenotypes correspond specific disease states yet construct tensor data samples different temporal lengths properly capture temporal relationship specific individual data sample remains open challenge paper propose novel collective non-negative tensor factorization cntf model patient represented temporal tensor temporal tensors factorized collectively phenotype definitions shared across patients proposed cntf model also flexible incorporate non-temporal data modality rnn-based temporal regularization validate proposed model using mimic-iii dataset empirical results show learned phenotypes clinically interpretable moreover proposed cntf model outperforms state-of-the-art computational phenotyping models mortality prediction task	negative
read + verify: machine reading comprehension with unanswerable questions	machine reading comprehension unanswerable questions aims abstain answering answer inferred addition extract answers previous works usually predict additional “ no-answer ” probability detect unanswerable cases however fail validate answerability question verifying legitimacy predicted answer address problem propose novel read-then-verify system utilizes neural reader extract candidate answers produce no-answer probabilities also leverages answer verifier decide whether predicted answer entailed input snippets moreover introduce two auxiliary losses help reader better handle answer extraction well no-answer detection investigate three different architectures answer verifier experiments squad 2.0 dataset show system obtains score 74.2 f1 test set achieving state-of-the-art results time submission aug. 28th 2018	negative
enriching word embeddings with a regressor instead of labeled corpora	propose novel method enriching word-embeddings without need labeled corpus instead show relying regressor – trained small lexicon predict pseudo-labels – significantly improves performance current techniques rely human-derived sentence-level labels entire corpora approach enables enrichment corpora labels wikipedia exploring utility general approach sentiment non-sentiment-focused tasks show enriching embeddings twitter wikipedia-based embeddings provide notable improvements performance binary sentiment classification semeval tasks embedding analogy task document classification importantly approach notably better generalizable state-of-the-art approaches enriching labeled unlabeled corpora	positive
enhanced random forest algorithms for partially monotone ordinal classification	one factors hindering use classification models decision making predictions may contradict expectations domains finance medicine ability include knowledge monotone nondecreasing relationships sought increase accuracy user satisfaction one successful classifiers attempts made random forest ideally solution would maximise accuracy b low complexity scale well c guarantee global monotonicity cater multi-class paper first reviews state-of-theart literature statistical libraries identifies opportunities improvement new rule-based method proposed maximal accuracy variant faster approximate variant simulated real datasets used perform comprehensive ordinal classification benchmarking monotone forest literature proposed approaches shown reduce bias induced monotonisation thereby improve accuracy	negative
running time analysis of moea/d with crossover on discrete optimization problem	decomposition-based multiobjective evolutionary algorithms moeas class popular methods solving multiobjective optimization problems mops widely studied numerical experiments successfully applied practice however know little algorithms theoretical aspect paper present running time analysis simple moea crossover based moea/d framework moea/d-c four discrete optimization problems rigorous theoretical analysis shows moea/d-c obtain set pareto optimal solutions cover pareto front problems expected running time apparently lower one without crossover moreover moea/d-c needs decompose mop scalar optimization subproblems according several simple weight vectors result suggests use crossover decomposition-based moea simplify setting weight vector different problems make algorithm efficient study theoretically explains decomposition-based moeas work well computational experiments provides insights design moeas mops future research	negative
when do envy-free allocations exist?	consider fair division setting indivisible items allocated among n agents agents additive utilities agents ’ utilities individual items independently sampled distribution previous work shown envy-free allocation likely exist ω n log n n n left open question determining phase transition non-existence existence occurs show surprisingly fact universal point transition— instead transition governed divisibility relation n. one hand divisible n envy-free allocation exists high probability long ≥ 2n hand “ almost ” divisible envy-free allocation unlikely exist even θ n log n /log log n	positive
block: bilinear superdiagonal fusion for visual question answering and visual relationship detection	multimodal representation learning gaining interest within deep learning community bilinear models provide interesting framework find subtle combination modalities number parameters grows quadratically input dimensions making practical implementation within classical deep learning pipelines challenging paper introduce block new multimodal fusion based block-superdiagonal tensor decomposition leverages notion block-term ranks generalizes concepts rank mode ranks tensors already used multimodal fusion allows define new ways optimizing tradeoff expressiveness complexity fusion model able represent fine interactions modalities maintaining powerful mono-modal representations demonstrate practical interest fusion model using block two challenging tasks visual question answering vqa visual relationship detection vrd design end-to-end learnable architectures representing relevant interactions modalities extensive experiments show block compares favorably respect state-of-the-art multimodal fusion models vqa vrd tasks code available https //github.com/cadene/block.bootstrap.pytorch	negative
exploring human-like reading strategy for abstractive text summarization	recent artificial intelligence studies witnessed great interest abstractive text summarization although remarkable progress made deep neural network based methods generating plausible high-quality abstractive summaries remains challenging task human-like reading strategy rarely explored abstractive text summarization however able improve effectiveness summarization considering process reading comprehension logical thinking motivated humanlike reading strategy follows hierarchical routine propose novel hybrid learning model abstractive text summarization hats model consists three major components knowledge-based attention network multitask encoder-decoder network generative adversarial network consistent different stages human-like reading strategy verify effectiveness hats conduct extensive experiments two real-life datasets cnn/daily mail gigaword datasets experimental results demonstrate hats achieves impressive results datasets	negative
diversity-driven extensible hierarchical reinforcement learning	hierarchical reinforcement learning hrl recently shown promising advances speeding learning improving exploration discovering intertask transferable skills recent works focus hrl two levels i.e. master policy manipulates subpolicies turn manipulate primitive actions however hrl multiple levels usually needed many real-world scenarios whose ultimate goals highly abstract actions primitive therefore paper propose diversitydriven extensible hrl dehrl extensible scalable framework built learned levelwise realize hrl multiple levels dehrl follows popular assumption diverse subpolicies useful i.e. subpolicies believed useful diverse however existing implementations diversity assumption usually drawbacks makes inapplicable hrl multiple levels consequently propose novel diversity-driven solution achieve assumption dehrl experimental studies evaluate dehrl nine baselines four perspectives two domains results show dehrl outperforms state-of-the-art baselines four aspects	negative
ipomdp-net: a deep neural network for partially observable multi-agent planning using interactive pomdps	paper introduces ipomdp-net neural network architecture multi-agent planning partial observability embeds interactive partially observable markov decision process i-pomdp model qmdp planning algorithm solves model neural network architecture ipomdp-net fully differentiable allows end-to-end training learning phase train ipomdp-net various fixed randomly generated environments reinforcement learning setting assuming observable reinforcements unknown randomly initialized model functions planning phase test trained network new unseen variants environments planning setting using trained model plan without reinforcements empirical results show model-based ipomdp-net outperforms state-of-the-art modelfree network generalizes better larger unseen environments approach provides general neural computing architecture multi-agent planning using i-pomdps suggests multi-agent setting model agents benefits decision-making resulting policy higher quality better generalizability	positive
explicitly imposing constraints in deep networks via conditional gradients gives improved generalization and faster convergence	number results recently demonstrated benefits incorporating various constraints training deep architectures vision machine learning advantages range guarantees statistical generalization better accuracy compression support general constraints within widely used libraries remains scarce broader deployment within many applications benefit remains under-explored part reason stochastic gradient descent sgd workhorse training deep neural networks natively deal constraints global scope well paper revisit classical first order scheme numerical optimization conditional gradients cg thus far limited applicability training deep models show via rigorous analysis various constraints naturally handled modifications algorithm provide convergence guarantees show suite immediate benefits possible — training resnets fewer layers better accuracy simply substituting version cg faster training gans 50 fewer epochs image inpainting applications provably better generalization guarantees using efficiently implementable forms recently proposed regularizers	negative
gradient-based inference for networks with output constraints	practitioners apply neural networks increasingly complex problems natural language processing syntactic parsing semantic role labeling rich output structures many structured-prediction problems require deterministic constraints output values example sequence-to-sequence syntactic parsing require sequential outputs encode valid trees hidden units might capture properties network always able learn constraints training data alone practitioners must resort post-processing paper present inference method neural networks enforces deterministic constraints outputs without performing rule-based post-processing expensive discrete search instead spirit gradient-based training enforce constraints gradient-based inference gbi input test-time nudge continuous model weights network ’ unconstrained inference procedure generates output satisfies constraints study efficacy gbi three tasks hard constraints semantic role labeling syntactic parsing sequence transduction case algorithm satisfies constraints improves accuracy even underlying network stateof-the-art	positive
abstracting causal models	consider sequence successively restrictive definitions abstraction causal models starting notion introduced rubenstein et al 2017 called exact transformation applies probabilistic causal models moving notion uniform transformation applies deterministic causal models allow differences hidden “ right ” choice distribution abstraction interventions interest determined map low-level states high-level states strong abstraction takes seriously potential interventions model allowed interventions show procedures combining micro-variables macro-variables instances notion strong abstraction examples considered rubenstein et al	positive
measurement maximizing adaptive sampling with risk bounding functions	autonomous exploration mobile agent must adapt new measurements seek high reward disturbances cause probability collision must traded expected reward paper considers autonomous agent tasked maximizing measurements gaussian process subject unbounded disturbances seek adaptive policy maximum allowed probability failure constrained function expected reward policy found using extension monte carlo tree search mcts bounds probability failure apply mcts sequence approximating problems allows constraint satisfying actions found anytime manner innovation lies defining approximating problems replanning strategy probability failure constraint guaranteed satisfied true policy approach need plan measurements explicitly constrain planning based measurements observed best knowledge approach first enforce probability failure constraints adaptive sampling experiments real bathymetric data simulated measurements show algorithm allows agent take dangerous actions reward justifies risk verify monte carlo simulations failure bounds satisfied	negative
paraphrase diversification using counterfactual debiasing	problem generating set diverse paraphrase sentences 1 compromising original meaning original sentence 2 imposing diversity various semantic aspects lexical syntactic structure examined existing work paraphrase generation focused former latter trained fixed style transfer transferring positive negative sentiments even cost losing semantics work consider style transfer means imposing diversity paraphrasing correctness constraint target sentence must remain paraphrase original sentence however goal maximize diversity set k generated paraphrases denoted diversified paraphrase dp problem key contribution deciding style guidance generation towards direction increasing diversity output respect generated previously pre-materializing training data style decisions impractical train biased data debiasing guidance compared state-of-the-art methods proposed model generate diverse yet semantically consistent paraphrase sentences model trained mscoco dataset achieves highest embedding scores .94/.95/.86 similar state-of-the-art results lower mbleu score diverse 8.73	positive
combining deep learning and qualitative spatial reasoning to learn complex structures from sparse examples with noise	many modern machine learning approaches require vast amounts training data learn new concepts conversely human learning often requires examples—sometimes one—from learner abstract structural concepts present novel approach introducing new spatial structures ai agent combining deep learning qualitative spatial relations various heuristic search algorithms agent extracts spatial relations sparse set noisy examples block-based structures trains convolutional sequential models relation sets create novel examples similar structures agent begins placing blocks virtual table uses cnn predict similar complete example structure placement lstm predict likely set remaining moves needed complete recommends one using heuristic search verify agent learned concept observing virtual block-building activities wherein ranks potential subsequent action toward building learned concept empirically assess approach human participants ’ ratings block structures initial results qualitative evaluations structures generated trained agent show generalized concepts training data heuristics perform best within search space might improve learning execution	negative
learning personalized end-to-end goal-oriented dialog	existing works dialog systems consider conversation content neglecting personality user bot interacting begets several unsolved issues paper present personalized end-to-end model attempt leverage personalization goal-oriented dialogs first introduce profile model encodes user profiles distributed embeddings refers conversation history similar users preference model captures user preferences knowledge base entities handle ambiguity user requests two models combined personalized memn2n experiments show proposed model achieves qualitative performance improvements state-of-the-art methods human evaluation also outperforms approaches terms task completion rate user satisfaction	negative
dialogue generation: from imitation learning to inverse reinforcement learning	performance adversarial dialogue generation models relies quality reward signal produced discriminator reward signal poor discriminator sparse unstable may lead generator fall local optimum produce nonsense replies alleviate first problem first extend recently proposed adversarial dialogue generation method adversarial imitation learning solution framework adversarial inverse reinforcement learning propose new reward model dialogue generation provide accurate precise reward signal generator training evaluate performance resulting model automatic metrics human evaluations two annotation settings experimental results demonstrate model generate high-quality responses achieve higher overall performance state-of-the-art	positive
hybrid attention-based prototypical networks for noisy few-shot relation classification	existing methods relation classification rc primarily rely distant supervision ds large-scale supervised training datasets readily available although ds automatically annotates adequate amounts data model training coverage data still quite limited meanwhile many long-tail relations still suffer data sparsity intuitively people grasp new knowledge learning instances thus provide different view rc formalizing rc few-shot learning fsl problem however current fsl models mainly focus low-noise vision tasks makes hard directly deal diversity noise text paper propose hybrid attention-based prototypical networks problem noisy few-shot rc design instancelevel feature-level attention schemes based prototypical networks highlight crucial instances features respectively significantly enhances performance robustness rc models noisy fsl scenario besides attention schemes accelerate convergence speed rc models experimental results demonstrate hybrid attention-based models require fewer training iterations outperform state-of-the-art baseline models code datasets released https //github.com/thunlp/ hatt-proto	negative
autosense model for word sense induction	word sense induction wsi task automatically discovering multiple senses meanings word three main challenges domain adaptability novel sense detection sense granularity flexibility current latent variable models known solve first two challenges flexible different word sense granularities differ much among words aardvark one sense play 50 senses current models either require hyperparameter tuning nonparametric induction number senses find ineffective thus aim eliminate requirements solve sense granularity problem proposing autosense latent variable model based two observations 1 senses represented distribution topics 2 senses generate pairings target word neighboring word observations alleviate problem throwing garbage senses b additionally inducing fine-grained word senses results show great improvements stateof-the-art models popular wsi datasets also show autosense able learn appropriate sense granularity word finally apply autosense unsupervised author name disambiguation task sense granularity problem evident show autosense evidently better competing models share data code https //github.com/rktamplayo/autosense	negative
debguer: a tool for bug prediction and diagnosis	paper present debguer tool web-based tool prediction isolation software bugs debguer partial implementation learn diagnose plan ldp paradigm recently introduced paradigm integrating artificial intelligence ai software bug detection correction process ldp diagnosis dx algorithm used suggest possible explanations – diagnoses – observed bug needed test planning algorithm subsequently used suggest testing diagnosis test planning algorithms consider fault prediction model associates software component e.g. class method likelihood contains bug debguer implements first two components ldp bug prediction learn bug diagnosis diagnose provides easy-to-use web interface successfully tested 12 projects	positive
efficient and effective incomplete multi-view clustering	incomplete multi-view clustering imvc optimally fuses multiple pre-specified incomplete views improve clustering performance among various excellent solutions recently proposed multiple kernel k-means incomplete kernels mkkm-ik forms benchmark redefines imvc joint optimization problem clustering kernel matrix imputation tasks alternately performed convergence though demonstrating promising performance various applications observe manner kernel matrix imputation mkkm-ik would incur intensive computational storage complexities overcomplicated optimization limitedly improved clustering performance paper propose efficient effective incomplete multi-view clustering ee-imvc algorithm address issues instead completing incomplete kernel matrices ee-imvc proposes impute incomplete base matrix generated incomplete views learned consensus clustering matrix carefully develop three-step iterative algorithm solve resultant optimization problem linear computational complexity theoretically prove convergence conduct comprehensive experiments study proposed ee-imvc terms clustering accuracy running time evolution learned consensus clustering matrix convergence indicated algorithm significantly consistently outperforms state-of-the-art algorithms much less running time memory	negative
algorithms for average regret minimization	paper study problem realm multicriteria decision making goal select given set d-dimensional objects minimum sized subset s0 bounded regret thereby regret measures unhappiness users would like select favorite object set select favorite object subset s0 previous work focused bounding maximum regret determined unhappy user propose consider average regret instead determined sum un happiness possible users show regret measure comes desirable properties supermodularity allows construct approximation algorithms furthermore introduce regret minimizing permutation problem discuss extensions algorithms recently proposed k-regret measure theoretical results accompanied experiments variety inputs 7	positive
how does knowledge of the auc constrain the set of possible ground-truth labelings?	recent work privacy-preserving machine learning considered datamining competitions kaggle could potentially “ hacked ” either intentionally inadvertently using information oracle reports classifier ’ accuracy test set blum hardt 2015 hardt ullman 2014 zheng 2015 whitehill 2016 binary classification tasks particular one common accuracy metrics area roc curve auc paper explore mathematical structure auc computed n-vector real-valued “ guesses ” respect ground-truth labels assumption perfect knowledge test set auc c=p/q show knowing c constrains set w possible ground-truth labelings derive algorithm compute exact number labelings enumerate efficiently also provide empirical evidence surprisingly number compatible labelings actually decrease n grows test set-dependent threshold reached finally show w efficiently whittled pairs oracle queries infer groundtruth test labels complete certainty	negative
robustness guarantees for bayesian inference with gaussian processes	bayesian inference gaussian processes widely used applications ranging robotics control biological systems many applications safety-critical require characterization uncertainty associated learning model formal guarantees predictions paper define robustness measure bayesian inference input perturbations given probability test point compact set input space containing test point prediction learning model remain δ−close points set δ 0. measures used provide formal probabilistic guarantees absence adversarial examples employing theory gaussian processes derive upper bounds resulting robustness utilising borell-tis inequality propose algorithms computation evaluate techniques two examples gp regression problem fully-connected deep neural network rely weak convergence gps study adversarial examples mnist dataset	positive
a two-individual based evolutionary algorithm for the flexible job shop scheduling problem	population-based evolutionary algorithms usually manage large number individuals maintain diversity search complex time-consuming paper propose evolutionary algorithm using two individuals called master-apprentice evolutionary algorithm mae solving flexible job shop scheduling problem fjsp ensure diversity quality evolution mae integrates tabu search procedure recombination operator based path relinking using novel distance definition effective individual updating strategy taking account multiple complex constraints fjsp experiments 313 widely-used public instances show mae improves previous best known results 47 instances matches best known results except 3 remaining instances consuming computational time current state-of-the-art metaheuristics mae additionally establishes solution quality records 10 hard instances whose previous best values established well-known industrial solver state-of-the-art exact method	positive
mncn: a multilingual ngram-based convolutional network for aspect category detection in online reviews	advent internet caused significant growth number opinions expressed products services e-commerce websites aspect category detection one challenging subtasks aspect-based sentiment analysis deals categorizing given review sentence set predefined categories research efforts field devoted english language reviews large number reviews languages left unexplored paper propose multilingual method perform aspect category detection reviews different languages makes use deep convolutional neural network multilingual word embeddings best knowledge method first attempt performing aspect category detection multiple languages simultaneously empirical results multilingual dataset provided semeval workshop demonstrate effectiveness proposed method1	negative
unsupervised fake news detection on social media: a generative approach	social media become one main channels people access consume news due rapidness low cost news dissemination however properties social media also make hotbed fake news dissemination bringing negative impacts individuals society therefore detecting fake news become crucial problem attracting tremendous research effort existing methods fake news detection supervised require extensive amount time labor build reliably annotated dataset search alternative paper investigate could detect fake news unsupervised manner treat truths news users ’ credibility latent random variables exploit users ’ engagements social media identify opinions towards authenticity news leverage bayesian network model capture conditional dependencies among truths news users ’ opinions users ’ credibility solve inference problem propose efficient collapsed gibbs sampling approach infer truths news users ’ credibility without labelled data experiment results two datasets show proposed method significantly outperforms compared unsupervised methods	negative
adaptive planning with evidence based prediction for improved fluency in routine human-robot collaborative tasks	thesis work intends explore development shared mental model autonomous agent human aim promote fluency continuing interactions defined repetitive tasks repetitive actions experimentation increasing iterations wish robot learn behavior affects partner accomplish propose model encodes human robot actions probabilistic space describing temporal transition points activities purpose model passive predictive power understanding future actions associate also encode latent effect robot ’ action future actions associate	negative
querying attributed dl-lite ontologies using provenance semirings	attributed description logic recently proposed formalism targeted graph-based representation formats enriches description logic concepts roles finite sets attribute-value pairs called annotations one important uses annotations record provenance information work first investigate complexity satisfiability query answering attributed dl-liter ontologies propose new semantics based provenance semirings integrating provenance information query answering finally establish complexity results satisfiability query answering semantics	negative
on reinforcement learning for full-length game of starcraft	starcraft ii poses grand challenge reinforcement learning main difficulties include huge state space varying action space long horizon etc paper investigate set techniques reinforcement learning full-length game starcraft ii investigate hierarchical approach hierarchy involves two levels abstraction one macro-actions extracted expert ’ demonstration trajectories reduce action space order magnitude yet remain effective two-layer hierarchical architecture modular easy scale also investigate curriculum transfer learning approach trains agent simplest opponent harder ones 64×64 map using restrictive units train agent single machine 4 gpus 48 cpu threads achieve winning rate 99 difficulty level-1 built-in ai curriculum transfer learning algorithm mixture combat model achieve 93 winning rate difficult noncheating built-in ai level-7 within days hope study could shed light future research large-scale reinforcement learning	negative
classifier-agnostic saliency map extraction	extracting saliency maps indicate parts image important classification requires many tricks achieve satisfactory performance using classifier-dependent methods instead propose classifier-agnostic saliency map extraction allows find parts image classifier could use one given advance way extract much higher quality saliency maps	negative
depthwise convolution is all you need for learning multiple visual domains	growing interest designing models deal images different visual domains exists universal structure different visual domains captured via common parameterization use single model domains rather one model per domain model aware relationships different domains also trained work new domains less resources however identify reusable structure model easy paper propose multi-domain learning architecture based depthwise separable convolution proposed approach based assumption images different domains share cross-channel correlations domain-specific spatial correlations proposed model compact minimal overhead applied new domains additionally introduce gating mechanism promote soft sharing different domains evaluate approach visual decathlon challenge benchmark testing ability multi-domain models experiments show approach achieve highest score requiring 50 parameters compared state-of-the-art approaches	positive
improving one-class collaborative filtering via ranking-based implicit regularizer	one-class collaborative filtering occf problems vital many applications recommender systems news music recommendation suffers sparsity issues lacks negative examples address problem state-of-the-arts assigned smaller weights unobserved samples performed low-rank approximation however ground-truth ratings unobserved samples usually set zero ill-defined paper propose ranking-based implicit regularizer provide new general framework occf avert ground-truth ratings unobserved samples exploit regularize ranking-based loss function design efficient optimization algorithms learn model parameters finally evaluate three realworld datasets results show proposed regularizer significantly improves ranking-based algorithms proposed framework outperforms state-of-the-art occf algorithms	negative
towards task understanding in visual settings	consider problem understanding real world tasks depicted visual images existing image captioning methods excel producing natural language descriptions visual scenes involving human tasks often need understanding exact task undertaken rather literal description scene leverage insights real world task understanding systems propose framework composed convolutional neural networks external hierarchical task ontology produce task descriptions input images detailed experiments highlight efficacy extracted descriptions could potentially find way many applications including image alt text generation	positive
argumentation for explainable scheduling	mathematical optimization offers highly-effective tools finding solutions problems well-defined goals notably scheduling however optimization solvers often unexplainable black boxes whose solutions inaccessible users users interact define novel paradigm using argumentation empower interaction optimization solvers users supported tractable explanations certify refute solutions solution solver interest user context ‘ what-if ’ scenarios specifically define argumentative natural language explanations schedule feasible efficient satisfying fixed user decisions based models fundamental makespan scheduling problem terms abstract argumentation frameworks afs define three types afs whose stable extensions one-to-one correspondence schedules feasible efficient satisfying fixed decisions respectively extract argumentative explanations afs natural language explanations argumentative ones	negative
robust ordinal embedding from contaminated relative comparisons	existing ordinal embedding methods usually follow twostage routine outlier detection first employed pick inconsistent comparisons embedding learned clean data however learning multi-stage manner well-known suffer sub-optimal solutions paper propose unified framework jointly identify contaminated comparisons derive reliable embeddings merits method three-fold 1 virtue proposed unified framework sub-optimality traditional methods largely alleviated 2 proposed method aware global inconsistency minimizing corresponding cost traditional methods involve local inconsistency 3 instead considering nuclear norm heuristics adopt exact solution rank equality constraint studies supported experiments simulated examples real-world data proposed framework provides us promising tool robust ordinal embedding contaminated comparisons	negative
a dual attention network with semantic embedding for few-shot learning	despite recent success deep neural networks remains challenging efficiently learn new visual concepts limited training data address problem prevailing strategy build meta-learner learns prior knowledge learning small set annotated data however existing meta-learning approaches rely global representation images meta-learner complex model structures sensitive background clutter difficult interpret propose novel meta-learning method few-shot classification based two simple attention mechanisms one spatial attention localize relevant object regions task attention select similar training data label prediction implement method via dual-attention network design semantic-aware meta-learning loss train meta-learner network end-to-end manner validate model three few-shot image classification datasets extensive ablative study approach shows competitive performances datasets fewer parameters facilitating future research code data split available https //github.com/tonysy/stanet-pytorch	negative
certifying the true error: machine learning in coq with verified generalization guarantees	present mlcert novel system practical mechanized proof generalization learning procedures bounding expected error terms training test error mlcert mechanized prove generalization bounds inside theorem prover coq thus bounds machine checked coq ’ proof checker mlcert practical extract learning procedures defined coq executable code thus procedures proved generalization bounds trained deployed real systems mlcert well documented open source thus expect usable even without coq expertise validate mlcert compatible external tools tensorflow use prove generalization bounds neural networks trained using tensorflow extended mnist data set	positive
connecting the digital and physical world: improving the robustness of adversarial attacks	deep learning models achieved unprecedented success various domains also growing concern adversarial attacks related applications recent results show adding small amount perturbations image imperceptible humans resulting adversarial examples force classifier make targeted mistakes far existing works focus crafting adversarial examples digital domain limited efforts devoted understanding physical domain attacks work explore feasibility generating robust adversarial examples remain effective physical domain core idea use image-to-image translation network simulate digital-to-physical transformation process generating robust adversarial examples validate method conduct large-scale physical-domain experiment involves manually taking 3000 physical domain photos results show method outperforms existing ones large margin demonstrates high level robustness transferability	negative
scalable and efficient pairwise learning to achieve statistical accuracy	pairwise learning important learning topic machine learning community loss function involves pairs samples e.g. auc maximization metric learning existing pairwise learning algorithms perform well generality scalability efficiency simultaneously address challenging problems paper first analyze relationship statistical accuracy regularized empire risk pairwise loss based relationship propose scalable efficient adaptive doubly stochastic gradient algorithm adadsg generalized regularized pairwise learning problems importantly prove overall computational cost adadsg n achieve statistical accuracy full training set size n best theoretical result pairwise learning best knowledge experimental results variety real-world datasets confirm effectiveness adadsg algorithm also show adadsg significantly better scalability efficiency existing pairwise learning algorithms	positive
random feature maps for the itemset kernel	although kernel methods efficiently use feature combinations without computing directly scale well size training dataset factorization machines fms related models hand enable feature combinations efficiently optimization generally requires solving non-convex problem present random feature maps itemset kernel uses feature combinations includes anova kernel all-subsets kernel standard dot product linear models using one proposed maps used alternative kernel methods fms resulting better scalability training evaluation also present theoretical results proposed map discuss relationship factorization machines linear models using proposed map anova kernel relate proposed feature maps prior work furthermore show maps calculated efficiently using signed circulant matrix projection technique finally demonstrate effectiveness using proposed maps real-world datasets..	negative
bounded suboptimal search with learned heuristics for multi-agent systems	wide range discrete planning problems solved optimally using graph search algorithms however optimal search quickly becomes infeasible increased complexity problem case heuristics guide planning process towards goal state increase performance considerably unfortunately heuristics often unavailable need manual time-consuming engineering building upon recent results applying deep learning learn generalized reactive policies propose learn heuristics imitation learning learning heuristics based optimal examples used guide classical search algorithm solve unseen tasks however directly applying learned heuristics search algorithms a∗ breaks optimality guarantees since learned heuristics necessarily admissible therefore propose novel method utilizes learned heuristics guide focal search a∗ variant a∗ guarantees bounded suboptimality ii compare complexity performance jointly learning individual policies multiple robots approach learns one policy robots iii thoroughly examine learned policies generalize previously unseen environments demonstrate considerably improved performance simulated complex dynamic coverage problem	negative
state abstraction as compression in apprenticeship learning	state abstraction give rise models environments compressed useful thereby enabling efficient sequential decision making work offer first formalism analysis trade-off compression performance made context state abstraction apprenticeship learning build rate-distortion theory classic blahut-arimoto algorithm information bottleneck method develop algorithm computing state abstractions approximate optimal tradeoff compression performance illustrate power algorithmic structure offer insights effective abstraction compression reinforcement learning mixture analysis visuals experimentation	negative
approximate kernel selection with strong approximate consistency	kernel selection fundamental generalization performance kernel-based learning algorithms approximate kernel selection efficient kernel selection approach exploits convergence property kernel selection criteria computational virtue kernel matrix approximation convergence property measured notion approximate consistency existing nyström approximations whose sampling distributions independent specific learning task hand difficult establish strong approximate consistency mainly focus quality low-rank matrix approximation rather performance kernel selection criterion used conjunction approximate matrix paper propose novel nyström approximate kernel selection algorithm customizing criterion-driven adaptive sampling distribution nyström approximation adaptively reduces error approximate accurate criteria theoretically derive strong approximate consistency proposed nyström approximate kernel selection algorithm finally empirically evaluate approximate consistency algorithm compared state-of-the-art methods	negative
quota: the quantile option architecture for reinforcement learning	paper propose quantile option architecture quota exploration based recent advances distributional reinforcement learning rl quota decision making based quantiles value distribution mean quota provides new dimension exploration via making use optimism pessimism value distribution demonstrate performance advantage quota challenging video games physical robot simulators	positive
attention-aware sampling via deep reinforcement learning for action recognition	deep learning based methods achieved remarkable progress action recognition existing works mainly focus designing novel deep architectures achieve video representations learning action recognition methods treat sampled frames equally average frame-level predictions testing stage however within video discriminative actions may occur sparsely frames frames irrelevant ground truth may even lead wrong prediction result think strategy selecting relevant frames would important key enhance existing deep learning based action recognition paper propose attentionaware sampling method action recognition aims discard irrelevant misleading frames preserve discriminative frames formulate process mining key frames videos markov decision process train attention agent deep reinforcement learning without extra labels agent takes features predictions baseline model input generates importance scores frames moreover approach extensible applied different existing deep learning based action recognition models achieve competitive action recognition performance two widely used action recognition datasets	negative
mamic: macro and micro curriculum for robotic reinforcement learning	generating curriculum guided learning involves subjecting agent easier goals first gradually increasing difficulty work takes similar direction proposes dual curriculum scheme solving robotic manipulation tasks sparse rewards called mamic includes macro curriculum scheme divides task multiple subtasks followed micro curriculum scheme enables agent learn discovered subtasks show combining macro micro curriculum strategies help overcoming major exploratory constraints considered robot manipulation tasks without engineer complex rewards also illustrate meaning usage individual curricula performance scheme analysed fetch environments	positive
migan: malware image synthesis using gans	majority advancement deep learning dl occurred domains computer vision natural language processing abundant training data available major obstacle leveraging dl techniques malware analysis lack sufficiently big labeled datasets paper take first steps towards building model synthesize labeled dataset malware images using gan model utilized perform data augmentation training classifier furthermore model shared publicly community reap benefits dataset without sharing original dataset first show underlying idiosyncrasies malware images existing data augmentation techniques well traditional gan training fail produce quality artificial samples next propose new method training gan explicitly embed prior domain knowledge dataset training procedure show improvements training stability sample quality assessed different metrics experiments show substantial improvement baselines promise using generative model malware visualization systems	negative
optimizing discount and reputation trade-offs in e-commerce systems: characterization and online learning	feedback-based reputation systems widely deployed e-commerce systems evidences showed earning reputable label sellers systems may take substantial amount time implies reduction profit propose enhance sellers ’ reputation via price discounts however challenges 1 demands buyers depend discount reputation 2 demands unknown seller address challenges first formulate profit maximization problem via semimarkov decision process smdp explore optimal trade-offs selecting price discounts prove monotonicity optimal profit optimal discount based monotonicity design qlfp q-learning forward projection algorithm infers optimal discount historical transaction data conduct experiments dataset show qlfp algorithm improves profit high 50 classical q-learning speedy q-learning algorithm qlfp algorithm also improves profit high four times case providing price discount	negative
performancenet: score-to-audio music generation with multi-band convolutional residual network	music creation typically composed two parts composing musical score performing score instruments make sounds recent work made much progress automatic music generation symbolic domain attempts made build ai model render realistic music audio musical scores directly synthesizing audio sound sample libraries often leads mechanical deadpan results since musical scores contain performance-level information subtle changes timing dynamics moreover task may sound like text-to-speech synthesis problem fundamental differences since music audio rich polyphonic sounds build ai performer propose paper deep convolutional model learns end-to-end manner score-to-audio mapping symbolic representation music called pianorolls audio representation music called spectrograms model consists two subnets contournet uses u-net structure learn correspondence pianorolls spectrograms give initial result texturenet uses multi-band residual network refine result adding spectral texture overtones timbre train model generate music clips violin cello flute dataset moderate size also present result user study shows model achieves higher mean opinion score mos naturalness emotional expressivity wavenet-based model two off-the-shelf synthesizers open source code https //github.com/bwang514/performancenet	negative
automatic generation of leveled visual assessments for young learners	images essential tool communicating children particularly younger ages still developing emergent literacy skills hence assessments use images assess conceptual knowledge visual literacy important component learning process creating assessments scale challenging task led several techniques proposed automatic generation textual assessments however none focuses generating image-based assessments understand manual process creating visual assessments interviewed primary school teachers based findings preliminary study present novel approach uses image semantics generate visual multiple choice questions vmcqs young learners wherein options presented form images propose metric measure semantic similarity two images use identify four options – one answer three distractor images – given question also use metric generating vmcqs two difficulty levels – easy hard quantitative evaluation show system-generated vmcqs comparable vmcqs created experts hence establishing effectiveness approach	negative
from horn-sriq to datalog: a data-independent transformation that preserves assertion entailment	ontology-based access large data-sets recently gained lot attention access data efficiently one approach rewrite ontology datalog use powerful datalog engines compute implicit entailments existing rewriting techniques support description logics dls elh horn-shiq go one step present one data-independent rewriting technique horn-sriq⊓ extension horn-shiq supports role chain axioms expressive feature prominently used many real-world ontologies evaluated rewriting technique large known corpus ontologies experiments show resulting rewritings moderate size approach efficient state-of-the-art dl reasoners reasoning data-intensive ontologies	negative
sphmc: spectral hamiltonian monte carlo	stochastic gradient hamiltonian monte carlo sghmc methods widely used sample certain probability distributions incorporating kernel density derivatives and/or given datasets instead exploring new samples kernel spaces piece work proposed novel sghmc sampler namely spectral hamiltonian monte carlo sphmc produces high dimensional sparse representations given datasets sparse sensing sghmc inspired compressed sensing assume given samples low-dimensional measurements certain high-dimensional sparse vectors continuous probability distribution exists high-dimensional space specifically given dictionary sparse coding sphmc first derives novel likelihood evaluator probability distribution loss function lasso samples high-dimensional distribution using stochastic langevin dynamics derivatives logarithm likelihood metropolis–hastings sampling addition new samples low-dimensional measuring spaces regenerated using sampled high-dimensional vectors dictionary extensive experiments conducted evaluate proposed algorithm using real-world datasets performance comparisons three real-world applications demonstrate superior performance sphmc beyond baseline methods	negative
knowledge transfer via distillation of activation boundaries formed by hidden neurons	activation boundary neuron refers separating hyperplane determines whether neuron activated deactivated long considered neural networks activations neurons rather exact output values play important role forming classificationfriendly partitions hidden feature space however far know aspect neural networks considered literature knowledge transfer paper propose knowledge transfer method via distillation activation boundaries formed hidden neurons distillation propose activation transfer loss minimum value boundaries generated student coincide teacher since activation transfer loss differentiable design piecewise differentiable loss approximating activation transfer loss proposed method student learns separating boundary activation region deactivation region formed neuron teacher experiments various aspects knowledge transfer verified proposed method outperforms current state-of-the-art	negative
synergistic image and feature adaptation: towards cross-modality domain adaptation for medical image segmentation	paper presents novel unsupervised domain adaptation framework called synergistic image feature adaptation sifa effectively tackle problem domain shift domain adaptation become important hot topic recent studies deep learning aiming recover performance degradation applying neural networks new testing domains proposed sifa elegant learning diagram presents synergistic fusion adaptations image feature perspectives particular simultaneously transform appearance images across domains enhance domain-invariance extracted features towards segmentation task feature encoder layers shared perspectives grasp mutual benefits end-to-end learning procedure without using annotation target domain learning unified model guided adversarial losses multiple discriminators employed various aspects extensively validated method challenging application crossmodality medical image segmentation cardiac structures experimental results demonstrate sifa model recovers degraded performance 17.2 73.0 outperforms state-of-the-art methods significant margin	positive
loss-balanced task weighting to reduce negative transfer in multi-task learning	settings related prediction tasks integrated multi-task learning models often improve performance relative independent single-task models however even average task performance improves individual tasks may experience negative transfer multi-task model ’ predictions worse single-task model ’ show prevalence negative transfer computational chemistry case study 128 tasks introduce framework provides foundation reducing negative transfer multitask models loss-balanced task weighting approach dynamically updates task weights model training control influence individual tasks	negative
incomplete label multi-task deep learning for spatio-temporal event subtype forecasting	due potentially significant benefits society forecasting spatio-temporal societal events currently attracting considerable attention researchers beyond merely predicting occurrence future events practitioners looking information specific subtypes future events order allocate appropriate amounts types resources manage events associated social risks however forecasting event subtypes far complex merely extending binary prediction cover multiple classes 1 different locations require different models handle characteristic event subtype patterns due spatial heterogeneity 2 historically many locations experienced incomplete set event subtypes thus limiting local model ’ ability predict previously “ unseen ” subtypes 3 subtle discrepancy among different event subtypes requires discriminative profound representations societal events order address challenges concurrently propose spatial incomplete multi-task deep learning simda framework capable effectively forecasting subtypes future events new framework formulates spatial locations tasks handle spatial heterogeneity event subtypes learns joint deep representation subtypes across tasks furthermore based “ first law geography ” spatiallyclosed tasks share similar event subtype patterns adjacent tasks share knowledge effectively optimizing proposed model amounts new nonconvex strongly-coupled problem propose new algorithm based alternating direction method multipliers admm decompose complex problem subproblems solved efficiently extensive experiments six real-world datasets demonstrate effectiveness efficiency proposed model	negative
turbo learning framework for human-object interactions recognition and human pose estimation	human-object interactions hoi recognition pose estimation two closely related tasks human pose essential cue recognizing actions localizing interacted objects meanwhile human action interacted objects ’ localizations provide guidance pose estimation paper propose turbo learning framework perform hoi recognition pose estimation simultaneously first two modules designed enforce message passing tasks i.e pose aware hoi recognition module hoi guided pose estimation module two modules form closed loop utilize complementary information iteratively trained end-to-end manner proposed method achieves state-of-the-art performance two public benchmarks including verbs coco v-coco hico-det datasets	negative
counting and sampling markov equivalent directed acyclic graphs	exploring directed acyclic graphs dags markov equivalence class pivotal infer causal effects discover causal dag via appropriate interventional data consider counting uniform sampling dags markov equivalent given dag problems efficiently reduce counting moral acyclic orientations given undirected connected chordal graph n vertices give two algorithms first algorithm requires 2nn4 arithmetic operations improving previous superexponential upper bound second requires k 2kk2n operations k size largest clique graph bounded-degree graphs bound linear n. single run algorithms enable uniform sampling equivalence class computational cost linear graph size empirical results indicate algorithms superior previously presented algorithms range inputs graphs hundreds vertices thousands edges processed second desktop computer	negative
on fair cost sharing games in machine learning	machine learning game theory known exhibit strong link mutually provide solutions models allowing study analyze optimal behaviour set agents paper take closer look special class games known fair cost sharing games machine learning perspective show particular kind games agents choose selfish behaviour cooperation shared costs natural link several machine learning scenarios including collaborative learning homogeneous heterogeneous sources data demonstrate game-theoretical results bounding ratio best nash equilibrium approximate counterpart optimal solution given game used provide upper bound gain achievable collaborative learning expressed expected risk sample complexity homogeneous heterogeneous cases respectively believe established link spur many possible future implications learning scenarios well privacy-aware learning among noticeable examples	negative
implicit argument prediction as reading comprehension	implicit arguments detected solely syntactic cues make harder extract predicate-argument tuples present new model implicit argument prediction draws reading comprehension casting predicate-argument tuple missing argument query also draw pointer networks multi-hop computation model shows good performance argument cloze task well nominal implicit argument prediction task	negative
policy optimization with model-based explorations	model-free reinforcement learning methods proximal policy optimization algorithm ppo successfully applied complex decision-making problems atari games however methods suffer high variances high sample complexity hand model-based reinforcement learning methods learn transition dynamics sample efficient often suffer bias transition estimation make use model-based model-free learning central problem reinforcement learning	negative
a topic-aware reinforced model for weakly supervised stance detection	analyzing public attitudes plays important role opinion mining systems stance detection aims determine text whether author favor neutral towards given target one challenge task text may explicitly express attitude towards target existing approaches utilize target content alone build models moreover although weakly supervised approaches proposed ease burden manually annotating largescale training data approaches confronted noisy labeling problem address two issues paper propose topic-aware reinforced model tarm weakly supervised stance detection model consists two complementary components 1 detection network incorporates target-related topic information representation learning identifying stance effectively 2 policy network learns eliminate noisy instances auto-labeled data based off-policy reinforcement learning two networks alternately optimized improve ’ performances experimental results demonstrate proposed model tarm outperforms state-of-the-art approaches	negative
on the role of syntactic graph convolutions for identifying and classifying argument components	paper focuses fundamental research combines syntactic knowledge neural studies utilize syntactic information argument component identification classification ac-i/c tasks argument mining following paper ’ contributions 1 propose way incorporating syntactic gcn multi-task learning models ac-i/c tasks 2 demonstrate valid effectiveness proposed syntactic gcn fair experiments datasets also found syntactic gcns promising lexically independent scenarios code experiments available reproducibility.1	positive
structural causal bandits with non-manipulable variables	causal knowledge sought throughout data-driven fields due explanatory power potential value inform decision-making targeted system well-understood terms causal components one able design precise surgical interventions bring certain desired outcomes idea leveraging causal understanding system improve decision-making studied literature rubric structural causal bandits lee bareinboim 2018 setting 1 pulling arm corresponds performing causal intervention set variables 2 associated rewards governed underlying causal mechanisms one key assumption work observed variable x system manipulable means intervening making x x always realizable many real-world scenarios however stringent requirement instance scientific evidence may support obesity shortens life ’ feasible manipulate obesity directly example decreasing amount soda consumption pearl 2018 paper study relaxed version structural causal bandit problem variables manipulable specifically develop procedure takes argument partially specified causal knowledge identifies possibly-optimal arms structural bandits non-manipulable variables introduce algorithm uncovers non-trivial dependence structure among possibly-optimal arms finally corroborate findings simulations shows mab solvers enhanced causal knowledge leveraging newly discovered dependence structure among arms consistently outperform causal-insensitive solvers	positive
partners in crime: manipulating the deferred acceptance algorithm through an accomplice	introduce new manipulation strategy available women men-proposing stable matching called manipulation accomplice strategy woman team potential male “ accomplice ” manipulates behalf obtain better match investigate stability matching obtained manipulation provide algorithm compute strategies show benefit compared single-woman manipulation strategies	negative
validation of growing knowledge graphs by abductive text evidences	paper proposes validation mechanism newly added triples growing knowledge graph given logical theory knowledge graph text corpus new triple validated mechanism computes sorted list explanations new triple facilitate validation explanation called abductive text evidence set pairs form triple window appending set triples left knowledge graph enforces entailment new triple logical theory every sentence window right contained text corpus explains degree triple left true angle practice special class abductive text evidences called tep-based abductive text evidence proposed constructed explanation patterns seen knowledge graph accordingly method computing complete set tep-based abductive text evidences proposed moreover method sorting abductive text evidences based distantly supervised learning proposed evaluate proposed validation mechanism four knowledge graphs logical theories constructed four great classical masterpieces chinese literature experimental results datasets demonstrate efficiency effectiveness proposed mechanism	positive
solving large extensive-form games with strategy constraints	extensive-form games common model multiagent interactions imperfect information two-player zerosum games typical solution concept nash equilibrium unconstrained strategy set player many situations however would like constrain set possible strategies example constraints natural way model limited resources risk mitigation safety consistency past observations behavior secondary objectives agent small games optimal strategies linear constraints found solving linear program however state-of-the-art algorithms solving large games handle general constraints work introduce generalized form counterfactual regret minimization provably finds optimal strategies feasible set convex constraints demonstrate effectiveness algorithm finding strategies mitigate risk security games opponent modeling poker games given partial observations private information	negative
mixture of expert/imitator networks: scalable semi-supervised learning framework	current success deep neural networks dnns increasingly broad range tasks involving artificial intelligence strongly depends quality quantity labeled training data general scarcity labeled data often observed many natural language processing tasks one important issues addressed semisupervised learning ssl promising approach overcoming issue incorporating large amount unlabeled data paper propose novel scalable method ssl text classification tasks unique property method mixture expert/imitator networks imitator networks learn “ imitate ” estimated label distribution expert network unlabeled data potentially contributes set features classification experiments demonstrate proposed method consistently improves performance several types baseline dnns also demonstrate method data better performance property promising scalability amount unlabeled data	positive
unsupervised controllable text formalization	propose novel framework controllable natural language transformation realizing requirement parallel corpus practically unsustainable controllable generation tasks unsupervised training scheme introduced crux framework deep neural encoder-decoder reinforced text-transformation knowledge auxiliary modules called scorers scorers based off-the-shelf language processing tools decide learning scheme encoder-decoder based actions apply framework text-transformation task formalizing input text improving readability grade degree required formalization controlled user run-time experiments public datasets demonstrate efficacy model towards transforming given text formal style b varying amount formalness output text based specified input control code datasets released academic use	positive
trafficpredict: trajectory prediction for heterogeneous traffic-agents	safely efficiently navigate complex urban traffic autonomous vehicles must make responsible predictions relation surrounding traffic-agents vehicles bicycles pedestrians etc. challenging critical task explore movement patterns different traffic-agents predict future trajectories accurately help autonomous vehicle make reasonable navigation decision solve problem propose long short-term memory-based lstm-based realtime traffic prediction algorithm trafficpredict approach uses instance layer learn instances ’ movements interactions category layer learn similarities instances belonging type refine prediction order evaluate performance collected trajectory datasets large city consisting varying conditions traffic densities dataset includes many challenging scenarios vehicles bicycles pedestrians move among one another evaluate performance trafficpredict new dataset highlight higher accuracy trajectory prediction comparing prior prediction methods	negative
comparing sample-wise learnability across deep neural network models	estimating relative importance sample training set important practical theoretical value importance sampling curriculum learning kind focus individual samples invokes concept samplewise learnability easy correctly learn sample cf pac learnability paper approach sample-wise learnability problem within deep learning context propose measure learnability sample given deep neural network dnn model basic idea train given model training set sample aggregate hits misses entire training epochs experiments show samplewise learnability measure collected way highly linearly correlated across different dnn models resnet-20 vgg-16 mobilenet suggesting measure provide deep general insights data ’ properties expect method help develop better curricula training help us better understand data	negative
consensual affine transformations for partial valuation aggregation	consider task aggregating scores provided experts scored subset objects rated since experts see subset objects lack global information overall quality objects well global range quality inherently reliable information get experts therefore relative scores objects scored	positive
remote management of boundary protection devices with information restrictions	boundary protection devices bpds used us government mission partners regulate flow information across networks differing security levels bpds provide several critical functions including preventing unauthorized sharing sanitizing information preventing cyber attacks application national security critical infrastructure environments e.g. military missions nuclear power plants clean water distribution systems calls comprehensive load monitoring system provides resilience scalability well automated vendor neutral configuration management system efficiently respond security threats machine speed design one-way traffic control systems however presents challenges dynamic load adaptation techniques require access application server performance metrics across network boundaries moreover structured review approval process regulates configuration use presents two significant challenges 1 adaptation techniques alter configuration bpds must predictable understandable pre-approved administrators 2 software installed bpds completing stringent accreditation process challenges often lead manual configuration management practices inefficient ineffective many cases hammerhead prototype developed part sharc project addresses challenges using knowledge representation rule-oriented adaptation bundle format extensible open-source constraint solver	positive
randomized wagering mechanisms	wagering mechanisms one-shot betting mechanisms elicit agents ’ predictions event deterministic wagering mechanisms existing impossibility result shown incompatibility desirable theoretical properties particular pareto optimality profitable side bet allocation achieved together weak incentive compatibility weak budget balance individual rationality paper expand design space wagering mechanisms allow randomization ask whether randomized wagering mechanisms achieve previously considered desirable properties including pareto optimality answer question positively two classes randomized wagering mechanisms one simple randomized lottery-type implementation existing deterministic wagering mechanisms ii another family randomized wagering mechanisms named surrogate wagering mechanisms robust noisy ground truth surrogate wagering mechanisms inspired idea learning noisy labels natarajan et al 2013 well recent extension idea information elicitation without verification setting liu chen 2018 show broad set randomized wagering mechanisms satisfy desirable theoretical properties	negative
improved knowledge graph embedding using background taxonomic information	knowledge graphs used represent relational information terms triples enable learning domains embedding models tensor factorization models used make predictions new triples often background taxonomic information terms subclasses subproperties also taken account show existing fully expressive a.k.a universal models provably respect subclass subproperty information show minimal modifications existing knowledge graph completion method enables injection taxonomic information moreover prove model fully expressive assuming lower-bound size embeddings experimental results public knowledge graphs show despite simplicity approach surprisingly effective	negative
robuststl: a robust seasonal-trend decomposition algorithm for long time series	decomposing complex time series trend seasonality remainder components important task facilitate time series anomaly detection forecasting although numerous methods proposed still many time series characteristics exhibiting real-world data addressed properly including 1 ability handle seasonality fluctuation shift abrupt change trend reminder 2 robustness data anomalies 3 applicability time series long seasonality period paper propose novel generic time series decomposition algorithm address challenges specifically extract trend component robustly solving regression problem using least absolute deviations loss sparse regularization based extracted trend apply non-local seasonal filtering extract seasonality component process repeated accurate decomposition obtained experiments different synthetic real-world time series datasets demonstrate method outperforms existing solutions	negative
latent dirichlet allocation for internet price war	current internet market makers facing intense competitive environment personalized price reductions discounted coupons provided peers attract customers much investment spent catch ’ competitors participants price cut war often incapable winning due lack information others ’ strategies customers ’ preference formalize problem stochastic game imperfect incomplete information develop variant latent dirichlet allocation lda infer latent variables current market environment represents preferences customers strategies competitors tests simulated experiments open dataset real data show subsuming available market information market maker ’ competitors model exhibits significant improvement understanding market environment finding best response strategies internet price war work marks first successful learning method infer latent information environment price war lda modeling sets example related competitive applications follow	positive
what should i learn first: introducing lecturebank for nlp education and prerequisite chain learning	recent years witnessed rising popularity natural language processing nlp related fields artificial intelligence ai machine learning ml many online courses resources available even without strong background field often student curious specific topic quite know begin studying answer question “ one learn first ” apply embedding-based method learn prerequisite relations course concepts domain nlp introduce lecturebank dataset containing 1,352 english lecture files collected university courses classified according existing taxonomy well 208 manually-labeled prerequisite relation topics publicly available 1. dataset useful educational purposes lecture preparation organization well applications reading list generation additionally experiment neural graph-based networks non-neural classifiers learn prerequisite relations dataset	negative
ontology-based query answering for probabilistic temporal data	investigate ontology-based query answering data temporal probabilistic might occur contexts stream reasoning situation recognition uncertain data present framework allows represent temporal probabilistic data introduce query language complex temporal probabilistic patterns described specifically language combines conjunctive queries operators linear time logic well probability operators analyse complexities evaluating queries language various settings cases combining temporal probabilistic dimension way comes cost increased complexity also determine cases increase avoided	negative
examining political trustworthiness through text-based measures of ideology	work shows value word-level statistical data us congressional record studying ideological positions dynamic behavior senators using classification techniques machine learning predict senators ’ party near-perfect accuracy also develop text-based ideology scores embed politician ’ ideological position one-dimensional policy space using scores find speech diverges voting positions may result higher vote totals explain behavior show politicians use speech move closer party ’ average position results provide empirical support political economy models commitment also add growing literature machine-learning-based text analysis social science contexts	negative
profiles, proxies, and assumptions: decentralized, communications-resilient planning, allocation, and scheduling	degraded communications expected large-scale disaster response military operations nevertheless require rapid concerted actions distributed decision makers limited visibility changing situation charge limited set resources describe laplata novel architecture addresses challenges separating mission planning allocation/scheduling scalability cost negotiation describe formal algorithms achieve near-optimal performance according mission completion percentage subject matter expert review assumption-based planning replanning profileassisted cooperative allocation schedule negotiation validate approach realistic problem specification compare results subject matter expert solutions	negative
cost-sensitive learning to rank	formulate cost-sensitive learning rank problem learning prioritize limited resources mitigate costly outcomes develop improved ranking models solve problem verified experiments diverse domains forest fire prevention crime prevention preventing storm caused outages electrical networks	negative
defending elections against malicious spread of misinformation	integrity democratic elections depends voters ’ access accurate information however modern media environments dominated social media provide malicious actors unprecedented ability manipulate elections via misinformation fake news study zerosum game attacker attempts subvert election propagating fake new story misinformation set advertising channels defender attempts limit attacker ’ impact computing equilibrium game challenging even pure strategy sets players exponential nevertheless give provable polynomial-time approximation algorithms computing defender ’ minimax optimal strategy across range settings encompassing different population structures well models information available player experimental results confirm algorithms provide nearoptimal defender strategies showcase variations difficulty defending elections depending resources knowledge available defender	negative
training deep neural networks in generations: a more tolerant teacher educates better students	focus problem training deep neural network generations flowchart order optimize target network student another network teacher architecture first trained used provide part supervision signals next stage strategy leads higher accuracy many aspects e.g. teacher-student optimization helps still need explorations	negative
solving multiagent planning problems with concurrent conditional effects	work present novel approach solving concurrent multiagent planning problems several agents act parallel approach relies compilation concurrent multiagent planning classical planning allowing us use off-the-shelf classical planner solve original multiagent problem solution directly interpreted concurrent plan satisfies given set concurrency constraints avoiding exponential blowup associated concurrent actions planner first handle action effects conditional agents theoretically show compilation sound complete empirically show compilation solve challenging multiagent planning problems require concurrent actions	negative
from recommendation systems to facility location games	recommendation systems extremely popular tools matching users contents however content providers strategic basic principle matching users closest content users contents modeled points semantic space may yield low social welfare due fact content providers strategic optimize offered content recommended many users possible motivated modern applications propose widely studied framework facility location games study recommendation systems strategic content providers conceptual contribution introduction mediator facility location models pursuit better social welfare aim designing mediators induce game high social welfare equilibrium b intervene little possible service latter introduce notion intervention cost quantifies much damage mediator may cause social welfare off-equilibrium profile adopted case study high-welfare low-intervention mediator design consider one-dimensional segment user domain propose mediator implements socially optimal strategy profile unique equilibrium profile show tight bound intervention cost ultimately consider extensions highlight open questions general agenda	negative
granger-causal attentive mixtures of experts: learning important features with neural networks	knowledge importance input features towards decisions made machine-learning models essential increase understanding models underlying data present new approach estimating feature importance neural networks based idea distributing features interest among experts attentive mixture experts ame ames use attentive gating networks trained granger-causal objective learn jointly produce accurate predictions well estimates feature importance single model experiments show feature importance estimates provided ames compare favourably provided state-of-theart methods ii ames significantly faster estimating feature importance existing methods iii associations discovered ames consistent reported domain experts	negative
interactive attention transfer network for cross-domain sentiment classification	cross-domain sentiment classification refers utilizing useful knowledge source domain help sentiment classification target domain labeled data existing methods mainly concentrate extracting common features domains unfortunately fully consider effects aspect e.g. battery life reviewing electronic product information sentences order better solve problem propose interactive attention transfer network iatn crossdomain sentiment classification iatn provides interactive attention transfer mechanism better transfer sentiment across domains incorporating information sentences aspects specifically iatn comprises two attention networks one identify common features domains domain classification aims extract information aspects using common features bridge conduct interactive attention learning two networks sentences aspects influence final sentiment representation extensive experiments amazon reviews dataset crowdfunding reviews dataset demonstrate effectiveness universality method also give interpretable way track attention information sentiment	positive
the spectacl of nonconvex clustering: a spectral approach to density-based clustering	comes clustering nonconvex shapes two paradigms used find suitable clustering minimum cut maximum density popular algorithms incorporating paradigms spectral clustering dbscan paradigms pros cons minimum cut clusterings sensitive noise density-based clusterings trouble handling clusters varying densities paper propose spectacl method combining advantages approaches solving two mentioned drawbacks method easy implement spectral clustering theoretically founded optimize proposed density criterion clusterings experiments synthetic real-world data demonstrate approach provides robust reliable clusterings	positive
knowledge refinement via rule selection	several different applications including data transformation entity resolution rules used capture aspects knowledge application hand often large set rules generated automatically semi-automatically challenge refine encapsulated knowledge selecting subset rules based expected operational behavior rules available data paper carry systematic complexity-theoretic investigation following rule selection problem given set rules specified horn formulas pair input database output database find subset rules minimizes total error number false positive false negative errors arising selected rules first establish computational hardness results decision problems underlying minimization problem well upper lower bounds approximability investigate bi-objective optimization version rule selection problem total error size selected rules taken account show testing membership pareto front bi-objective optimization problem dp-complete finally show similar dp-completeness result holds bi-level optimization version rule selection problem one minimizes first total error size	negative
verifying robustness of gradient boosted models	gradient boosted models fundamental machine learning technique robustness small perturbations input important quality measure machine learning models literature lacks method prove robustness gradient boosted models	positive
towards sentence-level brain decoding with distributed representations	decoding human brain activities based linguistic representations actively studied recent years however previous studies exclusively focus word-level representations little learned decoding whole sentences brain activation patterns work effort mend gap paper build decoders associate brain activities sentence stimulus via distributed representations currently dominant sentence representation approach natural language processing nlp carry systematic evaluation covering widely-used baselines state-of-the-art sentence representation models demonstrate well different types sentence representations decode brain activation patterns give empirical explanations performance difference moreover explore sentences neurally represented brain compare sentence representation ’ correspondence different brain areas associated high-level cognitive functions find supervised structured representation models accurately probe language atlas human brain best knowledge work first comprehensive evaluation distributed sentence representations brain decoding hope work contribute decoding brain activities nlp representation models understanding linguistic items neurally represented	negative
“reverse gerrymandering”: manipulation in multi-group decision making	district-based manipulation gerrymandering usually taken refer agents fixed location external division imposed upon however many real-world setting external fixed division – organizational chart company markets particular product cases agents may wish move around “ reverse gerrymandering ” tries maximize influence across company ’ subunits resources “ working ” allocated areas needed	negative
leveraging observations in bandits: between risks and benefits	imitation learning widely used speed learning novice agents allowing leverage existing data experts allowing agent influenced external observations benefit learning process also puts agent risk following sub-optimal behaviours paper study problem context bandits specifically consider agent learner interacting bandit-style decision task also observe target policy interacting environment learner observes target ’ actions rewards obtained introduce new bandit optimism modifier uses conditional optimism contingent actions target order guide agent ’ exploration analyze effect modification well-known upper confidence bound algorithm proving preserves regret upper-bound order lnt even presence poor target derive dependency expected regret general target policy provide empirical results showing great benefits well certain limitations inherent observational learning multi-armed bandit setting experiments conducted using targets satisfying theoretical assumptions high probability thus narrowing gap theory application	negative
logic-based sequential decision-making	deep reinforcement learning drl gained great success learning directly high-dimensional sensory inputs yet notorious lack interpretability interpretability subtasks critical hierarchical decision-making increases transparency black-box-style drl approach helps rl practitioners understand high-level behavior system better paper introduce symbolic planning drl propose framework symbolic deep reinforcement learning sdrl handle high-dimensional sensory inputs symbolic planning task-level interpretability enabled relating symbolic actions options framework features planner – controller – meta-controller architecture takes charge subtask scheduling data-driven subtask learning subtask evaluation respectively three components cross-fertilize eventually converge optimal symbolic plan along learned subtasks bringing together advantages long-term planning capability symbolic knowledge end-to-end reinforcement learning directly high-dimensional sensory input experimental results validate interpretability subtasks along improved data efficiency compared state-of-the-art approaches	negative
an optimal rewiring strategy for cooperative multiagent social learning	multiagent coordination cooperative multiagent systems mass widely studied fixed-agent repeated interaction setting static social learning framework however two aspects dynamics real-world mass currently missing first network topologies dynamically change course interaction second interaction utilities pair agents may identical known prior issues mentioned increase difficulty coordination paper consider multiagent social learning dynamic environment agents alter connections interact randomly chosen neighbors unknown utilities beforehand propose optimal rewiring strategy select beneficial peers maximize accumulated payoffs long-run interactions empirically demonstrate effects approach large-scale mass	negative
deep reinforcement learning for syntactic error repair in student programs	novice programmers often struggle formal syntax programming languages traditional classroom setting make progress help real time feedback instructors often impossible get massive open online course mooc setting syntactic error repair techniques huge potential assist scale towards design novel programming language correction framework amenable reinforcement learning framework allows agent mimic human actions text navigation editing demonstrate agent trained self-exploration directly raw input program text without either supervision prior knowledge formal syntax programming language evaluate technique publicly available dataset containing 6975 erroneous c programs typographic errors written students introductory programming course technique fixes 1699 24.4 programs completely 1310 18.8 program partially outperforming deepfix state-of-the-art syntactic error repair technique uses fully supervised neural machine translation approach	negative
relaxing and restraining queries for obda	advocate use ontologies relaxing restraining queries retrieve either less answers enabling exploration given dataset propose set rewriting rules relax restrain conjunctive queries cqs datasets mediated ontology written dialect dl-lite complex role inclusions cris addition cri enables representation knowledge data involving ordered hierarchies categories style multi-dimensional data models although cris general destroy first-order rewritability cqs identify settings cqs remain rewritable	negative
enhancing evolutionary conversion rate optimization via multi-armed bandit algorithms	conversion rate optimization means designing web interfaces visitors perform desired action register purchase site one promising approach implemented sentient ascend optimize design using evolutionary algorithms evaluating candidate design online actual visitors evaluations costly noisy several challenges emerge available visitor traffic used efficiently good solutions identified reliably high conversion rate maintained optimization paper proposes new technique address issues traffic allocated candidate solutions using multi-armed bandit algorithm using traffic evaluations useful best-arm identification mode best candidate identified reliably end evolution campaign mode overall conversion rate optimized throughout entire evolution process multi-armed bandit algorithms thus improve performance reliability machine discovery noisy real-world environments	positive
dan: deep attention neural network for news recommendation	rapid information explosion news making personalized news recommendation users becomes increasingly challenging problem many existing recommendation methods regard recommendation procedure static process achieved better recommendation performance however usually fail dynamic diversity news user ’ interests ignore importance sequential information user ’ clicking selection paper taking full advantages convolution neural network cnn recurrent neural network rnn attention mechanism propose deep attention neural network dan news recommendation dan model presents use attention-based parallel cnn aggregating user ’ interest features attention-based rnn capturing richer hidden sequential features user ’ clicks combines features new recommendation conduct experiment real-world news data sets experimental results demonstrate superiority effectiveness proposed dan model	negative
robust optimization over multiple domains	work study problem learning single model multiple domains unlike conventional machine learning scenario domain corresponding model multiple domains i.e. applications/users may share machine learning model due maintenance loads cloud computing services example digit-recognition model applicable hand-written digits house numbers car plates etc therefore ideal model cloud computing perform well applicable domain address new challenge cloud computing develop framework robust optimization multiple domains lieu minimizing empirical risk aim learn model optimized adversarial distribution multiple domains hence propose learn model adversarial distribution simultaneously stochastic algorithm efficiency theoretically analyze convergence rate convex non-convex models best knowledge first study convergence rate learning robust non-convex model practical algorithm furthermore demonstrate robustness framework convergence rate enhanced appropriate regularizers adversarial distribution empirical study real-world fine-grained visual categorization digits recognition tasks verifies effectiveness efficiency proposed framework	negative
difficulty-aware attention network with confidence learning for medical image segmentation	medical image segmentation key step various applications image-guided radiation therapy diagnosis recently deep neural networks provided promising solutions automatic image segmentation however often perform good regular samples i.e. easy-to-segment samples since datasets dominated easy regular samples medical images due huge inter-subject variations disease-specific effects subjects exist several difficult-to-segment cases often overlooked previous works address challenge propose difficulty-aware deep segmentation network confidence learning end-to-end segmentation proposed framework two main contributions 1 besides segmentation network also propose fully convolutional adversarial network confidence learning provide voxel-wise region-wise confidence information segmentation network relax adversarial learning confidence learning decreasing priority adversarial learning avoid training imbalance generator discriminator 2 propose difficulty-aware attention mechanism properly handle hard samples hard regions considering structural information may go beyond shortcomings focal loss propose fusion module selectively fuse concatenated feature maps encoder-decoder architectures experimental results clinical challenge datasets show proposed network achieve state-of-the-art segmentation accuracy analysis also indicates individual component proposed network contributes overall performance improvement	negative
addressing the under-translation problem from the entropy perspective	neural machine translation nmt drawn much attention due promising translation performance recent years however under-translation problem still remains big challenge paper focus under-translation problem attempt find kinds source words likely ignored analysis observe source word large translation entropy inclined dropped address problem propose coarse-to-fine framework coarse-grained phase introduce simple strategy reduce entropy highentropy words constructing pseudo target sentences fine-grained phase propose three methods including pre-training method multitask method two-pass method encourage neural model correctly translate high-entropy words experimental results various translation tasks show method significantly improve translation quality substantially reduce under-translation cases high-entropy words	negative
active mini-batch sampling using repulsive point processes	convergence speed stochastic gradient descent sgd improved actively selecting mini-batches explore sampling schemes similar data points less likely selected mini-batch particular prove repulsive sampling schemes lower variance gradient estimator generalizes recent work using determinantal point processes dpps mini-batch diversification zhang et al. 2017 broader class repulsive point processes first show phenomenon variance reduction diversified sampling generalizes particular non-stationary point processes show point processes may computationally much efficient dpps particular propose investigate poisson disk sampling—frequently encountered computer graphics community—for task show empirically approach improves standard sgd terms convergence speed well final model performance	positive
learning features and abstract actions for computing generalized plans	generalized planning concerned computation plans solve one multiple instances planning domain recently shown generalized plans expressed mappings feature values actions often computed fully observable non-deterministic fond planners actions plans however actions instances necessarily common instances abstract actions defined set common features formulation assumes features abstract actions given work address limitation showing learn automatically resulting account generalized planning combines learning planning novel way learner based max sat formulation yields features abstract actions sampled state transitions fond planner uses information suitably transformed produce general plans correctness guarantees given experimental results several domains reported	positive
a study of educational data mining: evidence from a thai university	educational data mining provides way predict student academic performance psychometric factor like time management one major issues affecting thai students ’ academic performance current data sources used predict students ’ performance limited manual collection data data single unit study generalised indicate overall academic performance study uses additional data source university log file predict academic performance investigates browsing categories internet access activities students respect time management studies single source data insufficient identify students at-risk failing academic studies furthermore paucity recent empirical studies area provide insights relationship students ’ academic performance internet access activities contribute area research employed two datasets web-browsing categories internet access activity types select best outcomes compared different weights time frequency domains found random forest technique provides best outcome datasets identify students at-risk failure also found data internet access activities reveals accurate outcomes data browsing categories alone combination two datasets reveals better picture students ’ internet usage thus identifies students academically at-risk failure work involves collecting internet access log file data analysing longer period relating period data collection events academic year	negative
linear kernel tests via empirical likelihood for high-dimensional data	propose framework analyzing comparing distributions without imposing parametric assumptions via empirical likelihood methods framework used study two fundamental statistical test problems two-sample test goodness-of-fit test two-sample test need determine whether two groups samples different distributions goodness-of-fit test examine likely set samples generated known target distribution specifically propose empirical likelihood ratio elr statistics two-sample test goodness-of-fit test linear time complexity show higher power i.e. probability correctly rejecting null hypothesis existing linear statistics high-dimensional data prove nonparametric wilks ’ theorems elr statistics illustrate limiting distributions proposed elr statistics chi-square distributions limiting distributions avoid bootstraps simulations determine threshold rejecting null hypothesis makes elr statistics efficient recently proposed linear statistic finite set stein discrepancy fssd also prove consistency elr statistics guarantees test power goes 1 number samples goes infinity addition experimentally demonstrate theoretically analyze fssd poor performance even fails test high-dimensional data finally conduct series experiments evaluate performance elr statistics compared state-of-the-art linear statistics	positive
on geometric alignment in low doubling dimension	real-world many problems formulated alignment two geometric patterns previously great amount research focus alignment 2d 3d patterns especially field computer vision recently alignment geometric patterns high dimension finds several novel applications attracted attentions however research still rather limited terms algorithms best knowledge existing approaches high dimensional alignment simple extensions counterparts 2d 3d cases often suffer issues high complexities paper propose effective framework compress high dimensional geometric patterns approximately preserve alignment quality consequence existing alignment approach applied compressed geometric patterns thus time complexity significantly reduced idea inspired observation high dimensional data often low intrinsic dimension adopt widely used notion “ doubling dimension ” measure extents compression resulting approximation finally test method random real datasets experimental results reveal running alignment algorithm compressed patterns achieve similar qualities comparing results original patterns running times including times cost compression substantially lower	negative
practical algorithms for multi-stage voting rules with parallel universes tiebreaking	stv ranked pairs rp two well-studied voting rules group decision-making proceed multiple rounds affected ties broken round however literature surprisingly vague ties broken propose first algorithms computing set alternatives winners tiebreaking mechanism stv rp also known parallel-universes tiebreaking put unfortunately put-winners np-complete compute stv rp standard search algorithms ai apply propose multiple dfs-based algorithms along pruning strategies heuristics sampling machine learning prioritize search direction significantly improve performance also propose novel ilp formulations put-winners stv rp respectively experiments synthetic realworld data show algorithms overall faster ilp	negative
approximate inference of outcomes in probabilistic elections	study complexity estimating probability outcome election probabilistic votes focus voting rules expressed positional scoring rules two models probabilistic voters uniform distribution completions partial voting profile consisting partial ordering candidates voter repeated insertion model rim candidates including special case mallows distribution past research established exact inference probability winning computationally hard p-hard additive polynomial-time approximation additive fpras attained sampling averaging often though need multiplicative approximation guarantees crucial important measures conditional probabilities unfortunately multiplicative approximation probability winning efficient conventional complexity assumptions since already np-complete determine whether probability nonzero contrastingly devise multiplicative polynomial-time approximations multiplicative fpras probability complement event namely losing election	negative
congestion graphs for automated time predictions	time prediction essential component decision making various artificial intelligence application areas including transportation systems healthcare manufacturing predictions required efficient resource allocation scheduling optimized routing temporal action planning work focus time prediction congested systems entities share scarce resources achieve accurate explainable time prediction setting features describing system congestion e.g. workload resource availability must considered features typically gathered using process knowledge i.e. insights interplay system ’ entities knowledge expensive gather may completely unavailable order automatically extract features data without prior process knowledge propose model congestion graphs grounded queueing theory show congestion graphs mined raw event data using queueing theory based assumptions information contained logs evaluate approach two real-world datasets healthcare systems scarce resources prevail emergency department outpatient cancer clinic experimental results show using automatic generation congestion features get 23 improvement terms relative error time prediction compared common baseline methods also detail congestion graphs used explain delays system	negative
interpretable predictive modeling for climate variables with weighted lasso	important family problems climate science focus finding predictive relationships various climate variables paper consider problem predicting monthly deseasonalized land temperature different locations worldwide based sea surface temperature sst contrary popular belief trade-off simple interpretable inaccurate models b complex accurate uninterpretable models introduce weighted lasso model problem yields interpretable results highly accurate covariate weights regularization weighted lasso pre-determined proportional spatial distance covariate sea surface location target land location establish finite sample estimation error bounds weighted lasso illustrate superior empirical performance interpretability complex models deep neural networks deep nets gradient boosted trees gbt also present detailed empirical analysis went wrong deep nets may serve helpful guideline application deep nets small sample scientific problems	negative
instance-level facial attributes transfer with geometry-aware flow	address problem instance-level facial attribute transfer without paired training data e.g. faithfully transferring exact mustache source face target face challenging task conventional semantic-level attribute transfer preserves generic attribute style instead instance-level traits propose use geometry-aware flow serves wellsuited representation modeling transformation instance-level facial attributes specifically leverage facial landmarks geometric guidance learn differentiable flows automatically despite large pose gap existed geometry-aware flow able warp source face attribute target face context generate warp-and-blend result compensate potential appearance gap source target faces propose hallucination sub-network produces appearance residual refine warp-and-blend result finally cycle-consistency framework consisting attribute transfer module attribute removal module designed abundant unpaired face images used training data extensive evaluations validate capability approach transferring instance-level facial attributes faithfully across large pose appearance gaps thanks flow representation approach readily applied generate realistic details high-resolution images1	negative
multi-winner contests for strategic diffusion in social networks	strategic diffusion encourages participants take active roles promoting stakeholders ’ agendas rewarding successful referrals social media continues transform way people communicate strategic diffusion become powerful tool stakeholders influence people ’ decisions behaviors desired objectives existing reward mechanisms strategic diffusion usually either vulnerable falsename attacks individually rational participants made successful referrals introduce novel multi-winner contests mwc mechanism strategic diffusion social networks mwc mechanism satisfies several desirable properties including false-name-proofness individual rationality budget constraint monotonicity subgraph constraint numerical experiments four real-world social network datasets demonstrate stakeholders significantly boost participants ’ aggregated efforts proper design competitions work sheds light design manipulation-resistant mechanisms appropriate contests	negative
theoretical analysis of label distribution learning	novel learning paradigm label distribution learning ldl explicitly models label ambiguity definition label description degree although lots work done deal real-world applications theoretical results ldl remain unexplored paper rethink ldl theoretical aspects towards analyzing learnability ldl firstly risk bounds three representative ldl algorithms aa-knn aa-bp sa-me provided aa-knn lipschitzness label distribution function assumed bound risk aa-bp sa-me rademacher complexity utilized give data-dependent risk bounds secondly generalized plug-in decision theorem proposed understand relation ldl classification uncovering approximation conditional probability distribution function absolute loss guarantees approaching optimal classifier also data-dependent error probability bounds presented corresponding ldl algorithms perform classification far know perhaps first research theory ldl	negative
towards fluid machine intelligence: can we make a gifted ai?	applications machine intelligence focused demonstrating crystallized intelligence crystallized intelligence relies accessing problem-specific knowledge skills experience stored long term memory paper challenge ai community design ais completely take tests fluid intelligence assess ability solve novel problems using problem-independent solving skills tests fluid intelligence nnat used extensively schools determine entry gifted education programs explain differences crystallized fluid intelligence importance capabilities machines demonstrating fluid intelligence pose several challenges ai community including machine taking test would considered gifted school districts state california importantly show existing work seemingly related fields transfer zero-shot life-long meta learning current form directly capable demonstrating fluid intelligence instead task-transductive mechanisms	negative
modeling coherence for discourse neural machine translation	discourse coherence plays important role translation one text however previous reported models focus improving performance individual sentence ignoring cross-sentence links dependencies affects coherence text paper propose use discourse context reward refine translation quality discourse perspective particular generate translation individual sentences first next deliberate preliminary produced translations train model learn policy produces discourse coherent text reward teacher practical results multiple discourse test datasets indicate model significantly improves translation quality state-of-the-art baseline system +1.23 bleu score moreover model generates discourse coherent text obtains +2.2 bleu improvements evaluated discourse metrics	negative
bootstrap estimated uncertainty of the environment model for model-based reinforcement learning	model-based reinforcement learning rl methods attempt learn dynamics model simulate real environment utilize model make better decisions however learned environment simulator often less model error would disturb making decision reduce performance propose bootstrapped model-based rl method bootstraps modules depth planning tree method quantify uncertainty environment model different state-action pairs lead agent explore pairs higher uncertainty reduce potential model errors moreover sample target values bootstrap distribution connect uncertainties current subsequent time-steps introduce prior mechanism improve exploration efficiency experiment results demonstrate method efficiently decreases model error outperforms treeqn stateof-the-art methods multiple atari games	negative
words can shift: dynamically adjusting word representations using nonverbal behaviors	humans convey intentions usage verbal nonverbal behaviors face-to-face communication speaker intentions often vary dynamically depending different nonverbal contexts vocal patterns facial expressions result modeling human language essential consider literal meaning words also nonverbal contexts words appear better model human language first model expressive nonverbal representations analyzing fine-grained visual acoustic patterns occur word segments addition seek capture dynamic nature nonverbal intents shifting word representations based accompanying nonverbal behaviors end propose recurrent attended variation embedding network raven models fine-grained structure nonverbal subword sequences dynamically shifts word representations based nonverbal cues proposed model achieves competitive performance two publicly available datasets multimodal sentiment analysis emotion recognition also visualize shifted word representations different nonverbal contexts summarize common patterns regarding multimodal variations word representations	negative
pcgan: partition-controlled human image generation	human image generation challenging task since affected many factors many human image generation methods focus generating human images conditioned given pose generated backgrounds often blurred paper propose novel partition-controlled gan generate human images according target pose background firstly human poses given images extracted foreground/background partitioned use secondly extract fuse appearance features pose features background features generate desired images experiments market-1501 deepfashion datasets show model generates realistic human images also produce human pose background want extensive experiments coco lip datasets indicate potential method	negative
learning diverse bayesian networks	much effort directed developing algorithms learning optimal bayesian network structures data given limited noisy data however optimal bayesian network often fails capture true underlying network structure one potentially address problem finding multiple likely bayesian networks k-best hope one recovers true model however often case best models come peak similar tend fail together moreover many models even optimal respective causal ordering thus unlikely useful paper proposes novel method finding set diverse top bayesian networks called modes network guaranteed optimal local neighborhood mode networks expected provide much better coverage true model based globallocal theorem showing mode bayesian network must optimal local scopes introduce a* search algorithm efficiently find top bayesian networks highly probable naturally diverse empirical evaluations show top mode models much better diversity well accuracy discovering true underlying models found k-best	negative
recommender systems: a healthy obsession	propose endurance sports rich novel domain recommender systems machine learning research sports like marathon running triathlons mountain biking become popular among recreational athletes exists growing opportunity develop solutions number interesting prediction classification recommendation challenges better support complex training competition needs athletes solutions potential improve health well-being large populations users promoting optimising exercise part productive healthy lifestyle	positive
a distillation approach to data efficient individual treatment effect estimation	potential using machine learning algorithms tool suggesting optimal interventions fueled significant interest developing methods estimating heterogeneous individual treatment effects ites observational data several methods estimating ites recently suggested methods assume constraints availability data time deployment test time assumption unrealistic settings data acquisition significant part analysis pipeline meaning data test case collected order predict ite work present data efficient individual treatment effect estimation deitee method exploits idea adjusting confounding hence collecting information confounders necessary test time deitee allows development rich models exploit variables train time identifies minimal set variables required estimate ite test time using 77 semi-synthetic datasets varying data generating processes show deitee achieves significant reductions number variables required test time little loss accuracy using real data demonstrate utility approach helping soon-to-be mothers make planning lifestyle decisions impact newborn health	positive
forbidden nodes aware community search	community search important problem network analysis attracted much attention recent years starts given nodes pays attention local network structures gets personalized resultant communities quickly paper argue many real scenarios nodes allowed appear community introduce new concept called forbidden nodes present new problem forbidden nodes aware community search describe scenarios	positive
action knowledge transfer for action prediction with partial videos	predicting action class partially observed videos known action prediction important task computer vision field many applications challenge action prediction mainly lies lack discriminative action information partially observed videos tackle challenge work propose transfer action knowledge learned fully observed videos improving prediction partially observed videos specifically develop two-stage learning framework action knowledge transfer first stage learn feature embeddings discriminative action classifier full videos knowledge learned embeddings classifier transferred partial videos second stage experiments ucf-101 hmdb-51 datasets show proposed action knowledge transfer method significantly improve performance action prediction especially actions small observation ratios e.g. 10 also experimentally illustrate method outperforms state-of-the-art action prediction systems	positive
neurox: a toolkit for analyzing individual neurons in neural networks	present toolkit facilitate interpretation understanding neural network models toolkit provides several methods identify salient neurons respect model external task user visualize selected neurons ablate measure effect model accuracy manipulate control behavior model test time analysis potential serve springboard various research directions understanding model better architectural choices model distillation controlling data biases toolkit available download.1	positive
machine learning with crowdsourcing: a brief summary of the past research and future directions	crowdsourcing systems labels obtained low cost facilitates creation training sets prediction model learning however labels obtained crowdsourcing often imperfect brings great challenges model learning since 2008 machine learning community noticed great opportunities brought crowdsourcing developed large number techniques deal inaccuracy randomness uncertainty issues learning crowdsourcing paper summarizes technical progress field past eleven years focus two fundamental issues data label quality prediction model quality data quality summarize ground truth inference methods machine learning based methods improve data quality prediction model quality summarize several learning paradigms developed crowdsourcing scenario finally discuss several promising future research directions attract researchers make contributions crowdsourcing	positive
near-neighbor methods in random preference completion	paper studies stylized yet natural learning-to-rank problem points critical incorrectness widely used nearest neighbor algorithm consider model n agents users xi i∈ n alternatives items yl l∈ associated latent feature vector agents rank items nondeterministically according plackett-luce model higher utility item agent likely item ranked high agent goal identify near neighbors arbitrary agent latent space prediction	positive
model-based diagnosis of hybrid systems using satisfiability modulo theory	currently detecting isolating faults hybrid systems often done manually help human operators paper present novel model-based diagnosis approach automatically diagnosing hybrid systems approach two parts first modelling dynamic system behaviour done well-known state space models using differential equations second state space models calculate boolean residuals observer-pattern novelty lies implementing observer pattern use symbolic system description specified satisfiability theory modulo linear arithmetic create static situation diagnosis algorithm decouple modelling diagnosis evaluating system description generates one boolean residual component residuals constitute fault symptoms find minimum cardinality diagnosis symptoms employ reiter ’ diagnosis lattice	positive
safe policy improvement with baseline bootstrapping in factored environments	present novel safe reinforcement learning algorithm exploits factored dynamics environment become less conservative focus problem settings policy already running interaction environment limited order safely deploy updated policy necessary provide confidence level regarding expected performance however algorithms safe policy improvement might require large number past experiences become confident enough change agent ’ behavior factored reinforcement learning hand known make good use data provided achieve better sample complexity exploiting independence features environment lacks confidence level study improve sample efficiency safe policy improvement baseline bootstrapping algorithm exploiting factored structure environment main result theoretical bound linear number parameters factored representation instead number states empirical analysis shows method improve policy using number samples potentially one order magnitude smaller flat algorithm	positive
a natural language corpus of common grounding under continuous and partially-observable context	common grounding process creating repairing updating mutual understandings critical aspect sophisticated human communication however traditional dialogue systems limited capability establishing common ground also lack task formulations introduce natural difficulty terms common grounding enabling easy evaluation analysis complex models paper propose minimal dialogue task requires advanced skills common grounding continuous partially-observable context based task formulation collected largescale dataset 6,760 dialogues fulfills essential requirements natural language corpora analysis dataset revealed important phenomena related common grounding need considered finally evaluate analyze baseline neural models simple subtask requires recognition created common ground show simple baseline models perform decently leave room improvement overall show proposed task fundamental testbed train evaluate analyze dialogue system ’ ability sophisticated common grounding	positive
semi-supervised learning for electron microscopy image segmentation	research field called connectomics aimed investigate structure connection neural system brain sensory organ living things earlier studies proposed method help experts suffer labeling three-dimensional reconstruction important process observe tiny neuronal structure detail paper proposed semi-supervised learning method performs pseudo-labeling makes possible automatically segment neuronal regions using small amount labeled data experimental result showed method outperformed normal supervised learning labeled samples accuracy sufficient yet	negative
efficiently combining human demonstrations and interventions for safe training of autonomous systems in real-time	paper investigates utilize different forms human interaction safely train autonomous systems realtime learning human demonstrations interventions implement two components cycle-of learning autonomous systems framework combining multiple modalities human interaction current effort employs human demonstrations teach desired behavior via imitation learning leverages intervention data correct undesired behaviors produced imitation learner teach novel tasks autonomous agent safely minutes training demonstrate method autonomous perching task using quadrotor continuous roll pitch yaw throttle commands imagery captured downward-facing camera high-fidelity simulated environment method improves task completion performance amount human interaction compared learning demonstrations alone also requiring average 32 less data achieve performance provides evidence combining multiple modes human interaction increase training speed overall performance policies autonomous systems	positive
vpds: an ai-based automated vehicle occupancy and violation detection system	high occupancy vehicle/high occupancy tolling hov/hot lanes operated based voluntary hov declarations drivers majority declarations wrong leverage faster hov lane speeds illegally herculean task manually regulate hov lanes identify violators therefore automated way counting number people car prudent fair tolling violator detection	negative
bayesian execution skill estimation	performance agents many domains continuous action spaces depends ability select good actions execute also ability execute planned actions precisely ability called agent ’ execution skill important characteristic agent significant impact success paper address problem estimating execution skill agent given observations agent acting domain observation includes executed action description state action executed reward received notably excludes action agent intended execute previously introduced problem demonstrated estimating agent ’ execution skill possible certain conditions previous method focused entirely reward agent received executed actions assumed agent able select optimal action state paper addresses execution skill estimation problem entirely different perspective focusing instead action executed present bayesian framework reasoning action observations show able outperform previous methods conditions also show flexibility framework allows applied settings previous limiting assumptions met success proposed method demonstrated experimentally toy domain well domain computational billiards	positive
a general planning-based framework for goal-driven conversation assistant	propose general framework goal-driven conversation assistant based planning methods aims rapidly build dialogue agent less handcrafting make interpretable efficient dialogue management various scenarios employing planning method dialogue actions efficiently defined reusable transition dialogue managed planner proposed framework consists pipeline natural language understanding intent labeler planning actions world model natural language generation learned attention-based neural network demonstrate approach creating conversational agents several independent domains	positive
the utility of sparse representations for control in reinforcement learning	investigate sparse representations control reinforcement learning representations widely used computer vision prevalence reinforcement learning limited sparse coding extracting representations new data computationally intensive begin demonstrating learning control policy incrementally representation standard neural network fails classic control domains whereas learning representation obtained neural network sparsity properties enforced effective provide evidence reason sparse representation provides locality avoids catastrophic interference particularly keeps consistent stable values bootstrapping discuss learn sparse representations explore idea distributional regularizers activation hidden nodes encouraged match particular distribution results sparse activation across time identify simple effective way obtain sparse representations afforded previously proposed strategies making practical investigation sparse representations reinforcement learning	negative
efficiently reasoning with interval constraints in forward search planning	paper present techniques reasoning natively quantitative/qualitative interval constraints statebased pddl planners considered important modeling solving problems timeline based planners reasoning pddl planners seen relatively little attention yet crucial step towards making pddl planners applicable real-world scenarios space missions main contribution extend planner optic reason natively allen interval constraints show approach outperforms mtp pddl planner capable handling similar constraints compilation pddl 2.1 order magnitude go present initial results indicating approach competitive timeline based planner mars rover domain showing potential pddl planners setting	positive
grading uncompilable programs	evaluators wish test candidates ability propose correct algorithmic approach solve programming problems recently several automated systems grading programs proposed none address uncompilable codes present first approach grade uncompilable codes provide semantic feedback using machine learning propose two methods allow us derive informative semantic features programs one approach makes program compilable correcting errors relaxes syntax/grammar rules help parse uncompilable codes compare relative efficacy approaches towards grading finally combine build algorithm rivals accuracy experts grading programs additionally show models learned compilable codes reused uncompilable codes present case studies companies able hire efficiently deploying technology	negative
get it scored using autosas — an automated system for scoring short answers	era moocs online exams taken millions candidates scoring short answers integral part becomes intractable evaluate human graders thus generic automated system capable grading responses designed deployed paper present fast scalable accurate approach towards automated short answer scoring sas propose explain design development system sas namely autosas given question along graded samples autosas learn grade prompt successfully paper lays features lexical diversity word2vec prompt content overlap plays pivotal role building proposed model also present methodology indicating factors responsible scoring answer trained model evaluated extensively used public dataset namely automated student assessment prize short answer scoring asap-sas autosas shows state-of-the-art performance achieves better results 8 question prompts measured quadratic weighted kappa qwk showing performance comparable humans	negative
sdrl: interpretable and data-efficient deep reinforcement learning leveraging symbolic planning	deep reinforcement learning drl gained great success learning directly high-dimensional sensory inputs yet notorious lack interpretability interpretability subtasks critical hierarchical decision-making increases transparency black-box-style drl approach helps rl practitioners understand high-level behavior system better paper introduce symbolic planning drl propose framework symbolic deep reinforcement learning sdrl handle high-dimensional sensory inputs symbolic planning task-level interpretability enabled relating symbolic actions options.this framework features planner – controller – meta-controller architecture takes charge subtask scheduling data-driven subtask learning subtask evaluation respectively three components cross-fertilize eventually converge optimal symbolic plan along learned subtasks bringing together advantages long-term planning capability symbolic knowledge end-to-end reinforcement learning directly high-dimensional sensory input experimental results validate interpretability subtasks along improved data efficiency compared state-of-the-art approaches	negative
cooperation enforcement and collusion resistance in repeated public goods games	enforcing cooperation among substantial agents one main objectives multi-agent systems however due existence inherent social dilemmas many scenarios free-rider problem may arise agents ’ long-run interactions things become even severer self-interested agents work collusion get extra benefits commonly accepted social dilemmas exists simple strategy agent whereby simultaneously manipulate utility opponents promote mutual cooperation among agents show strategies exist conventional repeated public goods game novelly identify find confronted strategies single opponent maximize utility via global cooperation colluding alliance get upper hand since full cooperation individually optimal single opponent stable cooperation among players achieved moreover experimentally show strategies still promote cooperation even opponents self-learning collusive	negative
automatic detection and compression for passive acoustic monitoring of the african forest elephant	work consider applying machine learning analysis compression audio signals context monitoring elephants sub-saharan africa earth ’ biodiversity increasingly threat sources anthropogenic change e.g resource extraction land use change climate change surveying animal populations critical developing conservation strategies however manually monitoring tropical forests deep oceans intractable species communicate acoustically researchers argued placing audio recorders habitats costeffective non-invasive method strategy known passive acoustic monitoring pam collaboration conservation efforts construct large labeled dataset passive acoustic recordings african forest elephant via crowdsourcing compromising thousands hours recordings wild using state-of-the-art techniques artificial intelligence improve upon previously proposed methods passive acoustic monitoring classification segmentation real-time detection elephant calls network bandwidth quickly becomes bottleneck efficient ways compress data needed audio compression schemes aimed human listeners unsuitable low-frequency elephant calls remedy provide novel end-to-end differentiable method compression audio signals adapted acoustic monitoring species dramatically improves naive coding strategies	negative
building causal graphs from medical literature and electronic medical records	large repositories medical data electronic medical record emr data recognized promising sources knowledge discovery effective analysis repositories often necessitate thorough understanding dependencies data example patient age ignored one might wrongly conclude causal relationship cataract hypertension confounding variables often identified causal graphs variables connected causal relationships current approaches automatically building graphs based text analysis medical literature yet result typically large graph low precision statistical methods constructing causal graphs observational data less suitable dealing large number covariates case emr data consequently confounding variables often identified medical domain experts via manual expensive time-consuming process	negative
counterfactual randomization: rescuing experimental studies from obscured confounding	randomized clinical trials rcts like conducted fda provide medical practitioners average effects treatments generally desirable observational studies due control unobserved confounders ucs viz. latent factors influence treatment recovery however recent results causal inference shown randomization results subsequent loss information ucs may impede treatment efficacy left uncontrolled practice bareinboim forney pearl 2015 paper presents novel experimental design noninvasively layered atop past future rcts expose presence ucs system also reveal patient- practitioner-specific treatment effects order improve decision-making applications given personalized medicine second opinions diagnosis employing offline results online recommender systems	negative
composable modular reinforcement learning	modular reinforcement learning mrl decomposes monolithic multiple-goal problem modules solve portion original problem modules ’ action preferences arbitrated determine action taken agent truly modular reinforcement learning would support decomposition modules composability separately written modules new modular reinforcement learning agents however performance mrl agents arbitrate module preferences using additive reward schemes degrades modules incomparable reward scales performance degradation means separately written modules composed new modular reinforcement learning agents as-is – may need modified align reward scales solve problem q-learningbased command arbitration algorithm demonstrate exhibit performance degradation existing approaches mrl thereby supporting composability	negative
partially observable multi-sensor sequential change detection: a combinatorial multi-armed bandit approach	paper explores machine learning address problem partially observable multi-sensor sequential change detection pomscd subset sensors observed monitor target system change-point detection online learning round contrast traditional multisensor sequential change detection tasks sensors observable pomscd much challenging learner needs detect on-the-fly whether change occurs based partially observed multi-sensor data streams also needs cleverly choose subset informative sensors observed next learning round order maximize overall sequential change detection performance paper present first online learning study tackle pomscd systemic rigorous way approach twofold novelties attempt detect changepoints partial observations effectively exploiting potential correlations sensors ii formulate sensor subset selection task multi-armed bandit mab problem develop effective adaptive sampling strategy using mab algorithms offer theoretical analysis proposed online learning solution validate empirical performance via extensive set numerical studies together case study real-world data sets	positive
chexpert: a large chest radiograph dataset with uncertainty labels and expert comparison	large labeled datasets driven deep learning methods achieve expert-level performance variety medical imaging tasks present chexpert large dataset contains 224,316 chest radiographs 65,240 patients design labeler automatically detect presence 14 observations radiology reports capturing uncertainties inherent radiograph interpretation investigate different approaches using uncertainty labels training convolutional neural networks output probability observations given available frontal lateral radiographs validation set 200 chest radiographic studies manually annotated 3 board-certified radiologists find different uncertainty approaches useful different pathologies evaluate best model test set composed 500 chest radiographic studies annotated consensus 5 board-certified radiologists compare performance model 3 additional radiologists detection 5 selected pathologies cardiomegaly edema pleural effusion model roc pr curves lie 3 radiologist operating points release dataset public standard benchmark evaluate performance chest radiograph interpretation models	negative
oversampling for imbalanced data via optimal transport	issue data imbalance occurs many real-world applications especially medical diagnosis normal cases usually much abnormal cases alleviate issue one important approaches oversampling method seeks synthesize minority class samples balance numbers different classes however existing methods barely consider global geometric information involved distribution minority class samples thus may incur distribution mismatching real synthetic samples paper relying optimal transport villani 2008 propose oversampling method exploiting global geometric information data make synthetic samples follow similar distribution minority class samples moreover introduce novel regularization based synthetic samples shift distribution minority class samples according loss information experiments toy real-world data sets demonstrate efficacy proposed method terms multiple metrics	negative
mai: an intelligent model acquisition interface for interactive specification of dialogue agents	state art automated conversational agents enterprise e.g customer support require lengthy design process experts loop figure specify complex conversation patterns demonstration looks prototype interface aims bring expertise required design agents well time taken specifically focus metawriter assist domain-writer design process complex conversation patterns derived simplifying abstractions interface level	negative
implanting rational knowledge into distributed representation at morpheme level	previously researchers paid attention creation unambiguous morpheme embeddings independent corpus information plays important role expressing exact meanings words parataxis languages like chinese paper constructing chinese lexical semantic ontology based word-formation propose novel approach implanting structured rational knowledge distributed representation morpheme level naturally avoiding heavy disambiguation corpus design template create instances pseudo-sentences merely pieces knowledge morphemes built lexicon exploit hierarchical information tackle data sparseness problem instance proliferation technique applied based similarity expand collection pseudo-sentences distributed representation morphemes trained pseudo-sentences using word2vec evaluation validate paradigmatic syntagmatic relations morpheme embeddings apply obtained embeddings word similarity measurement achieving significant improvements classical models 5 spearman scores 8 percentage points shows promising prospects adoption new source knowledge	positive
declarative question answering over knowledge bases containing natural language text with answer set programming	recent years machine learning ml based approaches popular approach developing endto-end question answering systems systems often struggle additional knowledge needed correctly answer questions proposed alternatives involve translating question natural language text logical representation use logical reasoning however alternative falters size text gets bigger address propose approach logical reasoning premises written natural language text proposed method uses recent features answer set programming asp call external nlp modules may based ml perform simple textual entailment test approach develop corpus based life cycle questions showed system achieves 18 performance gain compared standard mcq solvers	negative
refining abstraction heuristics during real-time planning	real-time planning planner must select next action within fixed time bound complete plan may found selected action might lead goal agent may need return current state preserve completeness real-time search methods incorporate learning heuristic values updated previous work real-time search used table-based heuristics values states updated individually paper explore use abstraction-based heuristics refining abstraction on-line update values multiple states including ones agent yet generated test idea empirically using cartesian abstractions fast downward planner results various benchmarks including sliding tile puzzle several ipc domains indicate approach improve performance compared traditional heuristic updating work brings abstraction refinement powerful technique offline planning real-time setting	negative
machine teaching for inverse reinforcement learning: algorithms and applications	inverse reinforcement learning irl infers reward function demonstrations allowing policy improvement generalization however despite much recent interest irl little work done understand minimum set demonstrations needed teach specific sequential decisionmaking task formalize problem finding maximally informative demonstrations irl machine teaching problem goal find minimum number demonstrations needed specify reward equivalence class demonstrator extend previous work algorithmic teaching sequential decision-making tasks showing reduction set cover problem enables efficient approximation algorithm determining set maximallyinformative demonstrations apply proposed machine teaching algorithm two novel applications providing lower bound number queries needed learn policy using active irl developing novel irl algorithm learn efficiently informative demonstrations standard irl approach	negative
you get what you share: incentives for a sharing economy	recent years range online applications facilitated resource sharing among users resulting significant increase resource utilization applications sharing one ’ resources skills agents increases social welfare general agent look agents whose available resources complement thereby forming natural sharing groups paper study settings large population self-organizes sharing groups many cases centralized optimization approaches creating optimal partition user population infeasible either central authority necessary information compute optimal partition power enforce partition instead central authority puts place incentive structure form utility sharing method letting participants form sharing groups first analyze simple equal-sharing method one typically encountered practice show lead highly inefficient equilibria propose shapley-sharing method show significantly improves overall social welfare	negative
multi-unit bilateral trade	characterise set dominant strategy incentive compatible dsic strongly budget balanced sbb ex-post individually rational ir mechanisms multi-unit bilateral trade setting setting single buyer single seller holds finite number k identical items mechanism decide many units item transferred seller buyer much money transferred buyer seller consider two classes valuation functions buyer seller valuations increasing number units possession specific class valuations increasing submodular	positive
personalized robot tutoring using the assistive tutor pomdp (at-pomdp)	selecting appropriate tutoring help actions account student ’ content mastery engagement level essential effective human tutors indicating critical need skills autonomous tutors work formulate robot-student tutoring help action selection problem assistive tutor partially observable markov decision process at-pomdp designed at-pomdp derived parameters based data prior robot-student tutoring study policy results solving at-pomdp allows robot tutor decide upon optimal tutoring help action give student maintaining belief student ’ mastery material engagement task approach validated between-subjects field study involved 4th grade students n=28 interacting social robot solving long division problems five sessions students received help robot using at-pomdp policy demonstrated significantly greater learning gains students received help robot fixed help action selection policy results demonstrate robust computational framework used effectively deliver diverse personalized tutoring support time students	negative
strategic tasks for explainable reinforcement learning	commonly used sequential decision making tasks games arcade learning environment ale provide rich observation spaces suitable deep reinforcement learning however consist mostly low-level control tasks limited use development explainable artificial intelligence xai due fine temporal resolution tasks many domains also lack built-in high level abstractions symbols existing tasks provide strategic decision-making rich observation spaces either difficult simulate intractable provide set new strategic decision-making tasks specialized development evaluation explainable ai methods built constrained mini-games within starcraft ii learning environment	positive
text assisted insight ranking using context-aware memory network	extracting valuable facts informative summaries multi-dimensional tables i.e insight mining important task data analysis business intelligence however ranking importance insights remains challenging unexplored task main challenge explicitly scoring insight giving rank requires thorough understanding tables costs lot manual efforts leads lack available training data insight ranking problem paper propose insight ranking model consists two parts neural ranking model explores data characteristics header semantics data statistical features memory network model introduces table structure context information ranking process also build dataset text assistance experimental results show approach largely improves ranking precision reported multi evaluation metrics	negative
solving partially observable stochastic games with public observations	many real-world problems dynamic interaction competitive agents partially observable stochastic games posgs among general formal models capture dynamic scenarios model captures stochastic events partial information players environment scenario fixed horizon solving posgs general setting intractable.therefore research focused subclasses posgs value game admit designing approximate optimal algorithms propose subclass two-player zero-sum games discounted-sum objective function—posgs public observations poposgs —where player able reconstruct beliefs player unobserved states results include 1 theoretical analysis po-posgs value functions showing convexity concavity beliefs maximizing minimizing player 2 novel algorithm approximating value game 3 practical demonstration scalability algorithm experimental results show algorithm closely approximate value non-trivial games hundreds states	negative
an affect-rich neural conversational model with biased attention and weighted cross-entropy loss	affect conveys important implicit information human communication capability correctly express affect human-machine conversations one major milestones artificial intelligence recent years extensive research open-domain neural conversational models conducted however embedding affect models still explored paper propose endto-end affect-rich open-domain neural conversational model produces responses appropriate syntax semantics also rich affect model extends seq2seq model adopts vad valence arousal dominance affective notations embed word affects addition model considers effect negators intensifiers via novel affective attention mechanism biases attention towards affect-rich words input sentences lastly train model affect-incorporated objective function encourage generation affect-rich words output responses evaluations based perplexity human evaluations show model outperforms state-of-the-art baseline model comparable size producing natural affect-rich responses	negative
generating multiple diverse responses for short-text conversation	neural generative models become popular achieved promising performance short-text conversation tasks generally trained build 1-to-1 mapping input post output response however given post often associated multiple replies simultaneously real applications previous research task mainly focuses improving relevance informativeness top one generated response post works study generating multiple accurate diverse responses post paper propose novel response generation model considers set responses jointly generates multiple diverse responses simultaneously reinforcement learning algorithm designed solve model experiments two short-text conversation tasks validate multiple responses generated model obtain higher quality larger diversity compared various state-ofthe-art generative models	negative
end-to-end knowledge-routed relational dialogue system for automatic diagnosis	beyond current conversational chatbots task-oriented dialogue systems attracted increasing attention move forward develop dialogue system automatic medical diagnosis converses patients collect additional symptoms beyond self-reports automatically makes diagnosis besides challenges conversational dialogue systems e.g topic transition coherency question understanding automatic medical diagnosis poses critical requirements dialogue rationality context medical knowledge symptom-disease relations existing dialogue systems madotto wu fung 2018 wei et al 2018 li et al 2017 mostly rely datadriven learning able encode extra expert knowledge graph work propose end-to-end knowledge-routed relational dialogue system kr-ds seamlessly incorporates rich medical knowledge graph topic transition dialogue management makes cooperative natural language understanding natural language generation novel knowledge-routed deep q-network kr-dqn introduced manage topic transitions integrates relational refinement branch encoding relations among different symptoms symptomdisease pairs knowledge-routed graph branch topic decision-making extensive experiments public medical dialogue dataset show kr-ds significantly beats stateof-the-art methods 8 diagnosis accuracy show superiority kr-ds newly collected medical dialogue system dataset challenging retaining original self-reports conversational data patients doctors	positive
be inaccurate but don’t be indecisive: how error distribution can affect user experience	system accuracy crucial factor influencing user experience intelligent interactive systems although accuracy known important little known role system ’ error distribution user experience paper study context background music selection tabletop games error distribution intelligent system affects user ’ perceived experience particular show supervised learning algorithms solely optimize prediction accuracy make system “ indecisive ” make system ’ errors sparsely distributed throughout game session hypothesize sparsely distributed errors harm users ’ perceived experience preferable use model somewhat inaccurate decisive model accurate often indecisive order test hypothesis introduce ensemble approach restrictive voting rule instead erring sparsely time errs consistently period time user study people watched videos dungeons dragons sessions supports hypothesis	negative
image block augmentation for one-shot learning	given one training instances novel classes oneshot learning task requires classifier generalizes novel classes directly training one-shot classifier may suffer insufficient training instances one-shot learning previous one-shot learning works investigate metalearning metric-based algorithms contrast paper proposes self-training jigsaw augmentation self-jig method one-shot learning particularly solve one-shot learning directly augmenting training images leveraging vast unlabeled instances precisely proposed self-jig algorithm synthesize new images labeled probe unlabeled gallery images labels gallery images predicted help augmentation process taken self-training scheme intrinsically argue provide useful way directly generating massive amounts training images novel classes extensive experiments ablation study evaluate efficacy also reveal insights proposed self-jig method	positive
a multi-agent communication framework for question-worthy phrase extraction and question generation	question generation aims produce questions automatically given piece text input existing research follows sequence-to-sequence fashion constructs single question based input considering question usually focuses specific fragment input especially scenario reading comprehension reasonable identify corresponding focus constructing question paper propose identify question-worthy phrases first generate questions assistance phrases introduce multi-agent communication framework taking phrase extraction question generation two agents learn two tasks simultaneously via message passing mechanism results experiments show effectiveness framework extract question-worthy phrases able improve performance question generation besides system able extract one question worthy phrases generate multiple questions accordingly	negative
analysis of joint multilingual sentence representations and semantic k-nearest neighbor graphs	multilingual sentence document representations becoming increasingly important build recent advances multilingual sentence encoders focus efficiency large-scale applicability specifically construct investigate k-nn graph joint space 566 million news sentences seven different languages show excellent multilingual retrieval quality un corpus 11.3m sentences extends zero-shot case never seen language provide detailed analysis multilingual sentence encoder twenty-one european languages learned graph sentence encoder language agnostic supports code switching	negative
controllable image-to-video translation: a case study on facial expression generation	recent advances deep learning made possible generate photo-realistic images using neural networks even extrapolate video frames input video clip paper sake furthering exploration interest realistic application study imageto-video translation particularly focus videos facial expressions problem challenges deep neural networks another temporal dimension comparing image-to-image translation moreover single input image fails existing video generation methods rely recurrent models propose user-controllable approach generate video clips various lengths single face image lengths types expressions controlled users end design novel neural network architecture incorporate user input skip connections propose several improvements adversarial training method neural network experiments user studies verify effectiveness approach especially would like highlight even face images wild downloaded web authors ’ photos model generate high-quality facial expression videos 50 labeled real amazon mechanical turk workers	negative
hirenet: a hierarchical attention model for the automatic analysis of asynchronous video job interviews	new technologies drastically change recruitment techniques research projects aim designing interactive systems help candidates practice job interviews studies aim automatic detection social signals e.g smile turn speech etc ... videos job interviews studies limited respect number interviews process also fact analyze simulated job interviews e.g students pretending apply fake position asynchronous video interviewing tools become mature products human resources market thus popular step recruitment process part project help recruiters collected corpus 7000 candidates asynchronous video job interviews real positions recording videos answering set questions propose new hierarchical attention model called hirenet aims predicting hirability candidates evaluated recruiters hirenet interview considered sequence questions answers containing salient socials signals two contextual sources information modeled hirenet words contained question job position model achieves better f1-scores previous approaches modality verbal content audio video results early late multimodal fusion suggest sophisticated fusion schemes needed improve monomodal results finally examples moments captured attention mechanisms suggest model could potentially used help finding key moments asynchronous job interview	negative
active preference learning based on generalized gini functions: application to the multiagent knapsack problem	consider problem actively eliciting preferences decision maker supervising collective decision process context fair multiagent combinatorial optimization individual preferences supposed known represented linear utility functions defined combinatorial domain social utility defined generalized gini social evaluation function gsf sake fairness gsf non-linear aggregation function parameterized weighting coefficients allow fine control equity requirement aggregation individual utilities paper focuses elicitation weights active learning context fair multiagent knapsack problem introduce compare several incremental decision procedures interleaving adaptive preference elicitation procedure combinatorial optimization algorithm determine gsf-optimal solution establish upper bound number queries provide numerical tests show efficiency proposed approach	positive
hierarchical context enabled recurrent neural network for recommendation	long user history inevitably reflects transitions personal interests time analyses user history require robust sequential model anticipate transitions decays user interests user history often modeled various rnn structures rnn structures recommendation system still suffer long-term dependency interest drifts resolve challenges suggest hcrnn three hierarchical contexts global local temporary interests structure designed withhold global long-term interest users reflect local sub-sequence interests attend temporary interests transition besides propose hierarchical context-based gate structure incorporate interest drift assumption suggest new rnn structure support hcrnn complementary bi-channel attention structure utilize hierarchical context experimented suggested structure sequential recommendation tasks citeulike movielens lastfm model showed best performances sequential recommendations	negative
flex: faithful linguistic explanations for neural net based model decisions	explaining decisions deep learning network imperative safeguard end-user trust explanations must intuitive descriptive faithfully explain model makes decisions work propose framework called flex faithful linguistic explanations generates post-hoc linguistic justifications rationalize decision convolutional neural network flex explains model ’ decision terms features responsible decision derive novel way associate features words introduce new decision-relevance metric measures faithfulness explanation model ’ reasoning experiment results two benchmark datasets demonstrate proposed framework generate discriminative faithful explanations compared state-of-the-art explanation generators also show flex generate explanations images unseen classes well automatically annotate objects images	negative
an innovative genetic algorithm for the quantum circuit compilation problem	quantum computing represents next big step towards speed boost computation promises major breakthroughs several disciplines including artificial intelligence paper investigates performance genetic algorithm optimize realization compilation nearest-neighbor compliant quantum circuits currrent technological limitations e.g. decoherence effect impose overall duration makespan quantum circuit realization minimized therefore makespanminimization problem compiling quantum algorithms present future quantum machines dragging increasing attention ai community genetic algorithm solution built utilizing novel chromosome encoding gene controls iterative selection quantum gate inserted solution lexicographic double-key ranking returned heuristic function recently published literature	negative
dynamic contracting under positive commitment	consider firm sells products arrive time buyer study problem notion call positive commitment seller allowed make binding positive promises buyer items arriving future allowed commit make offers buyer future model problem dynamic game seller chooses mechanism period subject sequential rationality constraint characterize perfect bayesian equilibrium dynamic game prove equilibrium efficient seller ’ revenue function buyer ’ ex ante utility commitment model particular goods sold advance buyer call positive commitment price	positive
domain agnostic real-valued specificity prediction	sentence specificity quantifies level detail sentence characterizing organization information discourse information useful many downstream applications specificity prediction systems predict coarse labels binary ternary trained tailored toward specific domains e.g. news goal work generalize specificity prediction domains labeled data available output nuanced realvalued specificity ratings.we present unsupervised domain adaptation system sentence specificity prediction specifically designed output real-valued estimates binary training labels calibrate values predictions appropriately regularize posterior distribution labels towards reference distribution show framework generalizes well three different domains 50 -68 mean absolute error reduction current state-of-the-art system trained news sentence specificity also demonstrate potential work improving quality informativeness dialogue generation systems	negative
understanding actors and evaluating personae with gaussian embeddings	understanding narrative content become increasingly popular topic nonetheless research identifying common types narrative characters personae impeded lack automatic broad-coverage evaluation methods argue computationally modeling actors provides benefits including novel evaluation mechanisms personae specifically propose two actor-modeling tasks cast prediction versatility ranking capture complementary aspects relation actors characters portray actor model present technique embedding actors movies character roles genres descriptive keywords gaussian distributions translation vectors gaussian variance corresponds actors ’ versatility empirical results indicate 1 technique considerably outperforms transe bordes et al 2013 ablation baselines 2 automatically identified persona topics bamman ’ connor smith 2013 yield statistically significant improvements tasks whereas simplistic persona descriptors including age gender perform inconsistently validating prior research	negative
learning optimal strategies to commit to	past decades various theories algorithms developed framework stackelberg games part innovations fielded scenarios national security defenses wildlife protections however one remaining difficulties literature theoretical works assume full information payoff matrices applications leader often prior knowledge follower ’ payoff matrix may gain information follower ’ utility function repeated interactions paper study problem learning optimal leader strategy stackelberg security games develop novel algorithms well new hardness results	positive
multi-source neural variational inference	learning multiple sources information important problem machine-learning research key challenges learning representations formulating inference methods take account complementarity redundancy various information sources paper formulate variational autoencoder based multi-source learning framework encoder conditioned different information source allows us relate sources via shared latent variables computing divergence measures individual source ’ posterior approximations explore variety options learn encoders integrate beliefs compute consistent posterior approximation visualise learned beliefs toy dataset evaluate methods learning shared representations structured output prediction showing trade-offs learning separate encoders information source furthermore demonstrate conflict detection redundancy increase robustness inference multi-source setting	negative
recurrent stacking of layers for compact neural machine translation models	encoder-decoder based sequence-to-sequence modeling common practice stack number recurrent convolutional feed-forward layers encoder decoder addition new layer improves sequence generation quality also leads significant increase number parameters paper propose share parameters across layers thereby leading recurrently stacked sequence-to-sequence model report extensive case study neural machine translation nmt using proposed method experimenting variety datasets empirically show translation quality model recurrently stacks single-layer 6 times despite significantly fewer parameters approaches model stacks 6 different layers also show method benefit prevalent way improving nmt i.e. extending training data pseudo-parallel corpora generated back-translation analyze effects recurrently stacked layers visualizing attentions models use recurrently stacked layers models finally explore limits parameter sharing share even parameters encoder decoder addition recurrent stacking layers	negative
verifiable and interpretable reinforcement learning through program synthesis	study problem generating interpretable verifiable policies reinforcement learning rl unlike popular deep reinforcement learning drl paradigm policy represented neural network aim work find policies represented highlevel programming languages programmatic policies several benefits including easily interpreted neural networks amenable verification scalable symbolic methods generation methods programmatic policies also provide mechanism systematically using domain knowledge guiding policy search interpretability verifiability policies provides opportunity deploy rl based solutions safety critical environments thesis draws extends work machine learning formal methods communities	negative
molecular property prediction: a multilevel quantum interactions modeling perspective	predicting molecular properties e.g. atomization energy essential issue quantum chemistry could speed much research progress drug designing substance discovery traditional studies based density functional theory dft physics proved time-consuming predicting large number molecules recently machine learning methods consider much rule-based information also shown potentials issue however complex inherent quantum interactions molecules still largely underexplored existing solutions paper propose generalizable transferable multilevel graph convolutional neural network mgcn molecular property prediction specifically represent molecule graph preserve internal structure moreover well-designed hierarchical graph neural network directly extracts features conformation spatial information followed multilevel interactions consequence multilevel overall representations utilized make prediction extensive experiments datasets equilibrium off-equilibrium molecules demonstrate effectiveness model furthermore detailed results also prove mgcn generalizable transferable prediction	positive
the rensselaer mandarin project — a cognitive and immersive language learning environment	rensselaer mandarin project enables group foreign language students improve functional understanding pronunciation vocabulary mandarin chinese authentic speaking situations virtual visit china students use speech gestures combinations thereof navigate immersive mixed reality stylized realism game experience interaction ai agents immersive technologies game mechanics environment developed black box theater equipped human-scale 360◦ panoramic screen 140h 200r arrays markerless motion tracking sensors speakers spatial audio	negative
unknown agents in friends oriented hedonic games: stability and complexity	study hedonic games friends appreciation agent considers agents friends enemies unknown agents although existing work assumed unknown agents impact agent ’ preference may preference depends number unknown agents coalition extend existing preference friends appreciation proposing two alternative attitudes toward unknown agents extraversion introversion depending whether unknown agents slightly positive negative impact preference agent prefers coalitions unknown agents show core stable outcomes individually stable outcomes may exist also prove deciding existence core existence individual stable coalition structure respectively npnp-complete np-complete	negative
mfpca: multiscale functional principal component analysis	consider problem performing dimension reduction heteroscedastic functional data variance different scales entire domain aim paper propose novel multiscale functional principal component analysis mfpca approach address heteroscedastic issue key ideas mfpca partition whole domain several subdomains according scale variance conduct usual functional principal component analysis fpca individual subdomain theoretically numerically show mfpca capture features areas low variance without estimating high-order principal components leading overall improvement performance dimension reduction heteroscedastic functional data contrast traditional fpca prioritizes optimizing performance subdomain larger data variance requires practically prohibitive number components characterize data region bearing relatively small variance	positive
quasi-perfect stackelberg equilibrium	equilibrium refinements important extensive-form i.e. tree-form games amend weaknesses nash equilibrium concept requiring sequential rationality beneficial properties one attractive refinement concepts quasi-perfect equilibrium quasiperfection studied extensive-form games poorly understood stackelberg settings—that settings leader commit strategy—which important modeling example security games paper introduce axiomatic definition quasi-perfect stackelberg equilibrium develop broad class game perturbation schemes lead limit class perturbation schemes strictly generalizes prior perturbation schemes introduced computation non-stackelberg quasi-perfect equilibria based perturbation schemes develop branch-and-bound algorithm computing quasi-perfect stackelberg equilibrium leverages perturbed variant linear program computing stackelberg extensive-form correlated equilibrium experiments show algorithm used find approximate quasi-perfect stackelberg equilibrium games thousands nodes	negative
bringing order to chaos – a compact representation of partial order in sat-based htn planning	htn planning provides expressive formalism model complex application domains widely used realworld applications however development domainindependent planning techniques models still lacking behind need informed statetransitions task hierarchy makes realisation search-based approaches difficult especially unrestricted partial ordering tasks htn domains recently translation htn planning problems propositional logic shown promising empirical results planners benefit unified representation state hierarchy require large formulae represent partial order paper introduce novel encoding htn planning sat contrast related work reasoning ordering relations left sat solver done beforehand results much smaller formulae shown evaluation planner outperforms previous sat-based approaches well state-of-the-art search-based htn planning	negative
incorporating network embedding into markov random field for better community detection	recent research community detection focuses learning representations nodes using different network embedding methods feeding normal features clustering algorithms however find though one may good results direct clustering based network embedding features ample room improvement seriously many real networks statisticallysignificant nodes play pivotal roles often divided incorrect communities using network embedding methods distance measures used capture spatial relationship nodes embedding nodes mapping feature vectors essentially coupled losing important structural information address problem propose general markov random field mrf framework incorporate coupling network embedding allows better detecting network communities smartly utilizing properties mrf new framework preserves advantages network embedding e.g low complexity high parallelizability applicability traditional machine learning also alleviates core drawback inadequate representations dependencies via making missing coupling relationships experiments real networks show new approach improves accuracy existing embedding methods e.g node2vec deepwalk mnmf corrects wrongly-divided statistically-significant nodes makes network embedding essentially suitable real community detection applications new approach also outperforms state-of-the-art conventional community detection methods	negative
found in translation: learning robust joint representations by cyclic translations between modalities	multimodal sentiment analysis core research area studies speaker sentiment expressed language visual acoustic modalities central challenge multimodal learning involves inferring joint representations process relate information modalities however existing work learns joint representations requiring modalities input result learned representations may sensitive noisy missing modalities test time recent success sequence sequence seq2seq models machine translation opportunity explore new ways learning joint representations may require input modalities test time paper propose method learn robust joint representations translating modalities method based key insight translation source target modality provides method learning joint representations using source modality input augment modality translations cycle consistency loss ensure joint representations retain maximal information modalities translation model trained paired multimodal data need data source modality test time final sentiment prediction ensures model remains robust perturbations missing information modalities train model coupled translationprediction objective achieves new state-of-the-art results multimodal sentiment analysis datasets cmu-mosi ictmmmo youtube additional experiments show model learns increasingly discriminative joint representations input modalities maintaining robustness missing perturbed modalities	negative
exact and approximate weighted model integration with probability density functions using knowledge compilation	weighted model counting recently extended weighted model integration used solve hybrid probabilistic reasoning problems problems involve discrete continuous probability distributions show standard knowledge compilation techniques sdds d-dnnfs apply weighted model integration use two novel solvers one exact one approximate solver furthermore extend class employable weight functions actual probability density functions instead mere polynomial weight functions	negative
predicting and analyzing language specificity in social media posts	computational linguistics specificity quantifies much detail engaged text important characteristic speaker intention language style useful nlp applications summarization argumentation mining yet date expert-annotated data sentence-level specificity scarce confined news genre addition systems predict sentence specificity classifiers trained produce binary labels general specific	positive
human-like sketch object recognition via analogical learning	deep learning systems perform well image recognition tasks however serious limitations including requiring far training data humans fooled adversarial examples contrast analogical learning relational representations tends far data-efficient requiring human-like amounts training data paper introduces approach combines automatically constructed qualitative visual representations analogical learning tackle hard computer vision problem object recognition sketches results mnist dataset novel dataset coloring book objects dataset provided comparison existing approaches indicates analogical generalization used identify sketched objects datasets several orders magnitude fewer examples deep learning systems require	negative
community detection in social networks considering topic correlations	network contents including node contents edge contents utilized community detection social networks thus topic community extracted semantic information plethora models integrating topic model network topologies proposed however key problem resolved semantic division community since definition community based topology community might involve several topics ach	negative
low-distortion social welfare functions	work implicit utilitarian voting advocates design preference aggregation methods maximize utilitarian social welfare respect latent utility functions based observed rankings alternatives approach successfully deployed order help people choose single alternative subset alternatives previously unclear apply approach design social welfare functions desired output ranking propose address problem assuming voters ’ utilities rankings induced unknown weights unknown utility functions moreover combinatorial subadditive structure despite extreme lack information voters ’ preferences show possible choose rankings worst-case gap social welfare optimal ranking called distortion larger polylogarithmic factors distortion associated much simpler problems experiments identify practical methods achieve nearoptimal social welfare average	negative
state-augmentation transformations for risk-sensitive reinforcement learning	framework mdp although general reward function takes three arguments—current state action successor state often simplified function two arguments—current state action former called transition-based reward function whereas latter called state-based reward function objective involves expected total reward simplification works perfectly however objective risk-sensitive simplification leads incorrect value propose three successively general state-augmentation transformations sats preserve reward sequences well reward distributions optimal policy risk-sensitive reinforcement learning risk-sensitive scenarios firstly prove every mdp stochastic transition-based reward function exists mdp deterministic state-based reward function given randomized policy first mdp exists corresponding policy second mdp markov reward processes share reward sequence secondly illustrate two situations require proposed sats inventory control problem one could using q-learning learning methods mdps transition-based reward functions could using methods markov processes deterministic state-based reward functions markov processes general reward functions show advantage sats considering value-at-risk example risk measure reward distribution instead measures mean variance distribution illustrate error reward distribution estimation reward simplification show sats enable variance formula work markov processes general reward functions	negative
an integral tag recommendation model for textual content	recommending suitable tags online textual content key building block better content organization consumption paper identify three pillars impact accuracy tag recommendation 1 sequential text modeling meaning intrinsic sequential ordering well different areas text might important implication corresponding tag 2 tag correlation meaning tags certain piece textual content often semantically correlated 3 content-tag overlapping meaning vocabularies content tags overlapped however none existing methods consider three aspects leading suboptimal tag recommendation paper propose integral model encode three aspects coherent encoder-decoder framework particular 1 encoder models semantics textual content via recurrent neural networks attention mechanism 2 decoder tackles tag correlation prediction path 3 shared embedding layer indicator function across encoder-decoder address content-tag overlapping experimental results three realworld datasets demonstrate proposed method significantly outperforms existing methods terms recommendation accuracy	negative
rotational diversity in multi-cycle assignment problems	multi-cycle assignment problems rotational diversity set tasks repeatedly assigned set agents multiple cycles goal achieve high diversity assignments tasks agents time assignments ’ profit maximized cycle due changing availability tasks agents planning ahead infeasible cycle independent assignment problem influenced previous choices approach multi-cycle assignment problem two-part problem profit maximization rotation combined one objective value solved general assignment problem rotational diversity maintained single execution costly assignment model simple yet effective method applicable different domains applications experiments show applicability multi-cycle variant multiple knapsack problem real-world case study test case selection assignment problem example software engineering domain test cases distributed compatible test machines	negative
evaluating recommender system stability with influence-guided fuzzing	recommender systems help users find products services may like lacking personal experience facing overwhelming set choices since unstable recommendations lead distrust loss profits poor user experience important test recommender system stability work present approach based inferred models influence underlie recommender systems guide generation dataset modifications assess recommender ’ stability implement approach evaluate several recommender algorithms using movielens dataset find influence-guided fuzzing effectively find small sets modifications cause significantly instability random approaches	negative
random dictators with a random referee: constant sample complexity mechanisms for social choice	study social choice mechanisms implicit utilitarian framework metric constraint goal minimize distortion worst case social cost ordinal mechanism relative underlying cardinal utilities consider two additional desiderata constant sample complexity squared distortion constant sample complexity means mechanism potentially randomized uses constant number ordinal queries regardless number voters alternatives squared distortion measure variance distortion randomized mechanism	negative
on the optimal efficiency of cost-algebraic a*	edelkamp et al 2005 proved a* given admissible heuristic guaranteed return optimal solution cost algebra traditional shortest path setting paper investigate cost-algebraic a* ’ optimal efficiency cost-algebraic setting conditions a* guaranteed expand fewest possible states traditional setting question examined detail dechter pearl 1985 identified five different situations a* optimally efficient show three continue hold cost-algebraic setting one also show one false hold even traditional setting introduce alternative hold cost-algebraic setting finally show well-known result due nilsson hold general cost-algebraic setting hold slightly less general setting	negative
look across elapse: disentangled representation learning and photorealistic cross-age face synthesis for age-invariant face recognition	despite remarkable progress face recognition related technologies reliably recognizing faces across ages still remains big challenge appearance human face changes substantially time resulting significant intraclass variations opposed current techniques ageinvariant face recognition either directly extract ageinvariant features recognition first synthesize face matches target age feature extraction argue desirable perform tasks jointly leverage end propose deep age-invariant model aim face recognition wild three distinct novelties first aim presents novel unified deep architecture jointly performing cross-age face synthesis recognition mutual boosting way second aim achieves continuous face rejuvenation/aging remarkable photorealistic identity-preserving properties avoiding requirement paired data true age testing samples third develop effective novel training strategies end-to-end learning whole deep architecture generates powerful age-invariant face representations explicitly disentangled age variation extensive experiments several cross-age datasets morph cacd fg-net demonstrate superiority proposed aim model state-of-the-arts benchmarking model one popular unconstrained face recognition datasets ijb-c additionally verifies promising generalizability aim recognizing faces wild	negative
a sharper generalization bound for divide-and-conquer ridge regression	study distributed machine learning problem n feature-response pairs partitioned among machines uniformly random goal approximately solve empirical risk minimization erm problem minimum amount communication divide-and-conquer dc method proposed several years ago lets every worker machine independently solve erm problem using local feature-response pairs driver machine combine solutions approach one-shot thereby extremely communication-efficient although dc method studied many prior works reasonable generalization bound established work	negative
interpretation of neural networks is fragile	order machine learning trusted many applications critical able reliably explain machine learning algorithm makes certain predictions reason variety methods developed recently interpret neural network predictions providing example feature importance maps scientific robustness security reasons important know extent interpretations altered small systematic perturbations input data might generated adversaries measurement biases paper demonstrate generate adversarial perturbations produce perceptively indistinguishable inputs assigned predicted label yet different interpretations systematically characterize robustness interpretations generated several widely-used feature importance interpretation methods feature importance maps integrated gradients deeplift imagenet cifar-10 cases experiments show systematic perturbations lead dramatically different interpretations without changing label extend results show interpretations based exemplars e.g influence functions similarly susceptible adversarial attack analysis geometry hessian matrix gives insight robustness general challenge current interpretation approaches	negative
non-compensatory psychological models for recommender systems	study consumer psychology reveals two categories consumption decision procedures compensatory rules non-compensatory rules existing recommendation models based latent factor models assume consumers follow compensatory rules i.e evaluate item multiple aspects compute weighted or/and summated score used derive rating ranking item however shown literature consumer behavior consumers adopt non-compensatory rules often compensatory rules main contribution paper study unexplored area utilizing non-compensatory rules recommendation models	negative
strong equivalence for epistemic logic programs made easy	epistemic logic programs elps answer set programming asp extended epistemic operators received renewed interest recent years led flurry new research well efficient solvers important question conditions sub-program replaced another one without changing meaning context problem known strong equivalence well-studied asp elps question approached embedding epistemic extensions equilibrium logics paper consider simpler direct characterization directly applicable language used state-of-the-art elp solvers also allows us give tight complexity bounds showing strong equivalence elps remains conp-complete asp use results provide syntactic characterizations tautological rules rule subsumption elps	negative
meaningful explanations of black box ai decision systems	black box ai systems automated decision making often based machine learning big data map user ’ features class score without exposing reasons problematic lack transparency also possible biases inherited algorithms human prejudices collection artifacts hidden training data may lead unfair wrong decisions focus urgent open challenge construct meaningful explanations opaque ai/ml systems introducing local-toglobal framework black box explanation articulated along three lines language expressing explanations terms logic rules statistical causal interpretation ii inference local explanations revealing decision rationale specific case auditing black box vicinity target instance iii bottom-up generalization many local explanations simple global ones algorithms optimize quality comprehensibility argue local-first approach opens door wide variety alternative solutions along different dimensions variety data sources relational text images etc variety learning problems multi-label classification regression scoring ranking variety languages expressing meaningful explanations variety means audit black box	positive
satellite detection of moving vessels in marine environments	growing need coverage large maritime areas mainly exclusive economic zone eez due difficulty accessing large areas use satellite based sensors efficient cost-effective way perform task vessel behavior prediction necessary ability detection moving vessels satellite imagery paper present algorithm selection best satellite observation window detect moving object first describe model vessel behavior prediction compare performance two base models use real marine traffic data ais compare ability predict vessel behavior time frame 1–24 hours present kingfisher maritime intelligence system uses algorithm track suspected vessels satellite sensor also present results algorithm operational scenarios kingfisher	positive
robustness envelopes for temporal plans	achieve practical execution planners must produce temporal plans degree run-time adaptability plans expressed simple temporal networks stn constrain timing action activations implicitly represent space choices plan executor	negative
learning and the unknown: surveying steps toward open world recognition	science attempts close gap man machine building systems capable learning must embrace importance unknown ability differentiate known unknown considered critical element intelligent self-learning system ability reject uncertain inputs long history machine learning including background garbage class account inputs interest paper explains neither genuinely sufficient handling unknown inputs – uncertain unknown unknowns need appear uncertain learning system past decade seen formalization development many open set algorithms provably bound risk unknown classes summarize state art core ideas results explain despite efforts date current techniques genuinely insufficient handling unknown inputs especially deep networks	negative
constraint-based sequential pattern mining with decision diagrams	constraint-based sequential pattern mining aims identifying frequent patterns sequential database items observing constraints defined item attributes introduce novel techniques constraint-based sequential pattern mining rely multi-valued decision diagram mdd representation database specifically representation accommodate multiple item attributes various constraint types including number non-monotone constraints evaluate applicability approach develop mdd-based prefix-projection algorithm compare performance typical generate-and-check variant well state-of-the-art constraint-based sequential pattern mining algorithm results show approach competitive superior methods terms scalability efficiency	positive
learning to adaptively scale recurrent neural networks	recent advancements recurrent neural network rnn research demonstrated superiority utilizing multiscale structures learning temporal representations time series currently multiscale rnns use fixed scales comply nature dynamical temporal patterns among sequences paper propose adaptively scaled recurrent neural networks asrnn simple efficient way handle problem instead using predefined scales asrnns able learn adjust scales based different temporal contexts making flexible modeling multiscale patterns compared multiscale rnns asrnns bestowed upon dynamical scaling capabilities much simpler structures easy integrated various rnn cells experiments multiple sequence modeling tasks indicate asrnns efficiently adapt scales based different sequence contexts yield better performances baselines without dynamical scaling abilities	negative
on the complexity of the inverse semivalue problem for weighted voting games	weighted voting games family cooperative games typically used model voting situations number agents players vote proposal games proposal accepted appropriately weighted sum votes exceeds prespecified threshold influence player voting outcome general proportional assigned weight various power indices proposed measure player ’ influence inverse power index problem problem designing weighted voting game achieves set target influences according predefined power index work study computational complexity inverse problem power index belongs class semivalues prove inverse problem computationally intractable broad family semivalues including regular semivalues special case general result establish computational hardness inverse problem banzhaf indices shapley values arguably popular power indices	negative
expert guided rule based prioritization of scientifically relevant images for downlinking over limited bandwidth from planetary orbiters	instruments onboard spacecraft acquire large amounts data transmitted low bandwidth consequently missions volume data collected greatly exceeds volume downlinked next orbit necessitates introduction intelligent autonomous decision making module maximizes return scientifically relevant dataset low bandwidth experts analyze propose iterative rule based approach guided expert knowledge represent scientifically interesting geological landforms respect expert selected attributes rules utilized assign priority based novel test instance respect rule high priority instances test set used iteratively update learned rules determine effectiveness proposed approach images acquired mars orbiter observe expert-acceptable prioritization order generated rules potentially increase return scientifically relevant observations	negative
dynamic explainable recommendation based on neural attentive models	providing explanations recommender system getting attention industry research communities existing explainable recommender models regard user preferences invariant generate static explanations however real scenarios user ’ preference always dynamic may interested different product features different states mismatching explanation user preference may degrade costumers ’ satisfaction confidence trust recommender system	positive
multi-agent discussion mechanism for natural language generation	introduce discussion mechanism multiagent communicating encoder-decoder architecture natural language generation nlg tasks prove applying discussion mechanism communication agents becomes effective generally speaking encoder-decoder architecture predicts target-sequence word word several time steps time step prediction agents discussion mechanism predict target word several discussion steps first step discussion agents make choice independently express decision agents next discussion step agents collect agents ’ decision update decisions express updated decisions others several iterations agents make final decision based well-communicated situation benefit discussion mechanism multiple encoders designed different structures fit specified input fetch different representations inputs.we train evaluate discussion mechanism table text generation text summarization image caption tasks respectively empirical results demonstrate proposed multi-agent discussion mechanism helpful maximizing utility communication agents	negative
learning to align question and answer utterances in customer service conversation with recurrent pointer networks	customers ask questions customer service staffs answer questions basic service manner customer service cs progress cs typical multi-round conversation however explicit corresponding relations among conversational utterances paper focuses obtaining explicit alignments question answer utterances cs important task dialogue analysis also able obtain lots valuable train data learning dialogue systems work propose end-to-end models aligning question q answer utterances cs conversation recurrent pointer networks rpn one hand rpn-based alignment models able model conversational contexts mutual influence different q-a alignments hand able address issue empty multiple alignments utterances unified manner construct dataset in-house online cs experimental results demonstrate proposed models effective learn alignments question answer utterances	positive
cognitive deficit of deep learning in numerosity	subitizing sense small natural numbers innate cognitive function humans primates responds visual stimuli prior development symbolic skills language arithmetic given successes deep learning dl tasks visual intelligence given primitivity number sense tantalizing question whether dl comprehend numbers perform subitizing somewhat disappointingly extensive experiments type cognitive psychology demonstrate examples-driven black box dl see superficial variations visual representations distill abstract notion natural number task children perform high accuracy confidence failure apparently due learning method cnn computational machinery recurrent neural network capable subitizing exist construct encoding mechanism mathematical morphology cnn convolutional kernels also investigate using subitizing test bed ways aid black box dl cognitive priors derived human insight findings mixed interesting pointing cognitive deficit pure dl measured successes boosting dl predetermined cognitive implements case study dl cognitive computing meaningful visual numerosity represents minimum level human intelligence	negative
early-stopping of scattering pattern observation with bayesian modeling	paper describes new machine-learning application speed small-angle neutron scattering sans experiments method based probabilistic modeling sans one scattering experiments observe microstructures materials two-dimensional patterns plane sans pattern obtained measurements takes long time obtain accurate experimental results sans pattern histogram detected neutrons shortening measurement time propose earlystopping method based gaussian mixture modeling prior generated b-spline regression results experiment using actual sans data carried examine accuracy method confirmed accuracy proposed method converged 4 minutes starting experiment normal sans takes 20 minutes	positive
functional connectivity network analysis with discriminative hub detection for brain disease identification	brain network analysis help reveal pathological basis neurological disorders facilitate automated diagnosis brain diseases exploring connectivity patterns human brain effectively representing brain network always fundamental task computeraided brain network analysis previous studies typically utilize human-engineered features represent brain connectivity networks features may well coordinated subsequent classifiers besides brain networks often equipped multiple hubs i.e. nodes occupying central position overall organization network providing essential clues describe connectivity patterns however existing studies often fail explore hubs brain connectivity networks address two issues propose connectivity network analysis method discriminative hub detection cnhd brain disease diagnosis using functional magnetic resonance imaging fmri data specifically incorporate feature extraction brain networks network-based classification unified model discriminative hubs automatically identified data via ℓ1-norm ℓ2,1-norm regularizers proposed cnhd method evaluated three real-world schizophrenia datasets fmri scans experimental results demonstrate method outperforms several state-of-the-art approaches disease diagnosis also effective automatically identifying disease-related network hubs human brain	positive
deep short text classification with knowledge powered attention	short text classification one important tasks natural language processing nlp unlike paragraphs documents short texts ambiguous since enough contextual information poses great challenge classification paper retrieve knowledge external knowledge source enhance semantic representation short texts take conceptual information kind knowledge incorporate deep neural networks purpose measuring importance knowledge introduce attention mechanisms propose deep short text classification knowledge powered attention stcka utilize concept towards short text cst attention concept towards concept set c-cs attention acquire weight concepts two aspects classify short text help conceptual information unlike traditional approaches model acts like human intrinsic ability make decisions based observation i.e. training data machines pays attention important knowledge also conduct extensive experiments four public datasets different tasks experimental results case studies show model outperforms state-of-the-art methods justifying effectiveness knowledge powered attention	negative
multi-interactive memory network for aspect based multimodal sentiment analysis	fundamental task sentiment analysis aspect-level sentiment analysis aims identify sentiment polarity specific aspect context previous work aspect-level sentiment analysis text-based prevalence multimodal user-generated content e.g text image internet multimodal sentiment analysis attracted increasing research attention recent years context aspect-level sentiment analysis multimodal data often important text-only data various correlations including impacts aspect brings text image well interactions associated text image however related work carried far intersection aspect-level multimodal sentiment analysis fill gap among first put forward new task aspect based multimodal sentiment analysis propose novel multi-interactive memory network mimn model task model includes two interactive memory networks supervise textual visual information given aspect learns interactive influences cross-modality data also self influences single-modality data provide new publicly available multimodal aspect-level sentiment dataset evaluate model experimental results demonstrate effectiveness proposed model new task	negative
generalized distance bribery	bribery problem elections asks whether external agent make distinguished candidate win prevent winning bribing voters problem studied respect weighted swap distance two votes elkind et al 2009 generalize definition introducing bound distance original bribed votes distance measures consider include restriction weighted swap distance variants footrule distance capture realworld models influence external agent may voters study constructive destructive variants distance bribery scoring rules obtain polynomial-time algorithms well np-hardness results case element-weighted swap element-weighted footrule distances give complete dichotomy result class pure scoring rules	positive
symmetry-breaking constraints for grid-based multi-agent path finding	describe new way reasoning symmetric collisions multi-agent path finding mapf 4-neighbor grids also introduce symmetry-breaking constraint resolve conflicts specialized technique allows us identify eliminate single step permutations two currently assigned incompatible paths permutation exactly cost current path one results new collision two agents show addition symmetry-breaking techniques lead exponential reduction size search space cbs popular framework mapf report significant improvements runtime success rate versus cbsh epea* – two recent state-of-the-art mapf algorithms	negative
simulation-based approach to efficient commonsense reasoning in very large knowledge bases	cognitive systems must reason large bodies general knowledge perform complex tasks real world however due intractability reasoning large expressive knowledge bases kbs many ai systems limited reasoning capabilities successful cognitive systems used variety machine learning axiom selection methods improve inference paper describe search heuristic uses monte-carlo simulation technique choose inference steps test efficacy approach large expressive kb cyc experimental results hundreds queries show method highly effective reducing inference time improving question-answering q/a performance	positive
estimating the days to success of campaigns in crowdfunding: a deep survival perspective	crowdfunding emerging mechanism entrepreneurs individuals solicit funding public creative ideas however platforms quite large proportion campaigns projects fail raise enough money backers ’ supports declared expiration date actually urgent predict exact success time campaigns problem well explored due series domain technical challenges paper notice implicit factor distribution backing behaviors positive impact estimating success time campaign therefore present focused study predicting two specific tasks i.e. backing distribution prediction success time prediction campaigns specifically propose seq2seq based model multi-facet priors smp integrate heterogeneous features jointly model backing distribution success time additionally keep change backing distributions smooth backing behaviors increases develop linear evolutionary prior backing distribution prediction furthermore due high failure rate success time campaigns unobservable model censoring phenomenon survival analysis perspective also develop non-increasing prior partial prior success time prediction finally conduct extensive experiments real-world dataset indiegogo experimental results clearly validate effectiveness smp	positive
hers: modeling influential contexts with heterogeneous relations for sparse and cold-start recommendation	classic recommender systems face challenges addressing data sparsity cold-start problems modeling user-item relation essential direction incorporate understand additional heterogeneous relations e.g. user-user item-item relations since user-item interaction often influenced users items form user ’ s/item ’ influential contexts induces important yet challenging issues including modeling heterogeneous relations interactions strength influence users/items influential contexts end design influential-context aggregation units icau aggregate user-user/item-item relations within given context influential context embeddings accordingly propose heterogeneous relations-embedded recommender system based icaus model interpret underlying motivation user-item interactions considering user-user item-item influences experiments two real-world datasets show highly improved recommendation quality made superiority handling cold-start problem addition demonstrate interpretability modeling influential contexts explaining recommendation results	positive
abduction-based explanations for machine learning models	growing range applications machine learning ml multitude settings motivates ability computing small explanations predictions made small explanations generally accepted easier human decision makers understand earlier work computing explanations based heuristic approaches providing guarantees quality terms close solutions cardinality- subset-minimal explanations paper develops constraint-agnostic solution computing explanations ml model proposed solution exploits abductive reasoning imposes requirement ml model represented sets constraints using target constraint reasoning system decision problem answered oracle experimental results obtained well-known datasets validate scalability proposed approach well quality computed solutions	positive
clipped matrix completion: a remedy for ceiling effects	consider problem recovering low-rank matrix clipped observations clipping conceivable many scientific areas obstructs statistical analyses hand matrix completion mc methods recover low-rank matrix various information deficits using principle low-rank completion however current theoretical guarantees low-rank mc apply clipped matrices deficit depends underlying values therefore feasibility clipped matrix completion cmc trivial paper first provide theoretical guarantee exact recovery cmc using trace-norm minimization algorithm furthermore propose practical cmc algorithms extending ordinary mc methods extension use squared hinge loss place squared loss reducing penalty overestimation clipped entries also propose novel regularization term tailored cmc combination two trace-norm terms theoretically bound recovery error regularization demonstrate effectiveness proposed methods experiments using synthetic benchmark data recommendation systems	positive
generation of policy-level explanations for reinforcement learning	though reinforcement learning greatly benefited incorporation neural networks inability verify correctness systems limits use current work explainable deep learning focuses explaining single decision terms input features making unsuitable explaining sequence decisions address need introduce abstracted policy graphs markov chains abstract states representation concisely summarizes policy individual decisions explained context expected future transitions additionally propose method generate abstracted policy graphs deterministic policies given learned value function set observed transitions potentially off-policy transitions used training since restrictions placed value function generated method compatible many existing reinforcement learning methods prove worst-case time complexity method quadratic number features linear number provided transitions |f|2|tr samples| applying method family domains show method scales well practice produces abstracted policy graphs reliably capture relationships within domains	negative
towards better accuracy and robustness with localized adversarial training	technology society grow increasingly dependent computer vision becomes important make sure technologies secure however even today ’ stateof-the-art classifiers easily fooled carefully manipulated images solutions increased robustness manipulated images come expense accuracy natural inputs work propose new training technique localized adversarial training results accurate classification natural adversarial images much 6.5 99.7 respectively	negative
towards sequence-to-sequence reinforcement learning for constraint solving with constraint-based local search	paper proposes framework solving constraint problems reinforcement learning rl sequence-tosequence recurrent neural networks approach constraint solving declarative machine learning problem variable-length input sequence variable-length output sequence predicted using randomly generated instances number constraint violations reward function problem-specific rl agent trained solve problem predicted solution candidate rl agent verified repaired cbls ensure solutions satisfy constraint model introduce framework components discuss early results future applications	positive
melding the data-decisions pipeline: decision-focused learning for combinatorial optimization	creating impact real-world settings requires artificial intelligence techniques span full pipeline data predictive models decisions components typically approached separately machine learning model first trained via measure predictive accuracy predictions used input optimization algorithm produces decision however loss function used train model may easily misaligned end goal make best decisions possible hand-tuning loss function align optimization difficult error-prone process often skipped entirely	positive
adversarial unsupervised representation learning for activity time-series	sufficient physical activity restful sleep play major role prevention cure many chronic conditions able proactively screen monitor chronic conditions would big step forward overall health rapid increase popularity wearable devices pro-vides significant new source making possible track user ’ lifestyle real-time paper propose novel unsupervised representation learning technique called activ-ity2vecthat learns “ summarizes ” discrete-valued ac-tivity time-series learns representations three com-ponents co-occurrence magnitude activ-ity levels time-segment ii neighboring context time-segment iii promoting subject-invariance ad-versarial training evaluate method four disorder prediction tasks using linear classifiers empirical evaluation demonstrates proposed method scales performs better many strong baselines adversarial regime helps improve generalizability representations promoting subject invariant features also show using representations level day works best since human activity structured terms daily routines	negative
acting and planning using operational models	common representation formalisms planning descriptive models abstractly describe actions tailored efficiently computing next state state transition system acting requires operational models describe things rich control structures closed-loop online decision-making using descriptive representations planning operational representations acting lead problems developing verifying consistency different models	positive
one for all: neural joint modeling of entities and events	previous work event extraction mainly focused predictions event triggers argument roles treating entity mentions provided human annotators unrealistic entity mentions usually predicted existing toolkits whose errors might propagated event trigger argument role recognition recent work addressed problem jointly predicting entity mentions event triggers arguments however work limited using discrete engineering features represent contextual information individual tasks interactions work propose novel model jointly perform predictions entity mentions event triggers arguments based shared hidden representations deep learning experiments demonstrate benefits proposed method leading state-of-the-art performance event extraction	positive
red-black heuristics for planning tasks with conditional effects	red-black planning state-of-the-art approach satisficing classical planning red-black planning heuristics heart planner mercury runner-up satisficing track international planning competition ipc 2014 major component four additional planners ipc 2018 including saarplan runner-up agile track mercury ’ exceptional performance amplified fact conditional effects handled planner trivial way simply compiling away conditional effects however important classical planning many domains require efficient modeling	negative
what and where the themes dominate in image	image captioning describe image natural language human benefited advances deep neural network achieved substantial progress performance however perspective human description scene fully considered task recently actually human description scene tightly related endogenous knowledge exogenous salient objects simultaneously implies content description confined known salient objects inspired observation paper proposes novel framework explicitly applies known salient objects image captioning framework known salient objects served themes guide description generation according property known salient object theme composed two components endogenous concept exogenous spatial attention feature specifically prediction word dominated concept spatial attention feature corresponding theme process caption prediction moreover introduce novel learning method distinctive learning dl get specificity generated captions like human descriptions formulates two constraints theme learning process encourage distinctiveness different images particularly reinforcement learning introduced framework address exposure bias problem training testing modes extensive experiments coco flickr30k datasets achieve superior results compared state-of-the-art methods	negative
learning to teach in cooperative multiagent reinforcement learning	collective human knowledge clearly benefited fact innovations individuals taught others communication similar human social groups agents distributed learning systems would likely benefit communication share knowledge teach skills problem teaching improve agent learning investigated prior works approaches make assumptions prevent application teaching general multiagent problems require domain expertise problems apply learning teach problem inherent complexities related measuring long-term impacts teaching compound standard multiagent coordination challenges contrast existing works paper presents first general framework algorithm intelligent agents learn teach multiagent environment algorithm learning coordinate teach reinforcement lectr addresses peer-to-peer teaching cooperative multiagent reinforcement learning agent approach learns advise uses received advice improve local learning importantly roles fixed agents learn assume role student and/or teacher appropriate moments requesting providing advice order improve teamwide performance learning empirical comparisons state-of-the-art teaching methods show teaching agents learn significantly faster also learn coordinate tasks existing methods fail	positive
very hard electoral control problems	important understand outcome election modified agent control structure election electoral control studied many election systems systems winner problem p control np election systems kemeny many desirable properties whose winner problems np thus systems control np fact show typically complete ∑p2 i.e. npnp second level polynomial hierarchy high level complexity approaches perform quite well solving np problems necessarily work ∑p2-complete problems however answer set programming suited express problems ∑p2 present encoding kemeny control	negative
dialoguernn: an attentive rnn for emotion detection in conversations	emotion detection conversations necessary step number applications including opinion mining chat history social media threads debates argumentation mining understanding consumer feedback live conversations currently systems treat parties conversation individually adapting speaker utterance paper describe new method based recurrent neural networks keeps track individual party states throughout conversation uses information emotion classification model outperforms state-of-the-art significant margin two different datasets	negative
adversarial learning for weakly-supervised social network alignment	nowadays common one natural person join multiple social networks enjoy different kinds services linking identical users across multiple social networks also known social network alignment important problem great research challenges existing methods usually link social identities pairwise sample level may lead undesirable performance number available annotations limited motivated isomorphism information paper consider identities social network whole perform social network alignment distribution level insight aim learn projection function minimize distance distributions user identities two social networks also incorporate available annotations learning guidance propose three models snnau snnab snnao learn projection function weakly-supervised adversarial learning framework empirically evaluate proposed models multiple datasets results demonstrate superiority proposals	negative
cubic lstms for video prediction	predicting future frames videos become promising direction research computer vision robot learning communities core problem involves moving object capture future motion prediction object capture specifies objects moving videos motion prediction describes future dynamics motivated analysis propose cubic long short-term memory cubiclstm unit video prediction cubiclstm consists three branches i.e. spatial branch capturing moving objects temporal branch processing motions output branch combining first two branches generate predicted frames stacking multiple cubiclstm units along spatial branch output branch evolving along temporal branch form cubic recurrent neural network cubicrnn experiment shows cubicrnn produces accurate video predictions prior methods synthetic real-world datasets	positive
blameworthiness in strategic games	multiple notions coalitional responsibility focus paper blameworthiness defined principle alternative possibilities coalition blamable statement statement true coalition strategy prevent main technical result sound complete bimodal logical system describes properties blameworthiness one-shot games	negative
polynomial-time probabilistic reasoning with partial observations via implicit learning in probability logics	standard approaches probabilistic reasoning require one possesses explicit model distribution question empirical learning models probability distributions partial observations problem efficient algorithms generally known work consider use bounded-degree fragments “ sum-of-squares ” logic probability logic prior work shown decide refutability fragments polynomial-time propose use fragments decide queries whether given probability distribution satisfies given system constraints bounds expected values show answering queries constraints bounds implicitly learned partial observations polynomial-time well known logic capable deriving many bounds useful probabilistic analysis show furthermore captures key polynomial-time fragments resolution thus fragments also quite expressive	negative
active learning of multi-class classification models from ordered class sets	paper study problem learning multi-class classification models limited set labeled examples obtained human annotator propose new machine learning framework learns multi-class classification models ordered class sets annotator may use express top class choice also competing classes still consideration ordered sets competing classes common example various diagnostic tasks paper first develop strategies learning multi-class classification models examples associated ordered class set information develop active learning strategy considers feedback evaluate benefit framework multiple datasets show class-order feedback active learning reduce annotation cost individually jointly	negative
deep hierarchical graph convolution for election prediction from geospatial census data	geographic information systems ’ gis research widely used within social physical sciences plays crucial role development implementation governments economic education environment transportation policy machine learning methods applied gis datasets uptake powerful deep learning cnn methodologies limited part due challenges posed complex often poorly structured nature data paper demonstrate utility gcnns gis analysis via multi-graph hierarchical spatial-filter gcnn network model context gis systems predict election outcomes using socio-economic features drawn 2016 australian census report marked improvement performance accuracy hierarchical gcnns benchmark generalised linear models standard gcnns especially semi-supervised tasks results indicate widespread potential gis-gcnn research methods enrich socio-economic gis analysis aiding social sciences policy development	positive
online multi-agent pathfinding	multi-agent pathfinding mapf problem moving group agents set target destinations avoiding collisions work study online version mapf new agents appear time several variants online mapf defined analyzed theoretically showing possible create optimal online mapf solver nevertheless propose effective online mapf algorithms balance solution quality runtime number plan changes agent makes execution	negative
fast iterative combinatorial auctions via bayesian learning	iterative combinatorial auctions cas often used multibillion dollar domains like spectrum auctions speed convergence one crucial factors behind choice specific design practical applications achieve fast convergence current cas require careful tuning price update rule balance convergence speed allocative efficiency brero lahaie 2018 recently introduced bayesian iterative auction design settings singleminded bidders bayesian approach allowed incorporate prior knowledge price update algorithm reducing number rounds convergence minimal parameter tuning paper generalize work settings restrictions bidder valuations introduce new bayesian ca design general setting uses monte carlo expectation maximization update prices round auction evaluate approach via simulations cats instances results show bayesian ca outperforms even highly optimized benchmark terms clearing percentage convergence speed	negative
measuring and mitigating unintended bias in text classification	introduce illustrate new approach measuring mitigating unintended bias machine learning models definition unintended bias parameterized test set subset input features illustrate used evaluate text classifiers using synthetic test set public corpus comments annotated toxicity wikipedia talk pages also demonstrate imbalances training data lead unintended bias resulting models therefore potentially unfair applications use set common demographic identity terms subset input features measure bias technique permits analysis common scenario demographic information authors readers unavailable bias mitigation must focus content text mitigation method introduce unsupervised approach based balancing training dataset demonstrate approach reduces unintended bias without compromising overall model quality	negative
path-specific counterfactual fairness	consider problem learning fair decision systems data sensitive attribute might affect decision along fair unfair pathways introduce counterfactual approach disregard effects along unfair pathways incur loss individual-specific information previous approaches method corrects observations adversely affected sensitive attribute uses form decision leverage recent developments deep learning approximate inference develop vae-type method widely applicable complex nonlinear models	positive
tracking logical difference in large-scale ontologies: a forgetting-based approach	paper explores logical difference two ontologies tracked using forgetting-based uniform interpolation ui -based approach idea rather computing entailments one ontology entailed ontology would computationally infeasible strongest entailments entailed ontology computed overcome drawbacks existing forgetting/uniform interpolation tools introduce new forgetting method designed task computing logical difference different versions large-scale ontologies method sound terminating compute uniform interpolants alc-ontologies large snomed ct ncit evaluation shows method achieve considerably better success rates 90 provides feasible approach computing logical difference large-scale ontologies case study different versions snomed ct ncit ontologies shows	positive
fair knapsack	study following multiagent variant knapsack problem given set items set voters value budget item endowed cost voter assigns item certain value goal select subset items total cost exceeding budget way consistent voters ’ preferences since preferences voters items vary significantly need way aggregating preferences order select socially best valid knapsack study three approaches aggregating voters ’ preferences motivated literature multiwinner elections fair allocation way introduce concepts individually best diverse fair knapsack study computational complexity including parameterized complexity complexity restricted domains aforementioned multiagent variants knapsack	negative
identification of causal effects in the presence of selection bias	cause-and-effect relations one valuable types knowledge sought throughout data-driven sciences since translate stable generalizable explanations well efficient robust decision-making capabilities inferring relations data however challenging task two common barriers goal known confounding selection biases former stems systematic bias introduced treatment assignment latter comes systematic bias collection units sample paper consider problem identifiability causal effects confounding selection biases simultaneously present first investigate problem identifiability available data biased prove algorithm proposed bareinboim tian 2015 fact complete namely whenever algorithm returns failure condition identifiability claim causal relation made method generalize setting addition biased data another piece external data available without bias may case subset covariates could measured without bias e.g. census examine problem identifiability combination biased unbiased data available propose new algorithm subsumes current state-of-the-art method based back-door criterion	positive
efficient region embedding with multi-view spatial networks: a perspective of locality-constrained spatial autocorrelations	urban regions places people live work consume entertain study investigate problem learning embedding space regions studying representations regions help us better understand patterns structures dynamics cities support urban planning ultimately make cities livable sustainable efforts made learning embeddings regions existing methods improved incorporating locality-constrained spatial autocorrelations encode-decode framework embedding strategy capable taking account intra-region structural information inter-region spatial autocorrelations end propose learn representations regions via new embedding strategy awareness locality-constrained spatial autocorrelations specifically first construct multi-view i.e. distance mobility connectivity poi-poi networks represent regions addition introduce two properties region embedding spatial autocorrelations global similarity regions ii top-k locality spatial autocorrelations locally approximately reside top k autocorrelated regions propose new encoder-decoder based formulation preserves two properties remaining efficient application exploit learned embeddings predict mobile checkin popularity regions finally extensive experiments real-world urban region data demonstrate effectiveness efficiency method	negative
balancing relevance and diversity in online bipartite matching via submodularity	bipartite matching problems vertices one side bipartite graph paired online variant one side graph available offline vertices side arrive online vertex arrives irrevocable immediate decision made algorithm either match available vertex drop examples problems include matching workers firms advertisers keywords organs patients much literature focuses maximizing total relevance—modeled via total weight—of matching however many real-world problems also important consider contributions diversity hiring diverse pool candidates displaying relevant diverse set ads paper propose online submodular bipartite matching osbm problem goal maximize submodular function f set matched edges objective general enough capture notion diversity e.g. weighted coverage function relevance e.g. traditional linear function —as well many natural objective functions occurring practice e.g. limited total budget advertising settings propose novel algorithms provable guarantees essentially optimal restricted various special cases also run experiments real-world synthetic datasets validate algorithms	positive
cafe: adaptive vdi workload prediction with multi-grained features	virtual desktop infrastructure vdi virtualization technology hosts desktop operating system centralized server data center private public cloud effective resource management crucial importance vdi customers maintaining sufficient virtual machines helps guarantee satisfactory user experience turning spare virtual machines helps save running cost generally existing techniques work passive manner either driving available capacity reactively configuring management schedules manually paper novel proactive resource management approach proposed aims predict vdi pool workload adaptively utilizing coarse fine historical descriptive cafe features specifically aggregate session count pool end users serves basis workload measurement predictive model induction extensive experiments real vdi customers data sets clearly validate effectiveness multi-grained features vdi workload prediction furthermore practical insights identified vdi data analytics also discussed	negative
learning multi-task communication with message passing for sequence learning	present two architectures multi-task learning neural sequence models approach allows relationships different tasks learned dynamically rather using ad-hoc pre-defined structure previous work adopt idea message-passing graph neural networks propose general graph multi-task learning framework different tasks communicate effective interpretable way conduct extensive experiments text classification sequence labelling evaluate approach multi-task learning transfer learning empirical results show models outperform competitive baselines also learn interpretable transferable patterns across tasks	positive
a deep sequential model for discourse parsing on multi-party dialogues	discourse structures beneficial various nlp tasks dialogue understanding question answering sentiment analysis paper presents deep sequential model parsing discourse dependency structures multi-party dialogues proposed model aims construct discourse dependency tree predicting dependency relations constructing discourse structure jointly alternately makes sequential scan elementary discourseunits edus 1 dialogue edu model decides previous edu current one link corresponding relation type predicted link relation type used build discourse structure incrementally structured encoder link prediction relation classification model utilizes local information represents concerned edus also global information encodes edu sequence discourse structure already built current step experiments show proposed model outperforms state-of-the-art baselines	negative
self-paced active learning: query the right thing at the right time	active learning queries labels oracle valuable instances reduce labeling cost many active learning studies informative representative instances preferred expected higher potential value improving model recently results self-paced learning show training model easy examples first gradually harder examples improve performance informative representative instances could easy hard querying valuable hard examples early stage may lead waste labeling cost paper propose self-paced active learning approach simultaneously consider potential value easiness instance try train model least cost querying right thing right time experimental results show proposed approach superior state-of-the-art batch mode active learning methods	negative
covariate shift adaptation on learning from positive and unlabeled data	goal binary classification identify whether input sample belongs positive negative classes usually supervised learning applied obtain classification rule real-world applications conceivable positive unlabeled data accessible learning called learning positive unlabeled data pu learning furthermore practice data distributions likely differ training testing due example time variation domain shift covariate shift dataset shift situation distributions covariates inputs differ training testing input-output relation paper address pu learning problem covariate shift propose importanceweighted pu learning method reveal situations importance-weighting necessary moreover derive convergence rate proposed method mild conditions experimentally demonstrate effectiveness	positive
pareto optimal allocation under compact uncertain preferences	assignment problem one well-studied settings multi-agent resource allocation aziz de haan rastegari 2017 considered problem additional feature agents ’ preferences involve uncertainty particular considered two uncertainty models neither necessarily compact paper focus three uncertain preferences models whose size polynomial number agents items consider several interesting computational questions regard pareto optimal assignments also present general characterization algorithmic results apply large classes uncertainty models	negative
exploiting the contagious effect for employee turnover prediction	talent turnover often costs large amount business time money performance therefore employee turnover prediction critical proactive talent management existing approaches turnover prediction mainly based profiling employees working environments important contagious effect employee turnovers largely ignored end paper propose contagious effect heterogeneous neural network cehnn turnover prediction integrating employee profiles environmental factors importantly influence turnover behaviors co-workers moreover global attention mechanism designed evaluate heterogeneous impact potential turnover behaviors attention mechanism improve interpretability turnover prediction provide actionable insights talent retention finally conduct extensive experiments case studies realworld dataset large company validate effectiveness contagious effect turnover prediction	negative
a model-free affective reinforcement learning approach to personalization of an autonomous social robot companion for early literacy education	personalized education technologies capable delivering adaptive interventions could play important role addressing needs diverse young learners critical time school readiness present innovative personalized social robot learning companion system utilizes children ’ verbal nonverbal affective cues modulate engagement maximize long-term learning gains propose affective reinforcement learning approach train personalized policy student educational activity child robot tell stories using personalized policy robot selects stories optimized child ’ engagement linguistic skill progression recruited 67 bilingual english language learners ages 4–6 years old participate between-subjects study evaluate system three-month deployment schools unique storytelling policy trained deliver personalized story curriculum child personalized group compared engagement learning outcomes non-personalized group fixed curriculum robot baseline group robot intervention personalization condition results show affective policy successfully personalized child boost engagement outcomes respect learning retaining target words well using target syntax structures compared children groups	positive
an improved quasi-polynomial algorithm for approximate well-supported nash equilibria	focus problem computing approximate nash equilibria bimatrix games particular consider notion approximate well-supported equilibria one standard approaches approximating equilibria already known one compute ε-well-supported nash equilibrium time log n/ε2 ε 0 games n pure strategies per player running time referred quasi-polynomial regarding faster algorithms remained open problem many years better running times small values approximation parameter known compute polynomial-time 0.6528-well-supported nash equilibrium paper investigate question propose much better quasi-polynomial time algorithm computes 1/2 ε -well-supported nash equilibrium time log logn1/ε/ε2 ε 0. algorithm based appropriately combining sampling arguments support enumeration solutions systems linear inequalities	negative
relating the structure of a problem and its explanation	ai becomes ubiquitous increasing interest computers able provide explanations conclusions paper proposes exploring relationship structure problem explanation nature challenge introduced series simple constraint satisfaction problems	negative
poll-confident voters in iterative voting	article deals strategic voting incomplete information propose descriptive model inspired political elections information vote intentions electorate comes public opinion polls social network modeled graph voters voters assumed confident poll update communicated results information get relatives social network consider iterative voting model based behavior study associated “ poll-confident ” dynamics context ask question manipulation polling institute	negative
deictic image mapping: an abstraction for learning pose invariant manipulation policies	applications deep reinforcement learning robotics often case want learn pose invariant policies policies invariant changes position orientation objects world example consider pegin-hole insertion task agent learns insert peg one hole would like policy generalize holes presented different poses unfortunately challenge using conventional methods paper proposes novel state action abstraction invariant pose shifts called deictic image maps used deep reinforcement learning provide broad conditions optimal abstract policies optimal underlying system finally show method help solve challenging robotic manipulation problems	negative
re-evaluating adem: a deeper look at scoring dialogue responses	automatically evaluating quality dialogue responses unstructured domains challenging problem adem lowe et al 2017 formulated automatic evaluation dialogue systems learning problem showed model able predict responses correlate significantly human judgements utterance system level system shown beaten word-overlap metrics bleu large margins start question whether adversary game adem model design battery targeted attacks neural network based adem evaluation system show automatic evaluation dialogue systems still long way go adem get confused variation simple reversing word order text report experiments several adversarial scenarios draw counterintuitive scores dialogue responses take systematic look scoring function proposed adem connect linear system theory predict shortcomings evident system also devise attack fool system rate response generation system favorable finally allude future research directions using adversarial attacks design truly automated dialogue evaluation system	negative
pareto-optimal allocation of indivisible goods with connectivity constraints	study problem allocating indivisible items agents additive valuations additional constraint bundles must connected underlying item graph previous work considered existence complexity fair allocations study problem finding allocation pareto-optimal easy find efficient allocation underlying graph path star problem np-hard many graph topologies even trees bounded pathwidth maximum degree 3. show path instances pareto-optimal allocation satisfies envy-freeness one good np-hard decide whether allocation exists even binary valuations also show path np-hard find pareto-optimal allocation satisfies maximin share show moving-knife algorithm find allocation agents binary valuations non-nested interval structure	negative
on the inducibility of stackelberg equilibrium for security games	strong stackelberg equilibrium sse standard solution concept stackelberg security games opposed weak stackelberg equilibrium wse sse assumes follower breaks ties favor leader widely acknowledged justified assertion defender often induce attacker choose preferred action making infinitesimal adjustment strategy unfortunately security games resource assignment constraints assertion might valid possible defender induce desired outcome result many results claimed literature may overly optimistic remedy first formally define utility guarantee defender strategy provide examples show utility sse higher utility guarantee second inspired analysis leader ’ payoff von stengel zamir 2004 provide solution concept called inducible stackelberg equilibrium ise owns highest utility guarantee always exists third show conditions ise coincides sse fact general case sse extremely worse respect utility guarantee moreover introducing ise invalidate existing algorithmic results problem computing ise polynomially reduces computing sse also provide algorithmic implementation computing ise experiments unveil empirical advantage ise sse	negative
challenges in the automatic analysis of students’ diagnostic reasoning	diagnostic reasoning key component many professions improve students ’ diagnostic reasoning skills educational psychologists analyse give feedback epistemic activities used students diagnosing particular hypothesis generation evidence generation evidence evaluation drawing conclusions however manual analysis highly time-consuming aim enable large-scale adoption diagnostic reasoning analysis feedback automating epistemic activity identification create first corpus task comprising diagnostic reasoning selfexplanations students two domains annotated epistemic activities based insights corpus creation task ’ characteristics discuss three challenges automatic identification epistemic activities using ai methods correct identification epistemic activity spans reliable distinction similar epistemic activities detection overlapping epistemic activities propose separate performance metric challenge thus provide evaluation framework future research indeed evaluation various state-of-the-art recurrent neural network architectures reveals current techniques fail address challenges	negative
on the proximity of markets with integral equilibria	study fisher markets admit equilibria wherein good integrally assigned agent strong existence computational guarantees known equilibria fisher markets additive valuations eisenberg gale 1959 orlin 2010 equilibria general assign goods fractionally agents hence fisher markets directly applicable context indivisible goods work show one always bypass hurdle bounded change agents ’ budgets obtain markets admit integral equilibrium refer markets pure markets show given fisher market additive valuations one efficiently compute “ near-by ” pure market accompanying integral equilibrium	negative
a genetic algorithm for finding a small and diverse set of recent news stories on a given subject: how we generate aaai’s ai-alert	paper describes genetic algorithm used select news stories artificial intelligence aaai ’ weekly aialert emailed nearly 11,000 subscribers week 1,500 news stories covering various aspects artificial intelligence machine learning discovered i2k connect ’ newsfinder agent challenge select 10 stories collection represent important news ai since stories topics necessarily repeat later weeks use click tracking supervised learning predict stories topics preferred readers instead must build representative selection stories priori using information story ’ topics content publisher date publication features paper describes genetic algorithm achieves task demonstrate effectiveness comparing several engagement metrics six months “ a/b testing ” experiments compare random story selection vs. simple scoring algorithm vs. new genetic algorithm	positive
discovering temporal patterns from insurance interaction data	insurance industry timely effective interaction customers core everyday operations processes key satisfactory customer experience interactions often result sequences data derived events occur time recurrent patterns provide valuable information used variety ways improve customer related work-flows paper demonstrate application recently proposed algorithm uncover time patterns takes account time events form patterns use temporal customer data generated two different use-cases satisfaction fraud show algorithm successfully detects patterns occur insurance context	negative
building human-machine trust via interpretability	developing human-machine trust prerequisite adoption machine learning systems decision critical settings e.g healthcare governance users develop appropriate trust systems understand systems make decisions interpretability helps users understand system learns also helps users contest system align intuition propose algorithm ava aggregate valuation antecedents generates consensus feature attribution retrieving local explanations capturing global patterns learned model empirical results show ava rivals current benchmarks	negative
belief change and non-monotonic reasoning sans compactness	belief change non-monotonic reasoning arguably different perspectives phenomenon namely jettisoning currently held beliefs response incompatible evidence investigations area typically assume among things underlying background logic compact whatever inferred set sentences x inferred finite subset x. recent research field shows compactness assumption dispensed without inflicting much damage agm paradigm belief change paper investigate impact relaxation non-monotonic logics instead particular show compactness guaranteed bridge agm paradigm belief change expectation logics remains unaffected “ return trip ” expectation logics agm paradigm longer guaranteed finally explore conditions guarantee given	positive
heuristic voting as ordinal dominance strategies	decision making uncertainty key component many ai settings particular voting scenarios strategic agents trying reach joint decision common approach handle uncertainty maximizing expected utility requires cardinal utility function well detailed probabilistic information however often probabilities easy estimate apply	negative
complexity of inconsistency-tolerant query answering in datalog+/– under cardinality-based repairs	querying inconsistent ontological knowledge bases important problem practice several inconsistencytolerant query answering semantics proposed including query answering relative repairs relative intersection repairs relative intersection closed repairs semantics one assumes input database erroneous notion repair describes maximally consistent subset input database different notions maximality subset cardinality maximality considered paper give precise picture computational complexity inconsistencytolerant boolean conjunctive query answering wide range datalog± languages cardinality-based versions three repair semantics	negative
making money from what you know - how to sell information?	information plays key role many decision situations rapid advancement communication technologies makes information providers accessible various information providing platforms found nowadays strategic sense goal maximize providers ’ expected profit paper consider common problem strategic information provider offering prospective buyers information disambiguate uncertainties buyers valuable decision making unlike prior work limit information provider ’ strategy price setting rather enable flexibility way information sold specifically enabling querying specific outcomes elimination subset non-true world states alongside traditional approach disclosing true world state prove case buyer self-interested information provider know true world state beforehand three methods i.e. disclosing true worldstate value offering check specific value eliminating random value equivalent yielding expected profit information provider case buyers human subjects using extensive set experiments show methods result substantially different outcomes furthermore using standard machine learning techniques information provider rather accurately predict performance different methods new problem settings hence substantially increase profit	negative
zero-shot neural transfer for cross-lingual entity linking	cross-lingual entity linking maps entity mention source language corresponding entry structured knowledge base different target language previous work relies heavily bilingual lexical resources bridge gap source target languages resources scarce unavailable many low-resource languages address problem investigate zero-shot cross-lingual entity linking assume bilingual lexical resources available source low-resource language specifically propose pivot-based	positive
revisiting projection-free optimization for strongly convex constraint sets	revisit frank-wolfe fw optimization strongly convex constraint sets provide faster convergence rate fw without line search showing previously overlooked variant fw indeed faster standard variant line search show fw converge global optimum even smooth functions convex quasi-convex locally-lipschitz also show general case smooth non-convex functions fw line search converges high probability stationary point rate 1/t long constraint set strongly convex—one fastest convergence rates non-convex optimization	negative
learning generalized temporal abstractions across both action and perception	learning temporal abstractions partial solutions task could reused similar even complicated tasks intuitively ingredient help agents plan learn reason efficiently multiple resolutions perceptions time like humans acquire skills build top already existing skills solve complicated tasks ai agents able learn develop skills continually hierarchically incrementally time research aim answer following question agent efficiently represent learn use knowledge world continual tasks work builds options framework provides novel extensions driven question introduce notion interest functions analogous temporally extended actions propose learning temporally extended perception key idea learn temporal abstractions unifying action perception	negative
a better algorithm for societal tradeoffs	societal tradeoffs problem agent perceives certain quantitative tradeoffs pairs activities goal aggregate tradeoffs across agents problem social choice specifically type quantitative judgment aggregation problem natural rule problem axiomatized conitzer et al aaai 2016 also provided several algorithms computing outcomes rule paper present significantly improved algorithm evaluate experimentally algorithm based tight connection minimum-cost flow exhibit also show algorithm improved without breakthroughs min-cost flow	negative
a whole new ball game: harvesting game data for player profiling	nowadays video games play important role human life longer purely associated escapism entertainment fact gaming become essential part daily routines give rise exponential growth various online game platforms participating platforms individuals generate multitude game data points example used automatic user profiling recommendation applications however literature automatic learning game data relatively sparse inspired us tackle problem player profiling first preliminary study specifically work approach task player gender prediction based various types game data initial experimental results inspire research user profiling game domain	negative
a fast machine learning workflow for rapid phenotype prediction from whole shotgun metagenomes	research microbiome emerging crucial science finds many applications healthcare food safety precision agriculture environmental studies huge amounts dna microbial communities sequenced analyzed scientists interested extracting meaningful biological information big data analyzing massive microbiome sequencing datasets embed functions interactions thousands different bacterial fungal viral species significant computational challenge artificial intelligence potential building predictive models provide insights specific cutting edge applications guiding diagnostics developing personalised treatments well maintaining soil health fertility current machine learning workflows predict traits host organisms commensal microbiome take account whole genetic material constituting microbiome instead basing analysis specific marker genes paper best knowledge introduce first machine learning workflow efficiently performs host phenotype prediction whole shotgun metagenomes computing similaritypreserving compact representations genetic material workflow enables prediction tasks classification regression terabytes raw sequencing data necessitate pre-prossessing expensive bioinformatics pipelines compare performance terms time accuracy uncertainty predictions four different classifiers precisely demonstrate ml workflow efficiently classify real data high accuracy using examples dog human metagenomic studies representing step forward towards real time diagnostics potential cloud applications	negative
greedy maximization of functions with bounded curvature under partition matroid constraints	investigate performance deterministic greedy algorithm problem maximizing functions partition matroid constraint consider non-monotone submodular functions monotone subadditive functions even though constrained maximization problems monotone submodular functions extensively studied little known greedy maximization non-monotone submodular functions monotone subadditive functions	negative
learning triggers for heterogeneous treatment effects	causal effect treatment vary person person based individual characteristics predispositions mining patterns individual-level effect differences problem known heterogeneous treatment effect estimation many important applications precision medicine recommender systems paper define study variant problem individuallevel threshold treatment needs reached order trigger effect one main contributions work estimate heterogeneous treatment effects fixed treatments also prescribe individualized treatments propose tree-based learning method find heterogeneity treatment effects experimental results multiple datasets show approach learn triggers better existing approaches	positive
cross-domain recommendation via coupled factorization machines	data across many business domains represented two coupled data sets correlations among coupled datasets studied literature making accurate cross-domain recommender systems however existing methods cross-domain recommendations mostly assume coupled mode data sets share identical latent factors limits discovery potentially useful domain-specific properties original data paper proposed novel cross-domain recommendation method called coupled factorization machine cofm addresses limitation compared existing models research first model uses factorization machines capture common characteristics coupled domains simultaneously preserving differences among experiments real-world datasets confirm advantages method making across-domain recommendations	negative
emergency department online patient-caregiver scheduling	emergency departments eds provide imperative source medical care central ed workflow patientcaregiver scheduling directed getting right patient right caregiver right time unfortunately common ed scheduling practices based ad-hoc heuristics may aligned complex partially conflicting ed ’ objectives	negative
using automated agents to teach negotiation	negotiation integral part daily lives regardless occupation although ubiquitous experience never taught negotiate lack training presents many consequences unfair salary negotiation geopolitical ramification ability resolve conflicts negotiate becoming critical due rise automated systems look replace various repetitive task jobs hopes improving human negotiation skills work seeks develop automated negotiation agents capable providing personalized feedback paper provide overview past current future work	negative
partial verification as a substitute for money	recent work shows use partial verification instead money implement truthful mechanisms paper develop tools answer following question given allocation rule made truthful payments minimal verification needed make truthful without techniques leverage geometric relationship type space set possible allocations	negative
dys: a framework for mixture models in quantification	quantification expanding research topic machine learning literature classification interested obtaining class individual observations quantification want estimate total number instances belong class subtle difference allows development several algorithms incur smaller consistent errors counting classes issued classifier among new quantification methods one particular family stands due accuracy simplicity ability operate imbalanced training samples mixture models mm despite desirable traits mm class algorithms lacks in-depth understanding concerning influence internal parameters performance paper generalize mm base framework called dys distribution y-similarity framework perform thorough evaluation critical design decisions mm models instance assess 15 dissimilarity functions compare histograms varying numbers bins 2 110 first time make connection quantification accuracy test sample size experiments covering 24 public benchmark datasets conclude tuned topsøe histogram distance function consistently leads smaller quantification errors therefore recommended general use notwithstanding hellinger distance ’ popularity rid mm models dependency choice number histogram bins introduce two dissimilarity functions operate directly observations show sord one measures presents performance slightly inferior tuned topsøe requiring sensible parameterization number bins	negative
multi-context system for optimization problems	paper proposes multi-context system optimization problems mcs-op introducing conditional costassignment bridge rules multi-context systems mcs novel feature facilitates definition preorder among equilibria based total incurred cost applied bridge rules application mcs-op paper describes mcs-op used modeling distributed constraint optimization problems dcop prominent class distributed optimization problems frequently employed multi-agent system mas research paper shows means example mcs-op expressive dcop hence could potentially useful modeling distributed optimization problems easily dealt using dcops also contains complexity analysis mcs-op	positive
personalized question routing via heterogeneous network embedding	question routing qr community-based question answering cqa websites aims recommending answerers high probabilities providing “ accepted answers ” new questions existing question routing algorithms simply predict ranking users based query content consequence question raiser information ignored hand lack learnable scoring functions explicitly compute ranking scores	negative
vistanet: visual aspect attention network for multimodal sentiment analysis	detecting sentiment expressed document key task many applications e.g. modeling user preferences monitoring consumer behaviors assessing product quality traditionally sentiment analysis task primarily relies textual content fueled rise mobile phones often cameras hand documents web e.g. reviews blog posts tweets increasingly multimodal nature photos addition textual content question arises whether visual component could useful sentiment analysis well work propose visual aspect attention network vistanet leveraging textual visual components observe many cases respect sentiment detection images play supporting role text highlighting salient aspects entity rather expressing sentiments independently text therefore instead using visual information features vistanet relies visual information alignment pointing important sentences document using attention experiments restaurant reviews showcase effectiveness visual aspect attention vis-à-vis visual features textual attention	negative
theory of minds: understanding behavior in groups through inverse planning	human social behavior structured relationships form teams groups tribes alliances scales human life structures guide multi-agent cooperation competition observe others underlying relationships typically unobservable hence must inferred humans make inferences intuitively flexibly often making rapid generalizations latent relationships underlie behavior sparse noisy observations rapid accurate inferences important determining cooperate compete cooperate order compete towards goal building machine-learning algorithms human-like social intelligence develop generative model multiagent action understanding based novel representation latent relationships called composable team hierarchies cth representation grounded formalism stochastic games multi-agent reinforcement learning use cth target bayesian inference yielding new algorithm understanding behavior groups infer hidden relationships well predict future actions multiple agents interacting together algorithm rapidly recovers underlying causal model agents relate spatial stochastic games observations patterns inference made algorithm closely correspond human judgments algorithm makes rapid generalizations people	negative
machine learning based heuristic search algorithms to solve birds of a feather card game	research conducted interdisciplinary team two undergraduate students faculty explore solutions birds feather bof research challenge bof newly-designed perfect-information solitaire-type game focus study design implement different algorithms evaluate effectiveness team compared provided depth-first search dfs heuristic algorithms monte carlo tree search mcts well novel heuristic search algorithm guided machine learning since studied algorithms converge solution solvable deal effectiveness approach measured quickly solution reached many nodes traversed solution reached employed methods potential provide artificial intelligence enthusiasts better understanding bof novel ways solve perfect-information games puzzles general results indicate proposed heuristic search algorithms guided machine learning provide significant improvement terms number nodes traversed provided dfs algorithm	negative
abox abduction via forgetting in alc	abductive reasoning generates explanatory hypotheses new observations using prior knowledge paper investigates use forgetting also known uniform interpolation perform abox abduction description logic alc ontologies non-abducibles specified forgetting signature contain concept role symbols resulting hypotheses semantically minimal consist disjunction abox axioms disjuncts independent explanations redundant respect background ontology disjuncts representing form hypothesis space observations hypotheses handled method contain atomic complex alc concepts excluding role assertions restricted horn clauses two approaches redundancy elimination explored practice full approximate using prototype implementation experiments performed corpus real world ontologies investigate practicality approaches across several settings	negative
convergence of learning dynamics in information retrieval games	consider game-theoretic model information retrieval strategic authors examine two different utility schemes authors aim maximizing exposure authors want maximize active selection content i.e. number clicks introduce study author learning dynamics contexts prove probability ranking principle prp forms basis current state-of-the-art ranking methods betterresponse learning dynamics converges pure nash equilibrium also show ranking methods induce strategic environment convergence may occur	positive
representing and learning grammars in answer set programming	paper introduce extension context-free grammars called answer set grammars asgs grammars allow annotations production rules written language answer set programming asp express context-sensitive constraints investigate complexity various classes asg respect two decision problems deciding whether given string belongs language asg deciding whether language asg non-empty specifically show complexity decision problems lowered restricting subset asp language used annotations aid applicability grammars computational problems require context-sensitive parsers partially known languages propose learning task inducing annotations asg characterise complexity task present algorithm solving evaluation prototype implementation also discussed	positive
iterated belief base revision: a dynamic epistemic logic approach	agm ’ belief revision one main paradigms study belief change operations context belief bases prioritised bases largely used specify agent ’ belief state whether representing agent ’ ‘ explicit beliefs ’ computational model belief state connection iterated agm-like operations encoding dynamic epistemic logics studied works considered well-known postulates iterated belief revision theory characterised means belief bases counterpart dynamic epistemic logic work investigates priority graphs syntactic representation preference relations deeply connected prioritised bases used characterise belief change operators focusing well-known postulates iterated belief change provide syntactic representations belief change operators dynamic context well new negative results regarding possibility representing iterated belief revision operation using transformations priority graphs	negative
hybrid reinforcement learning with expert state sequences	existing imitation learning approaches often require complete demonstration data including sequences actions states available paper consider realistic difficult scenario reinforcement learning agent access state sequences expert expert actions unobserved propose novel tensor-based model infer unobserved actions expert state sequences policy agent optimized via hybrid objective combining reinforcement learning imitation learning evaluated hybrid approach illustrative domain atari games empirical results show 1 agents able leverage state expert sequences learn faster pure reinforcement learning baselines 2 tensor-based action inference model advantageous compared standard deep neural networks inferring expert actions 3 hybrid policy optimization objective robust noise expert state sequences	negative
lipper: synthesizing thy speech using multi-view lipreading	lipreading lot potential applications domain surveillance video conferencing despite work building lipreading systems limited classifying silent videos classes representing text phrases however multiple problems associated making lipreading text-based classification task like dependence particular language vocabulary mapping thus paper propose multi-view lipreading audio system namely lipper models regression task model takes silent videos input produces speech output multi-view silent videos observe improvement single-view speech reconstruction results show presenting exhaustive set experiments speaker-dependent out-of-vocabulary speaker-independent settings compare delay values lipper speechreading systems order show real-time nature audio produced also perform user study audios produced order understand level comprehensibility audios produced using lipper	negative
quantifying uncertainties in natural language processing tasks	reliable uncertainty quantification first step towards building explainable transparent accountable artificial intelligent systems recent progress bayesian deep learning made quantification realizable paper propose novel methods study benefits characterizing model data uncertainties natural language processing nlp tasks empirical experiments sentiment analysis named entity recognition language modeling using convolutional recurrent neural network models show explicitly modeling uncertainties necessary measure output confidence levels also useful enhancing model performances various nlp tasks	negative
deception in finitely repeated security games	allocating resources defend targets attack often complicated uncertainty attacker ’ capabilities objectives underlying characteristics repeated interaction setting defender collect attack data time reduce uncertainty learn effective defense however clever attacker manipulate attack data mislead defender influencing learning process toward benefit investigate strategic deception part attacker private type information interacts repeatedly defender present detailed computation analysis players ’ optimal strategies given attacker may play deceptively computational experiments illuminate conditions conducive strategic deception quantify benefits attacker taking account attacker ’ deception capacity defender significantly mitigate loss misleading attack actions	negative
group decision diagram (gdd): a compact representation for permutations	permutation fundamental combinatorial object appeared various areas mathematics computer science artificial intelligence applications subset permutation group must maintained efficiently study develop new data structure called group decision diagram gdd maintain set permutations data structure combines zero-suppressed binary decision diagram computable subgroup chain permutation group data structure enables efficient operations membership testing set operations e.g. union intersection difference cartesian product experiments demonstrate data structure efficient i.e. 20–300 times faster existing methods permutation group considerably smaller symmetric group subsets constructed operations generators maintained	positive
a unified framework for planning in adversarial and cooperative environments	users ai systems may rely upon produce plans achieving desired objectives ai systems able compute obfuscated plans whose execution adversarial situations protects privacy well legible plans easy team members understand cooperative situations develop unified framework addresses dual problems computing plans desired level comprehensibility point view partially informed observer adversarial settings approach produces obfuscated plans observations consistent least k goals set decoy goals slightly varying framework present approach producing legible plans cooperative settings observation sequence projected plan consistent j goals set confounding goals addition show observability observer controlled either obfuscate convey actions plan goal known observer present theoretical results complexity analysis approach also present empirical evaluation show feasibility usefulness approaches using ipc domains	negative
subspace selection via dr-submodular maximization on lattices	subspace selection problem seeks subspace maximizes objective function constraint problem includes several important machine learning problems principal component analysis sparse dictionary selection problem often problems exactly approximately solved using greedy algorithms interested problems solved greedy algorithms classes objective functions constraints admit property	negative
tdsnn: from deep neural networks to deep spike neural networks with temporal-coding	continuous-valued deep convolutional networks dnns converted accurate rate-coding based spike neural networks snns however substantial computational energy costs caused multiple spikes limit use mobile embedded applications recent works shown newly emerged temporal-coding based snns converted dnns reduce computational load effectively paper propose novel method convert dnns temporal-coding snns called tdsnn combined characteristic leaky integrate-andfire lif neural model put forward new coding principle reverse coding design novel ticking neuron mechanism according evaluation proposed method achieves 42 total operations reduction average large networks comparing dnns 0.5 accuracy loss evaluation shows tdsnn may prove one key enablers make adoption snns widespread	positive
an equivalence between wagering and fair-division mechanisms	draw surprising direct mathematical equivalence class allocation mechanisms divisible goods studied context fair division class weakly budget-balanced wagering mechanisms designed eliciting probabilities equivalence rests intuition wagering allocation financial securities among bettors bettor ’ value security proportional belief likelihood future event equivalence leads theoretical advances new practical approaches fair division wagering known wagering mechanisms based proper scoring rules yield fair allocation mechanisms desirable properties including first strictly incentive compatible fair-division mechanism time allocation mechanisms make novel wagering rules including one requires ordinal uncertainty judgments one outperforms existing rules range simulations	negative
an abstraction-based method for verifying strategic properties in multi-agent systems with imperfect information	investigate verification multi-agent systems strategic properties expressed alternating-time temporal logic assumptions imperfect information perfect recall end develop three-valued semantics concurrent game structures upon define abstraction method prove concurrent game structures imperfect information admit perfect information abstractions preserve three-valued satisfaction present refinement procedure deal cases value specification undefined illustrate overall procedure variant train gate controller scenario imperfect information perfect recall	negative
how similar are two elections?	introduce election isomorphism problem family approximate variants refer disomorphism distance d-id problems metric preference orders show election isomorphism polynomial-time solvable d-isomorphism distance problems generalize various classic rank-aggregation methods e.g. kemeny litvak establish complexity problems including inapproximability provide initial experiments regarding ability solve practice	negative
computing argumentative explanations in bipolar argumentation frameworks	process arguing also process justifying explaining focus argumentative explanations abstract bipolar argumentation propose new defence acceptability semantics operates attack support relations use formalize two types explanations concise strong explanations also show compute explanations bipolar dispute trees	positive
a theory of state abstraction for reinforcement learning	reinforcement learning presents challenging problem agents must generalize experiences efficiently explore world learn feedback delayed often sparse making use limited computational budget abstraction essential endeavors abstraction agents form concise models surroundings behavior supporting effective decision making diverse complex environments end goal doctoral research characterize role abstraction plays reinforcement learning focus state abstraction offer three desiderata articulating means state abstraction useful introduce classes state abstractions provide partial path toward satisfying desiderata collectively develop theory state abstractions 1 preserve near-optimal behavior 2 learned computed efficiently 3 lower time data needed make effective decisions close discussing extensions results information theoretic paradigm abstraction extension hierarchical abstraction enjoys desirable properties	negative
understanding dropouts in moocs	massive open online courses moocs developed rapidly recent years attracted millions online users however central challenge extremely high dropout rate — recent reports show completion rate moocs 5 onah sinclair boyatt 2014 kizilcec piech schneider 2013 seaton et al 2014	negative
partial awareness	develop modal logic capture partial awareness logic three building blocks objects properties concepts properties unary predicates objects concepts boolean combinations properties take agent partially aware concept aware concept without aware properties define logic allows quantification objects properties agent reason unawareness apply logic contracts view syntactic objects dictate outcomes based truth formulas show agents unaware relevant properties referencing concepts agents partially aware improve welfare	negative
on-line learning of linear dynamical systems: exponential forgetting in kalman filters	kalman filter key tool time-series forecasting analysis show dependence prediction kalman filter past decaying exponentially whenever process noise non-degenerate therefore kalman filter may approximated regression recent observations surprisingly also show process noise essential exponential decay process noise may happen forecast depends past uniformly makes forecasting difficult	negative
probabilistic model checking of robots deployed in extreme environments	robots increasingly used carry critical missions extreme environments hazardous humans requires high degree operational autonomy uncertain conditions poses new challenges assuring robot ’ safety reliability paper develop framework probabilistic model checking layered markov model verify safety reliability requirements robots pre-mission stage runtime two novel estimators based conservative bayesian inference imprecise probability model sets priors introduced learn unknown transition parameters operational data demonstrate approach using data real-world deployment unmanned underwater vehicles extreme environments	negative
mirroring without overimitation: learning functionally equivalent manipulation actions	paper presents mirroring approach inspired neuroscience discovery mirror neurons transfer demonstrated manipulation actions robots designed address different embodiments human demonstrator robot approach extends classic robot learning demonstration lfd following aspects	positive
that’s mine! learning ownership relations and norms for robots	ability autonomous agents learn conform human norms crucial safety effectiveness social environments recent work led frameworks representation inference simple social rules research norm learning remains exploratory stage present robotic system capable representing learning inferring ownership relations norms ownership represented graph probabilistic relations objects owners along database predicate-based norms constrain actions permissible owned objects learn norms relations system integrates novel incremental norm learning algorithm capable one-shot learning induction specific examples ii bayesian inference ownership relations response apparent rule violations iii perceptbased prediction object ’ likely owners series simulated real-world experiments demonstrate competence flexibility system performing object manipulation tasks require variety norms followed laying groundwork future research acquisition application social norms	negative
disjunctive normal form for multi-agent modal logics based on logical separability	modal logics primary formalisms multi-agent systems major reasoning tasks logics intractable impedes applications multi-agent modal logics automatic planning one technique tackling intractability identify fragment called normal form multiagent logics expressive tractable reasoning tasks entailment checking bounded conjunction transformation forgetting instance dnf propositional logic tractable reasoning tasks paper first introduce notion logical separability define novel disjunctive normal form sdnf multiagent logic kn overcomes shortcomings existing approaches particular show every modal formula kn equivalently casted formula sdnf major reasoning tasks tractable propositional dnf also tractable sdnf moreover formulas sdnf enjoy property logical separability demonstrate usefulness approach apply sdnf multi-agent epistemic planning finally extend results three complex multi-agent logics dn k45n kd45n	negative
iterative classroom teaching	consider machine teaching problem classroom-like setting wherein teacher deliver examples diverse group students diversity stems differences initial internal states well learning rates prove teacher full knowledge learning dynamics students teach target concept entire classroom using min n log 1/ɛ exam-ples ambient dimension problem n number learners ɛ accuracy parameter show robustness teaching strategy teacher limited knowledge learners ’ internal dynamics provided noisy oracle study trade-off learners ’ workload teacher ’ cost teaching target concept experiments validate theoretical results suggest appropriately partitioning classroom homogenous groups provides balance two objectives	positive
how to combine tree-search methods in reinforcement learning	finite-horizon lookahead policies abundantly used reinforcement learning demonstrate impressive empirical success usually lookahead policies implemented specific planning methods monte carlo tree search e.g alphazero silver et al 2017b referring planning problem tree search reasonable practice implementations back value leaves information obtained root leveraged updating policy question potency approach namely latter procedure non-contractive general convergence guaranteed proposed enhancement straightforward simple use return optimal tree path back values descendants root leads γh-contracting procedure γ discount factor h tree depth establish results first introduce notion called multiple-step greedy consistency provide convergence rates two algorithmic instantiations enhancement presence noise injected tree search stage value estimation stage	negative
axiomatic characterization of data-driven influence measures for classification	study following problem given labeled dataset specific datapoint ∼x i-th feature influence classification ∼x identify family numerical influence measures — functions given datapoint ∼x assign numeric value φi ∼x every feature corresponding altering ’ value would influence outcome ∼x family term monotone influence measures mim uniquely derived set desirable properties axioms mim family constitutes provably sound methodology measuring feature influence classification domains values generated mim based dataset alone make queries classifier requirement naturally limits scope framework demonstrate effectiveness data	negative
hierarchical reinforcement learning for course recommendation in moocs	proliferation massive open online courses moocs demands effective way personalized course recommendation recent attention-based recommendation models distinguish effects different historical courses recommending different target courses however user interests many different courses attention mechanism perform poorly effects contributing courses diluted diverse historical courses address challenge propose hierarchical reinforcement learning algorithm revise user profiles tune course recommendation model revised profiles	positive
end-to-end safe reinforcement learning through barrier functions for safety-critical continuous control tasks	reinforcement learning rl algorithms found limited success beyond simulated applications one main reason absence safety guarantees learning process real world systems would realistically fail break optimal controller learned address issue propose controller architecture combines 1 model-free rl-based controller 2 model-based controllers utilizing control barrier functions cbfs 3 online learning unknown system dynamics order ensure safety learning general framework leverages success rl algorithms learn high-performance controllers cbf-based controllers guarantee safety guide learning process constraining set explorable polices utilize gaussian processes gps model system dynamics uncertainties	negative
qualitative spatial logic over 2d euclidean spaces is not finitely axiomatisable	several qualitative spatial logics used reasoning geospatial data sound complete axiomatisation metric spaces open whether axiomatisation also sound complete 2d euclidean spaces answer question negatively showing axiomatisations presented du et al 2013 du alechina 2016 complete 2d euclidean spaces moreover logics finitely axiomatisable	negative
predicting urban dispersal events: a two-stage framework through deep survival analysis on mobility data	urban dispersal events processes unusually large number people leave area short period early prediction dispersal events important mitigating congestion safety risks making better dispatching decisions taxi ride-sharing fleets existing work mostly focuses predicting taxi demand near future learning patterns historical data however fail case abnormality dispersal events abnormally high demand non-repetitive violate common assumptions smoothness demand change time instead paper argue dispersal events follow complex pattern trips related features past used predict events therefore formulate dispersal event prediction problem survival analysis problem propose two-stage framework dilsa deep learning model combined survival analysis developed predict probability dispersal event demand volume conduct extensive case studies experiments nyc yellow taxi dataset 20142016. results show dilsa predict events next 5 hours f1-score 0:7 average time error 18 minutes orders magnitude better state-of-the-art deep learning approaches taxi demand prediction	negative
on the distortion value of the elections with abstention	spatial voting theory distortion measure good winner proved deterministic voting mechanism guarantee distortion better 3 even simple metrics line study wish answer following question distortion value change allow less motivated agents abstain election	negative
building ethically bounded ai	ai agents deployed scenarios possibly unexpected situations need flexible adaptive creative achieving goal given thus certain level freedom choose best path goal inherent making ai robust flexible enough time however pervasive deployment ai life whether ai autonomous collaborating humans raises several ethical challenges ai agents aware follow appropriate ethical principles thus exhibit properties fairness virtues ethical principles define boundaries ai ’ freedom creativity however still challenge understand specify reason ethical boundaries ai agents combine appropriately subjective preferences goal specifications initial attempts employ either data-driven examplebased approach symbolic rule-based approach envision modular approach ai technique used essential ingredients decision making decision support systems paired contextual approach define combination relative weight world neither humans ai systems work isolation tightly interconnected e.g. internet things also envision compositional approach building ethically bounded ai ethical properties component fruitfully exploited derive overall system paper define motivate notion ethically-bounded ai describe two concrete examples outline outstanding challenges	negative
artificial intelligence competencies for data science undergraduate curricula	august 2017 acm education council initiated task force add broad interdisciplinary conversation data science articulation role computing discipline-specific contributions emerging field specifically task force seeking define computing contributions new field order provide guidance computer science similar departments offering data science programs study undergraduate level acm data science task force completed initial draft curricular report computing-knowledge areas identified report drawn across computing disciplines include several sub-areas ai short paper describes overall project highlights ai-relevant areas seeks open dialog ai competencies considered central data science undergraduate curriculum	negative
a comparative analysis of expected and distributional reinforcement learning	since introduction year ago distributional approaches reinforcement learning distributional rl produced strong results relative standard approach models expected values expected rl however aside convergence guarantees theoretical results investigating reasons behind improvements distributional rl provides paper begin investigation fundamental question analyzing differences tabular linear approximation non-linear approximation settings prove many realizations tabular linear approximation settings distributional rl behaves exactly expected rl cases two methods behave differently distributional rl fact hurt performance induce identical behaviour continue empirical analysis comparing distributional expected rl methods control settings non-linear approximators tease apart improvements distributional rl methods coming	negative
automated dispatch of helpdesk email tickets: pushing the limits with ai	ticket assignment/dispatch crucial part service delivery business lot scope automation optimization paper present end-to-end automated helpdesk email ticket assignment system also offered service objective system determine nature problem mentioned incoming email ticket automatically dispatch appropriate resolver group team resolution	negative
consensus in opinion formation processes in fully evolving environments	friedkin johnsen 1990 modeled opinion formation social networks dynamic process evolves rounds round agent updates expressed opinion weighted average innate belief opinions expressed previous round social neighbors stubbornness level agent represents tendency agent express opinion close innate belief	negative
imitation learning from observation	humans animals natural ability learn skills observation often simply seeing effects skills without direct knowledge underlying actions taken example observing actor jumping jack child copy despite knowing anything 's going inside actor 's brain nervous system main focus thesis extending ability artificial autonomous agents endeavor recently referred `` imitation learning observation '' imitation learning observation especially relevant today due accessibility many online videos used demonstrations robots meanwhile advances deep learning enabled us solve increasingly complex control tasks mapping visual input motor commands thesis contributes algorithms learn control policies state-only demonstration trajectories two types algorithms considered first type begins recovering missing action information demonstrations leverages existing imitation learning algorithms full state-action trajectories preliminary work shown learning inverse dynamics model agent self-supervised fashion inferring actions performed demonstrator enables sufficient action recovery purpose second type algorithm uses model-free end-to-end learning preliminary results indicate iteratively optimizing policy based closeness imitator 's expert 's state transitions leads policy closely mimics demonstrator 's trajectories	negative
optimizing in the dark: learning an optimal solution through a simple request interface	network resource reservation systems developed deployed driven demand substantial benefits providing performance predictability modern distributed applications however existing systems suffer limitations either inefficient finding optimal resource reservation cause private information e.g. network infrastructure exposed e.g. user paper design boxopt novel system leverages efficient oracle construction techniques optimization learning theory automatically swiftly learn optimal resource reservations without exchanging private information network user implement prototype boxopt demonstrate efficiency efficacy via extensive experiments using real network topology trace results show 1 boxopt 100 correctness ratio 2 95 requests boxopt learns optimal resource reservation within 13 seconds	negative
traffic updates: saying a lot while revealing a little	taking speed reports vehicles proven inexpensive way infer traffic conditions however due concerns privacy bandwidth every vehicle occupant may want transmit data location speed real time show drastically reduce number transmissions two ways based markov random field modeling traffic speed flow first show small number vehicles need report location give simple probabilistic method lets group vehicles decide subset transmit report preserving privacy coordinating without communication second approach computes potential value location ’ speed report emphasizing reports affect overall speed inferences omitting contribute little value methods significantly reduce amount communication necessary accurate speed inferences road network	negative
on the hardness of probabilistic inference relaxations	promising approach probabilistic inference attracted recent attention exploits reduction set model counting queries since probabilistic inference model counting p-hard various relaxations used practice hope relaxations allow efficient computation also providing rigorous approximation guarantees	positive
transforming underwriting in the life insurance industry	life insurance provides trillions dollars financial security hundreds millions individuals families worldwide life insurance companies must accurately assess individual-level mortality risk simultaneously maintain financial strength price products competitively traditional underwriting process used assess risk based manually examining applicant ’ health behavioral financial profile existence large historical data sets provides unprecedented opportunity artificial intelligence machine learning transform underwriting life insurance industry present overview rich application data set survival modeling combined develop life score deployed algorithmic underwriting system massmutual american mutual life insurance company serving millions clients novel evaluation framework show life score outperforms traditional underwriting 6 basis claims describe engagement actuaries medical doctors underwriters reinsurers paramount building algorithmic underwriting system predictive model core finally provide details deployed system highlight value includes saving millions dollars operational efficiency driving decisions behind tens billions dollars benefits	negative
designing preferences, beliefs, and identities for artificial intelligence	research artificial intelligence well economics related fields generally proceeds premise agent well-defined identity well-defined preferences outcomes well-defined beliefs world however design ai systems fact need specify boundaries one agent another system lie objective functions agents aim maximize extent even belief formation processes use	negative
satisfiability in strategy logic can be easier than model checking	design complex systems model-checking satisfiability arise two prominent decision problems model-checking requires designed system provided advance satisfiability allows check system even exists exceptions second problem turns harder first one complexity-theoretic standpoint paper investigate connection two problems non-trivial fragment strategy logic sl short sl extends ltl first-order quantifications strategies thus allowing explicitly reason strategic abilities agents multi-agent system satisfiability full logic known highly undecidable model-checking non-elementary	negative
generalized batch normalization: towards accelerating deep neural networks	utilizing recently introduced concepts statistics quantitative risk management present general variant batch normalization bn offers accelerated convergence neural network training compared conventional bn general show mean standard deviation always appropriate choice centering scaling procedure within bn transformation particularly relu follows normalization step present generalized batch normalization gbn transformation utilize variety alternative deviation measures scaling statistics centering choices naturally arise theory generalized deviation measures risk theory general used conjunction relu non-linearity underlying risk theory suggests natural arguably optimal choices deviation measure statistic utilizing suggested deviation measure statistic show experimentally training accelerated conventional bn often improved error rate well overall propose flexible bn transformation supported complimentary theoretical framework potentially guide design choices	negative
generating live soccer-match commentary from play data	address task generating live soccer-match commentaries play event data task characteristics commentary partially aligned events ii play event data contains many types categorical numerical attributes iii live commentaries often mention player names team names reasons propose encoder play event data enhanced gate mechanism also introduce attention mechanism events addition introduced placeholders reconstruction mechanism enable model copy appropriate player names team names input data conduct experiments play data english premier league provide discussion result including generated commentaries	negative
verification of rnn-based neural agent-environment systems	introduce agent-environment systems agent stateful executing relu recurrent neural network define study verification problem providing equivalences recurrent feed-forward neural networks bounded execution traces give sound complete procedure verification properties specified simplified version ltl bounded executions present implementation discuss experimental results obtained	negative
on resolving ambiguous anaphoric expressions in imperative discourse	anaphora resolution central problem natural language understanding study subclass problem involving object pronouns used simple imperative sentences e.g. “ pick up. ” specifically address cases situational contextual information required interpret pronouns current state-of-the art statisticallydriven coreference systems knowledge-based reasoning systems insufficient address cases paper introduce examples general class situated anaphora resolution problems propose proof-of-concept system disambiguating situated pronouns discuss general types reasoning might needed	negative
weighted abstract dialectical frameworks through the lens of approximation fixpoint theory	weighted abstract dialectical frameworks wadfs recently introduced extending abstract dialectical frameworks incorporate degrees acceptance paper propose different view wadfs develop semantics wadfs based approximation fixpoint theory abstract algebraic theory designed capture semantics various non-monotonic reasoning formalisms formalism deviates original definition basic assumptions fundamental assume ordering acceptance degrees discuss impact differences relationship two versions formalism advantages approaches offers furthermore study complexity various semantics	negative
multi-gcn: graph convolutional networks for multi-view networks, with applications to global poverty	rapid expansion mobile phone networks developing countries large-scale graph machine learning gained sudden relevance study global poverty recent applications range humanitarian response poverty estimation urban planning epidemic containment yet vast majority computational tools algorithms used applications account multi-view nature social networks people related myriad ways graph learning models treat relations binary paper develop graph-based convolutional network learning multi-view networks show method outperforms state-of-the-art semi-supervised learning algorithms three different prediction tasks using mobile phone datasets three different developing countries also show designed specifically use poverty research algorithm also outperforms existing benchmarks broader set learning tasks multi-view networks including node labelling citation networks	negative
evolving action abstractions for real-time planning in extensive-form games	key challenge planning systems real-time multiagent domains search large action spaces decide agent ’ next action previous works showed handcrafted action abstractions allow planning systems focus search subset promising actions paper show problem generating action abstractions cast problem selecting subset pure strategies pool options model selection subset pure strategies two-player game strategy set players powerset pool options— call game subset selection game present evolutionary algorithm solving game empirical results small matches µrts show evolutionary approach able converge nash equilibrium subset selection game also results larger matches show search algorithms using action abstractions derived evolutionary approach able substantially outperform state-of-the-art planning systems tested	negative
eliminating latent discrimination: train then mask	control latent discrimination predictive models provably remove questions heart algorithmic fairness impacts society paper define new operational fairness criteria inspired well-understood notion omitted variable-bias statistics econometrics notion fairness effectively controls sensitive features provides diagnostics deviations fair decision making establish analytical algorithmic results existence fair classifier context supervised learning results readily imply simple rather counter-intuitive strategy eliminating latent discrimination order prevent features proxying sensitive features need include sensitive features training phase exclude test/evaluation phase controlling effects evaluate performance algorithm several realworld datasets show fairness datasets improved small loss accuracy	positive
modular materialisation of datalog programs	seminaïve algorithm used materialise consequences datalog program also forms basis algorithms incrementally update materialisation input facts change certain combinations rules however handled much efficiently using custom algorithms integrate algorithms general reasoning approach handle arbitrary rules propose modular framework computing maintaining materialisation split datalog program modules handled using specialised algorithms handle remaining rules using semina¨ıve algorithm also present two algorithms computing transitive symmetric– transitive closure relation used within framework finally show empirically framework handle arbitrary datalog programs outperforming existing approaches often orders magnitude	negative
counterfactual reasoning in observational studies	identify appropriate action take intelligent agent must infer causal effects every possible action choices prominent example precision medicine attempts identify medical procedure benefit individual patient requires answering counterfactual questions `` '' would patient lived longer received alternative treatment '' '' phd attempt explore ways address challenges associated causal effect estimation focus devising methods enhance performance according individual-based measures opposed population-based measures	negative
forgetting in modular answer set programming	modular programming facilitates creation reuse large software recently gathered considerable interest context answer set programming asp setting forgetting elimination middle variables longer deemed relevant importance allows one e.g. simplify program make declarative even hide parts without affecting consequences parts relevant forgetting context asp extensively studied known limitations make unsuitable used modular asp paper present novel class forgetting operators show operators always successfully applied modular asp forget kinds atoms – input output hidden – overcoming impossibility results exist general asp additionally investigate conditions class operators preserves module theorem modular asp thus ensuring answer sets modules still composed module theorem always preserved allow reconfiguration modules	negative
reasoning over assumption-based argumentation frameworks via direct answer set programming encodings	focusing assumption-based argumentation aba central structured formalism ai argumentation propose new approach reasoning aba without preferences previous approaches apply either specialized algorithms translate aba reasoning reasoning abstract argumentation frameworks develop direct approach encoding aba reasoning tasks answer set programming significantly improves empirical performance current aba reasoning systems also give new complexity results reasoning aba+ suggesting integration preferential information aba results increased problem complexity several central argumentation semantics	negative
is everything going according to plan? expectations in goal reasoning agents	part motivated topics agency safety increasing interest goal reasoning form agency agents formulate goals one crucial aspects goal reasoning agents ability detect execution courses actions meet expectations present taxonomy different forms expectations used goal reasoning agents monitoring execution summarize contrast current understanding define check expectations based different knowledge sources used also identify gaps understanding expectations	positive
reverse-engineering satire, or “paper on computational humor accepted despite making serious advances”	humor essential human trait efforts understand humor called links humor foundations cognition well importance humor social engagement promising important subject study relevance artificial intelligence human– computer interaction previous computational work humor mostly operated coarse level granularity e.g. predicting whether entire sentence paragraph document etc. humorous step toward deep understanding humor seek fine-grained models attributes make given text humorous starting observation satirical news headlines tend resemble serious news headlines build analyze corpus satirical headlines paired nearly identical serious headlines corpus constructed via unfun.me online game incentivizes players make minimal edits satirical headlines goal making players believe results serious headlines edit operations used successfully remove humor pinpoint words concepts play key role making original satirical headline funny analysis reveals humor tends reside toward end headlines primarily noun phrases satirical headlines follow certain logical pattern term false analogy overall paper deepens understanding syntactic semantic structure satirical news headlines provides insights building humor-producing systems	negative
a framework for approval-based budgeting methods	define study general framework approval-based budgeting methods compare certain methods within framework axiomatic computational properties furthermore visualize behavior certain euclidean distributions analyze experimentally	negative
an exponential tail bound for the deleted estimate	accumulating evidence literature stability learning algorithms key characteristic permits learning algorithm generalize despite various insightful results direction seems overlooked dichotomy type stability-based generalization bounds literature one hand literature seems suggest exponential generalization bounds estimated risk optimal obtained stringent distribution independent computationally intractable notions stability uniform stability hand seems weaker notions stability hypothesis stability although distribution dependent amenable computation yield polynomial generalization bounds estimated risk suboptimal paper address gap two regimes results particular main question address whether possible derive exponential generalization bounds estimated risk using notion stability computationally tractable distribution dependent weaker uniform stability using recent advances concentration inequalities using notion stability weaker uniform stability distribution dependent amenable computation derive exponential tail bound concentration estimated risk hypothesis returned general learning rule estimated risk expressed terms deleted estimate interestingly note final bound similarities previous exponential generalization bounds deleted estimate particular result bousquet elisseeff 2002 regression case	negative
overcoming blind spots in the real world: leveraging complementary abilities for joint execution	simulators increasingly used train agents deploying real-world environments training simulation provides cost-effective way learn poorly modeled aspects simulator lead costly mistakes blind spots humans help guide agent towards identifying error regions humans blind spots noise execution study learning blind spots used manage hand-off decisions humans agents jointly act real-world neither trained evaluated fully formulation assumes agent blind spots result representational limitations simulation world leads agent ignore important features relevant acting open world approach blind spot discovery combines experiences collected simulation limited human demonstrations first step applies imitation learning demonstration data identify important features human using agent missing second step uses noisy labels extracted action mismatches agent human across simulation demonstration data train blind spot models show experiments two domains approach able learn succinct representation accurately captures blind spot regions avoids dangerous errors real world transfer control agent human	negative
labor division with movable walls: composing executable specifications with machine learning and search (blue sky idea)	artificial intelligence ai techniques including e.g. machine learning multi-agent collaboration planning heuristic search emerging ever-stronger tools solving hard problems real-world applications executable specification techniques es including e.g. statecharts scenario-based programming promising development approach offering intuitiveness ease enhancement compositionality amenability formal analysis propose approach integrating ai es techniques developing complex intelligent systems greatly simplify agile/spiral development maintenance processes approach calls automated detection whether certain goals sub-goals met clear division sub-goals solved ai solved es compositional incremental addition ai-based es-based components focusing particular gap current capability well-stated goal iterative refinement sub-goals solved ai smaller sub-sub-goals solved es ai describe principles approach advantages well key challenges suggestions tackle	negative
selecting compliant agents for opt-in micro-tolling	paper examines impact tolls social welfare context transportation network portion agents subject tolls specifically paper addresses question subset agents provides system benefit compliant approximate marginal cost tolling scheme since previous work suggests problem np-hard examine heuristic approach experimental results three real-world traffic scenarios suggest evaluating marginal impact given agent serves particularly strong heuristic selecting agent compliant results using heuristic selecting 7.6 agents compliant achieved increase 10.9 social welfare tolling presented heuristic approach conclusions help practitioners target specific agents participate opt-in tolling scheme	positive
inverse abstraction of neural networks using symbolic interpolation	neural networks real-world applications satisfy critical properties safety reliability analysis properties typically requires extracting information computing pre-images network transformations well-known explicit computation pre-images intractable introduce new methods computing compact symbolic abstractions pre-images computing overapproximations underapproximations layers abstraction pre-images enables formal analysis knowledge extraction without affecting standard learning algorithms use inverse abstractions automatically extract simple control laws compact representations pre-images corresponding unsafe outputs illustrate extracted abstractions interpretable used analyzing complex properties	negative
group fairness for the allocation of indivisible goods	consider problem fairly dividing collection indivisible goods among set players much existing literature fair division focuses notions individual fairness instance envy-freeness requires player prefer set goods allocated another player allocation observe algorithm satisfying individual fairness notions still treat groups players unfairly one group desiring goods allocated another main contribution notion group fairness implies existing notions individual fairness group fairness like individual fairness satisfied exactly indivisible goods thus introduce two “ one good ” style relaxations show somewhat surprisingly certain local optima nash welfare function satisfy relaxations computed pseudo-polynomial time local search experiments reveal faster computation stronger fairness guarantees practice	positive
scisummnet: a large annotated corpus and content-impact models for scientific paper summarization with citation networks	scientific article summarization challenging large annotated corpora available summary ideally include article ’ impacts research community paper provides novel solutions two challenges 1 develop release first large-scale manually-annotated corpus scientific papers computational linguistics enabling faster annotation 2 propose summarization methods integrate authors ’ original highlights abstract article ’ actual impacts community citations create comprehensive hybrid summaries conduct experiments demonstrate efficacy corpus training data-driven models scientific paper summarization advantage hybrid summaries abstracts traditional citation-based summaries large annotated corpus hybrid methods provide new framework scientific paper summarization research	negative
fair division with a secretive agent	study classic fair-division problems partial information setting paper respectively addresses fair division rent cake indivisible goods among agents cardinal preferences show settings appropriate valuations fair approximately fair division among n agents efficiently computed using valuations n − 1 agents nth secretive agent make arbitrary selection division proposed irrespective choice computed division admit overall fair allocation	negative
borda count in collective decision making: a summary of recent results	borda count one earliest important voting rules going far beyond voting summarize recent advances related borda computational social choice generally collective decision making first present variety well known attacks modeling strategic behavior voting—including manipulation control bribery—and discuss resistant borda terms computational complexity describe borda used maximize social welfare indivisible goods allocated agents ordinal preferences finally illustrate use borda forming coalitions players certain type hedonic game approaches central applications artificial intelligence	positive
probabilistic alternating-time µ-calculus	reasoning strategic abilities key ai system consisting multiple agents random behaviors propose probabilistic extension alternating µ-calculus amc named pamc reasoning strategic abilities agents stochastic multi-agent systems pamc subsumes existing logics amc pµtl usefulness pamc exemplified applications genetic regulatory networks show pamc model checking problem up∩co-up satisfiability problem exptime-complete amc moreover pamc admits small model property implement satisfiability checking procedure tool pamcsolver	negative
lifelong path planning with kinematic constraints for multi-agent pickup and delivery	multi-agent pickup delivery mapd problem models applications large number agents attend stream incoming pickup-and-delivery tasks token passing tp recent mapd algorithm efficient effective make tp even efficient effective using novel combinatorial search algorithm called safe interval path planning reservation table sippwrt single-agent path planning sippwrt uses advanced data structure allows fast updates lookups current paths agents online setting resulting mapd algorithm tp-sippwrt takes kinematic constraints real robots account directly planning computes continuous agent movements given velocities work non-holonomic robots rather discrete agent movements uniform velocity complete wellformed mapd instances demonstrate benefits automated warehouses using agent simulator standard robot simulator example demonstrate compute paths hundreds agents thousands tasks seconds efficient effective existing mapd algorithms use post-processing step adapt paths continuous agent movements given velocities	negative
primarily about primaries	much social choice literature examines direct voting systems voters submit ranked preferences candidates voting rule picks winner real-world elections decision-making processes often complex involve multiple stages instance one popular voting system filters candidates primaries first voters affiliated political party vote candidates party voting rule picks candidate party compete general election	negative
meta-path augmented response generation	propose chatbot namely mocha make good use relevant entities generating responses augmented meta-path information mocha able mention proper entities following conversation flow	positive
learning models of sequential decision-making with partial specification of agent behavior	artificial agents interact human artificial agents require models order reason agents ’ behavior addition predictive utility models maintaining model aligned agent ’ true generative model behavior critical effective human-agent interaction applications wherein observations partial specification agent ’ behavior available achieving model alignment challenging variety reasons one agent ’ decision factors often completely known prior approaches rely upon observations agents ’ behavior alone fail recover true model since multiple models explain observed behavior equally well achieve better model alignment provide novel approach capable learning aligned models conform partial knowledge agent ’ behavior central approach factored model behavior amm along bayesian nonparametric priors inference approach capable incorporating partial specifications constraints model learning evaluate approach experiments demonstrate improvements metrics model alignment	negative
minimum intervention cover of a causal graph	eliciting causal effects interventions observations one central concerns science increasingly artificial intelligence provide algorithm given causal graph g determines mic g minimum intervention cover g i.e. minimum set interventions suffices identifying every causal effect identifiable causal model characterized g. establish completeness do-calculus computing mic g mic g effectively offers efficient compilation information obtainable possible interventions causal model characterized g. minimum intervention cover finds applications variety contexts including counterfactual inference generalizing causal effects across experimental settings analyze computational complexity minimum intervention cover identify special cases practical interest mic g computed time polynomial size g	negative
learning set functions with limited complementarity	study pmac-learning real-valued set functions limited complementarity prove knowledge first nontrivial learnability result set functions exhibiting complementarity generalizing balcan harvey ’ result submodular functions prove nearly matching information theoretical lower bound number samples required complementing learnability result conduct numerical simulations show algorithm likely perform well practice	negative
deep neural networks constrained by decision rules	deep neural networks achieve high predictive accuracy learning latent representations complex data however reasoning behind decisions difficult humans understand hand rule-based approaches able justify decisions showing decision rules leading relatively low accuracy improve interpretability neural networks several techniques provide post-hoc explanations decisions made neural networks guarantee decisions always explained simple form like decision rules explanations generated decisions made neural networks	negative
linking educational resources on data science	availability massive datasets genetics neuroimaging mobile health subfields biology medicine promises new insights also poses significant challenges realize potential big data biomedicine national institutes health launched big data knowledge bd2k initiative funding several centers excellence biomedical data analysis training coordinating center tcc tasked facilitating online inperson training biomedical researchers data science major initiative bd2k tcc automatically identify describe organize data science training resources available web provide personalized training paths users paper describe construction erudite educational resource discovery index data science release linked data erudite contains 11,000 training resources including courses video tutorials conference talks materials metadata resources described uniformly using schema.org use machine learning techniques tag resource concepts data science education ontology developed describe resource content finally map references people organizations learning resources entities dbpedia dblp orcid embedding collection web linked data hope erudite provide framework foster open linked educational resources web	negative
an integrative framework for artificial intelligence education	modern introductory courses ai train students create intelligent systems provide broad coverage complex field paper identify problems common approaches teaching artificial intelligence suggest alternative principles courses adopt instead illustrate principles proposed course teaches students component methods pattern matching decision making also combination higher-level abilities reasoning sequential control plan generation integrated intelligent agents also present curriculum instantiates organization including sample programming exercises project requires system integration participants also gain experience building knowledge-based agents use software produce intelligent behavior	negative
incorporating behavioral constraints in online ai systems	ai systems learn reward feedback actions take increasingly deployed domains significant impact daily life however many cases online rewards guiding criteria additional constraints and/or priorities imposed regulations values preferences ethical principles detail novel online agent learns set behavioral constraints observation uses learned constraints guide making decisions online setting still reactive reward feedback define agent propose adopt novel extension classical contextual multi-armed bandit setting provide new algorithm called behavior constrained thompson sampling bcts allows online learning obeying exogenous constraints agent learns constrained policy implements observed behavioral constraints demonstrated teacher agent uses constrained policy guide reward-based online exploration exploitation characterize upper bound expected regret contextual bandit algorithm underlies agent provide case study real world data two application domains experiments show designed agent able act within set behavior constraints without significantly degrading overall reward performance	negative
wasserstein soft label propagation on hypergraphs: algorithm and generalization error bounds	inspired recent interests developing machine learning data mining algorithms hypergraphs investigate paper semi-supervised learning algorithm propagating ” soft labels ” e.g probability distributions class membership scores hypergraphs means optimal transportation borrowing insights wasserstein propagation graphs solomon et al 2014 re-formulate label propagation procedure message-passing algorithm renders naturally generalization applicable hypergraphs wasserstein barycenters furthermore pac learning framework provide generalization error bounds propagating one-dimensional distributions graphs hypergraphs using 2-wasserstein distance establishing algorithmic stability proposed semisupervised learning algorithm theoretical results also shed new lights upon deeper understandings wasserstein propagation graphs	negative
numerical optimization to ai, and back	impact numerical optimization modern data analysis quite significant today methods lie heart statistical machine learning applications domains spanning genomics finance medicine expanding scope applications complexity associated data continued raise expectations various criteria associated underlying algorithms broadly speaking research work classified two ai categories optimization ml opt-ml optimization cv opt-cv	negative
distributed community detection via metastability of the 2-choices dynamics	investigate behavior simple majority dynamics networks agents whose interaction topology exhibits community structure leveraging recent advancements analysis dynamics prove states nodes randomly initialized system rapidly stably converges configuration communities maintain internal consensus different states first analytical result behavior dynamics nonconsensus problems non-complete topologies based first symmetry-breaking analysis setting	negative
unbounded orchestrations of transducers for manufacturing	recently increasing interest using reactive synthesis techniques automate production manufacturing process plans previous work assumed set manufacturing resources known fixed advance paper consider general problem whether controller synthesized given sufficient resources unbounded setting types available manufacturing resources given want know whether possible manufacture product using resources type many resources type needed model manufacturing processes facilities transducers automata output show unbounded orchestration problem decidable pareto optimal set resources necessary manufacture product computable uni-transducers however multitransducers problem undecidable	negative
a pac framework for aggregating agents’ judgments	specifying objective function ai system pursue challenging especially decisions made system moral component input multiple stakeholders often required consider approaches query judgments individual examples aggregate judgments general policy propose formal learning-theoretic framework setting give general results translate classical results pac learning results framework subsequently show settings better results obtained working directly framework finally discuss model extended variety ways future research	negative
coverage centrality maximization in undirected networks	centrality metrics among main tools social network analysis central user network leads several benefits user central users highly influential play key roles within network therefore optimization problem increasing centrality network user recently received considerable attention given network target user v centrality maximization problem consists creating k new links incident v way centrality v maximized according centrality metric algorithms proposed literature based showing given centrality metric monotone submodular respect link addition however property hold several shortest-path based centrality metrics links undirected	positive
modelling autobiographical memory loss across life span	neurocomputational modelling long-term memory core topic computational cognitive neuroscience essential towards self-regulating brain-like ai systems paper study people generally lose memories emulate various memory loss phenomena using neurocomputational autobiographical memory model specifically based prior neurocognitive neuropsychology studies identify three neural processes namely overload decay inhibition lead memory loss memory formation storage retrieval respectively model validation collect memory dataset comprising one thousand life events emulate three key memory loss processes model parameters learnt memory recall behavioural patterns found human subjects different age groups emulation results show high correlation human memory recall performance across life span even another population used learning best knowledge paper first research work quantitative evaluations autobiographical memory loss using neurocomputational model	negative
complexity of abstract argumentation under a claim-centric view	abstract argumentation frameworks introduced dung part argumentation process arguments conflicts derived given knowledge base solely relation arguments used order identify acceptable sets arguments final step concerns acceptance status particular statements reviewing actual contents acceptable arguments complexity analysis abstract argumentation far neglected final step concerned argument names instead contents i.e claims outline paper slight deviation lead different complexity results therefore give comprehensive complexity analysis abstract argumentation claim-centric view analyse four main decision problems seven popular semantics addition also address complexity common sub-classes introduce novel parameterisations – exploit nature claims explicitly – along fixed-parameter tractability results	negative
matroid constrained fair allocation problem	consider problem allocating set indivisible goods among group homogeneous agents matroid constraints additive valuations fair manner propose novel algorithm computes fair allocation instances additive identical valuations even matroid constraints result provides computational anchor existential result fairness notion called ef1 envy-free one good biswas barman setting provide examples show fairness notions stronger ef1 always exist setting	positive
performance evaluation in machine learning: the good, the bad, the ugly, and the way forward	paper gives overview ways understanding performance evaluation measures machine-learned classifiers improved last twenty years also highlight range areas understanding still lacking leading ill-advised practices classifier evaluation suggests order make progress need develop proper measurement theory machine learning demonstrate example measurement theory might look like kinds new results would entail finally argue key properties classification ability data set difficulty unlikely directly observable suggesting need latent-variable models causal inference	positive
phonemd: learning to diagnose parkinson’s disease from smartphone data	parkinson ’ disease neurodegenerative disease affect person ’ movement speech dexterity cognition clinicians primarily diagnose parkinson ’ disease performing clinical assessment symptoms however misdiagnoses common one factor contributes misdiagnoses symptoms parkinson ’ disease may prominent time clinical assessment performed present machine-learning approach towards distinguishing people without parkinson ’ disease using long-term data smartphone-based walking voice tapping memory tests demonstrate attentive deep-learning models achieve significant improvements predictive performance strong baselines area receiver operating characteristic curve 0.85 data cohort 1853 participants also show models identify meaningful features input data results confirm smartphone data collected extended periods time could future potentially used digital biomarker diagnosis parkinson ’ disease	positive
pareto efficient auctions with interest rates	consider auction settings agents limited access monetary resources able make payments larger available resources taking loans certain interest rate setting strict generalization budget constrained utility functions corresponds infinite interest rates main result incentive compatible pareto-efficient auction divisible multi-unit setting 2 players able borrow money interest rate auction ascending price clock auction bears similarities clinching auction time considerable departure framework allocated goods de-allocated future given agents prices previously allocated goods raised	positive
migration as submodular optimization	migration presents sweeping societal challenges recently attracted significant attention scientific community one prominent approaches suggested employs optimization machine learning match migrants localities way maximizes expected number migrants find employment however relies strong additivity assumption argue hold practice due competition effects propose enhance data-driven approach explicitly optimizing effects specifically cast problem maximization approximately submodular function subject matroid constraints prove worst-case guarantees given classic greedy algorithm extend setting present three different models competition effects show give rise submodular objectives finally demonstrate via simulations approach leads significant gains across board	negative
convex formulations for fair principal component analysis	though growing literature fairness supervised learning incorporating fairness unsupervised learning less well-studied paper studies fairness context principal component analysis pca first define fairness dimensionality reduction definition interpreted saying reduction fair information protected class e.g. race gender inferred dimensionality-reduced data points next develop convex optimization formulations improve fairness respect definition pca kernel pca formulations semidefinite programs demonstrate effectiveness using several datasets conclude showing approach used perform fair respect age clustering health data may used set health insurance rates	negative
scalable robust kidney exchange	barter exchanges participants directly trade endowed goods constrained economic setting without money transactions barter exchanges often facilitated via central clearinghouse must match participants even face uncertainty—over participants existence quality potential trades leveraging robust combinatorial optimization techniques address uncertainty kidney exchange real-world barter market patients swap compatible paired donors provide two scalable robust methods handle two distinct types uncertainty kidney exchange—over quality existence potential match latter case directly addresses weakness stochastic-optimization-based methods kidney exchange clearing problem necessarily require explicit estimates probability transaction existing—a still-unsolved problem nascent market also propose novel scalable kidney exchange formulation eliminates need exponential-time constraint generation process competing formulations maintains provable optimality serves subsolver robust approach type uncertainty demonstrate benefits robustness real data large fielded kidney exchange united states conclude drawing parallels robustness notions fairness kidney exchange setting	positive
bird: engineering an efficient cnf-xor sat solver and its applications to approximate model counting	given boolean formula φ problem model counting also referred sat compute number solutions φ. model counting fundamental problem artificial intelligence wide range applications including probabilistic reasoning decision making uncertainty quantified information flow like motivated success sat solvers surge interest design hashing-based techniques approximate model counting past decade profiled state art approximate model counter approxmc2 observed 99.99 time consumed underlying sat solver cryptominisat observation motivated us ask design efficient underlying cnf-xor sat solver take advantage structure hashing-based algorithms would lead efficient approximate model counter	positive
fairly allocating many goods with few queries	investigate query complexity fair allocation indivisible goods two agents arbitrary monotonic valuations design algorithm computes allocation satisfying envy-freeness one good ef1 relaxation envy-freeness using logarithmic number queries show logarithmic query complexity bound also holds three agents additive valuations results suggest possible fairly allocate goods practice even number goods extremely large contrast prove computing allocation satisfying envyfreeness another relaxations envy-freeness good efx requires linear number queries even two agents identical additive valuations	negative
when do words matter? understanding the impact of lexical choice on audience perception using individual treatment effect estimation	studies across many disciplines shown lexical choice affect audience perception example users describe social media profile affect perceived socio-economic status however lack general methods estimating causal effect lexical choice perception specific sentence randomized controlled trials may provide good estimates scale potentially millions comparisons necessary consider lexical choices instead paper first offer two classes methods estimate effect perception changing one word another given sentence first class algorithms builds upon quasi-experimental designs estimate individual treatment effects observational data second class treats treatment effect estimation classification problem conduct experiments three data sources yelp twitter airbnb finding algorithmic estimates align well produced randomized-control trials additionally find possible transfer treatment effect classifiers across domains still maintain high accuracy	negative
towards automated semi-supervised learning	automated machine learning automl aims build appropriate machine learning model unseen dataset automatically i.e. without human intervention great efforts devoted automl typically focus supervised learning many applications however semisupervised learning ssl widespread current automl systems could well address ssl problems paper propose present automated learning system ssl auto-ssl first meta-learning enhanced meta-features employed quickly suggest instantiations ssl techniques likely perform quite well second large margin separation method proposed fine-tune hyperparameters importantly alleviate performance deterioration basic idea certain hyperparameter owns high quality predictive results unlabeled data may large margin separation extensive empirical results 200 cases demonstrate proposal one side achieves highly competitive better performance compared state-of-the-art automl system auto-sklearn classical ssl techniques side unlike classical ssl techniques often significantly degenerate performance proposal seldom suffers deficiency	negative
scalable distributed dl training: batching communication and computation	scalability distributed deep learning dl training parameter server architecture often communication constrained large clusters recent efforts use layer layer strategy overlap gradient communication backward computation reduce impact communication constraint scalability however approaches effectively applied overlap parameter communication forward computation paper propose design ibatch novel communication approach batches parameter communication forward computation overlap formulate batching decision optimization problem solve based greedy algorithm derive communication computation batches implement ibatch open-source dl framework bigdl perform evaluations various dl workloads experimental results show ibatch improves scalability cluster 72 nodes 73 default ps 41 layer layer strategy	negative
from lab to internship and back again: learning autonomous systems through creating a research and development ecosystem	research development r autonomous systems progresses interdisciplinary knowledge needed domains diverse artificial intelligence ai bi-ology psychology modeling simulation robotics r efforts necessarily interdisciplinary nature require technical well soft skills teamwork communication integration paper introduce 14 week summer long internship developing skills undergraduate science engineering interns r internship designed modular divided three parts training innovation application/integration end result internship 1 development ecosystem autonomy concepts 2 development robotics testing reasoning methods bayesian methods cognitive models basal ganglia 3 process future internships within modular construct collaboration full-time professional staff actively learned interns internship incorporates feedback loop educate per-form fundamental r future iterations internship leverage ecosystem adapt modular internship framework focus different innovations learning paradigms and/or applications	negative
capacity control of relu neural networks by basis-path norm	recently path norm proposed new capacity measure neural networks rectified linear unit relu activation function takes rescaling-invariant property relu account shown generalization error bound terms path norm explains empirical generalization behaviors relu neural networks better capacity measures moreover optimization algorithms take path norm regularization term loss function like path-sgd shown achieve better generalization performance however path norm counts values paths hence capacity measure based path norm could improperly influenced dependency among different paths also known path relu network represented small group linearly independent basis paths multiplication division operation indicates generalization behavior network depends basis paths motivated propose new norm basis-path norm based group linearly independent paths measure capacity neural networks accurately establish generalization error bound based basis path norm show explains generalization behaviors relu networks accurately previous capacity measures via extensive experiments addition develop optimization algorithms minimize empirical risk regularized basis-path norm experiments benchmark datasets demonstrate proposed regularization method achieves clearly better performance test set previous regularization approaches	negative
dictionary-guided editing networks for paraphrase generation	intuitive way human write paraphrase sentences replace words phrases original sentence corresponding synonyms make necessary changes ensure new sentences fluent grammatically correct propose novel approach modeling process dictionary-guided editing networks effectively conduct rewriting source sentence generate paraphrase sentences jointly learns selection appropriate word level phrase level paraphrase pairs context original sentence off-the-shelf dictionary well generation fluent natural language sentences specifically system retrieves set word level phrase level paraphrase pairs derived paraphrase database ppdb original sentence used guide decision words might deleted inserted soft attention mechanism sequence-to-sequence framework conduct experiments two benchmark datasets paraphrase generation namely mscoco quora dataset automatic evaluation results demonstrate dictionary-guided editing networks outperforms baseline methods human evaluation results indicate generated paraphrases grammatically correct relevant input sentence	negative
model ai assignments 2019	model ai assignments session seeks gather disseminate best assignment designs artificial intelligence ai education community recognizing assignments form core student learning experience present abstracts ten ai assignments 2019 session easily adoptable playfully engaging flexible variety instructor needs assignment specifications supporting resources may found http //modelai.gettysburg.edu	negative
popbots: designing an artificial intelligence curriculum for early childhood education	popbots hands-on toolkit curriculum designed help young children learn artificial intelligence ai building programming training interacting social robot today ’ children encounter ai forms smart toys computationally curated educational entertainment content however children yet empowered understand create technology existing computational thinking platforms made ideas like sequencing conditionals accessible young learners going beyond seek make ai concepts accessible designed popbots address specific learning needs children ages four seven adapting constructionist ideas ai curriculum paper describes designed curriculum evaluated effectiveness 80 pre-k kindergarten children found use social robot learning companion programmable artifact effective helping young children grasp ai concepts also identified teaching approaches greatest impact student ’ learning based make recommendations future modules iterations popbots platform	positive
compiling bayesian network classifiers into decision graphs	propose algorithm compiling bayesian network classifiers decision graphs mimic input output behavior classifiers particular compile bayesian network classifiers ordered decision graphs tractable exponentially smaller size decision trees tractability facilitates reasoning behavior bayesian network classifiers including explanation decisions make compilation algorithm comes guarantees time compilation size compiled decision graphs apply compilation algorithm classifiers literature discuss case studies show automatically explain decisions verify properties behavior	positive
joint representation learning for multi-modal transportation recommendation	multi-modal transportation recommendation goal recommending travel plan considers various transportation modes walking cycling automobile public transit connect among modes successful development multi-modal transportation recommendation systems help satisfy diversified needs travelers improve efficiency transport networks however existing transport recommender systems mainly focus unimodal transport planning end paper propose joint representation learning framework multi-modal transportation recommendation based carefully-constructed multi-modal transportation graph specifically first extract multi-modal transportation graph large-scale map query data describe concurrency users origin-destination od pairs transport modes provide effective solutions optimization problem develop anchor embedding transport modes initialize embeddings transport modes moreover infer user relevance od pair relevance incorporate regularize representation learning finally exploit learned representations online multimodal transportation recommendations indeed method deployed one largest navigation apps serve hundreds millions users extensive experimental results real-world map query data demonstrate enhanced performance proposed method multimodal transportation recommendations	negative
non-asymptotic uniform rates of consistency for k-nn regression	derive high-probability finite-sample uniform rates consistency k-nn regression optimal logarithmic factors mild assumptions moreover show k-nn regression adapts unknown lower intrinsic dimension automatically sup-norm apply k-nn regression rates establish new results estimating level sets global maxima function noisy observations	negative
desiderata for interpretability: explaining decision tree predictions with counterfactuals	explanations machine learning come many forms consensus regarding desired properties still emerging work collect organise explainability desiderata discuss used systematically evaluate properties quality explainable system using case class-contrastive counterfactual statements leads us propose novel method explaining predictions decision tree counterfactuals show model-specific approach exploits theoretical advantages counterfactual explanations hence improves decision tree interpretability decoupling quality interpretation depth width tree	negative
extension removal in abstract argumentation – an axiomatic approach	paper continues rather recent line research dynamics non-monotonic formalisms particular consider semantic changes dung ’ abstract argumentation formalism one studied problems context so-called enforcing problem concerned manipulating argumentation frameworks afs certain desired set arguments becomes extension study inverse problem namely extension removal problem possible – – modify given argumentation framework way certain undesired extensions longer generated analogously well known agm paradigm develop axiomatic approach removal problem i.e certain set axioms determine suitable manipulations although contraction elimination particular belief conceptually quite different extension removal surprisingly deep connections two turns postulates removal directly obtained reformulations agm contraction postulates prove series formal results including conditional unconditional existence semantical uniqueness removal operators well various impossibility results – show possible ways	positive
adaptive modeling for risk-aware decision making	thesis aims provide foundation risk-aware decision making decision making uncertainty core capability autonomous agent cornerstone long-term autonomy safety risk-aware decision making risk-aware model fully accounts known set risks environment respect problem consideration process decision making using model risk-aware decision making formulating risk-aware models critical robust reasoning uncertainty since impact using less accurate models may catastrophic extreme cases due overly optimistic view problems propose adaptive modeling framework helps balance trade-off model simplicity risk awareness different notions risks remaining computationally tractable	negative
evolution of collective fairness in hybrid populations of humans and agents	fairness plays fundamental role decision-making evidenced high incidence human behaviors result egalitarian outcomes often shown context dyadic interactions resorting ultimatum game peculiarities group interactions – corresponding effect eliciting fair actions – remain however astray focusing groups suggests several questions related effect group size group decision rules interrelation human agents ’ behaviors hybrid groups address topics test multiplayer version ultimatum game mug proposals made groups responders collectively accept reject firstly run online experiment evaluate humans react different group decision rules observe people become increasingly fair groups adopt stricter decision rules i.e. individuals required accept proposal accepted group secondly propose new analytical model shed light behaviors may evolved thirdly adapt model include agents fixed behaviors show including hardcoded pro-social agents favors evolutionary stability fair states even soft group decision rules suggests judiciously introducing agents particular behaviors population may leverage long-term social benefits	negative
convolutional spatial attention model for reading comprehension with multiple-choice questions	machine reading comprehension mrc multiplechoice questions requires machine read given passage select correct answer among several candidates paper propose novel approach called convolutional spatial attention csa model better handle mrc multiple-choice questions proposed model could fully extract mutual information among passage question candidates form enriched representations furthermore merge various attention results propose use convolutional operation dynamically summarize attention values within different size regions experimental results show proposed model could give substantial improvements various state-of- the-art systems race semeval-2018 task11 datasets	negative
the pure price of anarchy of pool block withholding attacks in bitcoin mining	bitcoin cryptocurrency built blockchain data structure generated significant academic commercial interest contrary prior expectations recent research shown participants protocol so-called “ miners ” always incentivized follow protocol study game induced one attack – pool block withholding attack – mining pools groups miners attack mining pools focus case two pools attacking potentially mining power system	positive
updates in human-ai teams: understanding and addressing the performance/compatibility tradeoff	ai systems deployed support human decision making high-stakes domains healthcare criminal justice many cases human ai form team human makes decisions reviewing ai ’ inferences successful partnership requires human develops insights performance ai system including failures study influence updates ai system setting updates increase ai ’ predictive performance may also lead behavioral changes odds user ’ prior experiences confidence ai ’ inferences show updates increase ai performance may actually hurt team performance introduce notion compatibility ai update prior user experience present methods studying role compatibility human-ai teams empirical results three high-stakes classification tasks show current machine learning algorithms produce compatible updates propose re-training objective improve compatibility update penalizing new errors objective offers full leverage performance/compatibility tradeoff across different datasets enabling compatible yet accurate updates	negative
a unified approach to online matching with conflict-aware constraints	online bipartite matching allocation models widely used analyze design markets internet advertising online labor crowdsourcing traditionally vertices one side market fixed known priori vertices side arrive online matched central agent offline side issue possible conflicts among offline agents emerges various real scenarios need match online agent set offline agents	negative
find me if you can: deep software clone detection by exploiting the contest between the plagiarist and the detector	code clone common software development usually leads software defects copyright infringement researchers paid significant attention code clone detection many methods proposed however patterns generating code clones always remain order fool clone detection systems plagiarists known clone creator usually conduct series tricky modifications code fragments make clone difficult detect existing clone detection approaches neglects dynamics “ contest ” plagiarist detectors doomed robust adversarial revision code paper propose novel clone detection approach namely acd mimic adversarial process plagiarist detector enables us build strong clone detector also model behavior plagiarists plagiarist model may turn help understand vulnerability current software clone detection tools experiments show learned policy plagiarist help us build stronger clone detector outperforms existing clone detection methods	positive
one-network adversarial fairness	currently great expansion impact machine learning algorithms lives prompting need objectives pure performance including fairness fairness means outcome automated decisionmaking system discriminate subgroups characterized sensitive attributes gender race given existing differentiable classifier make slight adjustments architecture including adding new hidden layer order enable concurrent adversarial optimization fairness accuracy framework provides one way quantify tradeoff fairness accuracy also leading strong empirical performance	positive
learning optimal and fair decision trees for non-discriminative decision-making	recent years automated data-driven decision-making systems enjoyed tremendous success variety fields e.g. make product recommendations guide production entertainment recently algorithms increasingly used assist socially sensitive decisionmaking e.g. decide admit degree program prioritize individuals public housing yet automated tools may result discriminative decision-making sense may treat individuals unfairly unequally based membership category minority resulting disparate treatment disparate impact violating moral ethical standards may happen training dataset biased e.g. individuals belonging particular group historically discriminated upon however may also happen training dataset unbiased errors made system affect individuals belonging category minority differently e.g. misclassification rates blacks higher whites paper unify definitions unfairness across classification regression propose versatile mixed-integer optimization framework learning optimal fair decision trees variants thereof prevent disparate treatment and/or disparate impact appropriate translates flexible schema designing fair interpretable policies suitable socially sensitive decision-making conduct extensive computational studies show framework improves state-of-the-art field typically relies heuristics yield non-discriminative decisions lower cost overall accuracy	negative
logistic regression on homomorphic encrypted data at scale	machine learning homomorphic encrypted data cryptographic method analyzing private and/or sensitive data keeping privacy training phase takes input encrypted training data outputs encrypted model without ever decrypting prediction phase uses encrypted model predict results new encrypted data phase decryption key needed thus data privacy ultimately guaranteed many applications various areas finance education genomics medical field sensitive private data several studies reported prediction phase studies conducted training phase	negative
concurrency debugging with maxsmt	current maximum satisfiability maxsat algorithms based successive calls powerful satisfiability sat solver able solve real-world instances many application domains moreover replacing sat solver satisfiability modulo theories smt solver enables effective maxsmt algorithms however maxsmt seldom used debugging multi-threaded software	positive
multiagent decision making for maritime traffic management	address problem maritime traffic management busy waterways increase safety navigation reducing congestion model maritime traffic large multiagent systems individual vessels agents vts authority regulatory agent develop maritime traffic simulator based historical traffic data incorporates realistic domain constraints uncertain asynchronous movement vessels also develop traffic coordination approach provides speed recommendation vessels different zones exploit nature collective interactions among agents develop scalable policy gradient approach scale real world problems empirical results synthetic real world problems show approach significantly reduce congestion keeping traffic throughput high	negative
a lightweight approach to academic research group management using online tools: spend more time on research and less on management	years taking trial-and-error approach managing moderate-size academic research group settled using set online tools protocols seem effective require relatively little effort use maintain inexpensive paper discusses approach communication project management document code management logistics hope researchers especially new faculty research scientists might find set tools protocols useful determining manage research group paper targeted toward research groups based mathematics engineering although faculty disciplines may find inspiration ideas	negative
a preliminary report of integrating science and computing teaching using logic programming	paper presents framework integrate science computing teaching using logic programming developed two modules one chemistry chemistry physics implemented elective course 8th graders clinical interviews video taped class observations exit interviews experiences class logic programming based approach accessible students	positive
on testing of uniform samplers	recent years seen unprecedented adoption artificial intelligence wide variety applications ranging medical diagnosis automobile industry security aircraft collision avoidance probabilistic reasoning key component modern artificial intelligence systems sampling techniques form core state art probabilistic reasoning systems	negative
surveys without questions: a reinforcement learning approach	‘ old world ’ instrument survey remains tool choice firms obtain ratings satisfaction experience customers realize interacting online firms avenues survey evolved emails links pop-ups browsing deficiencies persist include reliance ratings respondents infer customers ’ online interactions failing capture customer ’ interactions time since rating one-time snapshot inability tie back customers ’ ratings specific interactions ratings provided relate interactions overcome deficiencies extract proxy ratings clickstream data typically collected every customer ’ online interactions developing approach based reinforcement learning rl introduce new way interpret values generated value function rl proxy ratings approach need survey data training yet validation actual survey data proxy ratings yield reasonable performance results additionally offer new way draw insights values value function allow associating specific interactions proxy ratings introduce two new metrics represent ratings one customer-level aggregate-level click actions across customers defined around proportion pairwise successive actions show increase proxy ratings intuitive customer-level metric enables gauging dynamics ratings time better predictor purchase customer ratings survey aggregate-level metric allows pinpointing actions help hurt experience sum proxy ratings computed unobtrusively clickstream every action customer every session offer interpretable insightful alternative surveys	negative
predicting hurricane trajectories using a recurrent neural network	hurricanes cyclones circulating defined center whose closed wind speeds exceed 75 mph originating tropical subtropical waters landfall hurricanes result severe disasters accuracy predicting trajectory paths critical reduce economic loss save human lives given complexity nonlinearity weather data recurrent neural network rnn could beneficial modeling hurricane behavior propose application fully connected rnn predict trajectory hurricanes employed rnn fine grid reduce typical truncation errors utilized latitude longitude wind speed pressure publicly provided national hurricane center nhc predict trajectory hurricane 6-hour intervals results show proposed technique competitive methods currently employed nhc predict approximately 120 hours hurricane path	negative
on rational delegations in liquid democracy	liquid democracy proxy voting method proxies delegable propose study game-theoretic model liquid democracy address following question rational voter delegate vote study existence pure-strategy nash equilibria model group accuracy affected complement theoretical results means agent-based simulations study effects delegations group ’ accuracy variously structured social networks	negative
automated verification of social laws for continuous time multi-robot systems	designing multi-agent systems several agents work shared environment requires coordinating agents interfere one canonical approaches coordinating agents enacting social law applies restrictions agents ’ available actions good social law prevents agents interfering still allowing achieve goals recent work took first step towards reasoning social laws using automated planning showed verify given social law robust allows agents achieve goals regardless agents work relied classical planning formalism assumed actions instantaneous external scheduler chooses agent acts next however work directly applicable multi-robot systems real world actions take time agents act concurrently paper show robustness social law continuous time setting verified compilation temporal planning demonstrate work theoretically real robots	negative
augmenting markov decision processes with advising	paper introduces advice-mdps expansion markov decision processes generating policies take consideration advising desirability undesirability prohibition certain states actions advicemdps enable design designing semi-autonomous systems systems require operator support least handling certain situations efficiently handle unexpected complex environments operators advising augment planning model covering unexpected real-world irregularities advising swiftly augment degree autonomy system work without subsequent human intervention	positive
allocating interventions based on predicted outcomes: a case study on homelessness services	modern statistical machine learning methods increasingly capable modeling individual personalized treatment effects predictions could used allocate different interventions across populations based individual characteristics many domains like social services availability different possible interventions severely resource limited paper considers possible improvements allocation services context homelessness service provision major metropolitan area using data homeless system use counterfactual approach show potential substantial benefits terms reducing number families experience repeat episodes homelessness choosing optimal allocations based predicted outcomes fixed number beds different types homelessness service facilities changes allocation mechanism would without tradeoffs however significant fraction households predicted higher probability re-entry optimal allocation original one discuss efficiency equity fairness issues arise consider potential implications policy	negative
learning to address health inequality in the united states with a bayesian decision network	life-expectancy complex outcome driven genetic socio-demographic environmental geographic factors increasing socio-economic health disparities united states propagating longevity-gap making cause concern earlier studies probed individual factors integrated picture reveal quantifiable actions missing growing concern widening healthcare inequality caused artificial intelligence ai due differential access ai-driven services hence imperative explore exploit potential ai illuminating biases enabling transparent policy decisions positive social health impact work reveal actionable interventions decreasing longevitygap united states analyzing county-level data resource containing healthcare socio-economic behavioral education demographic features learn ensembleaveraged structure draw inferences using joint probability distribution extend bayesian decision network identifying policy actions draw quantitative estimates impact diversity preventive-care quality stablefamilies within unified framework decision network finally make analysis dashboard available interactive web-application enabling users policy-makers validate reported findings explore impact ones beyond reported work	negative
random walk decay centrality	propose new centrality measure called random walk decay centrality centralities literature based notion shortest paths new centrality measure stems random walk network provide axiomatic characterization show new centrality closely related pagerank detail show replacing one axiom called lack self-impact another one called edge swap results new axiomatization pagerank finally argue lack self-impact desirable various settings explain violating edge swap may beneficial may contribute promoting diversity centrality measure	positive
explainable, normative, and justified agency	paper pose new challenge ai researchers – develop intelligent systems support justified agency illustrate ability examples relate two basic topics receiving increased attention – agents explain decisions ones follow societal norms case describe target abilities consider design alternatives note open questions review prior research return justified agency offering hypothesis relation explanatory normative behavior conclude proposing testbeds experiments evaluate empirical claim encouraging researchers contribute crucial area	positive
moral permissibility of action plans	research classical planning far mainly concerned generating satisficing optimal plan however systems used make decisions relevant humans one also consider ethical consequences generated plans address challenge analyzing far possible generalize existing approaches machine ethics automatic planning systems traditionally ethical principles formulated actionbased manner allowing judge execution one action show judgment generalized plans study computational complexity making ethical judgment plans	negative
envisioning ai for k-12: what should every child know about ai?	ubiquity ai society means time ripe consider educated 21st century digital citizens know subject may 2018 association advancement artificial intelligence aaai computer science teachers association csta formed joint working group develop national guidelines teaching ai k-12 students inspired csta 's national standards k-12 computing education ai k-12 guidelines define students grade band know artificial intelligence machine learning robotics ai k-12 working group also creating online resource directory teachers find ai- related videos demos software activity descriptions incorporate lesson plans blue sky talk invites ai research community reflect big ideas ai every k-12 student know communicate public advances ai future impact society call action ai researchers become ai educators creating resources help teachers students understand work	negative
blameworthiness in multi-agent settings	provide formal definition blameworthiness settings multiple agents collaborate avoid negative outcome first provide method ascribing blameworthiness groups relative epistemic state distribution causal models describe outcome might arise show go ascription blameworthiness groups ascription blameworthiness individuals using standard notion cooperative game theory shapley value believe getting good notion blameworthiness group setting critical designing autonomous agents behave moral manner	positive
bayesian fairness	consider problem decision making fair underlying probabilistic model world known certainty argue recent notions fairness machine learning need explicitly incorporate parameter uncertainty hence introduce notion bayesian fairness suitable candidate fair decision rules using balance definition fairness introduced kleinberg mullainathan raghavan 2016 show bayesian perspective lead well-performing fair decision rules even high uncertainty	positive
value alignment, fair play, and the rights of service robots	ethics safety research artificial intelligence increasingly framed terms `` alignment '' human values interests argue turing 's call `` fair play machines '' early often overlooked contribution alignment literature turing 's appeal fair play suggests need correct human behavior accommodate machines surprising inversion value alignment treated today reflections `` fair play '' motivate novel interpretation turing 's notorious `` imitation game '' condition intelligence instead value alignment machine demonstrates minimal degree alignment norms conversation instance go undetected interrogated human carefully distinguish interpretation moral turing test motivated principle fair play instead depends imitation human moral behavior finally consider framework fair play used situate debate robot rights within alignment literature argue extending rights service robots operating public spaces `` fair '' precisely sense encourages alignment interests humans machines	negative
