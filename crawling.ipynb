{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_2019():\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collect_2018_to_2010():\n",
    "    years = [18, 17, 16, 16, 14, 13, 12, 11, 10]\n",
    "    webpage_url = \"https://aaai.org/Library/AAAI/aaai%scontents.php\"\n",
    "    for year in years:\n",
    "        print(2000+year)\n",
    "        paper_year = 2000 + year # <---------------- year\n",
    "        req = requests.get(webpage_url%str(year))\n",
    "        html = req.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        paper_url_list = soup.select('body')[0].select('#content')[0].select('#right')[0].select('#box6')[0].find('div', {'class': 'content'}).findAll('p', {'class': 'left'})\n",
    "        for paper_url in paper_url_list:\n",
    "            paper = paper_url.find('a').get('href').replace('view', 'viewPaper')\n",
    "            \n",
    "            if paper[-4:] == '.pdf':\n",
    "                continue\n",
    "                \n",
    "            paper_req = requests.get(paper)\n",
    "            paper_html = paper_req.text\n",
    "            paper_soup = BeautifulSoup(paper_html, 'html.parser')\n",
    "            print(paper)\n",
    "            paper_info = paper_soup.select('body')[0].select('#container')[0].select('#body')[0].select('#main')[0]\n",
    "            paper_details = paper_info.find('div',{'id': 'content'})\n",
    "            \n",
    "            paper_category = paper_info.find('div',{'id': 'breadcrumb'}).findAll('a')[3].text # <---------------- category\n",
    "            paper_title = paper_details.find('div',{'id': 'title'}).text # <--------------------------------- title\n",
    "            paper_author = paper_details.find('div',{'id': 'author'}).text.split(', ') # <---------------- author\n",
    "            paper_abstract = paper_details.find('div',{'id': 'abstract'}).find('div').text # <---------------- abstract\n",
    "            paper_keyword = paper_details.find('div',{'id': 'paperSubject'})  # <---------------- title\n",
    "            if paper_keyword == None:\n",
    "                paper_keyword = \"\"\n",
    "            else:\n",
    "                paper_keyword = paper_keyword.find('div').text.split('; ')\n",
    "            paper_download_path = paper_details.find('div',{'id': 'paper'}).find('a').get('href').replace('view', 'viewFile')\n",
    "                                                # <----------------  download_path\n",
    "            # remove break\n",
    "            break\n",
    "        #remove break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dir_path(url):\n",
    "    cnt = 1\n",
    "    while url[-cnt]!='/':\n",
    "        cnt = cnt + 1\n",
    "    return url[:-cnt+1]\n",
    "\n",
    "def collect_2008_to_1980():\n",
    "    years = [2008, 2007, 2006, 2005, 2004, 2002, 2000, 1999, 1998, 1997, 1996, 1994, 1993, 1992, 1991, 1990, 1988, 1987, 1986, 1984, 1983, 1982, 1980]\n",
    "    webpage_url = \"https://aaai.org/Library/AAAI/aaai%scontents.php\"\n",
    "    for year in years:\n",
    "        paper_year = year # <---------------- year\n",
    "        print(\"> %s\"%year)\n",
    "        req = requests.get(webpage_url%(str(year%100)).zfill(2))\n",
    "        html = req.text\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        html_tag_list = soup.find('body').select('#content')[0].select('#right')[0].select('#box6')[0].find('div', {'class': 'content'}).findAll(['p', 'h3', 'h4'])\n",
    "        paper_category = \"\"\n",
    "        for x in html_tag_list:\n",
    "            if x.name == 'p' and x.has_attr('class') and paper_category != \"\":\n",
    "                if x == html_tag_list[-1] and (x.text.find('Index') != -1 or x.text.find('index') != -1):\n",
    "                    continue\n",
    "\n",
    "                paper_tmp = x.find('a')\n",
    "                if paper_tmp == None:\n",
    "                    continue\n",
    "                else:\n",
    "                    paper_tmp = paper_tmp.get('href')\n",
    "                paper = get_dir_path(webpage_url%(str(year%100)).zfill(2)) + paper_tmp\n",
    "                \n",
    "                if paper[-4:] == '.pdf':\n",
    "                    continue\n",
    "\n",
    "                paper_req = requests.get(paper)\n",
    "                paper_html = paper_req.text\n",
    "                paper_soup = BeautifulSoup(paper_html, 'html.parser')\n",
    "                if paper_soup.find('body') == None:\n",
    "                    paper_title = paper_soup.find('h1').text # <---------------- title\n",
    "                    paper_info = paper_soup.findAll('p')\n",
    "                    paper_author = paper_info[0].text # <---------------- author\n",
    "                    paper_abstract = paper_info[1].text # <---------------- abstract\n",
    "                    paper_info = paper_info[2:]\n",
    "                    paper_keyword = \"\"\n",
    "                    for info in paper_info:\n",
    "                        if info.text.find('Subject') != -1:\n",
    "                            paper_keyword = info.text # <------------- paper_keyword\n",
    "                    paper_download_path = get_dir_path(paper)+paper_soup.find('h1').find('a').get('href') # <----------- path\n",
    "                else:\n",
    "                    paper_info = paper_soup.find('body').find('div')\n",
    "                    paper_title = paper_info.find('h1').text # <---------------- title\n",
    "                    paper_info = paper_info.findAll('p')\n",
    "                    paper_author = paper_info[0].text.split(', ') # <---------------- author\n",
    "                    paper_abstract = paper_info[1].text # <---------------- abstract\n",
    "                    paper_info = paper_info[2:]\n",
    "                    paper_keyword = \"\"\n",
    "                    for info in paper_info:\n",
    "                        if info.text.find('Subject') != -1:\n",
    "                            paper_keyword = info.text # <------------- paper_keyword\n",
    "                    paper_download_path = get_dir_path(paper)+paper_soup.find('h1').find('a').get('href') # <----------- path\n",
    "                #remove\n",
    "                break\n",
    "            \n",
    "            elif x.name == 'h3' or x.name == 'h4':\n",
    "                paper_category = x.text # <---------------- category\n",
    "        #remove\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
