{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "filename = 'AAAI_%s_info.tsv'\n",
    "save_path = ''\n",
    "\n",
    "def log_on_text(string):\n",
    "    f = open('log.txt', 'w')\n",
    "    f.write(string + '\\n')\n",
    "    f.close()\n",
    "\n",
    "_columns = ['conference_year', 'category', 'title', 'author', 'institution', 'abstract', 'download_url', 'pdf_file_path', 'keywords', 'publish_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_2019():\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collect_2018_to_2010():\n",
    "    years = [18, 17, 16, 16, 14, 13, 12, 11, 10]\n",
    "    webpage_url = \"https://aaai.org/Library/AAAI/aaai%scontents.php\"\n",
    "    for year in years:\n",
    "        with open(save_path + filename%str(2000+year), 'wt') as f:\n",
    "            tsv_writer = csv.writer(f, delimiter='\\t')\n",
    "            tsv_writer.writerow(_columns)\n",
    "            try:\n",
    "                log_on_text(str(2000+year))\n",
    "                #df = pd.DataFrame([], columns=_columns)\n",
    "                paper_year = 2000 + year # <---------------- year\n",
    "                req = requests.get(webpage_url%str(year))\n",
    "                html = req.text\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                paper_url_list = soup.select('body')[0].select('#content')[0].select('#right')[0].select('#box6')[0].find('div', {'class': 'content'}).findAll('p', {'class': 'left'})\n",
    "                for paper_url in paper_url_list:\n",
    "                    try:\n",
    "                        paper = paper_url.find('a').get('href').replace('view', 'viewPaper')\n",
    "\n",
    "                        if paper[-4:] == '.pdf':\n",
    "                            continue\n",
    "\n",
    "                        paper_req = requests.get(paper)\n",
    "                        paper_html = paper_req.text\n",
    "                        paper_soup = BeautifulSoup(paper_html, 'html.parser')\n",
    "                        #print(paper)\n",
    "                        paper_info = paper_soup.select('body')[0].select('#container')[0].select('#body')[0].select('#main')[0]\n",
    "                        paper_details = paper_info.find('div',{'id': 'content'})\n",
    "\n",
    "                        paper_category = paper_info.find('div',{'id': 'breadcrumb'}).findAll('a')[3].text # <---------------- category\n",
    "                        paper_title = paper_details.find('div',{'id': 'title'}).text # <--------------------------------- title\n",
    "                        paper_author = paper_details.find('div',{'id': 'author'}).text # <---------------- author\n",
    "                        paper_abstract = paper_details.find('div',{'id': 'abstract'}).find('div').text # <---------------- abstract\n",
    "                        paper_keyword = paper_details.find('div',{'id': 'paperSubject'})  # <---------------- title\n",
    "                        if paper_keyword == None:\n",
    "                            paper_keyword = \"\"\n",
    "                        else:\n",
    "                            paper_keyword = paper_keyword.find('div').text\n",
    "                        paper_download_path = paper_details.find('div',{'id': 'paper'}).find('a').get('href').replace('view', 'viewFile')\n",
    "                                                            # <----------------  download_path\n",
    "                        tsv_writer.writerow([paper_year, paper_category, paper_title, paper_author, '', paper_abstract, paper_download_path, '', paper_keyword, ''])\n",
    "                        # remove break\n",
    "                        # break\n",
    "                    except:\n",
    "                        log_on_text('>> error '+str(paper_url))\n",
    "                #df.to_csv('papers/aaai_paper_%s'%str(paper_year))\n",
    "                #remove break\n",
    "                #break\n",
    "            except:\n",
    "                log_on_text('> error '+str(2000+year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dir_path(url):\n",
    "    cnt = 1\n",
    "    while url[-cnt]!='/':\n",
    "        cnt = cnt + 1\n",
    "    return url[:-cnt+1]\n",
    "\n",
    "def collect_2008_to_1980():\n",
    "    years = [2008, 2007, 2006, 2005, 2004, 2002, 2000, 1999, 1998, 1997, 1996, 1994, 1993, 1992, 1991, 1990, 1988, 1987, 1986, 1984, 1983, 1982, 1980]\n",
    "    webpage_url = \"https://aaai.org/Library/AAAI/aaai%scontents.php\"\n",
    "    for year in years:\n",
    "        with open(save_path + filename%str(2000+year), 'wt') as f:\n",
    "            tsv_writer = csv.writer(f, delimiter='\\t')\n",
    "            tsv_writer.writerow(_columns)\n",
    "            try:\n",
    "                df = pd.DataFrame([], columns=_columns)\n",
    "                paper_year = year # <---------------- year\n",
    "                log_on_text(str(year))\n",
    "                req = requests.get(webpage_url%(str(year%100)).zfill(2))\n",
    "                html = req.text\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                html_tag_list = soup.find('body').select('#content')[0].select('#right')[0].select('#box6')[0].find('div', {'class': 'content'}).findAll(['p', 'h3', 'h4'])\n",
    "                paper_category = \"\"\n",
    "                for x in html_tag_list:\n",
    "                    try:\n",
    "                        if x.name == 'p' and x.has_attr('class') and paper_category != \"\":\n",
    "                            if x == html_tag_list[-1] and (x.text.find('Index') != -1 or x.text.find('index') != -1):\n",
    "                                continue\n",
    "\n",
    "                            paper_tmp = x.find('a')\n",
    "                            if paper_tmp == None:\n",
    "                                continue\n",
    "                            else:\n",
    "                                paper_tmp = paper_tmp.get('href')\n",
    "                            paper = get_dir_path(webpage_url%(str(year%100)).zfill(2)) + paper_tmp\n",
    "\n",
    "                            if paper[-4:] == '.pdf':\n",
    "                                continue\n",
    "\n",
    "                            paper_req = requests.get(paper)\n",
    "                            paper_html = paper_req.text\n",
    "                            paper_soup = BeautifulSoup(paper_html, 'html.parser')\n",
    "                            \n",
    "                            if paper_soup.find('body') == None:\n",
    "                                paper_title = paper_soup.find('h1').text # <---------------- title\n",
    "                                paper_info = paper_soup.findAll('p')\n",
    "                                paper_author = paper_info[0].text # <---------------- author\n",
    "                                paper_abstract = paper_info[1].text # <---------------- abstract\n",
    "                                paper_info = paper_info[2:]\n",
    "                                paper_keyword = \"\"\n",
    "                                for info in paper_info:\n",
    "                                    if info.text.find('Subject') != -1:\n",
    "                                        paper_keyword = info.text # <------------- paper_keyword\n",
    "                                paper_download_path = get_dir_path(paper)+paper_soup.find('h1').find('a').get('href') # <----------- path\n",
    "                            else:\n",
    "                                paper_info = paper_soup.find('body').find('div')\n",
    "                                paper_title = paper_info.find('h1').text # <---------------- title\n",
    "                                paper_info = paper_info.findAll('p')\n",
    "                                paper_author = paper_info[0].text # <---------------- author\n",
    "                                paper_abstract = paper_info[1].text # <---------------- abstract\n",
    "                                paper_info = paper_info[2:]\n",
    "                                paper_keyword = \"\"\n",
    "                                for info in paper_info:\n",
    "                                    if info.text.find('Subject') != -1:\n",
    "                                        paper_keyword = info.text # <------------- paper_keyword\n",
    "                                paper_download_path = get_dir_path(paper)+paper_soup.find('h1').find('a').get('href') # <----------- path\n",
    "                            #remove break\n",
    "                            #break\n",
    "                            tsv_writer.writerow([paper_year, paper_category, paper_title, paper_author, '', paper_abstract, paper_download_path, '', paper_keyword, ''])\n",
    "\n",
    "                        elif x.name == 'h3' or x.name == 'h4':\n",
    "                            paper_category = x.text # <---------------- category\n",
    "                    except:\n",
    "                        log_on_text(\">> error\" + str(year))\n",
    "                #remove break\n",
    "                #df.to_csv('papers/aaai_paper_%s'%str(paper_year))\n",
    "                #break\n",
    "            except:\n",
    "                log_on_text(\"> error\" + str(year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
